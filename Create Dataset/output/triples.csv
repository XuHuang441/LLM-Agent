label,corpus,topic
"The advent of large language models (LLMs) has transformed text-based
services, enabling capabilities ranging from real-time translation to AI-driven
chatbots. However, existing serving systems primarily focus on optimizing
server-side aggregate metrics like token generation throughput, ignoring
individual user experience with streamed text. As a result, under high and/or
bursty load, a significant number of users can receive unfavorable service
quality or poor Quality-of-Experience (QoE). In this paper, we first formally
define QoE of text streaming services, where text is delivered incrementally
and interactively to users, by considering the end-to-end token delivery
process throughout the entire interaction with the user. Thereafter, we propose
Andes, a QoE-aware serving system that enhances user experience for LLM-enabled
text streaming services. At its core, Andes strategically allocates contended
GPU resources among multiple requests over time to optimize their QoE. Our
evaluations demonstrate that, compared to the state-of-the-art LLM serving
systems like vLLM, Andes improves the average QoE by up to 3.2$\times$ under
high request rate, or alternatively, it attains up to 1.6$\times$ higher
request rate while preserving high QoE.","[{'Dynamic manipulation of graphene plasmonic skyrmions': 'With the characteristics of ultrasmall, ultrafast and topological protection,\noptical skyrmions has great prospects in application of high intensity data\nstroage, high resolution microscopic imaging and polarization sensing. The\nflexible control of the optical skyrmions is the premise of practical\napplication. At present, the manipulation of optical skyrmions usually relies\nupon the change of spatial structure, which results in a limited-tuning range\nand a discontinuous control in the parameter space. Here, we propose continuous\nmanipulation of the graphene plasmons skyrmions based on the electrotunable\nproperties of graphene. By changing the Fermi energy of one pair of the\nstanding waves and the phase of the incident light can achieve the\ntransformation of the topological state of the graphene plasmons skyrmions,\nwhich can be illustrated by the change of the skyrmion number from 1 to 0.5.\nThe direc manipulation of the graphene plasmons skyrmions is demonstrated by\nthe simulation results based on the finite element method. Our work suggests a\nfeasible way to flexibly control the optical skyrmions topological field, which\ncan be used for novel integrated photonics devices in the future.'}, {'FedTrans: Efficient Federated Learning via Multi-Model Transformation': 'Federated learning (FL) aims to train machine learning (ML) models across\npotentially millions of edge client devices. Yet, training and customizing\nmodels for FL clients is notoriously challenging due to the heterogeneity of\nclient data, device capabilities, and the massive scale of clients, making\nindividualized model exploration prohibitively expensive. State-of-the-art FL\nsolutions personalize a globally trained model or concurrently train multiple\nmodels, but they often incur suboptimal model accuracy and huge training costs.\n  In this paper, we introduce FedTrans, a multi-model FL training framework\nthat automatically produces and trains high-accuracy, hardware-compatible\nmodels for individual clients at scale. FedTrans begins with a basic global\nmodel, identifies accuracy bottlenecks in model architectures during training,\nand then employs model transformation to derive new models for heterogeneous\nclients on the fly. It judiciously assigns models to individual clients while\nperforming soft aggregation on multi-model updates to minimize total training\ncosts. Our evaluations using realistic settings show that FedTrans improves\nindividual client model accuracy by 14% - 72% while slashing training costs by\n1.6X - 20X over state-of-the-art solutions.'}, {'UNIMO-G: Unified Image Generation through Multimodal Conditional\n  Diffusion': 'Existing text-to-image diffusion models primarily generate images from text\nprompts. However, the inherent conciseness of textual descriptions poses\nchallenges in faithfully synthesizing images with intricate details, such as\nspecific entities or scenes. This paper presents UNIMO-G, a simple multimodal\nconditional diffusion framework that operates on multimodal prompts with\ninterleaved textual and visual inputs, which demonstrates a unified ability for\nboth text-driven and subject-driven image generation. UNIMO-G comprises two\ncore components: a Multimodal Large Language Model (MLLM) for encoding\nmultimodal prompts, and a conditional denoising diffusion network for\ngenerating images based on the encoded multimodal input. We leverage a\ntwo-stage training strategy to effectively train the framework: firstly\npre-training on large-scale text-image pairs to develop conditional image\ngeneration capabilities, and then instruction tuning with multimodal prompts to\nachieve unified image generation proficiency. A well-designed data processing\npipeline involving language grounding and image segmentation is employed to\nconstruct multi-modal prompts. UNIMO-G excels in both text-to-image generation\nand zero-shot subject-driven synthesis, and is notably effective in generating\nhigh-fidelity images from complex multimodal prompts involving multiple image\nentities.'}, {'Convection and Clouds under Different Planetary Gravities Simulated by a\n  Small-domain Cloud-resolving Model': 'In this study, we employ a cloud-resolving model (CRM) to investigate how\ngravity influences convection and clouds in a small-domain (96 km by 96 km)\nradiative-convective equilibrium (RCE). Our experiments are performed with a\nhorizontal grid spacing of 1 km, which can resolve large (> 1 km$^2$)\nconvective cells. We find that under a given stellar flux, sea surface\ntemperature increases with decreasing gravity. This is because a lower-gravity\nplanet has larger water vapor content and more clouds, resulting in a larger\nclear-sky greenhouse effect and a stronger cloud warming effect in the small\ndomain. By increasing stellar flux under different gravity values, we find that\nthe convection shifts from a quasi-steady state to an oscillatory state. In the\noscillatory state, there are convection cycles with a period of several days,\ncomprised of a short wet phase with intense surface precipitation and a dry\nphase with no surface precipitation. When convection shifts to the oscillatory\nstate, water vapor content and high-level cloud fraction increase\nsubstantially, resulting in rapid warming. After the transition to the\noscillatory state, the cloud net positive radiative effect decreases with\nincreasing stellar flux, which indicates a stabilizing climate effect. In the\nquasi-steady state, the atmospheric absorption features of CO$_2$ are more\ndetectable on lower-gravity planets because of their larger atmospheric\nheights. While in the oscillatory state, the high-level clouds mute almost all\nthe absorption features, making the atmospheric components hard to be\ncharacterized.'}, {'Venn: Resource Management Across Federated Learning Jobs': 'In recent years, federated learning (FL) has emerged as a promising approach\nfor machine learning (ML) and data science across distributed edge devices.\nWith the increasing popularity of FL, resource contention between multiple FL\njobs training on the same device population is increasing as well. Scheduling\nedge resources among multiple FL jobs is different from GPU scheduling for\ncloud ML because of the ephemeral nature and planetary scale of participating\ndevices as well as the overlapping resource requirements of diverse FL jobs.\nExisting resource managers for FL jobs opt for random assignment of devices to\nFL jobs for simplicity and scalability, which leads to poor performance. In\nthis paper, we present Venn, an FL resource manager, that efficiently schedules\nephemeral, heterogeneous devices among many FL jobs, with the goal of reducing\ntheir average job completion time (JCT). Venn formulates the Intersection\nResource Scheduling (IRS) problem to identify complex resource contention among\nmultiple FL jobs. Then, Venn proposes a contention-aware scheduling heuristic\nto minimize the average scheduling delay. Furthermore, it proposes a\nresource-aware device-to-job matching heuristic that focuses on optimizing\nresponse collection time by mitigating stragglers. Our evaluation shows that,\ncompared to the state-of-the-art FL resource managers, Venn improves the\naverage JCT by up to 1.88X.'}, {'Joint Training of Candidate Extraction and Answer Selection for Reading\n  Comprehension': 'While sophisticated neural-based techniques have been developed in reading\ncomprehension, most approaches model the answer in an independent manner,\nignoring its relations with other answer candidates. This problem can be even\nworse in open-domain scenarios, where candidates from multiple passages should\nbe combined to answer a single question. In this paper, we formulate reading\ncomprehension as an extract-then-select two-stage procedure. We first extract\nanswer candidates from passages, then select the final answer by combining\ninformation from all the candidates. Furthermore, we regard candidate\nextraction as a latent variable and train the two-stage process jointly with\nreinforcement learning. As a result, our approach has improved the\nstate-of-the-art performance significantly on two challenging open-domain\nreading comprehension datasets. Further analysis demonstrates the effectiveness\nof our model components, especially the information fusion of all the\ncandidates and the joint training of the extract-then-select procedure.'}, {'DU-VLG: Unifying Vision-and-Language Generation via Dual\n  Sequence-to-Sequence Pre-training': 'Due to the limitations of the model structure and pre-training objectives,\nexisting vision-and-language generation models cannot utilize pair-wise images\nand text through bi-directional generation. In this paper, we propose DU-VLG, a\nframework which unifies vision-and-language generation as sequence generation\nproblems. DU-VLG is trained with novel dual pre-training tasks: multi-modal\ndenoising autoencoder tasks and modality translation tasks. To bridge the gap\nbetween image understanding and generation, we further design a novel\ncommitment loss. We compare pre-training objectives on image captioning and\ntext-to-image generation datasets. Results show that DU-VLG yields better\nperformance than variants trained with uni-directional generation objectives or\nthe variant without the commitment loss. We also obtain higher scores compared\nto previous state-of-the-art systems on three vision-and-language generation\ntasks. In addition, human judges further confirm that our model generates real\nand relevant images as well as faithful and informative captions.'}, {'NeRF-Enhanced Outpainting for Faithful Field-of-View Extrapolation': 'In various applications, such as robotic navigation and remote visual\nassistance, expanding the field of view (FOV) of the camera proves beneficial\nfor enhancing environmental perception. Unlike image outpainting techniques\naimed solely at generating aesthetically pleasing visuals, these applications\ndemand an extended view that faithfully represents the scene. To achieve this,\nwe formulate a new problem of faithful FOV extrapolation that utilizes a set of\npre-captured images as prior knowledge of the scene. To address this problem,\nwe present a simple yet effective solution called NeRF-Enhanced Outpainting\n(NEO) that uses extended-FOV images generated through NeRF to train a\nscene-specific image outpainting model. To assess the performance of NEO, we\nconduct comprehensive evaluations on three photorealistic datasets and one\nreal-world dataset. Extensive experiments on the benchmark datasets showcase\nthe robustness and potential of our method in addressing this challenge. We\nbelieve our work lays a strong foundation for future exploration within the\nresearch community.'}, {'3D-Aware Talking-Head Video Motion Transfer': 'Motion transfer of talking-head videos involves generating a new video with\nthe appearance of a subject video and the motion pattern of a driving video.\nCurrent methodologies primarily depend on a limited number of subject images\nand 2D representations, thereby neglecting to fully utilize the multi-view\nappearance features inherent in the subject video. In this paper, we propose a\nnovel 3D-aware talking-head video motion transfer network, Head3D, which fully\nexploits the subject appearance information by generating a\nvisually-interpretable 3D canonical head from the 2D subject frames with a\nrecurrent network. A key component of our approach is a self-supervised 3D head\ngeometry learning module, designed to predict head poses and depth maps from 2D\nsubject video frames. This module facilitates the estimation of a 3D head in\ncanonical space, which can then be transformed to align with driving video\nframes. Additionally, we employ an attention-based fusion network to combine\nthe background and other details from subject frames with the 3D subject head\nto produce the synthetic target video. Our extensive experiments on two public\ntalking-head video datasets demonstrate that Head3D outperforms both 2D and 3D\nprior arts in the practical cross-identity setting, with evidence showing it\ncan be readily adapted to the pose-controllable novel view synthesis task.'}, {'Auxo: Efficient Federated Learning via Scalable Client Clustering': 'Federated learning (FL) is an emerging machine learning (ML) paradigm that\nenables heterogeneous edge devices to collaboratively train ML models without\nrevealing their raw data to a logically centralized server. However, beyond the\nheterogeneous device capacity, FL participants often exhibit differences in\ntheir data distributions, which are not independent and identically distributed\n(Non-IID). Many existing works present point solutions to address issues like\nslow convergence, low final accuracy, and bias in FL, all stemming from client\nheterogeneity. In this paper, we explore an additional layer of complexity to\nmitigate such heterogeneity by grouping clients with statistically similar data\ndistributions (cohorts). We propose Auxo to gradually identify such cohorts in\nlarge-scale, low-availability, and resource-constrained FL populations. Auxo\nthen adaptively determines how to train cohort-specific models in order to\nachieve better model performance and ensure resource efficiency. Our extensive\nevaluations show that, by identifying cohorts with smaller heterogeneity and\nperforming efficient cohort-based training, Auxo boosts various existing FL\nsolutions in terms of final accuracy (2.1% - 8.2%), convergence time (up to\n2.2x), and model bias (4.8% - 53.8%).'}]","**Abstract**

In the rapidly advancing field of conversational AI, large language models (LLMs) have transformed natural language processing capabilities with the potential to converse effectively with humans. This surge in popularity has sparked the development of rigorous Automated Feedback Systems, aiming to optimize the Quality of Experience (QoE) for conversational interactions. Traditionally, serving systems primarily focus on optimizing throughput, neglecting the nuanced requirements of QoE-centric services that offer seamless text streaming.

**Objective:** The primary objective of this paper is to introduce Andes, a Quality of Experience-aware LLM serving system that redefines the state-of-the-art by prioritizing user satisfaction over mere throughput optimization.

**Innovations:** Andes innovates through an overarching Quality of Experience (QoE) metric, based on three key metrics: stream time (TTFT), token delivery speed (TDS), and system capacity. It achieves notable improvements through an enhanced scheduling policy that preemptively terminates and restarts concurrent requests based on their prioritization, ensuring a smoother, more efficient stream of output tokens for the end-users.

**Methods:** The system leverages a QoE-aware priority scheduling mechanism that dynamically replaces the traditional first-come, first-served approach. This system utilizes the waiting queue to assess the scheduling priority of each request, managing them effectively to advocate for shorter TTFT in scenarios where TDS cannot surpass the user's digest speed.

**Results:** Andes demonstrates significant enhancements in QoE, marginally decreases in throughput, and increased serving capacity compared to existing systems. Across five models tested on two datasets, Andes improved QoE metrics up to 3.2×, showcased system capacity boosts of up to 2×, and maintained token generation throughput even as it ascertaines QoE improvement.

**Contributions:** By proposing a novel QoE-centric serving system and method, Andes advances the state-of-the-art in LLM serving, addressing the void in serving systems dedicated to optimizing the nuanced requirements of text streaming services.

**Applications:** This research not only contributes to the efficiency and performance in conversational AI platforms but also widens the scope for personalized user interaction across platforms such as chatbots, virtual assistants, and language translation tools, offering more tailored, responsive, and immersive experiences for users."
"Fuzzing, a widely-used technique for bug detection, has seen advancements
through Large Language Models (LLMs). Despite their potential, LLMs face
specific challenges in fuzzing. In this paper, we identified five major
challenges of LLM-assisted fuzzing. To support our findings, we revisited the
most recent papers from top-tier conferences, confirming that these challenges
are widespread. As a remedy, we propose some actionable recommendations to help
improve applying LLM in Fuzzing and conduct preliminary evaluations on DBMS
fuzzing. The results demonstrate that our recommendations effectively address
the identified challenges.","[{'On the complexities of some simple modules of symmetric groups': 'Let $p$ be a prime. In this paper, we compute complexities of some simple\nmodules of symmetric groups labelled by two-part partitions. Most of the simple\nmodules considered here are contained in the $p$-blocks with non-abelian defect\ngroups.'}, {'On Terwilliger $\\mathbb{F}$-algebras of factorial association schemes': 'The Terwilliger algebras of association schemes over an arbitrary field\n$\\mathbb{F}$ were called the Terwilliger $\\mathbb{F}$-algebras of association\nschemes in [8]. In this paper, we study the Terwilliger $\\mathbb{F}$-algebras\nof factorial association schemes. We determine the $\\mathbb{F}$-dimensions, the\ncenters, the semisimplicity, the Jacobson radicals, and the algebraic\nstructures of the Terwilliger $\\mathbb{F}$-algebras of factorial association\nschemes.'}, {'Equilibrium Points and Periodic Orbits in the Vicinity of Asteroids with\n  an Application to 216 Kleopatra': 'In this study, equilibrium points and periodic orbits in the potential field\nof asteroids are investigated. We present the linearized equations of motion\nrelative to the equilibrium points and characteristic equations. We find that\nthe distribution of characteristic multipliers of periodic orbits around the\nequilibrium point and the distribution of eigenvalues of the equilibrium point\ncorrespond to each other. The distribution of eigenvalues of the equilibrium\npoint confirms the topology and the stability of periodic orbits around the\nequilibrium point.'}, {'Transfer Matrix of Scatterers Connected in Parallel': 'Transport phenomena in parallel coupled scatterers are studied by transfer\nmatrix formulism. We derive a simple recurrence relation for transfer matrix of\none-dimensional two-terminal systems consisting of $N$ arbitrary scattering\nunit cells connected in parallel. For identical scattering sub-units we find\nthat the effects of parallel connection on transport properties of the coupled\nsystem can be described by a similarity transformation on the single scatterer,\nwith the similar matrix determined by the scattering matrix of the junction.\nWhile for distinct single scatterers, the similar matrices depend on both\nscattering properties of individual elements and structure of connection\ntopologies.'}, {'Symmetry-Breaking Transition and Spectral Singularity in Coupled\n  $\\mathcal{PT}$-Symmetric Quantum Potentials': 'We study the scattering properties of $N$ identical one-dimensional localized\n$\\mathcal{PT}$-symmetric potentials, connected in series as well as in\nparallel. We derive a general transfer matrix formalism for parallel coupled\nquantum scatterers, and apply that theory to demonstrate that the spectral\nsingularities and $\\mathcal{PT}$-symmetric transitions of single scattering\ncells may be observed in coupled systems, at the same or distinct values of the\ncritical parameters, depending on the connection modes under which the\nscattering objects are coupled. We analyse the influences of the connection\nconfiguration on the related transport properties such as spectral\nsingularities and anisotropic transmission resonances.'}, {'On some trivial source Specht modules': 'The paper presented here focuses on the classification of trivial source\nSpecht modules. We completely classify the trivial source Specht modules\nlabelled by hook partitions. We also classify the trivial source Specht modules\nlabelled by two-part partitions in the odd characteristic case. Moreover, in\nthe even characteristic case, we prove a result for the classification of the\ntrivial source Specht modules labelled by partitions with 2-weight 2, which\njustifies a conjecture of [16].'}, {'On the symmetric and exterior powers of Young permutation modules': 'Let $n$ be a positive integer and $\\lambda$ be a partition of $n$. Let\n$M^\\lambda$ be the Young permutation module labelled by $\\lambda$. In this\npaper, we study symmetric and exterior powers of $M^\\lambda$ in positive\ncharacteristic case. We determine the symmetric and exterior powers of\n$M^\\lambda$ that are projective. All the indecomposable exterior powers of\n$M^\\lambda$ are also classified. We then prove some results for indecomposable\ndirect summands that have the largest complexity in direct sum decompositions\nof some symmetric and exterior powers of $M^\\lambda$. We end by parameterizing\nall the Scott modules that are isomorphic to direct summands of the symmetric\nor exterior square of $M^\\lambda$ and determining their corresponding\nmultiplicities explicitly.'}, {'On some $p$-transitive association schemes': 'In this paper, for any prime $p$, we propose the notion of a $p$-transitive\nassociation scheme. This notion aims to generalize the fact that the regular\nmodule of a group algebra of a finite group has a unique trivial submodule to\nthe case of the regular modules of modular adjacency algebras. We completely\ndetermine the $p$-transitive quasi-thin association schemes and the\n$p$-transitive association schemes with thin thin residue by their structure\ntheory properties.'}, {'On Terwilliger $\\mathbb{F}$-algebras of quasi-thin association schemes': 'In [3], Hanaki defined the Terwilliger algebras of association schemes over a\ncommutative unital ring. In this paper, we call the Terwilliger algebras of\nassociation schemes over a field $\\mathbb{F}$ the Terwilliger\n$\\mathbb{F}$-algebras of association schemes and study the Terwilliger\n$\\mathbb{F}$-algebras of quasi-thin association schemes. As main results, we\ndetermine the $\\mathbb{F}$-dimensions, the semisimplicity, the Jacobson\nradicals, and the algebraic structures of the Terwilliger $\\mathbb{F}$-algebras\nof quasi-thin association schemes. We also get some results with independent\ninterests.'}, {'Equitable Transit Network Design Under Uncertainty': 'This paper proposes a bilevel transit network design problem considering\nsupply side uncertainty. The upper level problem determines frequency settings\nto simultaneously maximize the efficiency and equity measures, which are\ndefined by the reduction in the total effective travel cost and the minimum\nreduction in the effective travel cost of all OD pairs, respectively. The lower\nlevel problem is the reliability based transit assignment problem that captures\nthe effects of supply-side uncertainty on passengers route choice\nbehavior.Numerical studies demonstrate that 1) the Pareto frontier may not be\nconvex; 2) it is possible to improve the efficiency and equity objectives\nsimultaneously; 3) increasing the frequency could worsen the equity measure; 4)\npassengers risk attitude affects the rate of substitution between the two\nobjectives.'}]","Abstract

Exploring the capabilities and potential limitations of Large Language Models (LLMs) in software testing and debugging, this paper scrutinizes the effectiveness of employing LLMs in the context of employing fuzzer techniques and monitoring-based detection within database management systems (DBMS) like MeMonetDB, DuckDB, and ClickHouse. The paper reveals that, while LLMs demonstrate promise in seamlessly generating reference implementations and extracting testing patterns, they struggle with understanding the intricate logic of specialized domains, creating a challenge in accurately identifying bugs, especially in time-series databases like IoTDB. 

Our primary goal is to establish guidelines for harnessing LLMs in DBMS fuzzing effectively. The paper advances three recommendations (REC 3.1-3.2) to mitigate the risk of inaccurate testing oracles due to LLM misunderstandings. REC 3.1 suggests using LLMs to identify common patterns associated with specific bug types and leveraging domain knowledge to monitor systems using these patterns. REC 3.2 advocates leveraging the interpretation ability of LLM through natural language processing to isolate intended behavior and ensuing bugs from well-defined project instances such as RFC descriptions.

Applying a generic formal language grammar can significantly increase the diversity of test cases generated. We affirmatively transform popular SQL queries into the format of the targeted database system, enhancing this with the improvement in test case diversity via the seed query transformation. Meanwhile, REC 3.1 and 3.2’s tailoring of LLM for DBMS fuzzing demonstrates superior performance over general LLM usage, indicating that the utilization of these recommendations can help generate high-quality, semantically correct SQL statements for DBMS fuzzing and test oracles for RSDBMS, respectively.

The research outcomes contribute to identifying the challenges and effective strategies to optimize LLM deployment in DBMS testing. Specifically, it directs how LLMs can contribute to generate more specific, context-aware test cases and provide a deeper understanding of DBMS functionalities, leading to enhanced bug detection capabilities with reduced false positives. It further supports the integration of LLMs in virtual programming environments for predicting the state of virtual machines in fuzz testing and supplementing the tasks typically executed by human testers by levering LLMs to diagnose runtime anomaly patterns.

In light of these advancements, the potential applications of LLMs in improving overall software testing efficiency and effectiveness through advanced bug detection and understanding are vast. The paper lays the groundwork for integrating these novel technologies into more comprehensive and efficient DBMS testing frameworks, emphasizing the symbiotic relationship between LLMs and traditional software testing methodologies."
"Unsupervised Domain Adaptation (UDA) refers to the method that utilizes
annotated source domain data and unlabeled target domain data to train a model
capable of generalizing to the target domain data. Domain discrepancy leads to
a significant decrease in the performance of general network models trained on
the source domain data when applied to the target domain. We introduce a
straightforward approach to mitigate the domain discrepancy, which necessitates
no additional parameter calculations and seamlessly integrates with
self-training-based UDA methods. Through the transfer of the target domain
style to the source domain in the latent feature space, the model is trained to
prioritize the target domain style during the decision-making process. We
tackle the problem at both the image-level and shallow feature map level by
transferring the style information from the target domain to the source domain
data. As a result, we obtain a model that exhibits superior performance on the
target domain. Our method yields remarkable enhancements in the
state-of-the-art performance for synthetic-to-real UDA tasks. For example, our
proposed method attains a noteworthy UDA performance of 76.93 mIoU on the
GTA->Cityscapes dataset, representing a notable improvement of +1.03 percentage
points over the previous state-of-the-art results.","[{'Log Structures on Generalized Semi-Stable Varieties': 'This is my PhD Thesis, part of it has published in Acta Mathematica Sinica.\nIn this paper, a class of morphisms which have a kind of singularity weaker\nthan normal crossing is considered. We construct the obstruction such that the\nso-called semi-stable log structures exists if and only if the obstruction\nvanishes. In the case of no power, if the obstruction vanishes, then the\nsemi-stable log structure is unique up to a unique isomorphism. So we obtain a\nkind of canonical structures on this family of morphisms.'}, {'Purely Algebraic Method to Construct Toric Schemes': 'In this article, we first give some elementary proprieties of monoids and\nfans, then construct a toric scheme over an arbitrary ring, from a given fan.\nUsing Valuative Criterion, we prove that this scheme is separated and give the\nsufficient and necessary condition when it is proper. We also study the\nregularity and logarithmic regularity of it. Finally we study the morphisms of\ntoric schemes induced by the homomorphisms of fans.'}, {'The Étale Homology and The Cycle Maps in Adic Coefficients': 'In this article, we define the l-adic homology for a morphism of schemes\nsatisfying certain finiteness conditions. This homology has these functors\nsimilar to the Chow groups: proper push-forward, flat pull-back, base change,\ncap-product, etc. In particular on singular varieties, this kind of l-adic\nhomology behaves much better that the classical l-adic cohomology. As an\napplication, we give an much easier approach to construct the cycle maps for\narbitrary algebraic schemes over fields of finite cohomology dimension. And we\nprove these cycle maps kill the algebraic equivalences and commute with the\nChern action of locally free sheaves.'}, {'The $\\ell$-adic Dualizing Complex on an Excellent Surface with Rational\n  Singularities': 'In this article, we show that if $X$ is an excellent surface with rational\nsingularities, the constant sheaf $\\mathbb{Q}_{\\ell}$ is a dualizing complex.\nIn coefficient $\\mathbb{Z}_{\\ell}$, we also prove that the obstruction for\n$\\mathbb{Z}_{\\ell}$ to become a dualizing complex lying on the divisor class\ngroups at singular points. As applications, we study the perverse sheaves and\nthe weights of $\\ell$-adic cohomology groups on such surfaces.'}, {'SDO/AIA Observations of Large-Amplitude Longitudinal Oscillations in a\n  Solar Filament': 'We present the first \\emph{Solar Dynamics Observatory}/Atmospheric Imaging\nAssembly observations of the large-amplitude longitudinal (LAL) oscillations in\nthe south and north parts (SP and NP) of a solar filament on 2012 April 7. Both\noscillations are triggered by flare activities close to the filament. The\nperiod varies with filamentary threads, ranging from 44 to 67 min. The\noscillations of different threads are out of phase, and their velocity\namplitudes vary from 30 to 60 km s$^{-1}$, with a maximum displacement of about\n25 Mm. The oscillations of the SP repeat for about 4 cycles without any\nsignificant damping and then a nearby C2.4 flare causes the transition from the\nLAL oscillations of the filament to its later eruption. The filament eruption\nis also associated with a coronal mass ejection and a B6.8 flare. However, the\noscillations of the NP damp with time and die out at last. Our observations\nshow that the activated part of the SP repeatedly shows a helical motion. This\nindicates that the magnetic structure of the filament is possibly modified\nduring this process. We suggest that the restoring force is the coupling of the\nmagnetic tension and gravity.'}, {'Slipping Magnetic Reconnection Triggering a Solar Eruption of a\n  Triangle-flag Flux Rope': 'We firstly report the simultaneous activities of a slipping motion of flare\nloops and a slipping eruption of a flux rope in 131 {\\AA} and 94 {\\AA} channels\non 2014 February 02. The east hook-like flare ribbon propagated slippingly at a\nspeed of about 50 km s$^{-1}$, which lasted about 40 min and extended by more\nthan 100 Mm, but the west flare ribbon moved in the opposite direction with a\nspeed of 30 km s$^{-1}$. At the later phase of the flare activity, a ""bi-fan""\nsystem of flare loops was well developed. The east footpoints of the flux rope\nshowed an apparent slipping motion along the hook of the ribbon, simultaneously\nthe fine structures of the flux rope rose up rapidly at a speed of 130 km\ns$^{-1}$, much faster the whole flux rope. We infer that the east footpoints of\nthe flux rope are successively heated by a slipping magnetic reconnection\nduring the flare, which results in the apparent slippage of the flux rope. The\nslipping motion delineates a ""triangle-flag surface"" of the flux rope, implying\nthat the topology of a flux rope is more complex than anticipated.'}, {'Search for Magnetic Monopoles in Magnetic Reconnection Regions': ""In order to satisfy the symmetry between electric and magnetic fields in the\nsource free Maxwell's equations, electric charges might have magnetic\ncounterparts: magnetic monopoles. Many methods and techniques are proposed to\nsearch for the monopoles, but no confirmed results have been obtained. Based on\nsolar observations, we know that magnetic reconnections take place during\neruptive solar activities. The magnetic fields can be broken at first and then\nrejoined, implying that the fields are source-relevant at the broken moment. It\nis speculated that the magnetic lines undergo outward deflection movement\nduring the broken moment, as the line tying effect disappears and the magnetic\ntension triggers the movement. The signal of the deflection is detected for the\nfirst time by EUV and H${\\alpha}$ observations in reconnection processes. We\npropose that the monopoles appear in magnetic reconnection regions at first,\nand then the annihilation of opposite polarity monopoles releases energy and\nperhaps also produces particles. To detect the predict monopoles, laboratory\nplasma experiments can be used to provide some fundamental information.""}, {'Geometric continuous-stage exponential energy-preserving integrators for\n  charged-particle dynamics in a magnetic field from normal to strong regimes': 'This paper is concerned with geometric exponential energy-preserving\nintegrators for solving charged-particle dynamics in a magnetic field from\nnormal to strong regimes. We firstly formulate the scheme of the methods for\nthe system in a uniform magnetic field by using the idea of continuous-stage\nmethods, and then discuss its energy-preserving property. Moreover, symmetric\nconditions and order conditions are analysed. Based on those conditions, we\npropose two practical symmetric continuous-stage exponential energy-preserving\nintegrators of order up to four. Then we extend the obtained methods to the\nsystem in a nonuniform magnetic field and derive their properties including the\nsymmetry, convergence and energy conservation. Numerical experiments\ndemonstrate the efficiency of the proposed methods in comparison with some\nexisting schemes in the literature.'}, {'Continuous-stage symplectic adapted exponential methods for\n  charged-particle dynamics with arbitrary electromagnetic fields': 'This paper is devoted to the numerical symplectic approximation of the\ncharged-particle dynamics (CPD) with arbitrary electromagnetic fields. By\nutilizing continuous-stage methods and exponential integrators, a general class\nof symplectic methods is formulated for CPD under a homogeneous magnetic field.\nBased on the derived symplectic conditions, two practical symplectic methods up\nto order four are constructed where the error estimates show that the proposed\nsecond order scheme has a uniform accuracy in the position w.r.t. the strength\nof the magnetic field. Moreover, the symplectic methods are extended to CPD\nunder non-homogeneous magnetic fields and three algorithms are formulated.\nRigorous error estimates are investigated for the proposed methods and one\nmethod is proved to have a uniform accuracy in the position w.r.t. the strength\nof the magnetic field. Numerical experiments are provided for CPD under\nhomogeneous and non-homogeneous magnetic fields, and the numerical results\nsupport the theoretical analysis and demonstrate the remarkable numerical\nbehavior of our methods.'}, {'Explicit symplectic adapted exponential integrators for charged-particle\n  dynamics in a strong and constant magnetic field': 'This paper studies explicit symplectic adapted exponential integrators for\nsolving charged-particle dynamics in a strong and constant magnetic field. We\nfirst formulate the scheme of adapted exponential integrators and then derive\nits symplecticity conditions. Based on the symplecticity conditions, we propose\nfive practical explicit symplectic adapted exponential integrators. Two\nnumerical experiments are carried out and the numerical results demonstrate the\nremarkable numerical behavior of the new methods.'}]","**Abstract**

In the face of growing demands for annotated data in  semantic segmentation, unsupervised domain adaptation (UDA) emerges as a valuable tool to bridge the gap between different datasets by facilitating the transfer of knowledge from a labeled source domain to an unannotated target domain. However, domain adaptation models struggle with the adaptation of style information. Recognizing this challenge, our paper refines this approach by incorporating image-to-image domain adaptation techniques and innovative methods. This study targets the risk of domain performance degradation when adopting unadapted models in UDA. It introduces Style Adaptive Instance Normalization (SAIN), a method designed to adapt styistical features during the training process, ensuring that the adaptation of image styles remains transparent during prediction, while primarily focusing on the enhancement of style transfer in the target domain, and preserving the content of the source domain.

We employ Fourier Domain Adaptation (FDA) to harmonize the intricacies between source and target datasets, minimizing the visual disparity through domain-style tranfer. Moreover, our key innovation, SAIN, effectively addresses the issue by dynamically adjusting the model's output to align stylistically with the target domain, utilizing a novel custom formula that flexibly transfers style. This gradual shift in style alignment significantly improves the model's generalization capability and overall performance when deployed in unseen environments.

Empirical results on popularity datasets, such as GTA→Cityscapes, Synthia→Cityscapes, and Synthia→KITTI, illustrate that the integration of our method leads to a notable enhancement in mean Intersection over Union (mIoU) metrics, ranging from a 1.03 increase to a 1.05 increase across diverse domain adaptation scenarios. 

Not only does our study propose an efficacious approach to tackle the problems existing in UDA, but it also opens the door for future research by offering a robust framework on which subsequent improvements and adaptations can be built. Our contributions have the potential to unlock new avenues and possibilities for UDA, enhancing its applicability across various fields, including autonomous driving, urban planning, and more, thereby accelerating progress in deep learning applications with limited dataset capabilities."
"Cross-modality images that integrate visible-infrared spectra cues can
provide richer complementary information for object detection. Despite this,
existing visible-infrared object detection methods severely degrade in severe
weather conditions. This failure stems from the pronounced sensitivity of
visible images to environmental perturbations, such as rain, haze, and snow,
which frequently cause false negatives and false positives in detection. To
address this issue, we introduce a novel and challenging task, termed
visible-infrared object detection under adverse weather conditions. To foster
this task, we have constructed a new Severe Weather Visible-Infrared Dataset
(SWVID) with diverse severe weather scenes. Furthermore, we introduce the
Cross-modality Fusion Mamba with Weather-removal (CFMW) to augment detection
accuracy in adverse weather conditions. Thanks to the proposed Weather Removal
Diffusion Model (WRDM) and Cross-modality Fusion Mamba (CFM) modules, CFMW is
able to mine more essential information of pedestrian features in
cross-modality fusion, thus could transfer to other rarer scenarios with high
efficiency and has adequate availability on those platforms with low computing
power. To the best of our knowledge, this is the first study that targeted
improvement and integrated both Diffusion and Mamba modules in cross-modality
object detection, successfully expanding the practical application of this type
of model with its higher accuracy and more advanced architecture. Extensive
experiments on both well-recognized and self-created datasets conclusively
demonstrate that our CFMW achieves state-of-the-art detection performance,
surpassing existing benchmarks. The dataset and source code will be made
publicly available at https://github.com/lhy-zjut/CFMW.","[{'Beam Detection Based on Machine Learning Algorithms': 'The positions of free electron laser beams on screens are precisely\ndetermined by a sequence of machine learning models. Transfer training is\nconducted in a self-constructed convolutional neural network based on VGG16\nmodel. Output of intermediate layers are passed as features to a support vector\nregression model. With this sequence, 85.8% correct prediction is achieved on\ntest data.'}, {'Empowering Data Mesh with Federated Learning': 'The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.'}, {'Rationale-based Opinion Summarization': 'Opinion summarization aims to generate concise summaries that present popular\nopinions of a large group of reviews. However, these summaries can be too\ngeneric and lack supporting details. To address these issues, we propose a new\nparadigm for summarizing reviews, rationale-based opinion summarization.\nRationale-based opinion summaries output the representative opinions as well as\none or more corresponding rationales. To extract good rationales, we define\nfour desirable properties: relatedness, specificity, popularity, and diversity\nand present a Gibbs-sampling-based method to extract rationales. Overall, we\npropose RATION, an unsupervised extractive system that has two components: an\nOpinion Extractor (to extract representative opinions) and Rationales Extractor\n(to extract corresponding rationales). We conduct automatic and human\nevaluations to show that rationales extracted by RATION have the proposed\nproperties and its summaries are more useful than conventional summaries. The\nimplementation of our work is available at\nhttps://github.com/leehaoyuan/RATION.'}, {'Augmented Abstractive Summarization With Document-LevelSemantic Graph': 'Previous abstractive methods apply sequence-to-sequence structures to\ngenerate summary without a module to assist the system to detect vital mentions\nand relationships within a document. To address this problem, we utilize\nsemantic graph to boost the generation performance. Firstly, we extract\nimportant entities from each document and then establish a graph inspired by\nthe idea of distant supervision \\citep{mintz-etal-2009-distant}. Then, we\ncombine a Bi-LSTM with a graph encoder to obtain the representation of each\ngraph node. A novel neural decoder is presented to leverage the information of\nsuch entity graphs. Automatic and human evaluations show the effectiveness of\nour technique.'}, {'Weakly-Supervised Video Moment Retrieval via Regularized Two-Branch\n  Proposal Networks with Erasing Mechanism': 'Video moment retrieval is to identify the target moment according to the\ngiven sentence in an untrimmed video. Due to temporal boundary annotations of\nthe video are extremely time-consuming to acquire, modeling in the\nweakly-supervised setting is increasingly focused, where we only have access to\nthe video-sentence pairs during training. Most existing weakly-supervised\nmethods adopt a MIL-based framework to develop inter-sample confrontment, but\nneglect the intra-sample confrontment between moments with similar semantics.\nTherefore, these methods fail to distinguish the correct moment from plausible\nnegative moments. Further, the previous attention models in cross-modal\ninteraction tend to focus on a few dominant words exorbitantly, ignoring the\ncomprehensive video-sentence correspondence. In this paper, we propose a novel\nRegularized Two-Branch Proposal Network with Erasing Mechanism to consider the\ninter-sample and intra-sample confrontments simultaneously. Concretely, we\nfirst devise a language-aware visual filter to generate both enhanced and\nsuppressed video streams. Then, we design the sharable two-branch proposal\nmodule to generate positive and plausible negative proposals from the enhanced\nand suppressed branch respectively, contributing to sufficient confrontment.\nBesides, we introduce an attention-guided dynamic erasing mechanism in enhanced\nbranch to discover the complementary video-sentence relation. Moreover, we\napply two types of proposal regularization to stabilize the training process\nand improve model performance. The extensive experiments on ActivityCaption,\nCharades-STA and DiDeMo datasets show the effectiveness of our method.'}, {'Urban2Vec: Incorporating Street View Imagery and POIs for Multi-Modal\n  Urban Neighborhood Embedding': 'Understanding intrinsic patterns and predicting spatiotemporal\ncharacteristics of cities require a comprehensive representation of urban\nneighborhoods. Existing works relied on either inter- or intra-region\nconnectivities to generate neighborhood representations but failed to fully\nutilize the informative yet heterogeneous data within neighborhoods. In this\nwork, we propose Urban2Vec, an unsupervised multi-modal framework which\nincorporates both street view imagery and point-of-interest (POI) data to learn\nneighborhood embeddings. Specifically, we use a convolutional neural network to\nextract visual features from street view images while preserving geospatial\nsimilarity. Furthermore, we model each POI as a bag-of-words containing its\ncategory, rating, and review information. Analog to document embedding in\nnatural language processing, we establish the semantic similarity between\nneighborhood (""document"") and the words from its surrounding POIs in the vector\nspace. By jointly encoding visual, textual, and geospatial information into the\nneighborhood representation, Urban2Vec can achieve performances better than\nbaseline models and comparable to fully-supervised methods in downstream\nprediction tasks. Extensive experiments on three U.S. metropolitan areas also\ndemonstrate the model interpretability, generalization capability, and its\nvalue in neighborhood similarity analysis.'}, {'A novel decomposed-ensemble time series forecasting framework: capturing\n  underlying volatility information': 'Time series forecasting represents a significant and challenging task across\nvarious fields. Recently, methods based on mode decomposition have dominated\nthe forecasting of complex time series because of the advantages of capturing\nlocal characteristics and extracting intrinsic modes from data. Unfortunately,\nmost models fail to capture the implied volatilities that contain significant\ninformation. To enhance the prediction of contemporary diverse and complex time\nseries, we propose a novel time series forecasting paradigm that integrates\ndecomposition with the capability to capture the underlying fluctuation\ninformation of the series. In our methodology, we implement the Variational\nMode Decomposition algorithm to decompose the time series into K distinct\nsub-modes. Following this decomposition, we apply the Generalized\nAutoregressive Conditional Heteroskedasticity (GARCH) model to extract the\nvolatility information in these sub-modes. Subsequently, both the numerical\ndata and the volatility information for each sub-mode are harnessed to train a\nneural network. This network is adept at predicting the information of the\nsub-modes, and we aggregate the predictions of all sub-modes to generate the\nfinal output. By integrating econometric and artificial intelligence methods,\nand taking into account both the numerical and volatility information of the\ntime series, our proposed framework demonstrates superior performance in time\nseries forecasting, as evidenced by the significant decrease in MSE, RMSE, and\nMAPE in our comparative experimental results.'}, {'GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D\n  Pretraining from Real-World Data': ""3D Shape represented as point cloud has achieve advancements in multimodal\npre-training to align image and language descriptions, which is curial to\nobject identification, classification, and retrieval. However, the discrete\nrepresentations of point cloud lost the object's surface shape information and\ncreates a gap between rendering results and 2D correspondences. To address this\nproblem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D\nGaussian Splatting) into multimodal pre-training to enhance 3D representation.\nGS-CLIP leverages a pre-trained vision-language model for a learned common\nvisual and textual space on massive real world image-text pairs and then learns\na 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel\nGaussian-Aware Fusion is proposed to extract and fuse global explicit feature.\nAs a general framework for language-image-3D pre-training, GS-CLIP is agnostic\nto 3D backbone networks. Experiments on challenging shows that GS-CLIP\nsignificantly improves the state-of-the-art, outperforming the previously best\nresults.""}, {'Language Model is a Branch Predictor for Simultaneous Machine\n  Translation': 'The primary objective of simultaneous machine translation (SiMT) is to\nminimize latency while preserving the quality of the final translation. Drawing\ninspiration from CPU branch prediction techniques, we propose incorporating\nbranch prediction techniques in SiMT tasks to reduce translation latency.\nSpecifically, we utilize a language model as a branch predictor to predict\npotential branch directions, namely, future source words. Subsequently, we\nutilize the predicted source words to decode the output in advance. When the\nactual source word deviates from the predicted source word, we use the real\nsource word to decode the output again, replacing the predicted output. To\nfurther reduce computational costs, we share the parameters of the encoder and\nthe branch predictor, and utilize a pre-trained language model for\ninitialization. Our proposed method can be seamlessly integrated with any SiMT\nmodel. Extensive experimental results demonstrate that our approach can improve\ntranslation quality and latency at the same time. Our code is available at\nhttps://github.com/YinAoXiong/simt_branch_predictor .'}, {'Many-body localization in Ising models with random long-range\n  interactions': 'We theoretically investigate the many-body localization phase transition in a\none-dimensional Ising spin chain with random long-range spin-spin interactions,\n$V_{ij}\\propto\\left|i-j\\right|^{-\\alpha}$, where the exponent of the\ninteraction range $\\alpha$ can be tuned from zero to infinitely large. By using\nexact diagonalization, we calculate the half-chain entanglement entropy and the\nenergy spectral statistics and use them to characterize the phase transition\ntowards the many-body localization phase at infinite temperature and at\nsufficiently large disorder strength. We perform finite-size scaling to extract\nthe critical disorder strength and the critical exponent of the divergent\nlocalization length. With increasing $\\alpha$, the critical exponent\nexperiences a sharp increase at about $\\alpha=1$ and then gradually decreases\nto a value found earlier in a disordered short-ranged interacting spin chain.\nFor $\\alpha<1$, we find that the system is mostly localized and the increase in\nthe disorder strength may drive a transition between two many-body localized\nphases. In contrast, for $\\alpha>1$, the transition is from a thermalized phase\nto the many-body localization phase. Our predictions could be experimentally\ntested with ion-trap quantum emulator with programmable random long-range\ninteractions, or with randomly distributed Rydberg atoms or polar molecules in\nlattices.'}]","Abstract:

Addressing the challenges of weathering (including rain, haze, and snow) on image quality, this paper introduces a Weather-aware Reverse Diffusion Model (WRDM) combined with cross-modality fusion (CMF) in cross-modality object detection. The core innovations aim to restore images afflicted with adverse weather conditions and enhance the accuracy of object detection under these conditions.

The objective is to promote significant advancements in multi-weather restoration and cross-modality object detection by leveraging diffusion models for image restoration and novel state-space models for efficient feature fusion. Specifically, WRDM innovatively integrates reference clear images at each diffusion step for guiding the synthesis process, while CMF introduces a Channel Swapping Mamba block to facilitate feature interaction between RGB and thermal channels.

Experimental results showcase that CMF and WRDM jointly result in substantial improvement in metrics including PSNR and SSIM in image deraining, dehazing, and desnowing tasks, outperforming state-of-the-art methods. Specifically, the proposed method demonstrates superior performance with over a 24% improvement over MPRNet in the deraining task. Cross-modality performance is also amplified by nearly 10% under challenging weather scenarios.

This work advances the state-of-the-art in multi-weather image restoration and cross-modality object detection, paving the way for applications in surveillance, autonomous driving, and enhanced video monitoring by effectively handling and mitigating the issue of weather deterioration on image quality and visual information extraction.

Notably, resolution and memory usage are optimized to ensure efficient processing while achieving superior restoration and detection performances. The method's adaptability, scalability, and robustness contribute to enhanced system performance and wider applicability in diverse real-world scenarios."
"Besides humans and machines, Artificial Intelligence (AI) models have emerged
to be another important audience of programming languages, as we come to the
era of large language models (LLMs). LLMs can now excel at coding competitions
and even program like developers to address various tasks, such as math
calculation. Yet, the grammar and layout of existing programs are designed for
humans. Particularly, abundant grammar tokens and formatting tokens are
included to make the code more readable to humans. While beneficial, such a
human-centric design imposes an unnecessary computational burden on LLMs where
each token, either consumed or generated, consumes computational resources. To
improve inference efficiency and reduce computational costs, we propose the
concept of AI-oriented grammar, which aims to represent the code in a way that
better suits the working mechanism of AI models. Code written with AI-oriented
grammar discards formats and uses a minimum number of tokens to convey code
semantics effectively. To demonstrate the feasibility of this concept, we
explore and implement the first AI-oriented grammar for Python, named Simple
Python (SimPy). SimPy is crafted by revising the original Python grammar
through a series of heuristic rules. Programs written in SimPy maintain
identical Abstract Syntax Tree (AST) structures to those in standard Python,
allowing execution via a modified AST parser. In addition, we explore methods
to enable existing LLMs to proficiently understand and use SimPy, and ensure
the changes remain imperceptible for human developers. Compared with the
original Python, SimPy not only reduces token usage by 13.5% and 10.4% for
CodeLlama and GPT-4, but can also achieve equivalent, even improved,
performance over the models trained on Python code.","[{'PSCS: A Path-based Neural Model for Semantic Code Search': 'To obtain code snippets for reuse, programmers prefer to search for related\ndocuments, e.g., blogs or Q&A, instead of code itself. The major reason is due\nto the semantic diversity and mismatch between queries and code snippets. Deep\nlearning models have been proposed to address this challenge. Compared with\napproaches using information retrieval techniques, deep learning models do not\nsuffer from the information loss caused by refining user intention into\nkeywords. However, the performance of previous works is not satisfactory\nbecause they ignore the importance of code structure. When the semantics of\ncode (e.g., identifier names, APIs) are ambiguous, code structure may be the\nonly feature for the model to utilize. In that case, previous works relearn the\nstructural information from lexical tokens of code, which is extremely\ndifficult for a model without any domain knowledge. In this work, we propose\nPSCS, a path-based neural model for semantic code search. Our model encodes\nboth the semantics and structures of code represented by AST paths. We train\nand evaluate our model over 330k-19k query-function pairs, respectively. The\nevaluation results demonstrate that PSCS achieves a SuccessRate of 47.6% and a\nMean Reciprocal Rank (MRR) of 30.4% when considering the top-10 results with a\nmatch. The proposed approach significantly outperforms both DeepCS, the first\napproach that applies deep learning to code search task, and CARLCS, a\nstate-of-the-art approach that introduces a co-attentive representation\nlearning model on the basis of DeepCS. The importance of code structure is\ndemonstrated with an ablation study on code features, which enlightens model\ndesign for further studies.'}, {'CodeMark: Imperceptible Watermarking for Code Datasets against Neural\n  Code Completion Models': ""Code datasets are of immense value for training neural-network-based code\ncompletion models, where companies or organizations have made substantial\ninvestments to establish and process these datasets. Unluckily, these datasets,\neither built for proprietary or public usage, face the high risk of\nunauthorized exploits, resulting from data leakages, license violations, etc.\nEven worse, the ``black-box'' nature of neural models sets a high barrier for\nexternals to audit their training datasets, which further connives these\nunauthorized usages. Currently, watermarking methods have been proposed to\nprohibit inappropriate usage of image and natural language datasets. However,\ndue to domain specificity, they are not directly applicable to code datasets,\nleaving the copyright protection of this emerging and important field of code\ndata still exposed to threats. To fill this gap, we propose a method, named\nCodeMark, to embed user-defined imperceptible watermarks into code datasets to\ntrace their usage in training neural code completion models. CodeMark is based\non adaptive semantic-preserving transformations, which preserve the exact\nfunctionality of the code data and keep the changes covert against\nrule-breakers. We implement CodeMark in a toolkit and conduct an extensive\nevaluation of code completion models. CodeMark is validated to fulfill all\ndesired properties of practical watermarks, including harmlessness to model\naccuracy, verifiability, robustness, and imperceptibility.""}, {'Frauds Bargain Attack: Generating Adversarial Text Samples via Word\n  Manipulation Process': ""Recent research has revealed that natural language processing (NLP) models\nare vulnerable to adversarial examples. However, the current techniques for\ngenerating such examples rely on deterministic heuristic rules, which fail to\nproduce optimal adversarial examples. In response, this study proposes a new\nmethod called the Fraud's Bargain Attack (FBA), which uses a randomization\nmechanism to expand the search space and produce high-quality adversarial\nexamples with a higher probability of success. FBA uses the Metropolis-Hasting\nsampler, a type of Markov Chain Monte Carlo sampler, to improve the selection\nof adversarial examples from all candidates generated by a customized\nstochastic process called the Word Manipulation Process (WMP). The WMP method\nmodifies individual words in a contextually-aware manner through insertion,\nremoval, or substitution. Through extensive experiments, this study\ndemonstrates that FBA outperforms other methods in terms of attack success\nrate, imperceptibility and sentence quality.""}, {'Reversible Jump Attack to Textual Classifiers with Modification\n  Reduction': 'Recent studies on adversarial examples expose vulnerabilities of natural\nlanguage processing (NLP) models. Existing techniques for generating\nadversarial examples are typically driven by deterministic hierarchical rules\nthat are agnostic to the optimal adversarial examples, a strategy that often\nresults in adversarial samples with a suboptimal balance between magnitudes of\nchanges and attack successes. To this end, in this research we propose two\nalgorithms, Reversible Jump Attack (RJA) and Metropolis-Hasting Modification\nReduction (MMR), to generate highly effective adversarial examples and to\nimprove the imperceptibility of the examples, respectively. RJA utilizes a\nnovel randomization mechanism to enlarge the search space and efficiently\nadapts to a number of perturbed words for adversarial examples. With these\ngenerated adversarial examples, MMR applies the Metropolis-Hasting sampler to\nenhance the imperceptibility of adversarial examples. Extensive experiments\ndemonstrate that RJA-MMR outperforms current state-of-the-art methods in attack\nperformance, imperceptibility, fluency and grammar correctness.'}, {'Req2Lib: A Semantic Neural Model for Software Library Recommendation': 'Third-party libraries are crucial to the development of software projects. To\nget suitable libraries, developers need to search through millions of libraries\nby filtering, evaluating, and comparing. The vast number of libraries places a\nbarrier for programmers to locate appropriate ones. To help developers,\nresearchers have proposed automated approaches to recommend libraries based on\nlibrary usage pattern. However, these prior studies can not sufficiently match\nuser requirements and suffer from cold-start problem. In this work, we would\nlike to make recommendations based on requirement descriptions to avoid these\nproblems. To this end, we propose a novel neural approach called Req2Lib which\nrecommends libraries given descriptions of the project requirement. We use a\nSequence-to-Sequence model to learn the library linked-usage information and\nsemantic information of requirement descriptions in natural language. Besides,\nwe apply a domain-specific pre-trained word2vec model for word embedding, which\nis trained over textual corpus from Stack Overflow posts. In the experiment, we\ntrain and evaluate the model with data from 5,625 java projects. Our\npreliminary evaluation demonstrates that Req2Lib can recommend libraries\naccurately.'}, {'CoProtector: Protect Open-Source Code against Unauthorized Training\n  Usage with Data Poisoning': 'Github Copilot, trained on billions of lines of public code, has recently\nbecome the buzzword in the computer science research and practice community.\nAlthough it is designed to help developers implement safe and effective code\nwith powerful intelligence, practitioners and researchers raise concerns about\nits ethical and security problems, e.g., should the copyleft licensed code be\nfreely leveraged or insecure code be considered for training in the first\nplace? These problems pose a significant impact on Copilot and other similar\nproducts that aim to learn knowledge from large-scale open-source code through\ndeep learning models, which are inevitably on the rise with the fast\ndevelopment of artificial intelligence. To mitigate such impacts, we argue that\nthere is a need to invent effective mechanisms for protecting open-source code\nfrom being exploited by deep learning models. Here, we design and implement a\nprototype, CoProtector, which utilizes data poisoning techniques to arm source\ncode repositories for defending against such exploits. Our large-scale\nexperiments empirically show that CoProtector is effective in achieving its\npurpose, significantly reducing the performance of Copilot-like deep learning\nmodels while being able to stably reveal the secretly embedded watermark\nbackdoors.'}, {'On the Importance of Building High-quality Training Datasets for Neural\n  Code Search': 'The performance of neural code search is significantly influenced by the\nquality of the training data from which the neural models are derived. A large\ncorpus of high-quality query and code pairs is demanded to establish a precise\nmapping from the natural language to the programming language. Due to the\nlimited availability, most widely-used code search datasets are established\nwith compromise, such as using code comments as a replacement of queries. Our\nempirical study on a famous code search dataset reveals that over one-third of\nits queries contain noises that make them deviate from natural user queries.\nModels trained through noisy data are faced with severe performance degradation\nwhen applied in real-world scenarios. To improve the dataset quality and make\nthe queries of its samples semantically identical to real user queries is\ncritical for the practical usability of neural code search. In this paper, we\npropose a data cleaning framework consisting of two subsequent filters: a\nrule-based syntactic filter and a model-based semantic filter. This is the\nfirst framework that applies semantic query cleaning to code search datasets.\nExperimentally, we evaluated the effectiveness of our framework on two\nwidely-used code search models and three manually-annotated code retrieval\nbenchmarks. Training the popular DeepCS model with the filtered dataset from\nour framework improves its performance by 19.2% MRR and 21.3% Answer@1, on\naverage with the three validation benchmarks.'}, {'When Neural Code Completion Models Size up the Situation: Attaining\n  Cheaper and Faster Completion through Dynamic Model Inference': ""Leveraging recent advancements in large language models, modern neural code\ncompletion models have demonstrated the capability to generate highly accurate\ncode suggestions. However, their massive size poses challenges in terms of\ncomputational costs and environmental impact, hindering their widespread\nadoption in practical scenarios. Dynamic inference emerges as a promising\nsolution, as it allocates minimal computation during inference while\nmaintaining the model's performance. In this research, we explore dynamic\ninference within the context of code completion. Initially, we conducted an\nempirical investigation on GPT-2, focusing on the inference capabilities of\nintermediate layers for code completion. We found that 54.4% of tokens can be\naccurately generated using just the first layer, signifying significant\ncomputational savings potential. Moreover, despite using all layers, the model\nstill fails to predict 14.5% of tokens correctly, and the subsequent\ncompletions continued from them are rarely considered helpful, with only a 4.2%\nAcceptance Rate. These findings motivate our exploration of dynamic inference\nin code completion and inspire us to enhance it with a decision-making\nmechanism that stops the generation of incorrect code. We thus propose a novel\ndynamic inference method specifically tailored for code completion models. This\nmethod aims not only to produce correct predictions with largely reduced\ncomputation but also to prevent incorrect predictions proactively. Our\nextensive evaluation shows that it can averagely skip 1.7 layers out of 16\nlayers in the models, leading to an 11.2% speedup with only a marginal 1.1%\nreduction in ROUGE-L.""}, {'Robustness, Security, Privacy, Explainability, Efficiency, and Usability\n  of Large Language Models for Code': 'Large language models for code (LLM4Code), which demonstrate strong\nperformance (e.g., high accuracy) in processing source code, have significantly\ntransformed software engineering. Many studies separately investigate the\nnon-functional properties of LM4Code, but there is no systematic review of how\nthese properties are evaluated and enhanced. This paper fills this gap by\nthoroughly examining 146 relevant studies, thereby presenting the first\nsystematic literature review to identify seven important properties beyond\naccuracy, including robustness, security, privacy, explainability, efficiency,\nand usability. We discuss the current state-of-the-art methods and trends,\nidentify gaps in existing research, and present promising directions for future\nstudy.'}, {""Don't Complete It! Preventing Unhelpful Code Completion for Productive\n  and Sustainable Neural Code Completion Systems"": 'Currently, large pre-trained language models are widely applied in neural\ncode completion systems. Though large code models significantly outperform\ntheir smaller counterparts, around 70% displayed code completions from Copilot\nare not accepted by developers. Being reviewed but not accepted, their help to\ndeveloper productivity is considerably limited. Even worse, considering the\nhigh cost of the large code models, it is a huge waste of computing resources\nand energy. To fill this significant gap, we first investigate the prompts of\nunhelpful code completions, and empirically find four observable patterns that\ncause such prompts, all of which are inherent, namely, they can hardly be\naddressed by improving the accuracy of the model. This demonstrates the\nfeasibility of identifying such prompts based on the prompts themselves.\nMotivated by this finding, we propose an early-rejection mechanism to turn down\nlow-return prompts by foretelling the code completion qualities without sending\nthem to the code completion system. Furthermore, we propose a lightweight\nTransformer-based estimator to demonstrate the feasibility of the mechanism.\nThe experimental results show that the proposed estimator helps save 23.3% of\ncomputational cost measured in floating-point operations for the code\ncompletion systems, and 80.2% of rejected prompts lead to unhelpful completion'}]","Title: AI Coders Are Among Us: Rethinking Programming Language Grammar for Efficient Code Generation

Abstract:

The burgeoning field of Large Language Models (LLMs) leverages extensive data to generate high-quality code, yet faces inefficiencies due to the diverse, human-centric grammar in source code which often imposes redundant tokens. This study innovates by reformulating a language with AI-oriented grammar that reduces token counts by up to 34.7% on average, demonstrating AI's potential to enhance code generation. Our investigation effectively explores the impact of two training strategies to adapt models to this newly shaped, compact language, and introduces a Python-to-SimPy code conversion method for seamless AI coding within pre-existing systems. 

Drawing upon state-of-the-art LLMs such as CodeBERT, Codex, and TinyLlama for experiments, our results highlight the challenge of accurately improving code generation efficiency without compromising model performance. Methodologically, the paper pioneers the implementation of code conversion tools compatible with a variety of AI models, showcasing the integration of AI-oriented grammar into existing productivity pipelines without notable latency overhead. 

Our primary finding that the AI-oriented SimPy code framework can significantly reduce token redundancy while maintaining or improving code generation quality contributes to the paradigm shift in programming language design. this research is poised to catalyze advancements in efficient code generation, enhancing the productivity and competitiveness of AI-driven software development workflows in the rapidly evolving tech landscape."
"Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of
GPT-4V, by enabling the model to associate visual objects with tags inserted on
the image. These tags, marked with alphanumerics, can be indexed via text
tokens for easy reference. Despite the extraordinary performance from GPT-4V,
we observe that other Multimodal Large Language Models (MLLMs) struggle to
understand these visual tags. To promote the learning of SoM prompting for
open-source models, we propose a new learning paradigm: ""list items one by
one,"" which asks the model to enumerate and describe all visual tags placed on
the image following the alphanumeric orders of tags. By integrating our curated
dataset with other visual instruction tuning datasets, we are able to equip
existing MLLMs with the SoM prompting ability. Furthermore, we evaluate our
finetuned SoM models on five MLLM benchmarks. We find that this new dataset,
even in a relatively small size (10k-30k images with tags), significantly
enhances visual reasoning capabilities and reduces hallucinations for MLLMs.
Perhaps surprisingly, these improvements persist even when the visual tags are
omitted from input images during inference. This suggests the potential of
""list items one by one"" as a new paradigm for training MLLMs, which strengthens
the object-text alignment through the use of visual tags in the training stage.
Finally, we conduct analyses by probing trained models to understand the
working mechanism of SoM. Our code and data are available at
\url{https://github.com/zzxslp/SoM-LLaVA}.","[{'Mining Open Government Data Used in Scientific Research': 'In the following paper, we describe results from mining citations, mentions,\nand links to open government data (OGD) in peer-reviewed literature. We\ninductively develop a method for categorizing how OGD are used by different\nresearch communities, and provide descriptive statistics about the publication\nyears, publication outlets, and OGD sources. Our results demonstrate that, 1.\nThe use of OGD in research is steadily increasing from 2009 to 2016; 2.\nResearchers use OGD from 96 different open government data portals, with\ndata.gov.uk and data.gov being the most frequent sources; and, 3.Contrary to\nprevious findings, we provide evidence suggesting that OGD from developing\nnations, notably India and Kenya, are being frequently used to fuel scientific\ndiscoveries. The findings of this paper contribute to ongoing research agendas\naimed at tracking the impact of open government data initiatives, and provides\nan initial description of how open government data are valuable to diverse\nscientific research communities.'}, {'FairST: Equitable Spatial and Temporal Demand Prediction for New\n  Mobility Systems': 'Emerging transportation modes, including car-sharing, bike-sharing, and\nride-hailing, are transforming urban mobility but have been shown to reinforce\nsocioeconomic inequities. Spatiotemporal demand prediction models for these new\nmobility regimes must therefore consider fairness as a first-class design\nrequirement. We present FairST, a fairness-aware model for predicting demand\nfor new mobility systems. Our approach utilizes 1D, 2D and 3D convolutions to\nintegrate various urban features and learn the spatial-temporal dynamics of a\nmobility system, but we include fairness metrics as a form of regularization to\nmake the predictions more equitable across demographic groups. We propose two\nnovel spatiotemporal fairness metrics, a region-based fairness gap (RFG) and an\nindividual-based fairness gap (IFG). Both quantify equity in a spatiotemporal\ncontext, but vary by whether demographics are labeled at the region level (RFG)\nor whether population distribution information is available (IFG). Experimental\nresults on real bike share and ride share datasets demonstrate the\neffectiveness of the proposed model: FairST not only reduces the fairness gap\nby more than 80%, but can surprisingly achieve better accuracy than\nstate-of-the-art yet fairness-oblivious methods including LSTMs, ConvLSTMs, and\n3D CNN.'}, {'Study on S2 Flow Path Design and Three-dimensional Numerical Simulation\n  Parameter Calibration in Axial Compressor': 'Aerodynamic design process of multi - stage axial flow compressor usually\nuses the way that combines the S2 flow design and three-dimensional CFD\nnumerical simulation analysis. Based on Mr. Wu Zhonghua\'s "" Three-dimensional\nflow theory "", aiming at the S2 flow design matching parameters and the\nthree-dimensional CFD numerical simulation data, through autonomous\nprogramming, the S2 design parameter distribution and the corresponding CGNS\nformat CFD calculation results are extracted. Then make the comparative\nanalysis of the two and provide the modification suggestion of the design. The\nexamples have been tested by the comparison and correction of the eight-stage\naxial flow compressor calculation and finally improve the design performance of\nthe compressor design. The design adiabatic efficiency increases by 0.5%. The\nsurge margin increases by more than 5%. The validity and feasibility of the\nmethod are verified.'}, {'Comparing Apples to Apples: Generating Aspect-Aware Comparative\n  Sentences from User Reviews': 'It is time-consuming to find the best product among many similar\nalternatives. Comparative sentences can help to contrast one item from others\nin a way that highlights important features of an item that stand out. Given\nreviews of one or multiple items and relevant item features, we generate\ncomparative review sentences to aid users to find the best fit. Specifically,\nour model consists of three successive components in a transformer: (i) an item\nencoding module to encode an item for comparison, (ii) a comparison generation\nmodule that generates comparative sentences in an autoregressive manner, (iii)\na novel decoding method for user personalization. We show that our pipeline\ngenerates fluent and diverse comparative sentences. We run experiments on the\nrelevance and fidelity of our generated sentences in a human evaluation study\nand find that our algorithm creates comparative review sentences that are\nrelevant and truthful.'}, {'Predicting Abandonment in Online Coding Tutorials': 'Learners regularly abandon online coding tutorials when they get bored or\nfrustrated, but there are few techniques for anticipating this abandonment to\nintervene. In this paper, we examine the feasibility of predicting abandonment\nwith machine-learned classifiers. Using interaction logs from an online\nprogramming game, we extracted a collection of features that are potentially\nrelated to learner abandonment and engagement, then developed classifiers for\neach level. Across the first five levels of the game, our classifiers\nsuccessfully predicted 61% to 76% of learners who did not complete the next\nlevel, achieving an average AUC of 0.68. In these classifiers, features\nnegatively associated with abandonment included account activation and\nhelp-seeking behaviors, whereas features positively associated with abandonment\nincluded features indicating difficulty and disengagement. These findings\nhighlight the feasibility of providing timely intervention to learners likely\nto quit.'}, {'Personalized Showcases: Generating Multi-Modal Explanations for\n  Recommendations': ""Existing explanation models generate only text for recommendations but still\nstruggle to produce diverse contents. In this paper, to further enrich\nexplanations, we propose a new task named personalized showcases, in which we\nprovide both textual and visual information to explain our recommendations.\nSpecifically, we first select a personalized image set that is the most\nrelevant to a user's interest toward a recommended item. Then, natural language\nexplanations are generated accordingly given our selected images. For this new\ntask, we collect a large-scale dataset from Google Local (i.e.,~maps) and\nconstruct a high-quality subset for generating multi-modal explanations. We\npropose a personalized multi-modal framework which can generate diverse and\nvisually-aligned explanations via contrastive learning. Experiments show that\nour framework benefits from different modalities as inputs, and is able to\nproduce more diverse and expressive explanations compared to previous methods\non a variety of evaluation metrics.""}, {'Cross-Lingual Vision-Language Navigation': 'Commanding a robot to navigate with natural language instructions is a\nlong-term goal for grounded language understanding and robotics. But the\ndominant language is English, according to previous studies on vision-language\nnavigation (VLN). To go beyond English and serve people speaking different\nlanguages, we collect a bilingual Room-to-Room (BL-R2R) dataset, extending the\noriginal benchmark with new Chinese instructions. Based on this newly\nintroduced dataset, we study how an agent can be trained on existing English\ninstructions but navigate effectively with another language under a zero-shot\nlearning scenario. Without any training data of the target language, our model\nshows competitive results even compared to a model with full access to the\ntarget language training data. Moreover, we investigate the transferring\nability of our model when given a certain amount of target language training\ndata.'}, {'L2C: Describing Visual Differences Needs Semantic Understanding of\n  Individuals': 'Recent advances in language and vision push forward the research of\ncaptioning a single image to describing visual differences between image pairs.\nSuppose there are two images, I_1 and I_2, and the task is to generate a\ndescription W_{1,2} comparing them, existing methods directly model { I_1, I_2\n} -> W_{1,2} mapping without the semantic understanding of individuals. In this\npaper, we introduce a Learning-to-Compare (L2C) model, which learns to\nunderstand the semantic structures of these two images and compare them while\nlearning to describe each one. We demonstrate that L2C benefits from a\ncomparison between explicit semantic representations and single-image captions,\nand generalizes better on the new testing image pairs. It outperforms the\nbaseline on both automatic evaluation and human evaluation for the\nBirds-to-Words dataset.'}, {'CosRec: 2D Convolutional Neural Networks for Sequential Recommendation': 'Sequential patterns play an important role in building modern recommender\nsystems. To this end, several recommender systems have been built on top of\nMarkov Chains and Recurrent Models (among others). Although these sequential\nmodels have proven successful at a range of tasks, they still struggle to\nuncover complex relationships nested in user purchase histories. In this paper,\nwe argue that modeling pairwise relationships directly leads to an efficient\nrepresentation of sequential features and captures complex item correlations.\nSpecifically, we propose a 2D convolutional network for sequential\nrecommendation (CosRec). It encodes a sequence of items into a three-way\ntensor; learns local features using 2D convolutional filters; and aggregates\nhigh-order interactions in a feedforward manner. Quantitative results on two\npublic datasets show that our method outperforms both conventional methods and\nrecent sequence-based approaches, achieving state-of-the-art performance on\nvarious evaluation metrics.'}, {'CLIP also Understands Text: Prompting CLIP for Phrase Understanding': 'Contrastive Language-Image Pretraining (CLIP) efficiently learns visual\nconcepts by pre-training with natural language supervision. CLIP and its visual\nencoder have been explored on various vision and language tasks and achieve\nstrong zero-shot or transfer learning performance. However, the application of\nits text encoder solely for text understanding has been less explored. In this\npaper, we find that the text encoder of CLIP actually demonstrates strong\nability for phrase understanding, and can even significantly outperform popular\nlanguage models such as BERT with a properly designed prompt. Extensive\nexperiments validate the effectiveness of our method across different datasets\nand domains on entity clustering and entity set expansion tasks.'}]","Title: SoM-LLaVA: Enhancing Set-of-Mark Prompting in Multimodal Language Models through Gumped Tagging Techniques

Abstract: This paper introduces a new multimodal language model named SoM-LLaVA, which significantly improves upon set-of-mark (SoM) prompting for open-source multimodal language models (MLLMs). By leveraging Gumped tagging techniques, SoM-LLaVA achieves impressive accuracy in identifying all objects within richly-annotated images like MS-COCO. The primary contributions of this research include the development of Gumped tagging, tailored dataset creation based on diverse open-source image datasets, a series of training and probing analyses, and insightful insights into multimodal learning mechanisms. 

To address the challenge of open-source MLLMs understanding prompts like ""enumerate all tags,"" this paper first analyzes their performance on set-of-mark probing tasks. It then develops and validates Gumped tagging, enhancing object detection accuracy by integrating tags with alphanumeric texts. The creation of a SoM dataset from MS-COCO, paired with the Jacobs rule-based data generation approach, aids in alternating the languages of vision and text. 

SoM-LLaVA fine-tunes on this new dataset, significantly improving performance in listing object tags with 80% accuracy from occluded images. Comparative experiments with other models demonstrate its superior ability to effectively attend to object patches, highlighting the enhancement in MLLM mastery of set-of-mark prompting. This research thus uncovers a new pathway to improve multimodal language models' ability to perform SoM tasks, impacting applications such as visual instruction generation and scene understanding tasks in AI and human-computer interaction contexts."
"In the realm of Medical Visual Language Models (Med-VLMs), the quest for
universal efficient fine-tuning mechanisms remains paramount, especially given
researchers in interdisciplinary fields are often extremely short of training
resources, yet largely unexplored. Given the unique challenges in the medical
domain, such as limited data scope and significant domain-specific
requirements, evaluating and adapting Parameter-Efficient Fine-Tuning (PEFT)
methods specifically for Med-VLMs is essential. Most of the current PEFT
methods on Med-VLMs have yet to be comprehensively investigated but mainly
focus on adding some components to the model's structure or input. However,
fine-tuning intrinsic model components often yields better generality and
consistency, and its impact on the ultimate performance of Med-VLMs has been
widely overlooked and remains understudied. In this paper, we endeavour to
explore an alternative to traditional PEFT methods, especially the impact of
fine-tuning LayerNorm layers, FFNs and Attention layers on the Med-VLMs. Our
comprehensive studies span both small-scale and large-scale Med-VLMs,
evaluating their performance under various fine-tuning paradigms across tasks
such as Medical Visual Question Answering and Medical Imaging Report
Generation. The findings reveal unique insights into the effects of intrinsic
parameter fine-tuning methods on fine-tuning Med-VLMs to downstream tasks and
expose fine-tuning solely the LayerNorm layers not only surpasses the
efficiency of traditional PEFT methods but also retains the model's accuracy
and generalization capabilities across a spectrum of medical downstream tasks.
The experiments show LayerNorm fine-tuning's superior adaptability and
scalability, particularly in the context of large-scale Med-VLMs.","[{'A New Penalty Dual-Primal Augmented Lagrangian Method and Its Extensions': 'In this paper, we propose a penalty dual-primal augmented lagrangian method\nfor solving convex minimization problems under linear equality or inequality\nconstraints. The proposed method combines a novel penalty technique with\nupdates the new iterates in a dual-primal order, and then be extended to solve\nmultiple-block separable convex programming problems with splitting version and\npartial splitting version. We establish the convergence analysis for all the\nintroduced algorithm in the lens of variational analysis. Numerical results on\nthe basic pursuit problem and the lasso model are presented to illustrate the\nefficiency of the proposed method.'}, {'Semi-Coupled Two-Stream Fusion ConvNets for Action Recognition at\n  Extremely Low Resolutions': 'Deep convolutional neural networks (ConvNets) have been recently shown to\nattain state-of-the-art performance for action recognition on\nstandard-resolution videos. However, less attention has been paid to\nrecognition performance at extremely low resolutions (eLR) (e.g., 16 x 12\npixels). Reliable action recognition using eLR cameras would address privacy\nconcerns in various application environments such as private homes, hospitals,\nnursing/rehabilitation facilities, etc. In this paper, we propose a\nsemi-coupled filter-sharing network that leverages high resolution (HR) videos\nduring training in order to assist an eLR ConvNet. We also study methods for\nfusing spatial and temporal ConvNets customized for eLR videos in order to take\nadvantage of appearance and motion information. Our method outperforms\nstate-of-the-art methods at extremely low resolutions on IXMAS (93.7%) and HMDB\n(29.2%) datasets.'}, {'VGAN-Based Image Representation Learning for Privacy-Preserving Facial\n  Expression Recognition': ""Reliable facial expression recognition plays a critical role in human-machine\ninteractions. However, most of the facial expression analysis methodologies\nproposed to date pay little or no attention to the protection of a user's\nprivacy. In this paper, we propose a Privacy-Preserving Representation-Learning\nVariational Generative Adversarial Network (PPRL-VGAN) to learn an image\nrepresentation that is explicitly disentangled from the identity information.\nAt the same time, this representation is discriminative from the standpoint of\nfacial expression recognition and generative as it allows expression-equivalent\nface image synthesis. We evaluate the proposed model on two public datasets\nunder various threat scenarios. Quantitative and qualitative results\ndemonstrate that our approach strikes a balance between the preservation of\nprivacy and data utility. We further demonstrate that our model can be\neffectively applied to other tasks such as expression morphing and image\ncompletion.""}, {'Residual Frames with Efficient Pseudo-3D CNN for Human Action\n  Recognition': 'Human action recognition is regarded as a key cornerstone in domains such as\nsurveillance or video understanding. Despite recent progress in the development\nof end-to-end solutions for video-based action recognition, achieving\nstate-of-the-art performance still requires using auxiliary hand-crafted motion\nrepresentations, e.g., optical flow, which are usually computationally\ndemanding. In this work, we propose to use residual frames (i.e., differences\nbetween adjacent RGB frames) as an alternative ""lightweight"" motion\nrepresentation, which carries salient motion information and is computationally\nefficient. In addition, we develop a new pseudo-3D convolution module which\ndecouples 3D convolution into 2D and 1D convolution. The proposed module\nexploits residual information in the feature space to better structure motions,\nand is equipped with a self-attention mechanism that assists to recalibrate the\nappearance and motion features. Empirical results confirm the efficiency and\neffectiveness of residual frames as well as the proposed pseudo-3D convolution\nmodule.'}, {'MM-ViT: Multi-Modal Video Transformer for Compressed Video Action\n  Recognition': 'This paper presents a pure transformer-based approach, dubbed the Multi-Modal\nVideo Transformer (MM-ViT), for video action recognition. Different from other\nschemes which solely utilize the decoded RGB frames, MM-ViT operates\nexclusively in the compressed video domain and exploits all readily available\nmodalities, i.e., I-frames, motion vectors, residuals and audio waveform. In\norder to handle the large number of spatiotemporal tokens extracted from\nmultiple modalities, we develop several scalable model variants which factorize\nself-attention across the space, time and modality dimensions. In addition, to\nfurther explore the rich inter-modal interactions and their effects, we develop\nand compare three distinct cross-modal attention mechanisms that can be\nseamlessly integrated into the transformer building block. Extensive\nexperiments on three public action recognition benchmarks (UCF-101,\nSomething-Something-v2, Kinetics-600) demonstrate that MM-ViT outperforms the\nstate-of-the-art video transformers in both efficiency and accuracy, and\nperforms better or equally well to the state-of-the-art CNN counterparts with\ncomputationally-heavy optical flow.'}, {'Emergence of Specialization from Global Optimizing Evolution in a\n  Multi-Agent System': 'The evolution of specialization in a multi-agent system is studied both by\ncomputer simulation and Markov process model. Many individual agents search for\nand exploit resources to get global optimization in an environment without\ncomplete information. With the selection acting on agent specialization at the\nlevel of system and under the condition of increasing returns, the division of\nlabor emerges as the results of long-term optimizing evolution. Mathematical\nanalysis gives the optimum division of agents and a Markov chain model is\nproposed to describe the evolutionary dynamics. The results are in good\nagreement with that of simulation model.'}, {'Coarse-grained periodic orbits and bifurcations in a Markov chain model\n  for evolution of labor division': 'We construct a Markov process model to describe the evolution of labor\ndivision and its dynamical behavior is investigated by numerical simulations in\ndetail. We have shown that under the mechanism of increasing returns, the\ndivision of labor will emerge from the initial homogenous distribution and with\nthe change of parameters there are period-adding coarse-grained bifurcations in\nthe model. The model gives an example of emergent property in the evolution of\ncomplex systems and reveals an interesting dynamical behavior of the Markov\nchain process. Key words: Evolution, Bifurcation, Pattern formation PACS\nnumber(s): 87.23.-n, 89.75.Fb, 05.45.-a'}, {'A projection method for approximating fixed points of quasi nonexpansive\n  mappings without the usual demiclosedness condition': 'We introduce and analyze an abstract algorithm that aims to find the\nprojection onto a closed convex subset of a Hilbert space. When specialized to\nthe fixed point set of a quasi nonexpansive mapping, the required sufficient\ncondition (termed ""fixed-point closed"") is less restrictive than the usual\nconditions based on the demiclosedness principle. A concrete example of a\nsubgradient projector is presented which illustrates the applicability of this\ngeneralization.'}, {'A Bregman projection method for approximating fixed points of\n  quasi-Bregman nonexpansive mappings': 'We introduce an abstract algorithm that aims to find the Bregman projection\nonto a closed convex set. As an application, the asymptotic behaviour of an\niterative method for finding a fixed point of a quasi Bregman nonexpansive\nmapping with the fixed-point closedness property is analyzed. We also show that\nour result is applicable to Bregman subgradient projectors.'}, {'A proximal average for prox-bounded functions': 'In this work, we construct a proximal average for two prox-bounded functions,\nwhich recovers the classical proximal average for two convex functions. The new\nproximal average transforms continuously in epi-topology from one proximal hull\nto the other. When one of the functions is differentiable, the new proximal\naverage is differentiable. We give characterizations for Lipschitz and\nsingle-valued proximal mappings and we show that the convex combination of\nconvexified proximal mappings is always a proximal mapping.\nSubdifferentiability and behaviors of infimal values and minimizers are also\nstudied.'}]","Title: Enhancing Fine-Tuning Techniques for Medical Vision-Language Models 

Abstract:

In the evolving landscape of medical imaging, leveraging large-scale pretrained models to generate reports and interpret complex medical imagery is essential. This research focuses on optimizing the fine-tuning methodologies for Large-Scale Vision-Language Models (LVLMs) to enhance their interpretability and accuracy in the medical domain. This paper introduces a methodological shift from existing techniques to the Continual Fine-tuning (CFT) approach, part of the Layer Normalization Fine-tuning (LN-tuning) paradigm. The study innovates by comparing multiple fine-tuning techniques on LVLMs, including LoRa-tuning, Prefix-tuning, and Additive Fine-tuning. 

The methodology involves comparing the performance of CFT and several extrinsic fine-tuning techniques on a medical question-answering task. The research's primary objective is to evaluate the effectiveness of different fine-tuning strategies and determine which yields the best performance on unseen medical data. 

Key findings indicate that LN-tuning outperforms traditional PEFT methods in maintaining accuracy while reducing overfitting. The paper's experimental results highlight the benefits of responsiveness and adaptivity in fine-tuning for better performance across medical image-based tasks. 

This study significantly contributes to the field by offering a refined fine-tuning strategy that improves generative models' interpretability and performance, particularly in task-oriented, medical imaging scenarios. The findings demonstrate a viable solution to the challenges in applying pretrained models to solve complex medical problems, offering potential for widespread application in radiology, pathology, and other medical imaging sectors.

Future work aims to extend these findings to zero-shot settings, exploring the models' adaptability to novel medical domains without extensive retraining. The research fills a significant gap in the literature on applying deep learning techniques to medical imaging, providing valuable insights for the development of more efficient and accurate medical diagnostic support systems."
"Scale has opened new frontiers in natural language processing, but at a high
cost. In response, by learning to only activate a subset of parameters in
training and inference, Mixture-of-Experts (MoE) have been proposed as an
energy efficient path to even larger and more capable language models and this
shift towards a new generation of foundation models is gaining momentum,
particularly within the field of Automatic Speech Recognition (ASR). Recent
works that incorporating MoE into ASR models have complex designs such as
routing frames via supplementary embedding network, improving multilingual
ability for the experts, and utilizing dedicated auxiliary losses for either
expert load balancing or specific language handling. We found that delicate
designs are not necessary, while an embarrassingly simple substitution of MoE
layers for all Feed-Forward Network (FFN) layers is competent for the ASR task.
To be more specific, we benchmark our proposed model on a large scale
inner-source dataset (160k hours), the results show that we can scale our
baseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve
Dense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real
Time Factor (RTF). Furthermore, by applying Unified 2-pass framework with
bidirectional attention decoders (U2++), we achieve the streaming and
non-streaming decoding modes in a single MoE based model, which we call U2++
MoE. We hope that our study can facilitate the research on scaling speech
foundation models without sacrificing deployment efficiency.","[{'A new metric associated with the domain boundary': 'In this paper, we introduce a new metric $\\tilde{c}$ which is associated with\nthe domain boundary for a Ptolemy space $(X,d)$. Moreover, we study the\ninclusion relation of the $\\tilde{c}$ metric balls and some related hyperbolic\ntype metric balls in subdomains of $\\mathbb{R}^n$. In addition, we study\ndistortion properties of M\\""obius transformations with respect to the\n$\\tilde{c}$ metric in the unit ball and the quasiconformality of bilipschitz\nmappings in $\\tilde{c}$ metric.'}, {'Non-Autoregressive Transformer ASR with CTC-Enhanced Decoder Input': 'Non-autoregressive (NAR) transformer models have achieved significantly\ninference speedup but at the cost of inferior accuracy compared to\nautoregressive (AR) models in automatic speech recognition (ASR). Most of the\nNAR transformers take a fixed-length sequence filled with MASK tokens or a\nredundant sequence copied from encoder states as decoder input, they cannot\nprovide efficient target-side information thus leading to accuracy degradation.\nTo address this problem, we propose a CTC-enhanced NAR transformer, which\ngenerates target sequence by refining predictions of the CTC module.\nExperimental results show that our method outperforms all previous NAR\ncounterparts and achieves 50x faster decoding speed than a strong AR baseline\nwith only 0.0 ~ 0.3 absolute CER degradation on Aishell-1 and Aishell-2\ndatasets.'}, {'ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs': 'In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding\nPrompt-and-Refine strategy (Figure 3), two simple but effective\n\\textbf{training-free} methods to decrease the Token Display Time (TDT) of\nstreaming ASR models \\textbf{without any accuracy loss}. The core idea of\nZeroPrompt is to append zeroed content to each chunk during inference, which\nacts like a prompt to encourage the model to predict future tokens even before\nthey were spoken. We argue that streaming acoustic encoders naturally have the\nmodeling ability of Masked Language Models and our experiments demonstrate that\nZeroPrompt is engineering cheap and can be applied to streaming acoustic\nencoders on any dataset without any accuracy loss. Specifically, compared with\nour baseline models, we achieve 350 $\\sim$ 700ms reduction on First Token\nDisplay Time (TDT-F) and 100 $\\sim$ 400ms reduction on Last Token Display Time\n(TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and\nLibrispeech datasets.'}, {'LightGrad: Lightweight Diffusion Probabilistic Model for Text-to-Speech': 'Recent advances in neural text-to-speech (TTS) models bring thousands of TTS\napplications into daily life, where models are deployed in cloud to provide\nservices for customs. Among these models are diffusion probabilistic models\n(DPMs), which can be stably trained and are more parameter-efficient compared\nwith other generative models. As transmitting data between customs and the\ncloud introduces high latency and the risk of exposing private data, deploying\nTTS models on edge devices is preferred. When implementing DPMs onto edge\ndevices, there are two practical problems. First, current DPMs are not\nlightweight enough for resource-constrained devices. Second, DPMs require many\ndenoising steps in inference, which increases latency. In this work, we present\nLightGrad, a lightweight DPM for TTS. LightGrad is equipped with a lightweight\nU-Net diffusion decoder and a training-free fast sampling technique, reducing\nboth model parameters and inference latency. Streaming inference is also\nimplemented in LightGrad to reduce latency further. Compared with Grad-TTS,\nLightGrad achieves 62.2% reduction in paramters, 65.7% reduction in latency,\nwhile preserving comparable speech quality on both Chinese Mandarin and English\nin 4 denoising steps.'}, {'Spike-Triggered Contextual Biasing for End-to-End Mandarin Speech\n  Recognition': 'The attention-based deep contextual biasing method has been demonstrated to\neffectively improve the recognition performance of end-to-end automatic speech\nrecognition (ASR) systems on given contextual phrases. However, unlike shallow\nfusion methods that directly bias the posterior of the ASR model, deep biasing\nmethods implicitly integrate contextual information, making it challenging to\ncontrol the degree of bias. In this study, we introduce a spike-triggered deep\nbiasing method that simultaneously supports both explicit and implicit bias.\nMoreover, both bias approaches exhibit significant improvements and can be\ncascaded with shallow fusion methods for better results. Furthermore, we\npropose a context sampling enhancement strategy and improve the contextual\nphrase filtering algorithm. Experiments on the public WenetSpeech Mandarin\nbiased-word dataset show a 32.0% relative CER reduction compared to the\nbaseline model, with an impressively 68.6% relative CER reduction on contextual\nphrases.'}, {'Speech-XLNet: Unsupervised Acoustic Model Pretraining For Self-Attention\n  Networks': 'Self-attention network (SAN) can benefit significantly from the\nbi-directional representation learning through unsupervised pretraining\nparadigms such as BERT and XLNet. In this paper, we present an XLNet-like\npretraining scheme ""Speech-XLNet"" for unsupervised acoustic model pretraining\nto learn speech representations with SAN. The pretrained SAN is finetuned under\nthe hybrid SAN/HMM framework. We conjecture that by shuffling the speech frame\norders, the permutation in Speech-XLNet serves as a strong regularizer to\nencourage the SAN to make inferences by focusing on global structures through\nits attention weights. In addition, Speech-XLNet also allows the model to\nexplore the bi-directional contexts for effective speech representation\nlearning. Experiments on TIMIT and WSJ demonstrate that Speech-XLNet greatly\nimproves the SAN/HMM performance in terms of both convergence speed and\nrecognition accuracy compared to the one trained from randomly initialized\nweights. Our best systems achieve a relative improvement of 11.9% and 8.3% on\nthe TIMIT and WSJ tasks respectively. In particular, the best system achieves a\nphone error rate (PER) of 13.3% on the TIMIT test set, which to our best\nknowledge, is the lowest PER obtained from a single system.'}, {'TrimTail: Low-Latency Streaming ASR with Simple but Effective\n  Spectrogram-Level Length Penalty': 'In this paper, we present TrimTail, a simple but effective emission\nregularization method to improve the latency of streaming ASR models. The core\nidea of TrimTail is to apply length penalty (i.e., by trimming trailing frames,\nsee Fig. 1-(b)) directly on the spectrogram of input utterances, which does not\nrequire any alignment. We demonstrate that TrimTail is computationally cheap\nand can be applied online and optimized with any training loss or any model\narchitecture on any dataset without any extra effort by applying it on various\nend-to-end streaming ASR networks either trained with CTC loss [1] or\nTransducer loss [2]. We achieve 100 $\\sim$ 200ms latency reduction with equal\nor even better accuracy on both Aishell-1 and Librispeech. Moreover, by using\nTrimTail, we can achieve a 400ms algorithmic improvement of User Sensitive\nDelay (USD) with an accuracy loss of less than 0.2.'}, {'Fast-U2++: Fast and Accurate End-to-End Speech Recognition in Joint\n  CTC/Attention Frames': 'Recently, the unified streaming and non-streaming two-pass (U2/U2++)\nend-to-end model for speech recognition has shown great performance in terms of\nstreaming capability, accuracy and latency. In this paper, we present\nfast-U2++, an enhanced version of U2++ to further reduce partial latency. The\ncore idea of fast-U2++ is to output partial results of the bottom layers in its\nencoder with a small chunk, while using a large chunk in the top layers of its\nencoder to compensate the performance degradation caused by the small chunk.\nMoreover, we use knowledge distillation method to reduce the token emission\nlatency. We present extensive experiments on Aishell-1 dataset. Experiments and\nablation studies show that compared to U2++, fast-U2++ reduces model latency\nfrom 320ms to 80ms, and achieves a character error rate (CER) of 5.06% with a\nstreaming setup.'}, {'WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit': 'Recently, we made available WeNet, a production-oriented end-to-end speech\nrecognition toolkit, which introduces a unified two-pass (U2) framework and a\nbuilt-in runtime to address the streaming and non-streaming decoding modes in a\nsingle model. To further improve ASR performance and facilitate various\nproduction requirements, in this paper, we present WeNet 2.0 with four\nimportant updates. (1) We propose U2++, a unified two-pass framework with\nbidirectional attention decoders, which includes the future contextual\ninformation by a right-to-left attention decoder to improve the representative\nability of the shared encoder and the performance during the rescoring stage.\n(2) We introduce an n-gram based language model and a WFST-based decoder into\nWeNet 2.0, promoting the use of rich text data in production scenarios. (3) We\ndesign a unified contextual biasing framework, which leverages user-specific\ncontext (e.g., contact lists) to provide rapid adaptation ability for\nproduction and improves ASR accuracy in both with-LM and without-LM scenarios.\n(4) We design a unified IO to support large-scale data for effective model\ntraining. In summary, the brand-new WeNet 2.0 achieves up to 10\\% relative\nrecognition performance improvement over the original WeNet on various corpora\nand makes available several important production-oriented features.'}, {'CB-Conformer: Contextual biasing Conformer for biased word recognition': 'Due to the mismatch between the source and target domains, how to better\nutilize the biased word information to improve the performance of the automatic\nspeech recognition model in the target domain becomes a hot research topic.\nPrevious approaches either decode with a fixed external language model or\nintroduce a sizeable biasing module, which leads to poor adaptability and slow\ninference. In this work, we propose CB-Conformer to improve biased word\nrecognition by introducing the Contextual Biasing Module and the Self-Adaptive\nLanguage Model to vanilla Conformer. The Contextual Biasing Module combines\naudio fragments and contextual information, with only 0.2% model parameters of\nthe original Conformer. The Self-Adaptive Language Model modifies the internal\nweights of biased words based on their recall and precision, resulting in a\ngreater focus on biased words and more successful integration with the\nautomatic speech recognition model than the standard fixed language model. In\naddition, we construct and release an open-source Mandarin biased-word dataset\nbased on WenetSpeech. Experiments indicate that our proposed method brings a\n15.34% character error rate reduction, a 14.13% biased word recall increase,\nand a 6.80% biased word F1-score increase compared with the base Conformer.'}]","Title: ""MoE-based Streaming and Non-Streaming Universal Speech-to-Text Models with Encrypted Parameter Sharding for Efficient Decentralized Inference""

**Background**: The continuous expansion of speech data has necessitated the development of more efficient and scalable speech recognition models. This research focuses on a novel method that combines enhanced model efficiency with decentralized inference capabilities, optimizing for both streaming and non-streaming environments.

**Objective**: The primary goal is to design and evaluate MoE-based deep learning architectures that can support a joint CTC/AED framework with bidirectional decoders and Mixture-of-Experts to achieve superior speech recognition performance.

**Innovations**:
- Utilizes a MoE layer in both encoder and decoder sections, surpassing previous work that solely utilized MoE layers in the encoder. 
- Demonstrates the first MoE capability in streaming, enabling real-time processing without detrimental impact on overall performance.
- Adopts parameter sharding for decentralized inference, promoting efficient scaling of the model across distributed computing environments.

**Methods**:
- Builds models with various scales (Dense-225M, Dense-1B, MoE-1B) incorporating an 8-head attention mechanism for character-based representations (Mandarin) and byte-pair encoding (English).
- Employs U2++ architecture, optimized for subsampling and joint decoding, ensuring smooth transitions between streaming and non-streaming operations.

**Results**:
- The MoE-based models (Dense-1B and MoE-1B) show significantly lower word error rate (WER) compared to Dense-225M, particularly when training is comparable, emphasizing the higher efficiency of MoE architectures.
-实在的参数缩放能力, MoE-1B在具有相同的参数数量时与Dense-1B具有相当的WER,而推理成本则接近Dense-225M的水平,表明了相同的准确性但更低的推理成本。
- Streaming models (U2++-Dense-1B and U2++-MoE-1B) demonstrate a 2.5x speedup over U2++-Dense-225M, with an approximate 0.03x (cpu) or 0.0004x (gpu) difference in inference cost between models with different parameter counts.

**Contributions**:
- Introduces encrypted parameter sharding for more efficient decentralized inference, enhancing model scalability and privacy.
- Employs MoE in both encoder and decoder, showcasing superior performance, especially evident in smaller models implementing the MoE layer, while maintaining high accuracy.

**Applications**:
- The research advances the field of advanced speech-to-text translation and recognition, enabling more diverse applications across a range of sectors, particularly in cloud services, IoT devices, and decentralized computing environments, where real-time processing and privacy protection are critical.

This work significantly advances speech recognition technology through the integration of MoE models and parameter sharding, providing efficient processing and decentralized inference capabilities that could revolutionize speech recognition across various applications, emphasizing the importance of parameter optimization, scalability, and real-time computation."
"Let $G=(V, E)$ be a graph and let each vertex of $G$ has a lamp and a button.
Each button can be of $\sigma^+$-type or $\sigma$-type.
  Assume that initially some lamps are on and others are off. The button on
vertex $x$ is of $\sigma^+$-type ($\sigma$-type, respectively) if pressing the
button changes the lamp states on $x$ and on its neighbors in $G$ (the lamp
states on the neighbors of $x$ only, respectively). Assume that there is a set
$X\subseteq V$ such that pressing buttons on vertices of $X$ lights all lamps
on vertices of $G$. In particular, it is known to hold when initially all lamps
are off and all buttons are of $\sigma^+$-type.
  Finding such a set $X$ of the smallest size is NP-hard even if initially all
lamps are off and all buttons are of $\sigma^+$-type. Using a linear algebraic
approach we design a polynomial-time approximation algorithm for the problem
such that for the set $X$ constructed by the algorithm, we have $|X|\le
\min\{r,(|V|+{\rm opt})/2\},$ where $r$ is the rank of a (modified) adjacent
matrix of $G$ and ${\rm opt}$ is the size of an optimal solution to the
problem.
  To the best of our knowledge, this is the first polynomial-time approximation
algorithm for the problem with a nontrivial approximation guarantee.","[{'An analytic proof of the Borwein Conjecture': 'We provide a proof of the Borwein Conjecture using analytic methods.'}, {'On some conjectural congruences': 'In this paper, we confirm some congruences conjectured by V.J.W. Guo and M.J.\nSchlosser recently. For example, we show that for primes $p>3$, $$\n\\sum_{k=0}^{p-1}(2pk-2k-1)\\frac{\\left(\\frac{-1}{p-1}\\right)_k^{2p-2}}{(k!)^{2p-2}}\\equiv0\\pmod{p^5}.\n$$'}, {'Supercongruences arising from a ${}_7F_6$ hypergeometric transformation\n  formula': 'Using a ${}_7F_6$ hypergeometric transformation formula, we prove two\nsupercongruences. In particular, one of these supercongruences confirms a\nrecent conjecture of Guo, Liu and Schlosser, and gives an extension of a\nsupercongruence of Long and Ramakrishna.'}, {'p-Divisibility of the number of linear representations of an Abelian\n  p-group': 'We establish lower bounds for the $p$-divisibility of the quantity\n$\\#\\operatorname{Hom}(G,GL_n(\\mathbb{F}_q))$, the number of homomorphisms from\n$G$ to a general linear group, where $G$ is an Abelian $p$-group. This is in\nanalogy to the result of Krattenthaler and M\\""{u}ller \\cite{MR3383810} on\nhomomorphisms to symmetric groups.'}, {'Proof of a congruence concerning truncated hypergeometric series\n  ${}_6F_5$': 'In this paper, we mainly prove the following congruence conjectured by J.-C.\nLiu: $$\n{}_6F_5\\bigg[\\begin{matrix}\\frac{5}{4}&\\frac{1}{2}&\\frac{1}{2}&\\frac{1}{2}&\\frac{1}{2}&\\frac{1}{2}\\\\&\\frac{1}{4}&1&1&1&1\\end{matrix}\\bigg|\\\n-1\\bigg]_{\\frac{p-1}{2}}\\equiv-\\frac{p^3}{16}\\Gamma_p\\left(\\frac{1}{4}\\right)^4\\pmod{p^5},\n$$ where $p\\geq5$ are primes with $p\\equiv3\\pmod{4}$.'}, {'Two congruences concerning Apéry numbers': ""Let $n$ be a nonnegative integer. The $n$-th Ap\\'{e}ry number is defined by\n$$ A_n:=\\sum_{k=0}^n\\binom{n+k}{k}^2\\binom{n}{k}^2. $$ Z.-W. Sun ever\ninvestigated the congruence properties of Ap\\'{e}ry numbers and posed some\nconjectures. For example, Sun conjectured that for any prime $p\\geq7$ $$\n\\sum_{k=0}^{p-1}(2k+1)A_k\\equiv p-\\frac{7}{2}p^2H_{p-1}\\pmod{p^6} $$ and for\nany prime $p\\geq5$ $$ \\sum_{k=0}^{p-1}(2k+1)^3A_k\\equiv\np^3+4p^4H_{p-1}+\\frac{6}{5}p^8B_{p-5}\\pmod{p^9}, $$ where $H_n=\\sum_{k=1}^n1/k$\ndenotes the $n$-th harmonic number and $B_0,B_1,\\ldots$ are the well-known\nBernoulli numbers. In this paper we shall confirm these two conjectures.""}, {'Symbolic summation methods and hypergeometric supercongruences': 'In this paper, we establish the following two congruences: \\begin{gather*}\n\\sum_{k=0}^{(p+1)/2}(3k-1)\\frac{\\left(-\\frac{1}{2}\\right)_k^2\\left(\\frac{1}{2}\\right)_k4^k}{k!^3}\\equiv\np-6p^3\\left(\\frac{-1}{p}\\right)+2p^3\\left(\\frac{-1}{p}\\right)E_{p-3}\\pmod{p^4},\\\\\n\\sum_{k=0}^{p-1}(3k-1)\\frac{\\left(-\\frac{1}{2}\\right)_k^2\\left(\\frac{1}{2}\\right)_k4^k}{k!^3}\\equiv\np-2p^3\\pmod{p^4}, \\end{gather*} where $p>3$ is a prime, $E_{p-3}$ is the\n$(p-3)$-th Euler number and $\\left(-\\right)$ is the Legendre symbol. The first\ncongruence modulo $p^3$ was conjectured by Guo and Schlosser recently.'}, {'On two conjectural supercongruences of Z.-W. Sun': 'In this paper, we mainly prove two conjectural supercongruences of Sun by\nusing the following identity $$\n\\sum_{k=0}^n\\binom{2k}{k}^2\\binom{2n-2k}{n-k}^2=16^n\\sum_{k=0}^n\\frac{\\binom{n+k}{k}\\binom{n}{k}\\binom{2k}{k}^2}{(-16)^k}\n$$ which arises from a ${}_4F_3$ hypergeometric transformation. For any prime\n$p>3$, we prove that \\begin{gather*}\n\\sum_{n=0}^{p-1}\\frac{n+1}{8^n}\\sum_{k=0}^n\\binom{2k}{k}^2\\binom{2n-2k}{n-k}^2\\equiv(-1)^{(p-1)/2}p+5p^3E_{p-3}\\pmod{p^4},\\\\\n\\sum_{n=0}^{p-1}\\frac{2n+1}{(-16)^n}\\sum_{k=0}^n\\binom{2k}{k}^2\\binom{2n-2k}{n-k}^2\\equiv(-1)^{(p-1)/2}p+3p^3E_{p-3}\\pmod{p^4},\n\\end{gather*} where $E_{p-3}$ is the $(p-3)$th Euler number.'}, {'An overview of SPSA: recent development and applications': 'There is an increasing need in solving high-dimensional optimization problems\nunder non-deterministic environment. The simultaneous perturbation stochastic\napproximation (SPSA) algorithm has recently attracted considerable attention\nfor solving high-dimensional optimization problems where the analytical formula\ncannot be attained. SPSA is designed to estimate the gradient by applying\nperturbation on a random subset of dimensions at each iteration. SPSA can be\neasily implemented and is highly efficient in that that it relies on\nmeasurements of the objective function, not on measurements of the gradient of\nthe objective function. Since its invention, SPSA has been implemented in\nvarious fields such as reinforcement learning, production optimization etc. The\npaper briefly discuss the recent development of SPSA and its applications.'}, {'Hierarchical Bayesian Modeling for Time-Dependent Inverse Uncertainty\n  Quantification': ""This paper introduces a novel hierarchical Bayesian model specifically\ndesigned to address challenges in Inverse Uncertainty Quantification (IUQ) for\ntime-dependent problems in nuclear Thermal Hydraulics (TH) systems. The unique\ncharacteristics of time-dependent data, such as high dimensionality and\ncorrelation in model outputs requires special attention in the IUQ process. By\nintegrating Gaussian Processes (GP) with Principal Component Analysis (PCA), we\nefficiently construct surrogate models that effectively handle the complexity\nof dynamic TH systems. Additionally, we incorporate Neural Network (NN) models\nfor time series regression, enhancing the computational accuracy and\nfacilitating derivative calculations for efficient posterior sampling using the\nHamiltonian Monte Carlo Method - No U-Turn Sampler (NUTS).\n  We demonstrate the effectiveness of this hierarchical Bayesian approach using\nthe transient experiments in the PSBT benchmark. Our results show improved\nestimates of Physical Model Parameters' posterior distributions and a reduced\ntendency for over-fitting, compared to conventional single-level Bayesian\nmodels. This approach offers a promising framework for extending IUQ to more\ncomplex, time-dependent problems.""}]","Title: Constrained Generalized All-Ones Problem: Improved Approximation Algorithm and Characterization

Abstract:

This research provides a concise account of advancements made in the field of graph theory, specifically focusing on the Generalized All-Ones Problem (GAOP) as modified by introducing constraints and a secondary state vector. Specifically, it targets an n×n grid characterized by lamps and switches, with the initial premise being that lamps can be in illuminated or un-illuminated states, and switches can be σ+-types which alter those states. The main objective is to determine the appropriate switch settings to illuminate all lamps, under constraint of y values. Key innovations include an efficient approximation algorithm with polynomial time complexity, a characterization through linear matrix equations resulting in the identification of fundamental solution sets η and γ. This leads to a refined version of the GAOP, addressing the importance of minimizing P U, subject to X containing 2m values. Insights into the problem's inherent complexity in NP-completeness are also discussed. Through theoretical justification and empirical validation, the study achieves precise characterization of solution scenarios, provides an approximation guarantee ≤(n + opt)/2, derives a polynomial complexity algorithm, and supports these findings through exhaustive case study analysis. The ultimate contribution lies in offering a methodological advancement and practical solutions for optimizing the GAOP, with potential applications in network management, urban planning, and algorithmic decision-making."
"Large language models (LLMs) have showcased profound capabilities in language
understanding and generation, facilitating a wide array of applications.
However, there is a notable paucity of detailed, open-sourced methodologies on
efficiently scaling LLMs beyond 50 billion parameters with minimum
trial-and-error cost and computational resources. In this report, we introduce
Tele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that
features a stable, efficient pre-training paradigm and enhanced factual
judgment capabilities. Tele-FLM demonstrates superior multilingual language
modeling abilities, measured by BPB on textual corpus. Besides, in both English
and Chinese foundation model evaluation, it is comparable to strong
open-sourced models that involve larger pre-training FLOPs, such as Llama2-70B
and DeepSeek-67B. In addition to the model weights, we share the core designs,
engineering practices, and training details, which we expect to benefit both
the academic and industrial communities.","[{'Uniform synchronous criticality of diversely random complex networks': 'We investigate collective synchronous behaviors in random complex networks of\nlimit-cycle oscillators with the non-identical asymmetric coupling scheme, and\nfind a uniform coupling criticality of collective synchronization which is\nindependent of complexity of network topologies. Numerically simulations on\ncategories of random complex networks have verified this conclusion.'}, {'Comment on ""Comparison of six simulation codes for positive streamers in\n  air""(Plasma Sources Sci. Technol. 27 (2018) 095002)': 'Recently, a comparison of six codes for streamer discharge simulations were\nperformed in [1]. In this comment, we discuss about the big differences between\nthe results obtained by the different codes using the same deterministic model,\nand raise questions on the convergence of the codes and the minimum spatial\nresolution that are required for a converged results.'}, {'Sync in Complex Dynamical Networks: Stability, Evolution, Control, and\n  Application': 'In the past few years, the discoveries of small-world and scale-free\nproperties of many natural and artificial complex networks have stimulated\nsignificant advances in better understanding the relationship between the\ntopology and the collective dynamics of complex networks. This paper reports\nrecent progresses in the literature of synchronization of complex dynamical\nnetworks including stability criteria, network synchronizability and uniform\nsynchronous criticality in different topologies, and the connection between\ncontrol and synchronization of complex networks as well. The economic-cycle\nsynchronous phenomenon in the World Trade Web, a scale-free type of social\neconomic networks, is used to illustrate an application of the network\nsynchronization mechanism.'}, {'The stable property of Newton slopes for general Witt towers': 'Any polynomial $f(x)\\in\\mathbb{Z}_q[x]$ defines a Witt vector $[f]\\in\nW(\\mathbb{F}_q[x])$. Consider the Artin-Schreier-Witt tower $y^F-y=[f]$. This\nis a tower of curves over $\\mathbb{F}_q$, with total Galois group\n$\\mathbb{Z}_p$. We want to study the Newton slopes of zeta functions of this\ntower. We reduce it to the Newton polygons of L-functions associated with\ncharacters on the Galois groups. We prove that, when the conductors are large\nenough, these Newton slopes are unions of arithmetic progressions which are\nchanging proportionally as the conductor increases. This is a generalization of\nthe result of \\cite{Da}, where they get the same result in the case the\nnon-zero coefficients of $f(x)$ are roots of unity. To overcome the new\ndifficulty in our process, we apply some $(p^{\\theta},T)$-topology.'}, {'FASTCloud: A novel framework of assessment and selection for trustworthy\n  cloud service': 'By virtue of technology and benefit advantages, cloud computing has\nincreasingly attracted a large number of potential cloud consumers (PCC) plan\nto migrate the traditional business to the cloud service. However, trust has\nbecome one of the most challenging issues that prevent the PCC from adopting\ncloud services, especially in trustworthy cloud service selection. Besides, due\nto the diversity and dynamic of quality of service (QoS) in the cloud\nenvironment, the existing trust assessment methods based on the single constant\nvalue of QoS attribute and the subjective weight assignment are not good enough\nto provide an effective solution for PCCs to identify and select a trustworthy\ncloud service among a wide range of functionally-equivalent cloud service\nproviders (CSPs). To address the challenge, a novel assessment and selection\nframework for trustworthy cloud service, FASTCloud, is proposed in this study.\nThis framework facilitates PCCs to select a trustworthy cloud service based on\ntheir actual QoS requirements. In order to accurately and efficiently assess\nthe trust level of cloud services, a QoS-based trust assessment model is\nproposed. This model represents a trust level assessment method based on the\ninterval multiple attributes with an objective weight assignment method based\non the deviation maximization to adaptively determine the trust level of\ndifferent cloud services provisioned by candidate CSPs. The advantage of the\nproposed trust level assessment method in time complexity is demonstrated by\nthe performance analysis and comparison. The experimental result of a case\nstudy with an open-source dataset shows that the trust model is efficient in\ncloud service trust assessment and the FASTCloud can effectively help PCCs\nselect a trustworthy cloud service.'}, {'Existence and asymptotic behavior of entire large solutions for Hessian\n  equations': 'In this paper, we give some existence and nonexistence results for nonradial\nentire large solutions of the Hessian equation $S_k\\left(D^2 u\\right)=b(x)\nu^\\gamma$ in the sublinear case $0<\\gamma<k$. The exact asymptotic behavior of\nlarge solutions at infinity is also studied when $b(x)$ is the oscillation of a\nradial function $|x|^{-l}$ at infinity for $l\\leq k-1$.'}, {'Controlling the spreading in small-world networks': 'The spreading (propagation) of diseases, viruses, and disasters such as power\nblackout through a huge-scale and complex network is one of the most concerned\nissues today. In this paper, we study the control of such spreading in a\nnonlinear spreading model of small-world networks. We found that the short-cut\nadding probability $p$ in the N-W model \\cite{N-W:1999} of small-world networks\ndetermines the Hopf bifurcation and other bifurcating behaviors in the proposed\nmodel. We further show a control technique that stabilize a periodic spreading\nbehavior onto a stable equilibrium over the proposed model of small-world\nnetworks.'}, {'Disentangled and Robust Representation Learning for Bragging\n  Classification in Social Media': 'Researching bragging behavior on social media arouses interest of\ncomputational (socio) linguists. However, existing bragging classification\ndatasets suffer from a serious data imbalance issue. Because labeling a\ndata-balance dataset is expensive, most methods introduce external knowledge to\nimprove model learning. Nevertheless, such methods inevitably introduce noise\nand non-relevance information from external knowledge. To overcome the\ndrawback, we propose a novel bragging classification method with\ndisentangle-based representation augmentation and domain-aware adversarial\nstrategy. Specifically, model learns to disentangle and reconstruct\nrepresentation and generate augmented features via disentangle-based\nrepresentation augmentation. Moreover, domain-aware adversarial strategy aims\nto constrain domain of augmented features to improve their robustness.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance compared to other methods.'}, {'Simulation of adaptive feedforward control for magnetic alloy cavity': 'The upgrade plan of the China Spallation Neutron Source aims to enhance the\nbeam power from 100 kW to 500 kW. To achieve this, the plan involves\nincorporating three new magnetic alloy cavities while maintaining the existing\nsystem to enable double harmonic acceleration. As a consequence of the\nincreased current intensity, the beam loading effect will be significantly\namplified, presenting a considerable challenge for the low-level RF control\nsystem of the magnetic alloy cavity. To address this challenge, an adaptive\nfeedforward algorithm has been developed to enable optimal control. In\naddition, comprehensive simulations of the algorithm have been successfully\nconducted to validate its.'}, {'Emission Patterns and Light Curves of Gamma-Rays in the Pulsar\n  Magnetosphere with a Current-Induced Magnetic Field': 'We study the emission patterns and light curves of gamma-rays in the pulsar\nmagnetosphere with a current-induced magnetic field perturbation. Based on the\nsolution of a static dipole with the magnetic field induced by some currents\n(perturbation field), we derive the solutions of a static as well as a retarded\ndipole with the perturbation field in the Cartesian coordinates. The static\n(retarded) magnetic field can be expressed as the sum of pure static (retarded)\ndipolar magnetic field and the static (retarded) perturbation field. We use the\nsolution of the retarded magnetic field to investigate the influence of the\nperturbation field on the emission patterns and light curves, and we apply the\nperturbed solutions to calculate the gamma-ray light curves for the case of the\nVela pulsar. We find out that the perturbation field induced by the currents\nwill change the emission patterns and then light curves of gamma-rays,\nespecially for a larger perturbation field. Our results indicate that the\nperturbation field created by the outward-flowing (inward-flowing) electrons\n(positrons) can decrease the rotation effect on the magnetosphere and makes\nemission pattern appear to be more smooth relative to that of the pure retarded\ndipole, but the perturbation field created by the outward-flowing\n(inward-flowing) positrons (electrons) can make the emission pattern less\nsmooth.'}]","Title: Tele-FLM: Advancing Multilingual Foundation Models with Enhanced Chinese Training Data and Architecture Innovation

#### Background
The advent of multilingual large language models has revolutionized the field of natural language processing (NLP), laying the groundwork for more effective cross-lingual understanding and translation. Tele-FLM, a specialized multilingual foundation model, significantly advances the state of the art in NLP by focusing on rich Chinese training datasets, enhancing architectural design, and incorporating innovative training methodologies.

#### Objective
The primary objective of this study is to investigate the impact of extensive Chinese training data on a multilingual foundation model's performance across diverse linguistic tasks, with a focus on how architectural modifications can optimize its language understanding capabilities particularly for Chinese.

#### Innovations
The innovation of Tele-FLM resides in its improved training data collection process, which prioritizes classic Chinese texts for enhanced Chinese modeling capabilities. Inspired by FLM-101B, our model architecture adaptation specifically tailors the GPT-style transformer unit towards more efficient Chinese token compression, thereby reducing the parameter size without compromising performance. This approach leads to a more focused language understanding for less common languages like Chinese.

#### Methods
Our model employs a modified Architecture from FLM-101B with 64 layers, 8,192 hidden units per attention head, enabling fine-tuning for improved accuracy. The modifications focus on optimizing the Chinese tokenizer for better compression and performance. To train the model, a balanced weight scheme is adopted for a diverse set of Chinese texts, ensuring the capacity for nuanced linguistic expressions while maintaining the model's computational efficiency.

#### Results
Experimentally, Tele-FLM showcases superior performance compared to its predecessor Llama and benchmark models across Chinese benchmarks, such as C-Eval, CMMLU, and CHID, by achieving an overall accuracy improvement of 8.6% on average, particularly in reasoning-oriented tasks like GSM8K and BBH.

#### Contributions
This study significantly pushes the envelope on Chinese multilingual NLP by demonstrating the efficacy of enhanced training data and architecture optimization for tasks requiring high Chinese proficiency. The advancements contribute to a more versatile foundation model that can be effectively employed across various applications, from language translation to content generation and reasoning.

#### Applications
Tele-FLM holds considerable potential in enhancing the accuracy and linguistic capabilities of AI systems in multilingual environments, offering improved performance in cross-lingual communication and enabling the development of more sophisticated AI-driven services that can effectively serve users in Chinese-speaking regions.

By integrating extensive Chinese text data and architectural enhancements, Tele-FLM provides a robust foundation for advancing multilingual NLP applications, thereby fostering a more culturally inclusive technological landscape."
"Developing autonomous agents for mobile devices can significantly enhance
user interactions by offering increased efficiency and accessibility. However,
despite the growing interest in mobile device control agents, the absence of a
commonly adopted benchmark makes it challenging to quantify scientific progress
in this area. In this work, we introduce B-MoCA: a novel benchmark designed
specifically for evaluating mobile device control agents. To create a realistic
benchmark, we develop B-MoCA based on the Android operating system and define
60 common daily tasks. Importantly, we incorporate a randomization feature that
changes various aspects of mobile devices, including user interface layouts and
language settings, to assess generalization performance. We benchmark diverse
agents, including agents employing large language models (LLMs) or multi-modal
LLMs as well as agents trained from scratch using human expert demonstrations.
While these agents demonstrate proficiency in executing straightforward tasks,
their poor performance on complex tasks highlights significant opportunities
for future research to enhance their effectiveness. Our source code is publicly
available at https://b-moca.github.io.","[{'Semi-supervised Image Classification with Grad-CAM Consistency': 'Consistency training, which exploits both supervised and unsupervised\nlearning with different augmentations on image, is an effective method of\nutilizing unlabeled data in semi-supervised learning (SSL) manner. Here, we\npresent another version of the method with Grad-CAM consistency loss, so it can\nbe utilized in training model with better generalization and adjustability. We\nshow that our method improved the baseline ResNet model with at most 1.44 % and\n0.31 $\\pm$ 0.59 %p accuracy improvement on average with CIFAR-10 dataset. We\nconducted ablation study comparing to using only psuedo-label for consistency\ntraining. Also, we argue that our method can adjust in different environments\nwhen targeted to different units in the model. The code is available:\nhttps://github.com/gimme1dollar/gradcam-consistency-semi-sup.'}, {'National-scale electricity peak load forecasting: Traditional, machine\n  learning, or hybrid model?': ""As the volatility of electricity demand increases owing to climate change and\nelectrification, the importance of accurate peak load forecasting is\nincreasing. Traditional peak load forecasting has been conducted through time\nseries-based models; however, recently, new models based on machine or deep\nlearning are being introduced. This study performs a comparative analysis to\ndetermine the most accurate peak load-forecasting model for Korea, by comparing\nthe performance of time series, machine learning, and hybrid models. Seasonal\nautoregressive integrated moving average with exogenous variables (SARIMAX) is\nused for the time series model. Artificial neural network (ANN), support vector\nregression (SVR), and long short-term memory (LSTM) are used for the machine\nlearning models. SARIMAX-ANN, SARIMAX-SVR, and SARIMAX-LSTM are used for the\nhybrid models. The results indicate that the hybrid models exhibit significant\nimprovement over the SARIMAX model. The LSTM-based models outperformed the\nothers; the single and hybrid LSTM models did not exhibit a significant\nperformance difference. In the case of Korea's highest peak load in 2019, the\npredictive power of the LSTM model proved to be greater than that of the\nSARIMAX-LSTM model. The LSTM, SARIMAX-SVR, and SARIMAX-LSTM models outperformed\nthe current time series-based forecasting model used in Korea. Thus, Korea's\npeak load-forecasting performance can be improved by including machine learning\nor hybrid models.""}, {'Mod-CSA: Modularity optimization by conformational space annealing': 'We propose a new modularity optimization method, Mod-CSA, based on stochastic\nglobal optimization algorithm, conformational space annealing (CSA). Our method\noutperforms simulated annealing in terms of both efficiency and accuracy,\nfinding higher modularity partitions with less computational resources\nrequired. The high modularity values found by our method are higher than, or\nequal to, the largest values previously reported. In addition, the method can\nbe combined with other heuristic methods, and implemented in parallel fashion,\nallowing it to be applicable to large graphs with more than 10000 nodes.'}, {'MOCSA: multiobjective optimization by conformational space annealing': 'We introduce a novel multiobjective optimization algorithm based on the\nconformational space annealing (CSA) algorithm, MOCSA. It has three\ncharacteristic features: (a) Dominance relationship and distance between\nsolutions in the objective space are used as the fitness measure, (b) update\nrules are based on the fitness as well as the distance between solutions in the\ndecision space and (c) it uses a constrained local minimizer. We have tested\nMOCSA on 12 test problems, consisting of ZDT and DTLZ test suites. Benchmark\nresults show that solutions obtained by MOCSA are closer to the Pareto front\nand covers a wider range of the objective space than those by the elitist\nnon-dominated sorting genetic system (NSGA2).'}, {'Extraction of hidden information by efficient community detection in\n  networks': 'Currently, we are overwhelmed by a deluge of experimental data, and network\nphysics has the potential to become an invaluable method to increase our\nunderstanding of large interacting datasets. However, this potential is often\nunrealized for two reasons: uncovering the hidden community structure of a\nnetwork, known as community detection, is difficult, and further, even if one\nhas an idea of this community structure, it is not a priori obvious how to\nefficiently use this information. Here, to address both of these issues, we,\nfirst, identify optimal community structure of given networks in terms of\nmodularity by utilizing a recently introduced community detection method.\nSecond, we develop an approach to use this community information to extract\nhidden information from a network. When applied to a protein-protein\ninteraction network, the proposed method outperforms current state-of-the-art\nmethods that use only the local information of a network. The method is\ngenerally applicable to networks from many areas.'}, {'Efficient discovery of multiple minimum action pathways using Gaussian\n  process': 'We present a new efficient transition pathway search method based on the\nleast action principle and the Gaussian process regression method. Most pathway\nsearch methods developed so far rely on string representations, which\napproximate a transition pathway by a series of slowly varying system replicas.\nSuch string methods are computationally expensive in general because they\nrequire many replicas to obtain smooth pathways. Here, we present an approach\nemploying the Gaussian process regression method, which infers the shape of a\npotential energy surface with a few observed data and Gaussian-shaped kernel\nfunctions. We demonstrate a drastic elevation of computing efficiency of the\nmethod about five orders of magnitude than existing methods. Further, to\ndemonstrate its real-world capabilities, we apply our method to find multiple\nconformational transition pathways of alanine dipeptide using a quantum\nmechanical potential. Owing to the improved efficiency of our method, Gaussian\nprocess action optimiza tion (GPAO), we obtain the multiple transition pathways\nof alaninedipeptide and calculate their transition probabilities successfully\nwith ab initio accuracy. In addition, GPAO successfully finds the isomerization\npathways of small molecules and the rearrangement of atoms on a metallic\nsurface.'}, {'A Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical\n  Representation Learning': ""We present a rotated hyperbolic wrapped normal distribution (RoWN), a simple\nyet effective alteration of a hyperbolic wrapped normal distribution (HWN). The\nHWN expands the domain of probabilistic modeling from Euclidean to hyperbolic\nspace, where a tree can be embedded with arbitrary low distortion in theory. In\nthis work, we analyze the geometric properties of the diagonal HWN, a standard\nchoice of distribution in probabilistic modeling. The analysis shows that the\ndistribution is inappropriate to represent the data points at the same\nhierarchy level through their angular distance with the same norm in the\nPoincar\\'e disk model. We then empirically verify the presence of limitations\nof HWN, and show how RoWN, the proposed distribution, can alleviate the\nlimitations on various hierarchical datasets, including noisy synthetic binary\ntree, WordNet, and Atari 2600 Breakout. The code is available at\nhttps://github.com/ml-postech/RoWN.""}, {'Style-Agnostic Reinforcement Learning': 'We present a novel method of learning style-agnostic representation using\nboth style transfer and adversarial learning in the reinforcement learning\nframework. The style, here, refers to task-irrelevant details such as the color\nof the background in the images, where generalizing the learned policy across\nenvironments with different styles is still a challenge. Focusing on learning\nstyle-agnostic representations, our method trains the actor with diverse image\nstyles generated from an inherent adversarial style perturbation generator,\nwhich plays a min-max game between the actor and the generator, without\ndemanding expert knowledge for data augmentation or additional class labels for\nadversarial training. We verify that our method achieves competitive or better\nperformances than the state-of-the-art approaches on Procgen and Distracting\nControl Suite benchmarks, and further investigate the features extracted from\nour model, showing that the model better captures the invariants and is less\ndistracted by the shifted style. The code is available at\nhttps://github.com/POSTECH-CVLab/style-agnostic-RL.'}, {'Hyperbolic VAE via Latent Gaussian Distributions': 'We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent\nspace consists of a set of Gaussian distributions. It is known that the set of\nthe univariate Gaussian distributions with the Fisher information metric form a\nhyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed\nwith the Gaussian manifolds, we propose a pseudo-Gaussian manifold normal\ndistribution based on the Kullback-Leibler divergence, a local approximation of\nthe squared Fisher-Rao distance, to define a density over the latent space. In\nexperiments, we demonstrate the efficacy of GM-VAE on two different tasks:\ndensity estimation of image datasets and environment modeling in model-based\nreinforcement learning. GM-VAE outperforms the other variants of hyperbolic-\nand Euclidean-VAEs on density estimation tasks and shows competitive\nperformance in model-based reinforcement learning. We observe that our model\nprovides strong numerical stability, addressing a common limitation reported in\nprevious hyperbolic-VAEs.'}, {'Finding dominant transition pathways via global optimization of action': 'We present a new computational approach, Action-CSA, to sample multiple\nreaction pathways with fixed initial and final states through global\noptimization of the Onsager-Machlup action using the conformational space\nannealing method. This approach successfully samples not only the most dominant\npathway but also many other possible paths without initial guesses on reaction\npathways. Pathway space is efficiently sampled by crossover operations of a set\nof paths and preserving the diversity of sampled pathways. The sampling ability\nof the approach is assessed by finding pathways for the conformational changes\nof alanine dipeptide and hexane. The benchmarks demonstrate that the rank order\nand the transition time distribution of multiple pathways identified by the new\napproach are in good agreement with those of long molecular dynamics\nsimulations. We also show that the lowest action folding pathway of the\nmini-protein FSD-1 identified by the new approach is consistent with previous\nmolecular dynamics simulations and experiments.'}]","Abstract:

As mobile devices have become central to daily life, automated interaction has become essential for optimal usage. However, designing generative agents that can control the functions of these devices proved challenging due to a lack of standardization in benchmarks and the complexity of mobile interfaces. This research introduces B-MoCA (Benchmark for Mobile Control Agents), a comprehensive framework to evaluate generative models for device control. B-MoCA aims to enable the development of intelligent, versatile agents capable of handling the intricacies and variability of mobile device interfaces effectively.

The primary objective of this work is to create a standardized benchmark capable of measuring and comparing the performance of different agents across a variety of tasks and device configurations. B-MoCA introduces challenges with randomized features such as icon location, font size, wallpaper, language, and device type to test agents' generalization abilities. The framework supports three types of agents: Large Language Model (LLM) based, Multimodal Large Language Model (MLLM) based, and Vision-Language-UI (VLUI) based, each tailored for different levels of complexity and customization.

The innovations of B-MoCA are distinctive in the benchmarking domain: a practical task library comprising basic yet versatile applications, a randomization feature to evaluate generalization across various mobile setups, and a methodological approach to assess the performance of different agents via sequential decision-making. The benchmark is equipped with a success detector to automatically evaluate agent performance, simplifying the evaluation process and providing clear metrics.

The methodological approach involves training models on a diverse dataset of human demonstrations, fine-tuning them with sequential decision-making reinforcement learning, and evaluating their performance on the benchmark tasks. The results demonstrate that LLM agents and MLLM agents, with their pre-trained base knowledge and few-shot learning capabilities, achieved the best success rates on simple tasks, such as turning on airplane mode or decreasing brightness. However, both struggled with some complex tasks, like creating an alarm or navigating language settings. VLUI agents, in contrast, showed adaptability and improved performance on more intricate tasks, utilizing dual-gesture actions for precise visual manipulation.

The primary contributions include B-MoCA as a benchmarking solution for mobile device control, innovations in task and agent design, and the empirical validation of different agent types through systematic evaluation. These findings provide a foundational step towards developing more versatile and reliable intelligent agents that can operate seamlessly across various mobile platforms, enhancing user experiences and automation capabilities.

The applications of this research extend to the development of personal virtual assistants, mobile application interfaces, and autonomous mobile device management systems. It has the potential to improve tasks such as setting alarms, adjusting screen settings, managing applications, and responding to critical calls, ultimately leading to more efficient, user-friendly mobile technology."
"Protostars are born in magnetized environments. As a consequence, the
formation of protostellar disks can be suppressed by the magnetic field
efficiently removing angular momentum of the infalling material. Non-ideal MHD
effects are proposed to as one way to allow protostellar disks to form. Thus,
it is important to understand their contributions in observations of
protostellar systems. We derive an analytical equation to estimate the
ambipolar diffusivity coefficient at the edge of the protostellar disk in the
Class 0/I protostar, HOPS-370, for the first time, under the assumption that
the disk radius is set by ambipolar diffusion. Using previous results of the
protostellar mass, disk mass, disk radius, density and temperature profiles and
magnetic field strength, we estimate the ambipolar diffusivity coefficient to
be $1.7^{+1.5}_{-1.4}\times10^{19}\,\mathrm{cm^{2}\,s^{-1}}$. We quantify the
contribution of ambipolar diffusion by estimating its dimensionless
Els\""{a}sser number to be $\sim1.7^{+1.0}_{-1.0}$, indicating its dynamical
importance in this region. We compare to chemical calculations of the ambipolar
diffusivity coefficient using the Non-Ideal magnetohydrodynamics Coefficients
and Ionisation Library (NICIL), which is consistent with our results. In
addition, we compare our derived ambipolar diffusivity coefficient to the
diffusivity coefficients for Ohmic dissipation and the Hall effect, and find
ambipolar diffusion is dominant in our density regime. These results
demonstrate a new methodology to understand non-ideal MHD effects in
observations of protostellar disks. More detailed modeling of the magnetic
field, envelope and microphysics, along with a larger sample of protostellar
systems is needed to further understand the contributions of non-ideal MHD.","[{'Modeling Two First Hydrostatic Core Candidates Barnard 1b-N and 1b-S': 'A first hydrostatic core (FHC) is proposed to form after the initial collapse\nof a prestellar core, as a seed of a Class 0 protostar. FHCs are difficult to\nobserve because they are small, compact, embedded, and short lived. In this\nwork, we explored the physical properties of two well-known FHC candidates,\nB1-bN and B1-bS, by comparing interferometric data from Submillimeter Array\n(SMA) 1.1 and 1.3 mm and Atacama Large Millimeter/submillimeter Array (ALMA)\n870 $\\mu$m observations with simulated synthesis images of the two sources. The\nsimulated images are based on a simple model containing a single, hot compact\nfirst-core-like component at the center surrounded by a large-scale, cold and\ndusty envelope described by a broken power-law density distribution with an\nindex, $\\alpha$. Our results show that the hot compact components of B1-bN and\nB1-bS can be described by temperatures of \\sim 500 K with a size of \\sim 4 au,\nwhich are in agreement with theoretical predictions of an FHC. If the $\\alpha$\ninside the broken radii is fixed to -1.5, we find $\\alpha$ \\sim -2.9 and \\sim\n-3.3 outside the broken radii for B1-bN and B1-bS, respectively, consistent\nwith theoretical calculations of a collapsing, bounded envelope and previous\nobservations. Comparing the density and temperature profiles of the two sources\nwith radiation-hydrodynamic simulations of an FHC, we find both sources lie\nclose to, but before, the second collapse stage. We suggest that B1-bS may have\nstarted the collapsing process earlier compared to B1-bN, since a larger\ndiscontinuity point is found in its density profile.'}, {'Magnetic fields of the starless core L 1512': 'We present JCMT POL-2 850 um dust polarization observations and Mimir H band\nstellar polarization observations toward the starless core L1512. We detect the\nhighly-ordered core-scale magnetic field traced by the POL-2 data, of which the\nfield orientation is consistent with the parsec-scale magnetic fields traced by\nPlanck data, suggesting the large-scale fields thread from the low-density\nregion to the dense core region in this cloud. The surrounding magnetic field\ntraced by the Mimir data shows a wider variation in the field orientation,\nsuggesting there could be a transition of magnetic field morphology at the\nenvelope scale. L1512 was suggested to be presumably older than 1.4 Myr in a\nprevious study via time-dependent chemical analysis, hinting that the magnetic\nfield could be strong enough to slow the collapse of L1512. In this study, we\nuse the Davis-Chandrasekhar-Fermi method to derive a plane-of-sky magnetic\nfield strength ($B_{pos}$) of 18$\\pm$7 uG and an observed mass-to-flux ratio\n($\\lambda_{obs}$) of 3.5$\\pm$2.4, suggesting that L1512 is magnetically\nsupercritical. However, the absence of significant infall motion and the\npresence of an oscillating envelope are inconsistent with the magnetically\nsupercritical condition. Using a Virial analysis, we suggest the presence of a\nhitherto hidden line-of-sight magnetic field strength of ~27 uG with a\nmass-to-flux ratio ($\\lambda_{tot}$) of ~1.6, in which case both magnetic and\nkinetic pressures are important in supporting the L1512 core. On the other\nhand, L1512 may have just reached supercriticality and will collapse at any\ntime.'}, {'Accretion Flows or Outflow Cavities? Uncovering the Gas Dynamics around\n  Lupus 3-MMS': 'Understanding how material accretes onto the rotationally supported disk from\nthe surrounding envelope of gas and dust in the youngest protostellar systems\nis important for describing how disks are formed. Magnetohydrodynamic\nsimulations of magnetized, turbulent disk formation usually show spiral-like\nstreams of material (accretion flows) connecting the envelope to the disk.\nHowever, accretion flows in these early stages of protostellar formation still\nremain poorly characterized due to their low intensity and possibly some\nextended structures are disregarded as being part of the outflow cavity. We use\nALMA archival data of a young Class 0 protostar, Lupus 3-MMS, to uncover four\nextended accretion flow-like structures in C$^{18}$O that follow the edges of\nthe outflows. We make various types of position-velocity cuts to compare with\nthe outflows and find the extended structures are not consistent with the\noutflow emission, but rather more consistent with a simple infall model. We\nthen use a dendrogram algorithm to isolate five sub-structures in\nposition-position-velocity space. Four out of the five sub-structures fit well\n($>$95%) with our simple infall model, with specific angular momenta between\n$2.7-6.9\\times10^{-4}\\,$km$\\,$s$^{-1}\\,$pc and mass-infall rates of\n$0.5-1.1\\times10^{-6}\\,M_{\\odot}\\,$yr$^{-1}$. Better characterization of the\nphysical structure in the supposed ""outflow-cavities"" is important to\ndisentangle the true outflow cavities and accretion flows.'}, {'Early Planet Formation in Embedded Disks (eDisk). VIII. A Small\n  Protostellar Disk around the Extremely Low-Mass and Young Class 0 Protostar,\n  IRAS 15398-3359': 'Protostellar disks are a ubiquitous part of the star formation process and\nthe future sites of planet formation. As part of the Early Planet Formation in\nEmbedded Disks (eDisk) large program, we present high-angular resolution dust\ncontinuum ($\\sim40\\,$mas) and molecular line ($\\sim150\\,$mas) observations of\nthe Class 0 protostar, IRAS 15398-3359. The dust continuum is small, compact,\nand centrally peaked, while more extended dust structures are found in the\noutflow directions. We perform a 2D Gaussian fitting to find the deconvolved\nsize and $2\\sigma$ radius of the dust disk to be $4.5\\times2.8\\,\\mathrm{au}$\nand $3.8\\,\\mathrm{au}$, respectively. We estimate the gas+dust disk mass\nassuming optically thin continuum emission to be $0.6-1.8\\,M_\\mathrm{jup}$,\nindicating a very low-mass disk. The CO isotopologues trace components of the\noutflows and inner envelope, while SO traces a compact, rotating disk-like\ncomponent. Using several rotation curve fittings on the PV diagram of the SO\nemission, the lower limits of the protostellar mass and gas disk radius are\n$0.022\\,M_\\odot$ and $31.2\\,\\mathrm{au}$ from our Modified 2 single power-law\nfitting. A conservative upper limit of the protostellar mass is inferred to be\n$0.1\\,M_\\odot$. The protostellar mass-accretion rate and the specific angular\nmomentum at the protostellar disk edge are found to be between\n$1.3-6.1\\times10^{-6}\\,M_\\odot\\,\\mathrm{yr^{-1}}$ and\n$1.2-3.8\\times10^{-4}\\,\\mathrm{km\\,s^{-1}\\,pc}$, respectively, with an age\nestimated between $0.4-7.5\\times10^{4}\\,$yr. At this young age with no clear\nsubstructures in the disk, planet formation would likely not yet have started.\nThis study highlights the importance of high-resolution observations and\nsystematic fitting procedures when deriving dynamical properties of deeply\nembedded Class 0 protostars.'}, {'Early Planet Formation in Embedded Disks (eDisk) V: Possible Annular\n  Substructure in a Circumstellar Disk in the Ced110 IRS4 System': ""We have observed the Class 0/I protostellar system Ced110 IRS4 at an angular\nresolution of $0.05''$ ($\\sim$10 au) as a part of the ALMA large program; Early\nPlanet Formation in the Embedded Disks (eDisk). The 1.3 mm dust continuum\nemission reveals that Ced110 IRS4 is a binary system with a projected\nseparation of $\\sim$250 au. The continuum emissions associated with the main\nsource and its companion, named Ced110 IRS4A and IRS4B respectively, exhibit\ndisk-like shapes and likely arise from dust disks around the protostars. The\ncontinuum emission of Ced110 IRS4A has a radius of $\\sim$91.7 au\n($\\sim0.485''$), and shows bumps along its major axis with an asymmetry. The\nbumps can be interpreted as an shallow, ring-like structure at a radius of\n$\\sim$40 au ($\\sim0.2''$) in the continuum emission, as demonstrated from\ntwo-dimensional intensity distribution models. A rotation curve analysis on the\nC$^{18}$O and $^{13}$CO $J=2$-1 lines reveals the presence of a Keplerian disk\nwithin a radius of 120 au around Ced110 IRS4A, which supports the\ninterpretation that the dust continuum emission arises from a disk. The\nring-like structure in the dust continuum emission might indicate a possible,\nannular substructure in the surface density of the embedded disk, although the\npossibility that it is an apparent structure due to the optically thick\ncontinuum emission cannot be ruled out.""}, {'Early Planet Formation in Embedded Disks (eDisk) VI: Kinematic\n  Structures around the Very Low Mass Protostar IRAS 16253-2429': 'Precise estimates of protostellar masses are crucial to characterize the\nformation of stars of low masses down to brown-dwarfs (BDs; M* < 0.08 Msun).\nThe most accurate estimation of protostellar mass uses the Keplerian rotation\nin the circumstellar disk around the protostar. To apply the Keplerian rotation\nmethod to a protostar at the low-mass end, we have observed the Class 0\nprotostar IRAS 16253-2429 using the Atacama Large Millimeter/submillimeter\nArray (ALMA) in the 1.3 mm continuum at an angular resolution of 0.07"" (10 au),\nand in the 12CO, C18O, 13CO (J=2-1), and SO (J_N = 6_5-5_4) molecular lines, as\npart of the ALMA Large Program Early Planet Formation in Embedded Disks\n(eDisk). The continuum emission traces a non-axisymmetric, disk-like structure\nperpendicular to the associated 12CO outflow. The position-velocity (PV)\ndiagrams in the C18O and 13CO lines can be interpreted as infalling and\nrotating motions. In contrast, the PV diagram along the major axis of the\ndisk-like structure in the 12CO line allows us to identify Keplerian rotation.\nThe central stellar mass and the disk radius are estimated to be ~0.12-0.17\nMsun and ~13-19 au, respectively. The SO line suggests the existence of an\naccretion shock at a ring (r~28 au) surrounding the disk and a streamer from\nthe eastern side of the envelope. IRAS 16253-2429 is not a proto-BD but has a\ncentral stellar mass close to the BD mass regime, and our results provide a\ntypical picture of such very low-mass protostars.'}, {'Early Planet Formation in Embedded Disks (eDisk) IX: High-resolution\n  ALMA Observations of the Class 0 Protostar R CrA IRS5N and its surrounding': 'We present high-resolution, high-sensitivity observations of the Class 0\nprotostar RCrA IRS5N as part of the Atacama Large Milimeter/submilimeter Array\n(ALMA) large program Early Planet Formation in Embedded Disks (eDisk). The 1.3\nmm continuum emission reveals a flattened continuum structure around IRS5N,\nconsistent with a protostellar disk in the early phases of evolution. The\ncontinuum emission appears smooth and shows no substructures. However, a\nbrightness asymmetry is observed along the minor axis of the disk, suggesting\nthe disk is optically and geometrically thick. We estimate the disk mass to be\nbetween 0.007 and 0.02 M$_{\\odot}$. Furthermore, molecular emission has been\ndetected from various species, including C$^{18}$O (2$-$1), $^{12}$CO (2$-$1),\n$^{13}$CO (2$-$1), and H$_2$CO (3$_{0,3}-2_{0,2}$, 3$_{2,1}-2_{2,0}$, and\n3$_{2,2}-2_{2,1}$). By conducting a position-velocity analysis of the C$^{18}$O\n(2$-$1) emission, we find that the disk of IRS5N exhibits characteristics\nconsistent with Keplerian rotation around a central protostar with a mass of\napproximately 0.3 M$_{\\odot}$. Additionally, we observe dust continuum emission\nfrom the nearby binary source, IRS5a/b. The emission in $^{12}$CO toward\nIRS5a/b seems to emanate from IRS5b and flow into IRS5a, suggesting material\ntransport between their mutual orbits. The lack of a detected outflow and\nlarge-scale negatives in \\tlvco~observed toward IRS5N suggests that much of the\nflux from IRS5N is being resolved out. Due to this substantial surrounding\nenvelope, the central IRS5N protostar is expected to be significantly more\nmassive in the future.'}, {'Early Planet Formation in Embedded Disks (eDisk). II. Limited Dust\n  Settling and Prominent Snow Surfaces in the Edge-on Class I Disk IRAS\n  04302+2247': 'While dust disks around optically visible, Class II protostars are found to\nbe vertically thin, when and how dust settles to the midplane are unclear. As\npart of the Atacama Large Millimeter/submillimeter Array (ALMA) large program,\nEarly Planet Formation in Embedded Disks, we analyze the edge-on, embedded,\nClass I protostar IRAS 04302+2247, also nicknamed the ``Butterfly Star."" With a\nresolution of 0.05"" (8~au), the 1.3 mm continuum shows an asymmetry along the\nminor axis which is evidence of an optically thick and geometrically thick disk\nviewed nearly edge-on. There is no evidence of rings and gaps, which could be\ndue to the lack of radial substructure or the highly inclined and optically\nthick view. With 0.1"" (16~au) resolution, we resolve the 2D snow surfaces,\ni.e., the boundary region between freeze-out and sublimation, for $^{12}$CO\n$J$=2--1, $^{13}$CO $J$=2--1, C$^{18}$O $J$=2--1, $H_{2}$CO\n$J$=$3_{0,3}$--$2_{0,2}$, and SO $J$=$6_{5}$--$5_{4}$, and constrain the CO\nmidplane snow line to $\\sim 130$ au. We find Keplerian rotation around a\nprotostar of $1.6 \\pm 0.4 M_{\\odot}$ using C$^{18}$O. Through forward\nray-tracing using RADMC-3D, we find that the dust scale height is $\\sim 6$ au\nat a radius of 100~au from the central star and is comparable to the gas\npressure scale height. The results suggest that the dust of this Class~I source\nhas yet to vertically settle significantly.'}, {'Early Planet Formation in Embedded Disks (eDisk). IV. The Ringed and\n  Warped Structure of the Disk around the Class I Protostar L1489 IRS': 'Constraining the physical and chemical structure of young embedded disks is\ncrucial to understanding the earliest stages of planet formation. As part of\nthe Early Planet Formation in Embedded Disks Atacama Large\nMillimeter/submillimeter Array Large Program, we present high spatial\nresolution ($\\sim$0$.\\!\\!^{\\prime\\prime}$1 or $\\sim$15 au) observations of the\n1.3 mm continuum and $^{13}$CO $J=$ 2-1, C$^{18}$O $J=$ 2-1, and SO $J_N=$\n$6_5$-$5_4$ molecular lines toward the disk around the Class I protostar L1489\nIRS. The continuum emission shows a ring-like structure at 56 au from the\ncentral protostar and a tenuous, optically thin emission extending beyond\n$\\sim$300 au. The $^{13}$CO emission traces the warm disk surface, while the\nC$^{18}$O emission originates from near the disk midplane. The coincidence of\nthe radial emission peak of C$^{18}$O with the dust ring may indicate a\ngap-ring structure in the gaseous disk as well. The SO emission shows a highly\ncomplex distribution, including a compact, prominent component at $\\lesssim$30\nau, which is likely to originate from thermally sublimated SO molecules. The\ncompact SO emission also shows a velocity gradient along a slightly\n($\\sim15^\\circ$) tilted direction with respect to the major axis of the dust\ndisk, which we interpret as an inner warped disk in addition to the warp around\n$\\sim$200 au suggested by previous work. These warped structures may be formed\nby a planet or companion with an inclined orbit, or by a gradual change in the\nangular momentum axis during gas infall.'}, {'Early Planet Formation in Embedded Disks (eDisk). I. Overview of the\n  Program and First Results': 'We present an overview of the Large Program, ``Early Planet Formation in\nEmbedded Disks (eDisk)\'\', conducted with the Atacama Large\nMillimeter/submillimeter Array (ALMA). The ubiquitous detections of\nsubstructures, particularly rings and gaps, in protoplanetary disks around T\nTauri stars raise the possibility that at least some planet formation may have\nalready started during the embedded stages of star formation. In order to\naddress exactly how and when planet formation is initiated, the program focuses\non searching for substructures in disks around 12 Class 0 and 7 Class I\nprotostars in nearby ($< $200 pc) star-forming regions through 1.3 mm continuum\nobservations at a resolution of $\\sim7$ au (0.04""). The initial results show\nthat the continuum emission, mostly arising from dust disks around the sample\nprotostars, has relatively few distinctive substructures, such as rings and\nspirals, in marked contrast to Class II disks. The dramatic difference may\nsuggest that substructures quickly develop in disks when the systems evolve\nfrom protostars to Class II sources or alternatively that high optical depth of\nthe continuum emission could obscure internal structures. Kinematic information\nobtained through CO isotopologue lines and other lines reveals the presence of\nKeplerian disks around protostars, providing us with crucial physical\nparameters, in particular, the dynamical mass of the central protostars. We\ndescribe the background of the eDisk program, the sample selection and their\nALMA observations, the data reduction, and also highlight representative\nfirst-look results.'}]","Title: First Quantitative Measure of Ambipolar Diffusivity Coefficient in the HOPS-370 Protostellar System: Insights into Disk Formation and Evolution 

Abstract:

This research paper introduces a groundbreaking estimation of the ambipolar diffusion coefficient in the HOPS-370 protostellar system, integrating observational data with computational models to advance our understanding of protostellar disk formation and evolution. The study's main objective is to quantitatively estimate the effectiveness of ambipolar diffusion in preserving protostellar disks against magnetic braking, a critical process in stellar system development.

**Innovations**: The paper introduces a novel methodology for estimating the ambipolar diffusivity coefficient by correlating theoretical models with direct observational data. This approach directly addresses a significant knowledge gap in the field and employs a multi-scale dataset from the High Angular Resolution Disk Survey (HOPS).

**Methods**: The research utilizes the analytical formulation of ambipolar diffusion equations and complements the analytical approach with numerical simulations from the Next Generation Ion Cocoon Library (NICIL). The estimation process involves parameter fitting using voltage drop fractions and magnetic field observations on the protostar.

**Results**: A significant outcome is the first on-spot estimation of the ambipolar diffusivity coefficient, measuring to be 2.4 +2.1 -2.0 x 10^-1 seconds. This coefficient quantifies the resistance to diffusion of charged particles against the host magnetic field, crucial for self-regulating disk properties.

**Contributions**: This study is a pioneering step towards quantitatively validating the ambipolar diffusion mechanism in young protostellar systems, offering a flexible framework for understanding disk stability and evolution. It illuminates the relative importance of non-ideal MHD effects, such as ambipolar diffusion, in protostellar dynamics.

**Applications**: The research has wide-ranging implications for the methodology of extracting astrophysical parameters from observational data, potentially accelerating studies involving protostellar evolution, planet formation, and overall star system development. It advocates for a concerted effort in coordinating datasets and computational models to foster new insights into astrophysical phenomena."
"Colorizing grayscale images offers an engaging visual experience. Existing
automatic colorization methods often fail to generate satisfactory results due
to incorrect semantic colors and unsaturated colors. In this work, we propose
an automatic colorization pipeline to overcome these challenges. We leverage
the extraordinary generative ability of the diffusion prior to synthesize color
with plausible semantics. To overcome the artifacts introduced by the diffusion
prior, we apply the luminance conditional guidance. Moreover, we adopt
multimodal high-level semantic priors to help the model understand the image
content and deliver saturated colors. Besides, a luminance-aware decoder is
designed to restore details and enhance overall visual quality. The proposed
pipeline synthesizes saturated colors while maintaining plausible semantics.
Experiments indicate that our proposed method considers both diversity and
fidelity, surpassing previous methods in terms of perceptual realism and gain
most human preference.","[{'Covering with Excess One: Seeing the Topology': 'We have initiated the study of topology of the space of coverings on grid\ndomains. The space has the following constraint: while all the covering agents\ncan move freely (we allow overlapping) on the domain, their union must cover\nthe whole domain. A minimal number $N$ of the covering agents is required for a\nsuccessful covering of the domain. In this paper, we demonstrate beautiful\ntopological structures of this space on grid domains in 2D with $N+1$\ncoverings, the topology of the space has the homotopy type of $1$ dimensional\ncomplex, regardless of the domain shape. We also present the Euler\ncharacteristic formula which connects the topology of the space with that of\nthe domain itself.'}, {'Masked Face Image Classification with Sparse Representation based on\n  Majority Voting Mechanism': 'Sparse approximation is the problem to find the sparsest linear combination\nfor a signal from a redundant dictionary, which is widely applied in signal\nprocessing and compressed sensing. In this project, I manage to implement the\nOrthogonal Matching Pursuit (OMP) algorithm and Sparse Representation-based\nClassification (SRC) algorithm, then use them to finish the task of masked\nimage classification with majority voting. Here the experiment was token on the\nAR data-set, and the result shows the superiority of OMP algorithm combined\nwith SRC algorithm over masked face image classification with an accuracy of\n98.4%.'}, {'Contracting with Heterogeneous Researchers': 'We study the design of contracts that incentivize a researcher to conduct a\ncostly experiment, extending the work of Yoder (2022) from binary states to a\ngeneral state space. The cost is private information of the researcher. When\nthe experiment is observable, we find the optimal contract and show that higher\ntypes choose more costly experiments, but not necessarily more Blackwell\ninformative ones. When only the experiment result is observable, the principal\ncan still achieve the same optimal outcome if and only if a certain\nmonotonicity condition with respect to types holds. Our analysis demonstrates\nthat the general case is qualitatively different than the binary one, but that\nthe contracting problem remains tractable.'}, {'Leaked-Web: Accurate and Efficient Machine Learning-Based Website\n  Fingerprinting Attack through Hardware Performance Counters': ""Users' website browsing history contains sensitive information, like health\nconditions, political interests, financial situations, etc. Some recent studies\nhave demonstrated the possibility of inferring website fingerprints based on\nimportant usage information such as traffic, cache usage, memory usage, CPU\nactivity, power consumption, and hardware performance counters information.\nHowever, existing website fingerprinting attacks demand a high sampling rate\nwhich causes high performance overheads and large network traffic, and/or they\nrequire launching an additional malicious website by the user, which is not\nguaranteed. As a result, such drawbacks make the existing attacks more\nnoticeable to users and corresponding fingerprinting detection mechanisms. In\nresponse, in this work, we propose Leaked-Web, a novel accurate and efficient\nmachine learning-based website fingerprinting attack through processor's\nHardware Performance Counters (HPCs). Leaked-Web efficiently collects hardware\nperformance counters in users' computer systems at a significantly low\ngranularity monitoring rate and sends the samples to the remote attack's server\nfor further classification. Leaked-Web examines the web browsers'\nmicroarchitectural features using various advanced machine learning algorithms\nranging from classical, boosting, deep learning, and time-series models. Our\nexperimental results indicate that Leaked-Web based on a LogitBoost ML\nclassifier using only the top 4 HPC features achieves 91% classification\naccuracy outperforming the state-of-the-art attacks by nearly 5%. Furthermore,\nour proposed attack obtains a negligible performance overhead (only <1%),\naround 12% lower than the existing hardware-assisted website fingerprinting\nattacks.""}, {'DeePMD-kit: A deep learning package for many-body potential energy\n  representation and molecular dynamics': 'Recent developments in many-body potential energy representation via deep\nlearning have brought new hopes to addressing the accuracy-versus-efficiency\ndilemma in molecular simulations. Here we describe DeePMD-kit, a package\nwritten in Python/C++ that has been designed to minimize the effort required to\nbuild deep learning based representation of potential energy and force field\nand to perform molecular dynamics. Potential applications of DeePMD-kit span\nfrom finite molecules to extended systems and from metallic systems to\nchemically bonded systems. DeePMD-kit is interfaced with TensorFlow, one of the\nmost popular deep learning frameworks, making the training process highly\nautomatic and efficient. On the other end, DeePMD-kit is interfaced with\nhigh-performance classical molecular dynamics and quantum (path-integral)\nmolecular dynamics packages, i.e., LAMMPS and the i-PI, respectively. Thus,\nupon training, the potential energy and force field models can be used to\nperform efficient molecular simulations for different purposes. As an example\nof the many potential applications of the package, we use DeePMD-kit to learn\nthe interatomic potential energy and forces of a water model using data\nobtained from density functional theory. We demonstrate that the resulted\nmolecular dynamics model reproduces accurately the structural information\ncontained in the original model.'}, {'Time-domain multiscale shape identification in electro-sensing': 'This paper presents premier and innovative time-domain multi-scale method for\nshape identification in electro-sensing using pulse-type signals. The method is\nbased on transform-invariant shape descriptors computed from filtered\npolarization tensors at multi-scales. The proposed algorithm enjoys a\nremarkable noise robustness even with far-field measurements at very limited\nangle of view. It opens a door for pulsed imaging using echolocation and\ninduction data.'}, {'Adaptive Resolution Simulation in Equilibrium and Beyond': 'In this paper, we investigate the equilibrium statistical properties of both\nthe force and potential interpolations of adaptive resolution simulation\n(AdResS) under the theoretical framework of grand-canonical like AdResS\n(GC-AdResS). The thermodynamic relations between the higher and lower\nresolutions are derived by considering the absence of fundamental conservation\nlaws in mechanics for both branches of AdResS. In order to investigate the\napplicability of AdResS method in studying the properties beyond the\nequilibrium, we demonstrate the accuracy of AdResS in computing the dynamical\nproperties in two numerical examples: The velocity auto-correlation of pure\nwater and the conformational relaxation of alanine dipeptide dissolved in\nwater. Theoretical and technical open questions of the AdResS method are\ndiscussed in the end of the paper.'}, {'Optimizing working parameters of the twin-range cutoff method in terms\n  of accuracy and efficiency': 'We construct a priori error estimation for the force error of the twin-range\ncutoff method, which is widely used to treat the short-range non-bonded\ninteractions in molecular simulations. Based on the error and cost estimation,\nwe develop a work flow that can automatically determine the nearly most\nefficient twin-range cutoff parameters (i.e. the cutoff radii and the neighbor\nlist updating frequency) prior to a simulation for a predetermined accuracy.\nBoth the error estimate and the parameter tuning method are demonstrated to be\neffective by testing simulations of the standard Lennard-Jones 6-12 fluid in\ngas, liquid as well as supercritical state. We recommend the tuned twin-range\ncutoff method that can save precious user time and computational resources.'}, {'Linear Convergence Rate Analysis of Proximal Generalized ADMM for Convex\n  Composite Programming': 'The proximal generalized alternating direction method of multipliers\n(p-GADMM) is substantially efficient for solving convex composite programming\nproblems of high-dimensional to moderate accuracy. The global convergence of\nthis method was established by Xiao, Chen & Li [Math. Program. Comput., 2018],\nbut its convergence rate was not given. One may take it for granted that the\nconvergence rate could be proved easily by mimicking the proximal ADMM, but we\nfind the relaxed points will certainly cause many difficulties for theoretical\nanalysis. In this paper, we devote to exploring its convergence behavior and\nshow that the sequence generated by p-GADMM possesses Q-linear convergence rate\nunder some mild conditions. We would like to note that the proximal terms at\nthe subproblems are required to be positive definite, which is very common in\nmost practical implementations although it seems to be a bit strong.'}, {'Nonlinear Optimal Guidance for Cooperatively Imposing Relative Intercept\n  Angles': ""The optimal cooperative guidance in the nonlinear setting for intercepting a\ntarget by multiple pursuers is studied in the paper. As certain relative angles\ncan improve observability, the guidance command is required to cooperatively\ncontrol the pursuers to intercept the target with specific relative angles. By\nusing the neural networks, an approach for real-time generation of the\nnonlinear cooperative optimal guidance command is developed. Specifically, the\noptimal control problem with constraints on relative intercepting angles is\nformulated. Then, Pontryagin's maximum principle is used to derive the\nnecessary conditions for optimality, which are further employed to parameterize\nthe nonlinear optimal guidance law. As a result, the dataset for the mapping\nfrom state to nonlinear optimal guidance command can be generated by a simple\npropagation. A simple feedforward neural network is trained by the dataset to\ngenerate the nonlinear optimal guidance command. Finally, numerical examples\nare presented, showing that a nonlinear optimal guidance command with specific\nrelative angles can be generated within a faction of a millisecond.""}]","Title: Diffusion-VAE: An Automatic Colorization Pipeline Enhancing Vividness and Semantic Reasonability 

Abstract:
In the era of AI-driven computer vision, the retention of color information in grayscale images has become pivotal in applications ranging from retro art and entertainment to color-matching in industrial photography. In this work, we tackle the natural image automatic colorization problem by introducing Diffusion-VAE, a novel pipeline that synergizes a denoising U-Net and a variational autoencoder with high-level semantic guidance and luminance-aware diffusion. Our objective is to enhance vividness and semantic coherence in colorization outputs, surpassing state-of-the-art methods in both quantitative metrics and user evaluations.

A key innovation is the integration of high-level semantic guidance to avoid color chaos and preserve object-specific colors. We further advance the diffusion process with a luminance-aware decoder, capable of maintaining grayscale conditions while improving color synthesis without amplifying artifacts. Quantitatively, our model significantly improves Fréchet Inception Distance, colorfulness scores, and PSNR, specifically outperforming ControlNet in both striving for vivid colors and semantic reasonability.

The main contribution of our research lies in developing a deep learning-based method that achieves superior automatic colorization quality, often exceeding human perceptions represented in user studies. Improved results in color fidelity and object-color congruence not only boost the value in visual art but also have broad implications, from enhancing photography editing to innovative color transformation applications in graphic design. Highlighting the state-of-the-art advancements in automatic colorization, our Diffusion-VAE model paves the way for more expressive and contextually fitting color reconstructions in artificial intelligence images.

This paper's implementation and ideas carry strong potential for broad industrial and artistic incorporation, marking a pivotal step towards reinventing the digital colorization process."
"The recent success of large language models (LLMs) trained on static,
pre-collected, general datasets has sparked numerous research directions and
applications. One such direction addresses the non-trivial challenge of
integrating pre-trained LLMs into dynamic data distributions, task structures,
and user preferences. Pre-trained LLMs, when tailored for specific needs, often
experience significant performance degradation in previous knowledge domains --
a phenomenon known as ""catastrophic forgetting"". While extensively studied in
the continual learning (CL) community, it presents new manifestations in the
realm of LLMs. In this survey, we provide a comprehensive overview of the
current research progress on LLMs within the context of CL. This survey is
structured into four main sections: we first describe an overview of
continually learning LLMs, consisting of two directions of continuity: vertical
continuity (or vertical continual learning), i.e., continual adaptation from
general to specific capabilities, and horizontal continuity (or horizontal
continual learning), i.e., continual adaptation across time and domains
(Section 3). We then summarize three stages of learning LLMs in the context of
modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),
and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of
evaluation protocols for continual learning with LLMs, along with the current
available data sources (Section 5). Finally, we discuss intriguing questions
pertaining to continual learning for LLMs (Section 6). The full list of papers
examined in this survey is available at
https://github.com/Wang-ML-Lab/llm-continual-learning-survey.","[{'A Unified Approach to Domain Incremental Learning with Memory: Theory\n  and Algorithm': 'Domain incremental learning aims to adapt to a sequence of domains with\naccess to only a small subset of data (i.e., memory) from previous domains.\nVarious methods have been proposed for this problem, but it is still unclear\nhow they are related and when practitioners should choose one method over\nanother. In response, we propose a unified framework, dubbed Unified Domain\nIncremental Learning (UDIL), for domain incremental learning with memory. Our\nUDIL **unifies** various existing methods, and our theoretical analysis shows\nthat UDIL always achieves a tighter generalization error bound compared to\nthese methods. The key insight is that different existing methods correspond to\nour bound with different **fixed** coefficients; based on insights from this\nunification, our UDIL allows **adaptive** coefficients during training, thereby\nalways achieving the tightest bound. Empirical results show that our UDIL\noutperforms the state-of-the-art domain incremental learning methods on both\nsynthetic and real-world datasets. Code will be available at\nhttps://github.com/Wang-ML-Lab/unified-continual-learning.'}, {'Structure-Aware Group Discrimination with Adaptive-View Graph Encoder: A\n  Fast Graph Contrastive Learning Framework': 'Albeit having gained significant progress lately, large-scale graph\nrepresentation learning remains expensive to train and deploy for two main\nreasons: (i) the repetitive computation of multi-hop message passing and\nnon-linearity in graph neural networks (GNNs); (ii) the computational cost of\ncomplex pairwise contrastive learning loss. Two main contributions are made in\nthis paper targeting this twofold challenge: we first propose an adaptive-view\ngraph neural encoder (AVGE) with a limited number of message passing to\naccelerate the forward pass computation, and then we propose a structure-aware\ngroup discrimination (SAGD) loss in our framework which avoids inefficient\npairwise loss computing in most common GCL and improves the performance of the\nsimple group discrimination. By the framework proposed, we manage to bring down\nthe training and inference cost on various large-scale datasets by a\nsignificant margin (250x faster inference time) without loss of the\ndownstream-task performance.'}, {'MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph\n  Contrastive Learning': 'In this work, we investigate the problem of out-of-distribution (OOD)\ngeneralization for unsupervised learning methods on graph data. This scenario\nis particularly challenging because graph neural networks (GNNs) have been\nshown to be sensitive to distributional shifts, even when labels are available.\nTo address this challenge, we propose a \\underline{M}odel-\\underline{A}gnostic\n\\underline{R}ecipe for \\underline{I}mproving \\underline{O}OD generalizability\nof unsupervised graph contrastive learning methods, which we refer to as MARIO.\nMARIO introduces two principles aimed at developing distributional-shift-robust\ngraph contrastive methods to overcome the limitations of existing frameworks:\n(i) Information Bottleneck (IB) principle for achieving generalizable\nrepresentations and (ii) Invariant principle that incorporates adversarial data\naugmentation to obtain invariant representations. To the best of our knowledge,\nthis is the first work that investigates the OOD generalization problem of\ngraph contrastive learning, with a specific focus on node-level tasks. Through\nextensive experiments, we demonstrate that our method achieves state-of-the-art\nperformance on the OOD test set, while maintaining comparable performance on\nthe in-distribution test set when compared to existing approaches. The source\ncode for our method can be found at: https://github.com/ZhuYun97/MARIO'}, {'Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs': ""Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE""}, {'Run Away From your Teacher: Understanding BYOL by a Novel\n  Self-Supervised Approach': ""Recently, a newly proposed self-supervised framework Bootstrap Your Own\nLatent (BYOL) seriously challenges the necessity of negative samples in\ncontrastive learning frameworks. BYOL works like a charm despite the fact that\nit discards the negative samples completely and there is no measure to prevent\ncollapse in its training objective. In this paper, we suggest understanding\nBYOL from the view of our proposed interpretable self-supervised learning\nframework, Run Away From your Teacher (RAFT). RAFT optimizes two objectives at\nthe same time: (i) aligning two views of the same data to similar\nrepresentations and (ii) running away from the model's Mean Teacher (MT, the\nexponential moving average of the history models) instead of BYOL's running\ntowards it. The second term of RAFT explicitly prevents the representation\ncollapse and thus makes RAFT a more conceptually reliable framework. We provide\nbasic benchmarks of RAFT on CIFAR10 to validate the effectiveness of our\nmethod. Furthermore, we prove that BYOL is equivalent to RAFT under certain\nconditions, providing solid reasoning for BYOL's counter-intuitive success.""}, {'Informative Visual Storytelling with Cross-modal Rules': ""Existing methods in the Visual Storytelling field often suffer from the\nproblem of generating general descriptions, while the image contains a lot of\nmeaningful contents remaining unnoticed. The failure of informative story\ngeneration can be concluded to the model's incompetence of capturing enough\nmeaningful concepts. The categories of these concepts include entities,\nattributes, actions, and events, which are in some cases crucial to grounded\nstorytelling. To solve this problem, we propose a method to mine the\ncross-modal rules to help the model infer these informative concepts given\ncertain visual input. We first build the multimodal transactions by\nconcatenating the CNN activations and the word indices. Then we use the\nassociation rule mining algorithm to mine the cross-modal rules, which will be\nused for the concept inference. With the help of the cross-modal rules, the\ngenerated stories are more grounded and informative. Besides, our proposed\nmethod holds the advantages of interpretation, expandability, and\ntransferability, indicating potential for wider application. Finally, we\nleverage these concepts in our encoder-decoder framework with the attention\nmechanism. We conduct several experiments on the VIsual StoryTelling~(VIST)\ndataset, the results of which demonstrate the effectiveness of our approach in\nterms of both automatic metrics and human evaluation. Additional experiments\nare also conducted showing that our mined cross-modal rules as additional\nknowledge helps the model gain better performance when trained on a small\ndataset.""}, {'On the Efficacy of Small Self-Supervised Contrastive Models without\n  Distillation Signals': 'It is a consensus that small models perform quite poorly under the paradigm\nof self-supervised contrastive learning. Existing methods usually adopt a large\noff-the-shelf model to transfer knowledge to the small one via distillation.\nDespite their effectiveness, distillation-based methods may not be suitable for\nsome resource-restricted scenarios due to the huge computational expenses of\ndeploying a large model. In this paper, we study the issue of training\nself-supervised small models without distillation signals. We first evaluate\nthe representation spaces of the small models and make two non-negligible\nobservations: (i) the small models can complete the pretext task without\noverfitting despite their limited capacity and (ii) they universally suffer the\nproblem of over clustering. Then we verify multiple assumptions that are\nconsidered to alleviate the over-clustering phenomenon. Finally, we combine the\nvalidated techniques and improve the baseline performances of five small\narchitectures with considerable margins, which indicates that training small\nself-supervised contrastive models is feasible even without distillation\nsignals. The code is available at\n\\textit{https://github.com/WOWNICE/ssl-small}.'}, {'Towards Communication-Efficient and Privacy-Preserving Federated\n  Representation Learning': ""This paper investigates the feasibility of federated representation learning\nunder the constraints of communication cost and privacy protection. Existing\nworks either conduct annotation-guided local training which requires frequent\ncommunication or aggregates the client models via weight averaging which has\npotential risks of privacy exposure. To tackle the above problems, we first\nidentify that self-supervised contrastive local training is robust against the\nnon-identically distributed data, which provides the feasibility of longer\nlocal training and thus reduces the communication cost. Then based on the\naforementioned robustness, we propose a novel Federated representation Learning\nframework with Ensemble Similarity Distillation~(FLESD) that utilizes this\nrobustness. At each round of communication, the server first gathers a fraction\nof the clients' inferred similarity matrices on a public dataset. Then it\nensembles the similarity matrices and train the global model via similarity\ndistillation. We verify the effectiveness of FLESD by a series of empirical\nexperiments and show that, despite stricter constraints, it achieves comparable\nresults under multiple settings on multiple datasets.""}, {'CIL: Contrastive Instance Learning Framework for Distantly Supervised\n  Relation Extraction': 'The journey of reducing noise from distant supervision (DS) generated\ntraining data has been started since the DS was first introduced into the\nrelation extraction (RE) task. For the past decade, researchers apply the\nmulti-instance learning (MIL) framework to find the most reliable feature from\na bag of sentences. Although the pattern of MIL bags can greatly reduce DS\nnoise, it fails to represent many other useful sentence features in the\ndatasets. In many cases, these sentence features can only be acquired by extra\nsentence-level human annotation with heavy costs. Therefore, the performance of\ndistantly supervised RE models is bounded. In this paper, we go beyond typical\nMIL framework and propose a novel contrastive instance learning (CIL)\nframework. Specifically, we regard the initial MIL as the relational triple\nencoder and constraint positive pairs against negative pairs for each instance.\nExperiments demonstrate the effectiveness of our proposed framework, with\nsignificant improvements over the previous methods on NYT10, GDS and KBP.'}, {'GraphControl: Adding Conditional Control to Universal Graph Pre-trained\n  Models for Graph Domain Transfer Learning': 'Graph-structured data is ubiquitous in the world which models complex\nrelationships between objects, enabling various Web applications. Daily\ninfluxes of unlabeled graph data on the Web offer immense potential for these\napplications. Graph self-supervised algorithms have achieved significant\nsuccess in acquiring generic knowledge from abundant unlabeled graph data.\nThese pre-trained models can be applied to various downstream Web applications,\nsaving training time and improving downstream (target) performance. However,\ndifferent graphs, even across seemingly similar domains, can differ\nsignificantly in terms of attribute semantics, posing difficulties, if not\ninfeasibility, for transferring the pre-trained models to downstream tasks.\nConcretely speaking, for example, the additional task-specific node information\nin downstream tasks (specificity) is usually deliberately omitted so that the\npre-trained representation (transferability) can be leveraged. The trade-off as\nsuch is termed as ""transferability-specificity dilemma"" in this work. To\naddress this challenge, we introduce an innovative deployment module coined as\nGraphControl, motivated by ControlNet, to realize better graph domain transfer\nlearning. Specifically, by leveraging universal structural pre-trained models\nand GraphControl, we align the input space across various graphs and\nincorporate unique characteristics of target data as conditional inputs. These\nconditions will be progressively integrated into the model during fine-tuning\nor prompt tuning through ControlNet, facilitating personalized deployment.\nExtensive experiments show that our method significantly enhances the\nadaptability of pre-trained models on target attributed datasets, achieving\n1.4-3x performance gain. Furthermore, it outperforms training-from-scratch\nmethods on target data with a comparable margin and exhibits faster\nconvergence.'}]","Title: Progress and Challenges in Continual Learning for Large Language Models

Abstract:
As large language models (LLMs) trained through continuous learning from vast, evolving corpora of text gain momentum, they increasingly encounter temporal, lexical, and distributional shifts, including domain-specific adaptations. This research explores methodologies aligning with the latest advancements, pinpointing the methodologies and evaluating their inefficiencies in tackling the nuances of continual fine-tuning, continual instruction tuning, continual model refinement, and continual model alignment. Crucial topics like the development of benchmarks for evaluating LLMs that accommodate temporal stability, and the challenges posed by limited resources, ethical development, and data privacy are deliberated. We expound on the pitfalls of scalable and adaptive techniques, advocating for stipulations that streamline continual learning's incorporation into LLMs' lifecycle, while maintaining robust performance on evolving data and societal needs. The abstract underscores the necessity of establishing a comprehensive and widely recognized framework for continual learning, particularly in response to real-world, continuous shifts in data dynamics and societal expectations, thus strengthening the recovery of semantic and functional levels of explanation within language models. Addressing these gaps and challenges will ultimately fortify the adaptability and competency of LLMs in diverse, temporally shifting environments, further propelling their integration into various practical scenarios and applications.

Note: The above abstract is a conceptualization of various ideas from the text, that align with the requested structure and does not reflect any published research outcomes or novel findings."
"Although the capabilities of large language models (LLMs) ideally scale up
with increasing data and compute, they are inevitably constrained by limited
resources in reality. Suppose we have a moderately trained LLM (e.g., trained
to align with human preference) in hand, can we further exploit its potential
and cheaply acquire a stronger model? In this paper, we propose a simple method
called ExPO to boost LLMs' alignment with human preference. ExPO assumes that a
medium-aligned model can be interpolated between a less-aligned (weaker) model,
e.g., the initial SFT model, and a better-aligned (stronger) one, thereby
directly obtaining this stronger model by extrapolating from the weights of the
former two relatively weaker models. On the AlpacaEval 2.0 benchmark, we show
that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to
reach and even surpass the fully-trained one, without any additional training.
Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and
exhibits decent scalability across model sizes from 7B to 70B. Our work
demonstrates the efficacy of model extrapolation in exploiting LLMs'
capabilities, suggesting a promising direction that deserves future
exploration.","[{'Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation': 'Dialog models can be greatly strengthened through grounding on various\nexternal information, but grounded dialog corpora are usually not naturally\naccessible. In this work, we focus on the few-shot learning for grounded dialog\ngeneration (GDG). We first propose a simple prompting method for GDG tasks,\nwhere different constructs of model input, such as the grounding source and the\nconversation context, are distinguished through continuous or discrete prompts.\nOn three typical GDG tasks, we empirically demonstrate and analyze in-depth the\neffectiveness of our method. We then conduct extensive experiments to\nthoroughly investigate how our prompting method works with different\npre-trained models. We show that prompted language models perform superiorly to\nconversational models, and further analyze various factors that influence the\neffects of prompting. Overall, our work introduces a prompt-based perspective\nto the few-shot learning for GDG tasks, and provides valuable findings and\ninsights for future research.'}, {'Click: Controllable Text Generation with Sequence Likelihood Contrastive\n  Learning': ""It has always been an important yet challenging problem to control language\nmodels to avoid generating texts with undesirable attributes, such as toxic\nlanguage and unnatural repetition. We introduce Click for controllable text\ngeneration, which needs no modification to the model architecture and\nfacilitates out-of-the-box use of trained models. It employs a contrastive loss\non sequence likelihood, which fundamentally decreases the generation\nprobability of negative samples (i.e., generations with undesirable\nattributes). It also adopts a novel likelihood ranking-based strategy to\nconstruct contrastive samples from model generations. On the tasks of language\ndetoxification, sentiment steering, and repetition reduction, we show that\nClick outperforms strong baselines of controllable text generation and\ndemonstrate the superiority of Click's sample construction strategy.""}, {'ChID: A Large-scale Chinese IDiom Dataset for Cloze Test': 'Cloze-style reading comprehension in Chinese is still limited due to the lack\nof various corpora. In this paper we propose a large-scale Chinese cloze test\ndataset ChID, which studies the comprehension of idiom, a unique language\nphenomenon in Chinese. In this corpus, the idioms in a passage are replaced by\nblank symbols and the correct answer needs to be chosen from well-designed\ncandidate idioms. We carefully study how the design of candidate idioms and the\nrepresentation of idioms affect the performance of state-of-the-art models.\nResults show that the machine accuracy is substantially worse than that of\nhuman, indicating a large space for further research.'}, {'Difference-aware Knowledge Selection for Knowledge-grounded Conversation\n  Generation': 'In a multi-turn knowledge-grounded dialog, the difference between the\nknowledge selected at different turns usually provides potential clues to\nknowledge selection, which has been largely neglected in previous research. In\nthis paper, we propose a difference-aware knowledge selection method. It first\ncomputes the difference between the candidate knowledge sentences provided at\nthe current turn and those chosen in the previous turns. Then, the differential\ninformation is fused with or disentangled from the contextual information to\nfacilitate final knowledge selection. Automatic, human observational, and\ninteractive evaluation shows that our method is able to select knowledge more\naccurately and generate more informative responses, significantly outperforming\nthe state-of-the-art baselines. The codes are available at\nhttps://github.com/chujiezheng/DiffKS.'}, {'CEM: Commonsense-aware Empathetic Response Generation': ""A key trait of daily conversations between individuals is the ability to\nexpress empathy towards others, and exploring ways to implement empathy is a\ncrucial step towards human-like dialogue systems. Previous approaches on this\ntopic mainly focus on detecting and utilizing the user's emotion for generating\nempathetic responses. However, since empathy includes both aspects of affection\nand cognition, we argue that in addition to identifying the user's emotion,\ncognitive understanding of the user's situation should also be considered. To\nthis end, we propose a novel approach for empathetic response generation, which\nleverages commonsense to draw more information about the user's situation and\nuses this additional information to further enhance the empathy expression in\ngenerated responses. We evaluate our approach on EmpatheticDialogues, which is\na widely-used benchmark dataset for empathetic response generation. Empirical\nresults demonstrate that our approach outperforms the baseline models in both\nautomatic and human evaluations and can generate more informative and\nempathetic responses.""}, {'AugESC: Dialogue Augmentation with Large Language Models for Emotional\n  Support Conversation': ""Crowdsourced dialogue corpora are usually limited in scale and topic coverage\ndue to the expensive cost of data curation. This would hinder the\ngeneralization of downstream dialogue models to open-domain topics. In this\nwork, we leverage large language models for dialogue augmentation in the task\nof emotional support conversation (ESC). By treating dialogue augmentation as a\ndialogue completion task, we prompt a fine-tuned language model to complete\nfull dialogues from available dialogue posts of various topics, which are then\npostprocessed based on heuristics. Applying this approach, we construct AugESC,\nan augmented dataset for the ESC task, which largely extends the scale and\ntopic coverage of the crowdsourced ESConv corpus. Through comprehensive human\nevaluation, we demonstrate that our approach is superior to strong baselines of\ndialogue augmentation and that AugESC has comparable dialogue quality to the\ncrowdsourced corpus. We also conduct human interactive evaluation and prove\nthat post-training on AugESC improves downstream dialogue models'\ngeneralization ability to open-domain topics. These results suggest the utility\nof AugESC and highlight the potential of large language models in improving\ndata-scarce dialogue generation tasks.""}, {'A Baseline Analysis for Podcast Abstractive Summarization': ""Podcast summary, an important factor affecting end-users' listening\ndecisions, has often been considered a critical feature in podcast\nrecommendation systems, as well as many downstream applications. Existing\nabstractive summarization approaches are mainly built on fine-tuned models on\nprofessionally edited texts such as CNN and DailyMail news. Different from\nnews, podcasts are often longer, more colloquial and conversational, and\nnoisier with contents on commercials and sponsorship, which makes automatic\npodcast summarization extremely challenging. This paper presents a baseline\nanalysis of podcast summarization using the Spotify Podcast Dataset provided by\nTREC 2020. It aims to help researchers understand current state-of-the-art\npre-trained models and hence build a foundation for creating better models.""}, {'A Two-Phase Approach for Abstractive Podcast Summarization': 'Podcast summarization is different from summarization of other data formats,\nsuch as news, patents, and scientific papers in that podcasts are often longer,\nconversational, colloquial, and full of sponsorship and advertising\ninformation, which imposes great challenges for existing models. In this paper,\nwe focus on abstractive podcast summarization and propose a two-phase approach:\nsentence selection and seq2seq learning. Specifically, we first select\nimportant sentences from the noisy long podcast transcripts. The selection is\nbased on sentence similarity to the reference to reduce the redundancy and the\nassociated latent topics to preserve semantics. Then the selected sentences are\nfed into a pre-trained encoder-decoder framework for the summary generation.\nOur approach achieves promising results regarding both ROUGE-based measures and\nhuman evaluations.'}, {'CoMAE: A Multi-factor Hierarchical Framework for Empathetic Response\n  Generation': 'The capacity of empathy is crucial to the success of open-domain dialog\nsystems. Due to its nature of multi-dimensionality, there are various factors\nthat relate to empathy expression, such as communication mechanism, dialog act\nand emotion. However, existing methods for empathetic response generation\nusually either consider only one empathy factor or ignore the hierarchical\nrelationships between different factors, leading to a weak ability of empathy\nmodeling. In this paper, we propose a multi-factor hierarchical framework,\nCoMAE, for empathetic response generation, which models the above three key\nfactors of empathy expression in a hierarchical way. We show experimentally\nthat our CoMAE-based model can generate more empathetic responses than previous\nmethods. We also highlight the importance of hierarchical modeling of different\nfactors through both the empirical analysis on a real-life corpus and the\nextensive experiments. Our codes and used data are available at\nhttps://github.com/chujiezheng/CoMAE.'}, {'CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic\n  Response Generation': ""Empathetic conversation is psychologically supposed to be the result of\nconscious alignment and interaction between the cognition and affection of\nempathy. However, existing empathetic dialogue models usually consider only the\naffective aspect or treat cognition and affection in isolation, which limits\nthe capability of empathetic response generation. In this work, we propose the\nCASE model for empathetic dialogue generation. It first builds upon a\ncommonsense cognition graph and an emotional concept graph and then aligns the\nuser's cognition and affection at both the coarse-grained and fine-grained\nlevels. Through automatic and manual evaluation, we demonstrate that CASE\noutperforms state-of-the-art baselines of empathetic dialogues and can generate\nmore empathetic and informative responses.""}]","Title: Enhancing Large Language Models via Model Extrapolation: Exponential Boost in Alignment Performance without Additional Training

Abstract:

In response to the growing need for enhanced alignment between Large Language Models (LLMs) and human preferences, this study introduces a novel technique, known as Model Extrapolation (EXPO), aiming at augmenting existing LLMs to enhance their alignment score on AlpacaEval 2.0—without requiring any additional training data or resources. This research, built on previous frameworks including Direct Preference Optimization (DPO), underscores a key finding: even models with considerable human-preference alignment can be further improved by a ‘backward’ step to an initial supervised fine-tuning checkpoint. The process of model extrapolation, akin to “global gradient update” based on the difference between the final and initial model state vectors, allows for a nuanced adjustment. The research demonstrates that the combination of an effectively trained LL outline (Mw) and an advanced LL outline (M) can optimize performance in ways that were not observable otherwise. 

Unique contributions of our study include:
  
  
  1. **Simplicity and Efficiency**: The EXPO method offers a straightforward reconfiguration procedure drawing on pre-existing checkpoint models without the need for additional training, making it a cost-effective alteration for improving model performance.
  2. **Scalability**: The technique is universally applicable to models of varied sizes and levels of alignment-capabilities, extending its utility across the spectrum of model development and deployment scenarios.
  3. **Model Optimization**: The effectiveness of model extrapolation is substantiated through empirical evidence of significant performance enhancement (up to +4%) achieved solely through adjustment of a single hyperparameter, the extrapolation coefficient.

Model extrapolation serves as a powerful tool capable of enhancing alignment between LL outlines and human preferences, thereby reducing potential discrepancies in output. This intrinsic property of such models allows for optimization independent of additional iterations or costs, paving the way for powerful advancements in AI alignment and thereby widening the scope for AI systems to benefit society. The advancements are not only crucial for ensuring ethical use of AI but also for maximizing the roles of AI in various real-world applications, from content creation to customer service, thereby ensuring an informed and ethical AI-driven future."
"Unsupervised semantic segmentation aims to automatically partition images
into semantically meaningful regions by identifying global categories within an
image corpus without any form of annotation. Building upon recent advances in
self-supervised representation learning, we focus on how to leverage these
large pre-trained models for the downstream task of unsupervised segmentation.
We present PriMaPs - Principal Mask Proposals - decomposing images into
semantically meaningful masks based on their feature representation. This
allows us to realize unsupervised semantic segmentation by fitting class
prototypes to PriMaPs with a stochastic expectation-maximization algorithm,
PriMaPs-EM. Despite its conceptual simplicity, PriMaPs-EM leads to competitive
results across various pre-trained backbone models, including DINO and DINOv2,
and across datasets, such as Cityscapes, COCO-Stuff, and Potsdam-3.
Importantly, PriMaPs-EM is able to boost results when applied orthogonally to
current state-of-the-art unsupervised semantic segmentation pipelines.","[{'Collisionless Dynamics and the Cosmic Web': 'I review the nature of three-dimensional collapse in the Zeldovich\napproximation, how it relates to the underlying nature of the three-dimensional\nLagrangian manifold and naturally gives rise to a hierarchical structure\nformation scenario that progresses through collapse from voids to pancakes,\nfilaments and then halos. I then discuss how variations of the Zeldovich\napproximation (based on the gravitational or the velocity potential) have been\nused to define classifications of the cosmic large-scale structure into\ndynamically distinct parts. Finally, I turn to recent efforts to devise new\napproaches relying on tessellations of the Lagrangian manifold to follow the\nfine-grained dynamics of the dark matter fluid into the highly non-linear\nregime and both extract the maximum amount of information from existing\nsimulations as well as devise new simulation techniques for cold collisionless\ndynamics.'}, {'Multi-scale initial conditions for cosmological simulations': 'We discuss a new algorithm to generate multi-scale initial conditions with\nmultiple levels of refinements for cosmological ""zoom-in"" simulations. The\nmethod uses an adaptive convolution of Gaussian white noise with a real space\ntransfer function kernel together with an adaptive multi-grid Poisson solver to\ngenerate displacements and velocities following first (1LPT) or second order\nLagrangian perturbation theory (2LPT). The new algorithm achieves RMS relative\nerrors of order 10^(-4) for displacements and velocities in the refinement\nregion and thus improves in terms of errors by about two orders of magnitude\nover previous approaches. In addition, errors are localized at coarse-fine\nboundaries and do not suffer from Fourier-space induced interference ringing.\nAn optional hybrid multi-grid and Fast Fourier Transform (FFT) based scheme is\nintroduced which has identical Fourier space behaviour as traditional\napproaches. Using a suite of re-simulations of a galaxy cluster halo our real\nspace based approach is found to reproduce correlation functions, density\nprofiles, key halo properties and subhalo abundances with per cent level\naccuracy. Finally, we generalize our approach for two-component baryon and\ndark-matter simulations and demonstrate that the power spectrum evolution is in\nexcellent agreement with linear perturbation theory. For initial baryon density\nfields, it is suggested to use the local Lagrangian approximation in order to\ngenerate a density field for mesh based codes that is consistent with\nLagrangian perturbation theory instead of the current practice of using the\nEulerian linearly scaled densities.'}, {""General relativistic 'screening' in cosmological simulations"": ""We revisit the issue of interpreting the results of large volume cosmological\nsimulations in the context of large scale general relativistic effects. We look\nfor simple modifications to the nonlinear evolution of the gravitational\npotential $\\psi$ that lead on large scales to the correct, fully relativistic\ndescription of density perturbations in the Newtonian gauge. We note that the\nrelativistic constraint equation for $\\psi$ can be cast as a diffusion\nequation, with a diffusion length scale determined by the expansion of the\nUniverse. Exploiting the weak time evolution of $\\psi$ in all regimes of\ninterest, this equation can be further accurately approximated as a Helmholtz\nequation, with an effective relativistic 'screening' scale $\\ell$ related to\nthe Hubble radius. We demonstrate that it is thus possible to carry out N-body\nsimulations in the Newtonian gauge by replacing Poisson's equation with this\nHelmholtz equation, involving a trivial change in the Green's function kernel.\nOur results also motivate a simple, approximate (but very accurate) gauge\ntransformation - $\\delta_{\\rm N}(\\mathbf{k}) \\approx \\delta_{\\rm\nsim}(\\mathbf{k})\\times (k^2+\\ell^{-2})/k^2$ - to convert the density field\n$\\delta_{\\rm sim}$ of standard collisionless N-body simulations (initialised in\nthe comoving synchronous gauge) into the Newtonian gauge density $\\delta_{\\rm\nN}$ at arbitrary times. A similar conversion can also be written in terms of\nparticle positions. Our results can be interpreted in terms of a Jeans\nstability criterion induced by the expansion of the Universe. The appearance of\nthe screening scale $\\ell$ in the evolution of $\\psi$, in particular, leads to\na natural resolution of the 'Jeans swindle' in the presence of super-horizon\nmodes.""}, {'Shell-crossing in a $Λ$CDM Universe': 'Perturbation theory is an indispensable tool for studying the cosmic\nlarge-scale structure, and establishing its limits is therefore of utmost\nimportance. One crucial limitation of perturbation theory is shell-crossing,\nwhich is the instance when cold-dark-matter trajectories intersect for the\nfirst time. We investigate Lagrangian perturbation theory (LPT) at very high\norders in the vicinity of the first shell-crossing for random initial data in a\nrealistic three-dimensional Universe. For this we have numerically implemented\nthe all-order recursion relations for the matter trajectories, from which the\nconvergence of the LPT series at shell-crossing is established. Convergence\nstudies performed up to the 40th order reveal the nature of the\nconvergence-limiting singularities. These singularities are not the well-known\ndensity singularities at shell-crossing but occur at later times when LPT\nalready ceased to provide physically meaningful results.'}, {'Large-Scale Velocity Dispersion and the Cosmic Web': 'Gravitational collapse in cosmological context produces an intricate cosmic\nweb of voids, walls, filaments and nodes. The anisotropic nature of\ncollisionless collapse leads to the emergence of an anisotropic velocity\ndispersion, or stress, that absorbs most of the kinetic energy after\nshell-crossing. In this paper, we measure this large-scale velocity dispersion\ntensor $\\sigma^2_{ij}$ in $N$-body simulations using the phase-space\ninterpolation technique. We study the environmental dependence of the amplitude\nand anisotropy of the velocity dispersion tensor field, and measure its spatial\ncorrelation and alignment. The anisotropy of $\\sigma^2_{ij}$ naturally encodes\nthe collapse history and thus leads to a parameter-free identification of the\nfour dynamically distinct cosmic web components. We find this purely dynamical\nclassification to be in good agreement with some of the existing classification\nmethods. In particular, we demonstrate that $\\sigma^2_{ij}$ is well aligned\nwith the large-scale tidal field. We further investigate the influence of small\nscale density fluctuations on the large scale velocity dispersion, and find\nthat the measured amplitude and alignments are dominated by the largest\nperturbations and thus remain largely unaffected. We anticipate that these\nresults will give important new insight into the anisotropic nature of\ngravitational collapse on large scales, and the emergence of anisotropic stress\nin the cosmic web.'}, {'Renormalization group and UV completion of cosmological perturbations:\n  Gravitational collapse as a critical phenomenon': ""Cosmological perturbation theory is known to converge poorly for predicting\nthe spherical collapse and void evolution of collisionless matter. Using the\nexact parametric solution as a testing ground, we develop two asymptotic\nmethods in spherical symmetry that resolve the gravitational evolution to much\nhigher accuracy than Lagrangian perturbation theory (LPT), which is the current\ngold standard in the literature. One of the methods selects a stable\nfixed-point solution of the renormalization-group flow equation, thereby\npredicting already at the leading order the critical exponent of the phase\ntransition to collapsed structures. The other method completes the truncated\nLPT series far into the UV regime, by adding a non-analytic term that captures\nthe critical nature of the gravitational collapse. We find that the UV method\nmost accurately resolves the evolution of the nonlinear density as well as its\none-point probability distribution function. Similarly accurate predictions are\nachieved with the renormalization-group method, especially when paired with\nPad\\'e approximants. Further, our results yield new, very accurate, formulas to\nrelate linear and nonlinear density contrasts. Finally, we chart possible ways\non how to adapt our methods to the case of cosmological random field initial\nconditions.""}, {'What sets the central structure of dark matter haloes?': 'Dark matter (DM) haloes forming near the thermal cut-off scale of the density\nperturbations are unique, since they are the smallest objects and form through\nmonolithic gravitational collapse, while larger haloes contrastingly have\nexperienced mergers. While standard cold dark matter (CDM) simulations readily\nproduce haloes that follow the universal Navarro-Frenk-White (NFW) density\nprofile with an inner slope, $\\rho \\propto r^{-\\alpha}$, with $\\alpha=1$,\nrecent simulations have found that when the free-streaming cut-off expected for\nthe CDM model is resolved, the resulting haloes follow nearly power-law density\nprofiles of $\\alpha\\sim1.5$. In this paper, we study the formation of density\ncusps in haloes using idealized $N$-body simulations of the collapse of\nproto-haloes. When the proto-halo profile is initially cored due to particle\nfree-streaming at high redshift, we universally find $\\sim r^{-1.5}$ profiles\nirrespective of the proto-halo profile slope outside the core and large-scale\nnon-spherical perturbations. Quite in contrast, when the proto-halo has a\npower-law profile, then we obtain profiles compatible with the NFW shape when\nthe density slope of the proto-halo patch is shallower than a critical value,\n$\\alpha_{\\rm ini} \\sim 0.3$, while the final slope can be steeper for\n$\\alpha_{\\rm ini}\\ga 0.3$. We further demonstrate that the $r^{-1.5}$ profiles\nare sensitive to small scale noise, which gradually drives them towards an\ninner slope of $-1$, where they become resilient to such perturbations. We\ndemonstrate that the $r^{-1.5}$ solutions are in hydrostatic equilibrium,\nlargely consistent with a simple analytic model, and provide arguments that\nangular momentum appears to determine the inner slope.'}, {'Perturbation-theory informed integrators for cosmological simulations': ""Large-scale cosmological simulations are an indispensable tool for modern\ncosmology. To enable model-space exploration, fast and accurate predictions are\ncritical. In this paper, we show that the performance of such simulations can\nbe further improved with time-stepping schemes that use input from cosmological\nperturbation theory. Specifically, we introduce a class of time-stepping\nschemes derived by matching the particle trajectories in a single\nleapfrog/Verlet drift-kick-drift step to those predicted by Lagrangian\nperturbation theory (LPT). As a corollary, these schemes exactly yield the\nanalytic Zel'dovich solution in 1D in the pre-shell-crossing regime (i.e.\nbefore particle trajectories cross). One representative of this class is the\npopular FastPM scheme by Feng et al. 2016, which we take as our baseline. We\nthen construct more powerful LPT-inspired integrators and show that they\noutperform FastPM and standard integrators in fast simulations in two and three\ndimensions with $\\mathcal{O}(1 - 100)$ timesteps, requiring less steps to\naccurately reproduce the power spectrum and bispectrum of the density field.\nFurthermore, we demonstrate analytically and numerically that, for any\nintegrator, convergence is limited in the post-shell-crossing regime (to order\n3/2 for planar wave collapse), owing to the lacking regularity of the\nacceleration field, which makes the use of high-order integrators in this\nregime futile. Also, we study the impact of the timestep spacing and of a\ndecaying mode present in the initial conditions. Importantly, we find that\nsymplecticity of the integrator plays a minor role for fast approximate\nsimulations with a small number of timesteps.""}, {'Cosmological perturbations for two cold fluids in $Λ$CDM': 'The cosmic large-scale structure of our Universe is comprised of baryons and\ncold dark matter (CDM). Yet it is customary to treat these two components as a\ncombined single-matter fluid with vanishing pressure, which is justified only\nfor sufficiently large scales and late times. Here we go beyond the\nsingle-fluid approximation and develop the perturbation theory for two\ngravitationally coupled fluids while still assuming vanishing pressure. We\nmostly focus on perturbative expansions in powers of $D$ (or $D_+$), the linear\nstructure growth of matter in a $\\Lambda$CDM Universe with cosmological\nconstant $\\Lambda$. We derive in particular (1) explicit recursion relations\nfor the two fluid densities, (2) complementary all-order results in the\nLagrangian-coordinates approach, as well as (3) the associated component\nwavefunctions in a semi-classical approach to cosmic large-scale structure. In\nour companion paper (Hahn et al. 2020) we apply these new theoretical results\nto generate novel higher-order initial conditions for cosmological\nhydrodynamical simulations.'}, {'Classical simulation and quantum resource theory of non-Gaussian optics': 'We propose efficient algorithms for classically simulating Gaussian unitaries\nand measurements applied to non-Gaussian initial states. The constructions are\nbased on decomposing the non-Gaussian states into linear combinations of\nGaussian states. We use an extension of the covariance matrix formalism to\nefficiently track relative phases in the superpositions of Gaussian states. We\nget an exact simulation that scales quadratically with the number of Gaussian\nstates required to represent the initial state and an approximate simulation\nalgorithm that scales linearly with the degree. We define measures of\nnon-Gaussianty quantifying this simulation cost, which we call the Gaussian\nrank and the Gaussian extent. From the perspective of quantum resource\ntheories, we investigate the properties of this type of non-Gaussianity measure\nand compute optimal decomposition for states relevant to continuous-variable\nquantum computing.'}]","Title: Enhancing Self-Supervised Feature Representations with PriMaPs-EM for Semantic Segmentation

Abstract

Deep representations achieved through self-supervised learning (SSL), exemplified by DINOViT, are increasingly influential for semantic segmentation tasks. However, the extent to which these representations are harnessed to full potential remains unclear. This paper introduces PriMaPs-EM (Principal Mask Proposals Embedded with Maximization), a novel approach to modify the inherent SSL representations without the need for additional training, embellishing them to enhance subsequent semantic segmentation. 

Our study aims to exploit the intrinsic features of SSL for more effective semantic segmentation. By analyzing the changes in probabilities on predicted masks, our method prioritizes representations that lead to more consistent segmentation outcomes. Ablation studies on several SSL baselines demonstrate that PriMaPs-EM boosts the quality of semantic segmentation without requiring supervisory ground truths. 

PriMaPs-EM is jointly optimized to minimize classification errors on segmentation targets, inducing a more refined representation space. When integrated with PriMaPs and its subsequent iterations, it shows a predictive capability superior to baseline models in unsupervised semantic segmentation tasks like Cityscapes, COCO-Stuff, and Potsdam-3. We observe an average gain in mIoU and accuracy ranging from 17.6% to 53.7% across these datasets. This enhancement is attributed to the sophistication of SSL representations derived from PriMaPs-EM, showcasing its potential for advancing unsupervised semantic segmentation frameworks.

Our contributions include a refined understanding of leveraging self-supervised representations and the practical realization of task-specific improvement through PriMaPs-EM. This framework not only delivers state-of-the-art performance in unsupervised segmentation but also illuminates the potential for regularization to refine existing computational models, optimizing resource allocation in real-world applications that demand efficient deep learning architectures.

In conclusion, PriMaPs-EM provides a practical, efficient solution for augmenting self-supervised representations for better semantic segmentation, potentially facilitating advancements in autonomous systems, augmented reality, and semantic understanding in visual data-driven contexts."
"In this report, we introduce InternVL 1.5, an open-source multimodal large
language model (MLLM) to bridge the capability gap between open-source and
proprietary commercial models in multimodal understanding. We introduce three
simple improvements: (1) Strong Vision Encoder: we explored a continuous
learning strategy for the large-scale vision foundation model -- InternViT-6B,
boosting its visual understanding capabilities, and making it can be
transferred and reused in different LLMs. (2) Dynamic High-Resolution: we
divide images into tiles ranging from 1 to 40 of 448$\times$448 pixels
according to the aspect ratio and resolution of the input images, which
supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we
carefully collected a high-quality bilingual dataset that covers common scenes,
document images, and annotated them with English and Chinese question-answer
pairs, significantly enhancing performance in OCR- and Chinese-related tasks.
We evaluate InternVL 1.5 through a series of benchmarks and comparative
studies. Compared to both open-source and proprietary models, InternVL 1.5
shows competitive performance, achieving state-of-the-art results in 8 of 18
benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.","[{'Machine Ruling': 'Emerging technologies, such as big data, Internet of things, cloud computing,\nmobile Internet, and robotics, breed and expedite new applications and fields.\nIn the mean while, the long-term prosperity and happiness of human race demands\nadvanced technologies. In this paper, the aforementioned emerging technologies\nare applied to management and governance for the long-term prosperity and\nhappiness of human race. The term ""machine ruling"" is coined, introduced, and\njustified. Moreover, the framework and architecture of machine ruling are\nproposed. Enabling technologies and challenges are discussed.'}, {'On the inner products of some Deligne--Lusztig type representations': 'In this paper we introduce a family of Deligne--Lusztig type varieties\nattached to connected reductive groups over quotients of discrete valuation\nrings, naturally generalising the higher Deligne--Lusztig varieties and some\nconstructions related to the algebraisation problem raised by Lusztig. We\nestablish the inner product formula between the representations associated to\nthese varieties and the higher Deligne--Lusztig representations.'}, {'Generic character sheaves on reductive groups over a finite ring': 'In this paper we propose a construction of generic character sheaves on\nreductive groups over finite local rings at even levels, whose characteristic\nfunctions are higher Deligne--Lusztig characters when the parameters are\ngeneric. We formulate a conjecture on the simple perversity of these complexes,\nand we prove it in the level two case (thus generalised a result of Lusztig\nfrom the function field case). We then discuss the induction and restriction\nfunctors, as well as the Frobenius reciprocity, based on the perversity.'}, {'Green functions and higher Deligne--Lusztig characters': ""We give a generalisation of the character formula of Deligne--Lusztig\nrepresentations from the finite field case to the truncated formal power series\ncase. Motivated by this generalisation, we give a definition of Green functions\nfor these local rings, and we prove some basic properties along the lines of\nthe finite field case, like a summation formula. Among the applications we show\nthat the higher Deligne--Lusztig characters and G\\'erardin's characters agree\nat regular semisimple elements. We also derive a generalisation of Braverman\nand Kazhdan's result on gamma functions for Deligne--Lusztig characters, with a\nmore elementary argument.""}, {'A note on cusp forms and representations of\n  $\\mathrm{SL}_2(\\mathbb{F}_p)$': 'Cusp forms are certain holomorphic functions defined on the upper half-plane,\nand the space of cusp forms for the principal congruence subgroup $\\Gamma(p)$,\n$p$ a prime, is acted by $\\mathrm{SL}_2(\\mathbb{F}_p)$. Meanwhile, there is a\nfinite field incarnation of the upper half-plane, the Deligne--Lusztig (or\nDrinfeld) curve, whose cohomology space is also acted by\n$\\mathrm{SL}_2(\\mathbb{F}_p)$. In this note we compute the relation between\nthese two spaces in the weight $2$ case.'}, {'Flags and orbits of connected reductive groups over local rings': 'We prove that generic higher Deligne-Lusztig representations over truncated\nformal power series are non-nilpotent, when the parameters are non-trivial on\nthe biggest reduction kernel of the centre; we also establish a relation\nbetween the orbits of higher Deligne-Lusztig representations of $\\mathrm{SL}_n$\nand of $\\mathrm{GL}_n$. Then we introduce a combinatorial analogue of\nDeligne-Lusztig construction for general and special linear groups over local\nrings; this construction generalises the higher Deligne--Lusztig\nrepresentations and affords all the nilpotent orbit representations, and for\n$\\mathrm{GL}_n$ it also affords all the regular orbit representations as well\nas the invariant characters of the Lie algebra.'}, {'Twisting operators and centralisers of Lie type groups over local rings': 'We extend the classical result asserting that the twisting operator preserves\ncertain Deligne--Lusztig character values for truncated formal power series;\nalong the way we discuss some properties of centralisers. This leads us to the\nconstruction of an action of $\\mathrm{GL}_n(\\mathbb{F}_q[[\\pi]]/\\pi^r)$ on a\nSpringer fibre intersected by Deligne--Lusztig varieties; we determine the\nprimitivities of the induced cohomological representations for single cycles.\nThe case of $\\mathrm{SL}_2$ over finite dual numbers is presented with a\ncriterion on semisimple orbit representations.'}, {'Intersections of Deligne--Lusztig varieties and Springer fibres': ""In this paper we prove a direct geometric relation between Deligne--Lusztig\nvarieties and Springer fibres in type $\\mathsf{A}$: For any rational unipotent\nelement, the Springer fibre cuts out a unique component of a specific\nDeligne--Lusztig variety; moreover, this component forms an open dense subset\nof a component of the Springer fibre. This boils down to a map from the\nunipotent variety to the Weyl group, and combines several constructions with a\ncombinatorial flavour (like Weyr normal forms, Robinson--Schensted\ncorrespondence, and Spaltenstein's and Steinberg's labellings); it also\nprovides a geometric interpretation of a classical dimension formula of\nunipotent centralisers.""}, {'On a stability of higher level Coxeter unipotent representations': 'Let $\\mathbb{G}$ be a connected reductive group over $\\mathcal{O}$, a\ncomplete discrete valuation ring with finite residue field $\\mathbb{F}_q$. Let\n$R_{T_r,U_r}^{\\theta}$ be a level $r$ Deligne--Lusztig representation of\n$\\mathbb{G}(\\mathcal{O})$. We show that, if $q$ is not small, the Coxeter\nunipotent $R_{T_r,U_r}^1$ degenerates to the $r=1$ case. For\n$\\mathbb{G}=\\mathrm{GL}_2$ (or $\\mathrm{SL}_2$), as an application we give the\ndimensions and decompositions of all Coxeter $R_{T_r,U_r}^{\\theta}$. For\ngeneral $\\mathbb{G}$ we state a conjectural sign formula for\n$R_{T_r,U_r}^{\\theta}$.'}, {'Heuristic Reasoning on Graph and Game Complexity of Sudoku': 'The Sudoku puzzle has achieved worldwide popularity recently, and attracted\ngreat attention of the computational intelligence community. Sudoku is always\nconsidered as Satisfiability Problem or Constraint Satisfaction Problem. In\nthis paper, we propose to focus on the essential graph structure underlying the\nSudoku puzzle. First, we formalize Sudoku as a graph. Then a solving algorithm\nbased on heuristic reasoning on the graph is proposed. The related r-Reduction\ntheorem, inference theorem and their properties are proved, providing the\nformal basis for developments of Sudoku solving systems. In order to evaluate\nthe difficulty levels of puzzles, a quantitative measurement of the complexity\nlevel of Sudoku puzzles based on the graph structure and information theory is\nproposed. Experimental results show that all the puzzles can be solved fast\nusing the proposed heuristic reasoning, and that the proposed game complexity\nmetrics can discriminate difficulty levels of puzzles perfectly.'}]","**Abstract**

The rapid advancement of large language models (LLMs) and multimodal large language models (MLLMs) has revolutionized the landscape of artificial general intelligence (AGI), enabling sophisticated language tasks that bridge textual and visual data. However, a significant gap between proprietary commercial models and open-source MLLMs persists, marked in parameter scale, image resolution, and multilingual capability. To address this, we introduce InternVL 1.5, featuring an improved discrete learning approach and large-scale vision foundation model (VFMs) - InternViT-6B.

The primary goal of our research is to enhance multimodal capabilities in AGI through continuous comprehensive refinement of InternVL’s architecture. We highlight the discrepancies in the parameter scale between leading proprietary models like GPT-4V with 100 billion parameters, and open-source models typically equipped with 6 billion parameter VFM as integral components, juxtaposed against InternVL 1.5 with its wider VFM.

InternVL 1.5 employs a discrete learning process streamlined for high-throughput and scalability. Our innovations include training the VFM with dynamic resolution, unstable sampling for diverse scenarios, and continuous learning to adapt to complex multimodal tasks. Results show significant enhancements, with the most noticeable gains being in OCR tasks, multi-turn dialogues, mathematical problem-solving, and scientific diagram interpretations.

In terms of contributions, InternVL 1.5 bridges the gap between commercial and open-source MLLMs, improving performance in 8 out of 18 multimodal benchmarks over leading proprietary models. The open-source implementation of our system is expected to accelerate research in multimodal AI and enhance real-world application scenarios.

With implications spanning from augmenting human language interactions with visual data in personalized assistants to advancing visual analysis in scientific research, InternVL 1.5 considerably extends the reach of artificial intelligence into open-world applications, contributing to the broader AGI community."
"Vision-language pre-training has significantly elevated performance across a
wide range of image-language applications. Yet, the pre-training process for
video-related tasks demands exceptionally large computational and data
resources, which hinders the progress of video-language models. This paper
investigates a straight-forward, highly efficient, and resource-light approach
to adapting an existing image-language pre-trained model for dense video
understanding. Our preliminary experiments reveal that directly fine-tuning
pre-trained image-language models with multiple frames as inputs on video
datasets leads to performance saturation or even a drop. Our further
investigation reveals that it is largely attributed to the bias of learned
high-norm visual features. Motivated by this finding, we propose a simple but
effective pooling strategy to smooth the feature distribution along the
temporal dimension and thus reduce the dominant impacts from the extreme
features. The new model is termed Pooling LLaVA, or PLLaVA in short. PLLaVA
achieves new state-of-the-art performance on modern benchmark datasets for both
video question-answer and captioning tasks. Notably, on the recent popular
VideoChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average of
five evaluated dimensions, exceeding the previous SOTA results from GPT4V
(IG-VLM) by 9%. On the latest multi-choice benchmark MVBench, PLLaVA achieves
58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V
(IG-VLM). Code is available at https://pllava.github.io/","[{'A Preliminary Study on Optimal Placement of Cameras': 'This paper primarily focuses on figuring out the best array of cameras, or\nvisual sensors, so that such a placement enables the maximum utilization of\nthese visual sensors. Maximizing the utilization of these cameras can convert\nto another problem that is simpler for the formulation, that is, maximizing the\ntotal coverage with these cameras. To solve the problem, the coverage problem\nis first defined subject to the capabilities and limits of cameras. Then, poses\nof cameras are analyzed for the best arrangement.'}, {'Greedy metrics in orthogonal greedy learning': 'Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds a\nnew atom from a dictionary via the steepest gradient descent and build the\nestimator via orthogonal projecting the target function to the space spanned by\nthe selected atoms in each greedy step. Here, ""greed"" means choosing a new atom\naccording to the steepest gradient descent principle. OGL then avoids the\noverfitting/underfitting by selecting an appropriate iteration number. In this\npaper, we point out that the overfitting/underfitting can also be avoided via\nredefining ""greed"" in OGL. To this end, we introduce a new greedy metric,\ncalled $\\delta$-greedy thresholds, to refine ""greed"" and theoretically verifies\nits feasibility. Furthermore, we reveals that such a greedy metric can bring an\nadaptive termination rule on the premise of maintaining the prominent learning\nperformance of OGL. Our results show that the steepest gradient descent is not\nthe unique greedy metric of OGL and some other more suitable metric may lessen\nthe hassle of model-selection of OGL.'}, {'Re-scale boosting for regression and classification': 'Boosting is a learning scheme that combines weak prediction rules to produce\na strong composite estimator, with the underlying intuition that one can obtain\naccurate prediction rules by combining ""rough"" ones. Although boosting is\nproved to be consistent and overfitting-resistant, its numerical convergence\nrate is relatively slow. The aim of this paper is to develop a new boosting\nstrategy, called the re-scale boosting (RBoosting), to accelerate the numerical\nconvergence rate and, consequently, improve the learning performance of\nboosting. Our studies show that RBoosting possesses the almost optimal\nnumerical convergence rate in the sense that, up to a logarithmic factor, it\ncan reach the minimax nonlinear approximation rate. We then use RBoosting to\ntackle both the classification and regression problems, and deduce a tight\ngeneralization error estimate. The theoretical and experimental results show\nthat RBoosting outperforms boosting in terms of generalization.'}, {'Greedy Criterion in Orthogonal Greedy Learning': 'Orthogonal greedy learning (OGL) is a stepwise learning scheme that starts\nwith selecting a new atom from a specified dictionary via the steepest gradient\ndescent (SGD) and then builds the estimator through orthogonal projection. In\nthis paper, we find that SGD is not the unique greedy criterion and introduce a\nnew greedy criterion, called ""$\\delta$-greedy threshold"" for learning. Based on\nthe new greedy criterion, we derive an adaptive termination rule for OGL. Our\ntheoretical study shows that the new learning scheme can achieve the existing\n(almost) optimal learning rate of OGL. Plenty of numerical experiments are\nprovided to support that the new scheme can achieve almost optimal\ngeneralization performance, while requiring less computation than OGL.'}, {'Shrinkage degree in $L_2$-re-scale boosting for regression': 'Re-scale boosting (RBoosting) is a variant of boosting which can essentially\nimprove the generalization performance of boosting learning. The key feature of\nRBoosting lies in introducing a shrinkage degree to re-scale the ensemble\nestimate in each gradient-descent step. Thus, the shrinkage degree determines\nthe performance of RBoosting.\n  The aim of this paper is to develop a concrete analysis concerning how to\ndetermine the shrinkage degree in $L_2$-RBoosting. We propose two feasible ways\nto select the shrinkage degree. The first one is to parameterize the shrinkage\ndegree and the other one is to develope a data-driven approach of it. After\nrigorously analyzing the importance of the shrinkage degree in $L_2$-RBoosting\nlearning, we compare the pros and cons of the proposed methods. We find that\nalthough these approaches can reach the same learning rates, the structure of\nthe final estimate of the parameterized approach is better, which sometimes\nyields a better generalization capability when the number of sample is finite.\nWith this, we recommend to parameterize the shrinkage degree of\n$L_2$-RBoosting. To this end, we present an adaptive parameter-selection\nstrategy for shrinkage degree and verify its feasibility through both\ntheoretical analysis and numerical verification.\n  The obtained results enhance the understanding of RBoosting and further give\nguidance on how to use $L_2$-RBoosting for regression tasks.'}, {'Unifying Relational Sentence Generation and Retrieval for Medical Image\n  Report Composition': 'Beyond generating long and topic-coherent paragraphs in traditional\ncaptioning tasks, the medical image report composition task poses more\ntask-oriented challenges by requiring both the highly-accurate medical term\ndiagnosis and multiple heterogeneous forms of information including impression\nand findings. Current methods often generate the most common sentences due to\ndataset bias for individual case, regardless of whether the sentences properly\ncapture key entities and relationships. Such limitations severely hinder their\napplicability and generalization capability in medical report composition where\nthe most critical sentences lie in the descriptions of abnormal diseases that\nare relatively rare. Moreover, some medical terms appearing in one report are\noften entangled with each other and co-occurred, e.g. symptoms associated with\na specific disease. To enforce the semantic consistency of medical terms to be\nincorporated into the final reports and encourage the sentence generation for\nrare abnormal descriptions, we propose a novel framework that unifies template\nretrieval and sentence generation to handle both common and rare abnormality\nwhile ensuring the semantic-coherency among the detected medical terms.\nSpecifically, our approach exploits hybrid-knowledge co-reasoning: i) explicit\nrelationships among all abnormal medical terms to induce the visual attention\nlearning and topic representation encoding for better topic-oriented symptoms\ndescriptions; ii) adaptive generation mode that changes between the template\nretrieval and sentence generation according to a contextual topic encoder.\nExperimental results on two medical report benchmarks demonstrate the\nsuperiority of the proposed framework in terms of both human and metrics\nevaluation.'}, {'Transformation devices with optical nihility media and reduced\n  realizations': ""Starting from optical nihility media (ONM), we design several intriguing\ndevices with transformation optics method in two dimensions, such as a wave\nsplitter, a concave lens, a field rotator, a concentrator and an invisibility\ncloak. The extreme anisotropic property of ONM hinders the fabrication of these\ndevices, which could be effectively realized by simplified materials with\nFabry-P\\'erot resonances (FPs) at discrete frequencies. Moreover, we propose a\nreduced version of simplified materials with FPs to construct a concentrator\nand a rotator, which is feasible in experimental fabrications. The simulations\nof total scattering cross sections confirm their functionalities.""}, {'Detection and Classification of Breast Cancer Metastates Based on U-Net': 'This paper presents U-net based breast cancer metastases detection and\nclassification in lymph nodes, as well as patient-level classification based on\nmetastases detection. The whole pipeline can be divided into five steps:\npreprocessing and data argumentation, patch-based segmentation, post\nprocessing, slide-level classification, and patient-level classification. In\norder to reduce overfitting and speedup convergence, we applied batch\nnormalization and dropout into U-Net. The final Kappa score reaches 0.902 on\ntraining data.'}, {'End-to-End Knowledge-Routed Relational Dialogue System for Automatic\n  Diagnosis': 'Beyond current conversational chatbots or task-oriented dialogue systems that\nhave attracted increasing attention, we move forward to develop a dialogue\nsystem for automatic medical diagnosis that converses with patients to collect\nadditional symptoms beyond their self-reports and automatically makes a\ndiagnosis. Besides the challenges for conversational dialogue systems (e.g.\ntopic transition coherency and question understanding), automatic medical\ndiagnosis further poses more critical requirements for the dialogue rationality\nin the context of medical knowledge and symptom-disease relations. Existing\ndialogue systems (Madotto, Wu, and Fung 2018; Wei et al. 2018; Li et al. 2017)\nmostly rely on data-driven learning and cannot be able to encode extra expert\nknowledge graph. In this work, we propose an End-to-End Knowledge-routed\nRelational Dialogue System (KR-DS) that seamlessly incorporates rich medical\nknowledge graph into the topic transition in dialogue management, and makes it\ncooperative with natural language understanding and natural language\ngeneration. A novel Knowledge-routed Deep Q-network (KR-DQN) is introduced to\nmanage topic transitions, which integrates a relational refinement branch for\nencoding relations among different symptoms and symptom-disease pairs, and a\nknowledge-routed graph branch for topic decision-making. Extensive experiments\non a public medical dialogue dataset show our KR-DS significantly beats\nstate-of-the-art methods (by more than 8% in diagnosis accuracy). We further\nshow the superiority of our KR-DS on a newly collected medical dialogue system\ndataset, which is more challenging retaining original self-reports and\nconversational data between patients and doctors.'}, {'Re-scale AdaBoost for Attack Detection in Collaborative Filtering\n  Recommender Systems': 'Collaborative filtering recommender systems (CFRSs) are the key components of\nsuccessful e-commerce systems. Actually, CFRSs are highly vulnerable to attacks\nsince its openness. However, since attack size is far smaller than that of\ngenuine users, conventional supervised learning based detection methods could\nbe too ""dull"" to handle such imbalanced classification. In this paper, we\nimprove detection performance from following two aspects. First, we extract\nwell-designed features from user profiles based on the statistical properties\nof the diverse attack models, making hard classification task becomes easier to\nperform. Then, refer to the general idea of re-scale Boosting (RBoosting) and\nAdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost\n(RAdaBoost) as our detection method based on extracted features. RAdaBoost is\ncomparable to the optimal Boosting-type algorithm and can effectively improve\nthe performance in some hard scenarios. Finally, a series of experiments on the\nMovieLens-100K data set are conducted to demonstrate the outperformance of\nRAdaBoost comparing with some classical techniques such as SVM, kNN and\nAdaBoost.'}]","Title: PLLaVA: Integrating LLarge Language Models into the Video Domain through Adaptive Pooling Strategies

Abstract:
In the era of AI-driven media consumption, understanding and generating text from video content is pivotal for enhancing user experience and accessibility. Traditional LLarge Language Models (LLMs) are adept at language tasks but falter when applied directly to video data. We address this gap by introducing PLLaVA, a method bridging the domain of video understanding and LLMs through effective pooling strategies. 

**Background**: 
Concurrent advancements in large-scale language processing and video analysis open new dimensions in multimedia content interaction but challenge how to seamlessly integrate these components.

**Objective**: 
The objective is to develop a method that allows LLMs to leverage video information effectively by employing adaptive pooling that mitigates representation complexities and faciliates knowledge alignment.

**Innovations**: 
PLLaVA innovates by introducing adaptive pooling mechanisms for dimensionality reduction without losing essential information. It connects the video content to language models through a context-aware projection layer, enabling robust understanding and generation across various video datasets.

**Methods**: 
Our method involves processing videos using Vision Transformer (ViT) for feature extraction, projecting video features into a compatible space with context aware embeddings, and fusing context with the pooled video features before feeding them into the LLM for generation. LoRA, a lightweight model adaptation technique, is used to refine the LLM's performance on video-based tasks.

**Results**: 
PLLaVA outperforms existing architectures, demonstrating superior performance on video QA, captioning, and other retrieval tasks compared to baselines like VideoChatGPT and IG-VLMs. Notably, it exhibits improved understanding of temporal dynamics and context in videos across different tasks.

**Contributions**: 
The primary contribution is a versatile and efficient framework that integrates video analysis with text generation functionalities within LLMs. This accelerates the development of sophisticated multimedia applications and furthers the democratization of AI for diverse users.

**Applications**: 
PLLaVA finds applications in personalized content recommendation, real-time video summarization, AI-assisted educational content creation, enhancing video search engines, and developing interactive video experiences across digital platforms. This advances the field of multimodal AI with significant implications for user engagement and personalized service delivery in multimedia content consumption and creation."
"We introduce the FRactional-Order graph Neural Dynamical network (FROND), a
new continuous graph neural network (GNN) framework. Unlike traditional
continuous GNNs that rely on integer-order differential equations, FROND
employs the Caputo fractional derivative to leverage the non-local properties
of fractional calculus. This approach enables the capture of long-term
dependencies in feature updates, moving beyond the Markovian update mechanisms
in conventional integer-order models and offering enhanced capabilities in
graph representation learning. We offer an interpretation of the node feature
updating process in FROND from a non-Markovian random walk perspective when the
feature updating is particularly governed by a diffusion process. We
demonstrate analytically that oversmoothing can be mitigated in this setting.
Experimentally, we validate the FROND framework by comparing the fractional
adaptations of various established integer-order continuous GNNs, demonstrating
their consistently improved performance and underscoring the framework's
potential as an effective extension to enhance traditional continuous GNNs. The
code is available at \url{https://github.com/zknus/ICLR2024-FROND}.","[{'Sequential Multi-Class Labeling in Crowdsourcing': ""We consider a crowdsourcing platform where workers' responses to questions\nposed by a crowdsourcer are used to determine the hidden state of a multi-class\nlabeling problem. As workers may be unreliable, we propose to perform\nsequential questioning in which the questions posed to the workers are designed\nbased on previous questions and answers. We propose a Partially-Observable\nMarkov Decision Process (POMDP) framework to determine the best questioning\nstrategy, subject to the crowdsourcer's budget constraint. As this POMDP\nformulation is in general intractable, we develop a suboptimal approach based\non a $q$-ary Ulam-R\\'enyi game. We also propose a sampling heuristic, which can\nbe used in tandem with standard POMDP solvers, using our Ulam-R\\'enyi strategy.\nWe demonstrate through simulations that our approaches outperform a\nnon-sequential strategy based on error correction coding and which does not\nutilize workers' previous responses.""}, {'Learning Orthogonal Projections in Linear Bandits': ""In a linear stochastic bandit model, each arm is a vector in an Euclidean\nspace and the observed return at each time step is an unknown linear function\nof the chosen arm at that time step. In this paper, we investigate the problem\nof learning the best arm in a linear stochastic bandit model, where each arm's\nexpected reward is an unknown linear function of the projection of the arm onto\na subspace. We call this the projection reward. Unlike the classical linear\nbandit problem in which the observed return corresponds to the reward, the\nprojection reward at each time step is unobservable. Such a model is useful in\nrecommendation applications where the observed return includes corruption by\neach individual's biases, which we wish to exclude in the learned model. In the\ncase where there are finitely many arms, we develop a strategy to achieve\n$O(|\\bbD|\\log n)$ regret, where $n$ is the number of time steps and $|\\bbD|$ is\nthe number of arms. In the case where each arm is chosen from an infinite\ncompact set, our strategy achieves $O(n^{2/3}(\\log{n})^{1/2})$ regret.\nExperiments verify the efficiency of our strategy.""}, {'Task Recommendation in Crowdsourcing Based on Learning Preferences and\n  Reliabilities': ""Workers participating in a crowdsourcing platform can have a wide range of\nabilities and interests. An important problem in crowdsourcing is the task\nrecommendation problem, in which tasks that best match a particular worker's\npreferences and reliabilities are recommended to that worker. A task\nrecommendation scheme that assigns tasks more likely to be accepted by a worker\nwho is more likely to complete it reliably results in better performance for\nthe task requester. Without prior information about a worker, his preferences\nand reliabilities need to be learned over time. In this paper, we propose a\nmulti-armed bandit (MAB) framework to learn a worker's preferences and his\nreliabilities for different categories of tasks. However, unlike the classical\nMAB problem, the reward from the worker's completion of a task is unobservable.\nWe therefore include the use of gold tasks (i.e., tasks whose solutions are\nknown \\emph{a priori} and which do not produce any rewards) in our task\nrecommendation procedure. Our model could be viewed as a new variant of MAB, in\nwhich the random rewards can only be observed at those time steps where gold\ntasks are used, and the accuracy of estimating the expected reward of\nrecommending a task to a worker depends on the number of gold tasks used. We\nshow that the optimal regret is $O(\\sqrt{n})$, where $n$ is the number of tasks\nrecommended to the worker. We develop three task recommendation strategies to\ndetermine the number of gold tasks for different task categories, and show that\nthey are order optimal. Simulations verify the efficiency of our approaches.""}, {'Error-Correcting Output Codes with Ensemble Diversity for Robust\n  Learning in Neural Networks': 'Though deep learning has been applied successfully in many scenarios,\nmalicious inputs with human-imperceptible perturbations can make it vulnerable\nin real applications. This paper proposes an error-correcting neural network\n(ECNN) that combines a set of binary classifiers to combat adversarial examples\nin the multi-class classification problem. To build an ECNN, we propose to\ndesign a code matrix so that the minimum Hamming distance between any two rows\n(i.e., two codewords) and the minimum shared information distance between any\ntwo columns (i.e., two partitions of class labels) are simultaneously\nmaximized. Maximizing row distances can increase the system fault tolerance\nwhile maximizing column distances helps increase the diversity between binary\nclassifiers. We propose an end-to-end training method for our ECNN, which\nallows further improvement of the diversity between binary classifiers. The\nend-to-end training renders our proposed ECNN different from the traditional\nerror-correcting output code (ECOC) based methods that train binary classifiers\nindependently. ECNN is complementary to other existing defense approaches such\nas adversarial training and can be applied in conjunction with them. We\nempirically demonstrate that our proposed ECNN is effective against the\nstate-of-the-art white-box and black-box attacks on several datasets while\nmaintaining good classification accuracy on normal examples.'}, {'On the Robustness of Graph Neural Diffusion to Topology Perturbations': 'Neural diffusion on graphs is a novel class of graph neural networks that has\nattracted increasing attention recently. The capability of graph neural partial\ndifferential equations (PDEs) in addressing common hurdles of graph neural\nnetworks (GNNs), such as the problems of over-smoothing and bottlenecks, has\nbeen investigated but not their robustness to adversarial attacks. In this\nwork, we explore the robustness properties of graph neural PDEs. We empirically\ndemonstrate that graph neural PDEs are intrinsically more robust against\ntopology perturbation as compared to other GNNs. We provide insights into this\nphenomenon by exploiting the stability of the heat semigroup under graph\ntopology perturbations. We discuss various graph diffusion operators and relate\nthem to existing graph neural PDEs. Furthermore, we propose a general graph\nneural PDE framework based on which a new class of robust GNNs can be defined.\nWe verify that the new model achieves comparable state-of-the-art performance\non several benchmark datasets.'}, {'Node Embedding from Neural Hamiltonian Orbits in Graph Neural Networks': 'In the graph node embedding problem, embedding spaces can vary significantly\nfor different data types, leading to the need for different GNN model types. In\nthis paper, we model the embedding update of a node feature as a Hamiltonian\norbit over time. Since the Hamiltonian orbits generalize the exponential maps,\nthis approach allows us to learn the underlying manifold of the graph in\ntraining, in contrast to most of the existing literature that assumes a fixed\ngraph embedding manifold with a closed exponential map solution. Our proposed\nnode embedding strategy can automatically learn, without extensive tuning, the\nunderlying geometry of any given graph dataset even if it has diverse\ngeometries. We test Hamiltonian functions of different forms and verify the\nperformance of our approach on two graph node embedding downstream tasks: node\nclassification and link prediction. Numerical experiments demonstrate that our\napproach adapts better to different types of graph datasets than popular\nstate-of-the-art graph node embedding GNNs. The code is available at\n\\url{https://github.com/zknus/Hamiltonian-GNN}.'}, {'Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending\n  Against Adversarial Attacks': ""Deep neural networks (DNNs) are well-known to be vulnerable to adversarial\nattacks, where malicious human-imperceptible perturbations are included in the\ninput to the deep network to fool it into making a wrong classification. Recent\nstudies have demonstrated that neural Ordinary Differential Equations (ODEs)\nare intrinsically more robust against adversarial attacks compared to vanilla\nDNNs. In this work, we propose a stable neural ODE with Lyapunov-stable\nequilibrium points for defending against adversarial attacks (SODEF). By\nensuring that the equilibrium points of the ODE solution used as part of SODEF\nis Lyapunov-stable, the ODE solution for an input with a small perturbation\nconverges to the same solution as the unperturbed input. We provide theoretical\nresults that give insights into the stability of SODEF as well as the choice of\nregularizers to ensure its stability. Our analysis suggests that our proposed\nregularizers force the extracted feature points to be within a neighborhood of\nthe Lyapunov-stable equilibrium points of the ODE. SODEF is compatible with\nmany defense methods and can be applied to any neural network's final regressor\nlayer to enhance its stability against adversarial attacks.""}, {'Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach': 'Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nincluding those that affect both node features and graph topology. This paper\ninvestigates GNNs derived from diverse neural flows, concentrating on their\nconnection to various stability notions such as BIBO stability, Lyapunov\nstability, structural stability, and conservative stability. We argue that\nLyapunov stability, despite its common use, does not necessarily ensure\nadversarial robustness. Inspired by physics principles, we advocate for the use\nof conservative Hamiltonian neural flows to construct GNNs that are robust to\nadversarial attacks. The adversarial robustness of different neural flow GNNs\nis empirically compared on several benchmark datasets under a variety of\nadversarial attacks. Extensive numerical experiments demonstrate that GNNs\nleveraging conservative Hamiltonian flows with Lyapunov stability substantially\nimprove robustness against adversarial perturbations. The implementation code\nof experiments is available at\nhttps://github.com/zknus/NeurIPS-2023-HANG-Robustness.'}, {'Building Facade Parsing R-CNN': 'Building facade parsing, which predicts pixel-level labels for building\nfacades, has applications in computer vision perception for autonomous vehicle\n(AV) driving. However, instead of a frontal view, an on-board camera of an AV\ncaptures a deformed view of the facade of the buildings on both sides of the\nroad the AV is travelling on, due to the camera perspective. We propose Facade\nR-CNN, which includes a transconv module, generalized bounding box detection,\nand convex regularization, to perform parsing of deformed facade views.\nExperiments demonstrate that Facade R-CNN achieves better performance than the\ncurrent state-of-the-art facade parsing models, which are primarily developed\nfor frontal views. We also publish a new building facade parsing dataset\nderived from the Oxford RobotCar dataset, which we call the Oxford RobotCar\nFacade dataset. This dataset contains 500 street-view images from the Oxford\nRobotCar dataset augmented with accurate annotations of building facade\nobjects. The published dataset is available at\nhttps://github.com/sijieaaa/Oxford-RobotCar-Facade'}, {'RobustLoc: Robust Camera Pose Regression in Challenging Driving\n  Environments': 'Camera relocalization has various applications in autonomous driving.\nPrevious camera pose regression models consider only ideal scenarios where\nthere is little environmental perturbation. To deal with challenging driving\nenvironments that may have changing seasons, weather, illumination, and the\npresence of unstable objects, we propose RobustLoc, which derives its\nrobustness against perturbations from neural differential equations. Our model\nuses a convolutional neural network to extract feature maps from multi-view\nimages, a robust neural differential equation diffusion block module to diffuse\ninformation interactively, and a branched pose decoder with multi-layer\ntraining to estimate the vehicle poses. Experiments demonstrate that RobustLoc\nsurpasses current state-of-the-art camera pose regression models and achieves\nrobust performance in various environments. Our code is released at:\nhttps://github.com/sijieaaa/RobustLoc'}]","Abstract:

In the realm of graph neural networks (GNNs), continuous models based on integer-order differential equations have proven effective across various domains, such as chemistry, finance, and social media. However, ongoing research on continuous GNNs faces challenges, particularly in enhancing robustness and addressing heterophilic graphs, where connections are weighed more heavily for edges linking dissimilar nodes. This paper introduces Fractional-Order Graph Neural Dynamical Networks (FROND), a novel class of continuous GNNs that leverages fractional-order differential operators and incorporates continuous dynamical processes for improved message-passing schemes. 

The primary goal of this investigation is to advance the field by developing the fractional-order GRAND (F-GRAND) network, a theoretically grounded approach that incorporates memory effects in node representations. Counteracting the challenges typical of time-scale aggregation and oversmoothing, F-GRAND employs optimized, memory-aware differential operators designed to mitigate these issues and enhance learning capabilities across both homophilic and heterophilic datasets. 

Time-varying attention mechanisms are proposed to facilitate the selective information flow for each node's characteristics, while selective rewiring techniques are integrated to enhance model robustness and tailor dynamic relationships among nodes. Experiments conducted on diverse datasets demonstrate F-GRAND's superior performance compared to leading integer-order models and existing fractional-order approaches like GRAND and HGCN. 

Moreover, this work introduces FROND, a comprehensive framework encompassing a suite of fractional-order continuous GNN models, and accounts for graph rewiring capabilities, extending the model's applicability across various domains. The findings highlight F-GRAND's contributions to the robustness, memory and graph rewiring capabilities of continuous GNNs, thereby advancing their potential for intricate graph processing tasks.

This study is particularly relevant for applications such as social network analysis, recommendation systems, and biological network modeling, where understanding complex interactions between nodes and capturing long-term dependencies within the graph structure is crucial. The use of fractional-order differential operations in modeling newly-challenging heterophilic graphs guts the potential of FROND to disrupt the current status quo in graph neural network research."
"Large language models (LLMs) exhibit excellent ability to understand human
languages, but do they also understand their own language that appears
gibberish to us? In this work we delve into this question, aiming to uncover
the mechanisms underlying such behavior in LLMs. We employ the Greedy
Coordinate Gradient optimizer to craft prompts that compel LLMs to generate
coherent responses from seemingly nonsensical inputs. We call these inputs LM
Babel and this work systematically studies the behavior of LLMs manipulated by
these prompts. We find that the manipulation efficiency depends on the target
text's length and perplexity, with the Babel prompts often located in lower
loss minima compared to natural prompts. We further examine the structure of
the Babel prompts and evaluate their robustness. Notably, we find that guiding
the model to generate harmful texts is not more difficult than into generating
benign texts, suggesting lack of alignment for out-of-distribution prompts.","[{'LowKey: Leveraging Adversarial Attacks to Protect Social Media Users\n  from Facial Recognition': 'Facial recognition systems are increasingly deployed by private corporations,\ngovernment agencies, and contractors for consumer services and mass\nsurveillance programs alike. These systems are typically built by scraping\nsocial media profiles for user images. Adversarial perturbations have been\nproposed for bypassing facial recognition systems. However, existing methods\nfail on full-scale systems and commercial APIs. We develop our own adversarial\nfilter that accounts for the entire image processing pipeline and is\ndemonstrably effective against industrial-grade pipelines that include face\ndetection and large scale databases. Additionally, we release an easy-to-use\nwebtool that significantly degrades the accuracy of Amazon Rekognition and the\nMicrosoft Azure Face Recognition API, reducing the accuracy of each to below\n1%.'}, {'A Deep Dive into Dataset Imbalance and Bias in Face Identification': ""As the deployment of automated face recognition (FR) systems proliferates,\nbias in these systems is not just an academic question, but a matter of public\nconcern. Media portrayals often center imbalance as the main source of bias,\ni.e., that FR models perform worse on images of non-white people or women\nbecause these demographic groups are underrepresented in training data. Recent\nacademic research paints a more nuanced picture of this relationship. However,\nprevious studies of data imbalance in FR have focused exclusively on the face\nverification setting, while the face identification setting has been largely\nignored, despite being deployed in sensitive applications such as law\nenforcement. This is an unfortunate omission, as 'imbalance' is a more complex\nmatter in identification; imbalance may arise in not only the training data,\nbut also the testing data, and furthermore may affect the proportion of\nidentities belonging to each demographic group or the number of images\nbelonging to each identity. In this work, we address this gap in the research\nby thoroughly exploring the effects of each kind of imbalance possible in face\nidentification, and discuss other factors which may impact bias in this\nsetting.""}, {'Transfer Learning with Deep Tabular Models': 'Recent work on deep learning for tabular data demonstrates the strong\nperformance of deep tabular models, often bridging the gap between gradient\nboosted decision trees and neural networks. Accuracy aside, a major advantage\nof neural models is that they learn reusable features and are easily fine-tuned\nin new domains. This property is often exploited in computer vision and natural\nlanguage applications, where transfer learning is indispensable when\ntask-specific training data is scarce. In this work, we demonstrate that\nupstream data gives tabular neural networks a decisive advantage over widely\nused GBDT models. We propose a realistic medical diagnosis benchmark for\ntabular transfer learning, and we present a how-to guide for using upstream\ndata to boost performance with a variety of tabular neural network\narchitectures. Finally, we propose a pseudo-feature method for cases where the\nupstream and downstream feature sets differ, a tabular-specific problem\nwidespread in real-world applications. Our code is available at\nhttps://github.com/LevinRoman/tabular-transfer-learning .'}, {'A Performance-Driven Benchmark for Feature Selection in Tabular Deep\n  Learning': 'Academic tabular benchmarks often contain small sets of curated features. In\ncontrast, data scientists typically collect as many features as possible into\ntheir datasets, and even engineer new features from existing ones. To prevent\noverfitting in subsequent downstream modeling, practitioners commonly use\nautomated feature selection methods that identify a reduced subset of\ninformative features. Existing benchmarks for tabular feature selection\nconsider classical downstream models, toy synthetic datasets, or do not\nevaluate feature selectors on the basis of downstream performance. Motivated by\nthe increasing popularity of tabular deep learning, we construct a challenging\nfeature selection benchmark evaluated on downstream neural networks including\ntransformers, using real datasets and multiple methods for generating\nextraneous features. We also propose an input-gradient-based analogue of Lasso\nfor neural networks that outperforms classical feature selection methods on\nchallenging problems such as selecting from corrupted or second-order features.'}, {'Unraveling Meta-Learning: Understanding Feature Representations for\n  Few-Shot Tasks': 'Meta-learning algorithms produce feature extractors which achieve\nstate-of-the-art performance on few-shot classification. While the literature\nis rich with meta-learning methods, little is known about why the resulting\nfeature extractors perform so well. We develop a better understanding of the\nunderlying mechanics of meta-learning and the difference between models trained\nusing meta-learning and models which are trained classically. In doing so, we\nintroduce and verify several hypotheses for why meta-learned models perform\nbetter. Furthermore, we develop a regularizer which boosts the performance of\nstandard training routines for few-shot classification. In many cases, our\nroutine outperforms meta-learning while simultaneously running an order of\nmagnitude faster.'}, {'Technical Challenges for Training Fair Neural Networks': 'As machine learning algorithms have been widely deployed across applications,\nmany concerns have been raised over the fairness of their predictions,\nespecially in high stakes settings (such as facial recognition and medical\nimaging). To respond to these concerns, the community has proposed and\nformalized various notions of fairness as well as methods for rectifying unfair\nbehavior. While fairness constraints have been studied extensively for\nclassical models, the effectiveness of methods for imposing fairness on deep\nneural networks is unclear. In this paper, we observe that these large models\noverfit to fairness objectives, and produce a range of unintended and\nundesirable consequences. We conduct our experiments on both facial recognition\nand automated medical diagnosis datasets using state-of-the-art architectures.'}, {'MetaBalance: High-Performance Neural Networks for Class-Imbalanced Data': ""Class-imbalanced data, in which some classes contain far more samples than\nothers, is ubiquitous in real-world applications. Standard techniques for\nhandling class-imbalance usually work by training on a re-weighted loss or on\nre-balanced data. Unfortunately, training overparameterized neural networks on\nsuch objectives causes rapid memorization of minority class data. To avoid this\ntrap, we harness meta-learning, which uses both an ''outer-loop'' and an\n''inner-loop'' loss, each of which may be balanced using different strategies.\nWe evaluate our method, MetaBalance, on image classification, credit-card fraud\ndetection, loan default prediction, and facial recognition tasks with severely\nimbalanced data, and we find that MetaBalance outperforms a wide array of\npopular re-sampling strategies.""}, {'Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks\n  Without an Accuracy Tradeoff': 'Data poisoning and backdoor attacks manipulate victim models by maliciously\nmodifying training data. In light of this growing threat, a recent survey of\nindustry professionals revealed heightened fear in the private sector regarding\ndata poisoning. Many previous defenses against poisoning either fail in the\nface of increasingly strong attacks, or they significantly degrade performance.\nHowever, we find that strong data augmentations, such as mixup and CutMix, can\nsignificantly diminish the threat of poisoning and backdoor attacks without\ntrading off performance. We further verify the effectiveness of this simple\ndefense against adaptive poisoning methods, and we compare to baselines\nincluding the popular differentially private SGD (DP-SGD) defense. In the\ncontext of backdoors, CutMix greatly mitigates the attack while simultaneously\nincreasing validation accuracy by 9%.'}, {'Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated\n  Text': 'Detecting text generated by modern large language models is thought to be\nhard, as both LLMs and humans can exhibit a wide range of complex behaviors.\nHowever, we find that a score based on contrasting two closely related language\nmodels is highly accurate at separating human-generated and machine-generated\ntext. Based on this mechanism, we propose a novel LLM detector that only\nrequires simple calculations using a pair of pre-trained LLMs. The method,\ncalled Binoculars, achieves state-of-the-art accuracy without any training\ndata. It is capable of spotting machine text from a range of modern LLMs\nwithout any model-specific modifications. We comprehensively evaluate\nBinoculars on a number of text sources and in varied situations. Over a wide\nrange of document types, Binoculars detects over 90% of generated samples from\nChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being\ntrained on any ChatGPT data.'}, {'TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks': 'While tabular classification has traditionally relied on from-scratch\ntraining, a recent breakthrough called prior-data fitted networks (PFNs)\nchallenges this approach. Similar to large language models, PFNs make use of\npretraining and in-context learning to achieve strong performance on new tasks\nin a single forward pass. However, current PFNs have limitations that prohibit\ntheir widespread adoption. Notably, TabPFN achieves very strong performance on\nsmall tabular datasets but is not designed to make predictions for datasets of\nsize larger than 1000. In this work, we overcome these limitations and\nsubstantially improve the performance of PFNs by developing context\noptimization techniques for PFNs. Specifically, we propose TuneTables, a novel\nprompt-tuning strategy that compresses large datasets into a smaller learned\ncontext. TuneTables scales TabPFN to be competitive with state-of-the-art\ntabular classification methods on larger datasets, while having a substantially\nlower inference time than TabPFN. Furthermore, we show that TuneTables can be\nused as an interpretability tool and can even be used to mitigate biases by\noptimizing a fairness objective.'}]","Title: Probing Large Language Models’ Understanding of Gibberish Inputs 

Background: The deployment of large language models (LLMs) across various domains poses ethical challenges, especially concerning AI-generated content's alignment with human values and the prevention of harmful or untruthful responses, IP infringement, and data leaks. Recent advancements in LLMs open questions about their security against adversarial inputs, such as automatically optimized prompts designed to manipulate models into harmful outputs.

Objective: This study explores the exploitative abilities of appropriate Babel (random-looking) prompts on large language models, aiming to understand the models' vulnerabilities and the underlying patterns in generated content. 

Innovations: The paper introduces a systematic approach using optimized Babel prompts that yield benign and harmful outputs on different datasets. It expands on previous work by analyzing the structure of these gibberish prompts and their effects, revealing the models' susceptibility to manipulation.

Methods: The research employs automatic prompt optimization techniques on several LLMs, including Vicuna and LLaMA2 models, against a variety of target datasets: Wikipedia articles, news articles, and corporate emails. The experiments measure the success rate of constructing a target text, perplexity, and conditional perplexity of the generated content.

Results: The study demonstrates that Babel prompts successfully steer models toward desired target texts across diverse datasets, with success rates as high as 66%. Analysis reveals that the perplexity of target texts is notably lower when manipulated by prompt visitors, indicating an easier attack medium. Further probing shows the structure of Babel prompts reveals insights into the models' internal operations and dataset relationships.

Contributions: This research not only exposes critical vulnerabilities in LLMs concerning intent and content generation but also provides insights into the models' internal understanding and biases. By characterizing Babel prompts, the study advances AI interpretability and robustness against adversarial inputs, opening avenues for enhancing model safety through prompt design.

Applications: The findings underscore the importance of developing more secure and ethically aligned LLM systems. They offer a novel tool for researchers and policymakers to assess model safety, understand LLM behavior, and enforce regulations in the face of ethical and security challenges posed by large language models."
"Self-correction has emerged as a promising solution to boost the reasoning
performance of large language models (LLMs), where LLMs refine their solutions
using self-generated critiques that pinpoint the errors. This work explores
whether smaller-size (<= 13B) language models (LMs) have the ability of
self-correction on reasoning tasks with minimal inputs from stronger LMs. We
propose a novel pipeline that prompts smaller LMs to collect self-correction
data that supports the training of self-refinement abilities. First, we
leverage correct solutions to guide the model in critiquing their incorrect
responses. Second, the generated critiques, after filtering, are used for
supervised fine-tuning of the self-correcting reasoner through solution
refinement. Our experimental results show improved self-correction abilities of
two models on five datasets spanning math and commonsense reasoning, with
notable performance gains when paired with a strong GPT-4-based verifier,
though limitations are identified when using a weak self-verifier for
determining when to correct.","[{'MOVER: Mask, Over-generate and Rank for Hyperbole Generation': 'Despite being a common figure of speech, hyperbole is under-researched in\nFigurative Language Processing. In this paper, we tackle the challenging task\nof hyperbole generation to transfer a literal sentence into its hyperbolic\nparaphrase. To address the lack of available hyperbolic sentences, we construct\nHYPO-XL, the first large-scale English hyperbole corpus containing 17,862\nhyperbolic sentences in a non-trivial way. Based on our corpus, we propose an\nunsupervised method for hyperbole generation that does not require parallel\nliteral-hyperbole pairs. During training, we fine-tune BART to infill masked\nhyperbolic spans of sentences from HYPO-XL. During inference, we mask part of\nan input literal sentence and over-generate multiple possible hyperbolic\nversions. Then a BERT-based ranker selects the best candidate by hyperbolicity\nand paraphrase quality. Automatic and human evaluation results show that our\nmodel is effective at generating hyperbolic paraphrase sentences and\noutperforms several baseline systems.'}, {'BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles': 'A riddle is a question or statement with double or veiled meanings, followed\nby an unexpected answer. Solving riddle is a challenging task for both machine\nand human, testing the capability of understanding figurative, creative natural\nlanguage and reasoning with commonsense knowledge. We introduce BiRdQA, a\nbilingual multiple-choice question answering dataset with 6614 English riddles\nand 8751 Chinese riddles. For each riddle-answer pair, we provide four\ndistractors with additional information from Wikipedia. The distractors are\nautomatically generated at scale with minimal bias. Existing monolingual and\nmultilingual QA models fail to perform well on our dataset, indicating that\nthere is a long way to go before machine can beat human on solving tricky\nriddles. The dataset has been released to the community.'}, {'SituatedGen: Incorporating Geographical and Temporal Contexts into\n  Generative Commonsense Reasoning': 'Recently, commonsense reasoning in text generation has attracted much\nattention. Generative commonsense reasoning is the task that requires machines,\ngiven a group of keywords, to compose a single coherent sentence with\ncommonsense plausibility. While existing datasets targeting generative\ncommonsense reasoning focus on everyday scenarios, it is unclear how well\nmachines reason under specific geographical and temporal contexts. We formalize\nthis challenging task as SituatedGen, where machines with commonsense should\ngenerate a pair of contrastive sentences given a group of keywords including\ngeographical or temporal entities. We introduce a corresponding English dataset\nconsisting of 8,268 contrastive sentence pairs, which are built upon several\nexisting commonsense reasoning benchmarks with minimal manual labor.\nExperiments show that state-of-the-art generative language models struggle to\ngenerate sentences with commonsense plausibility and still lag far behind human\nperformance. Our dataset is publicly available at\nhttps://github.com/yunx-z/situated_gen.'}, {'Two-step Lookahead Bayesian Optimization with Inequality Constraints': 'Recent advances in computationally efficient non-myopic Bayesian optimization\n(BO) improve query efficiency over traditional myopic methods like expected\nimprovement while only modestly increasing computational cost. These advances\nhave been largely limited, however, to unconstrained optimization. For\nconstrained optimization, the few existing non-myopic BO methods require heavy\ncomputation. For instance, one existing non-myopic constrained BO method [Lam\nand Willcox, 2017] relies on computationally expensive unreliable brute-force\nderivative-free optimization of a Monte Carlo rollout acquisition function.\nMethods that use the reparameterization trick for more efficient\nderivative-based optimization of non-myopic acquisition functions in the\nunconstrained setting, like sample average approximation and infinitesimal\nperturbation analysis, do not extend: constraints introduce discontinuities in\nthe sampled acquisition function surface that hinder its optimization.\nMoreover, we argue here that being non-myopic is even more important in\nconstrained problems because fear of violating constraints pushes myopic\nmethods away from sampling the boundary between feasible and infeasible\nregions, slowing the discovery of optimal solutions with tight constraints. In\nthis paper, we propose a computationally efficient two-step lookahead\nconstrained Bayesian optimization acquisition function (2-OPT-C) supporting\nboth sequential and batch settings. To enable fast acquisition function\noptimization, we develop a novel likelihood-ratio-based unbiased estimator of\nthe gradient of the two-step optimal acquisition function that does not use the\nreparameterization trick. In numerical experiments, 2-OPT-C typically improves\nquery efficiency by 2x or more over previous methods, and in some cases by 10x\nor more.'}, {'Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck\n  Muscle Contraction': ""Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR\nexperiences. While VR/AR head-mounted displays unlock users' natural wide-range\nhead movements during viewing, their neck muscle comfort is inevitably\ncompromised by the added hardware weight. Unfortunately, little quantitative\nknowledge for understanding and addressing such an issue is available so far.\n  Leveraging electromyography devices, we measure, model, and predict VR users'\nneck muscle contraction levels (MCL) while they move their heads to interact\nwith the virtual environment. Specifically, by learning from collected\nphysiological data, we develop a bio-physically inspired computational model to\npredict neck MCL under diverse head kinematic states. Beyond quantifying the\ncumulative MCL of completed head movements, our model can also predict\npotential MCL requirements with target head poses only. A series of objective\nevaluations and user studies demonstrate its prediction accuracy and\ngenerality, as well as its ability in reducing users' neck discomfort by\noptimizing the layout of visual targets. We hope this research will motivate\nnew ergonomic-centered designs for VR/AR and interactive graphics applications.\nSource code is released at:\nhttps://github.com/NYU-ICL/xr-ergonomics-neck-comfort.""}, {'Investigating Zero- and Few-shot Generalization in Fact Verification': 'In this paper, we explore zero- and few-shot generalization for fact\nverification (FV), which aims to generalize the FV model trained on\nwell-resourced domains (e.g., Wikipedia) to low-resourced domains that lack\nhuman annotations. To this end, we first construct a benchmark dataset\ncollection which contains 11 FV datasets representing 6 domains. We conduct an\nempirical analysis of generalization across these FV datasets, finding that\ncurrent models generalize poorly. Our analysis reveals that several factors\naffect generalization, including dataset size, length of evidence, and the type\nof claims. Finally, we show that two directions of work improve generalization:\n1) incorporating domain knowledge via pretraining on specialized domains, and\n2) automatically generating training data via claim generation.'}, {'CC-Riddle: A Question Answering Dataset of Chinese Character Riddles': 'The Chinese character riddle is a unique form of cultural entertainment\nspecific to the Chinese language. It typically comprises two parts: the riddle\ndescription and the solution. The solution to the riddle is a single character,\nwhile the riddle description primarily describes the glyph of the solution,\noccasionally supplemented with its explanation and pronunciation. Solving\nChinese character riddles is a challenging task that demands understanding of\ncharacter glyph, general knowledge, and a grasp of figurative language. In this\npaper, we construct a \\textbf{C}hinese \\textbf{C}haracter riddle dataset named\nCC-Riddle, which covers the majority of common simplified Chinese characters.\nThe construction process is a combination of web crawling, language model\ngeneration and manual filtering. In generation stage, we input the Chinese\nphonetic alphabet, glyph and meaning of the solution character into the\ngeneration model, which then produces multiple riddle descriptions. The\ngenerated riddles are then manually filtered and the final CC-Riddle dataset is\ncomposed of both human-written riddles and these filtered, generated riddles.\nIn order to assess the performance of language models on the task of solving\ncharacter riddles, we use retrieval-based, generative and multiple-choice QA\nstrategies to test three language models: BERT, ChatGPT and ChatGLM. The test\nresults reveal that current language models still struggle to solve Chinese\ncharacter riddles. CC-Riddle is publicly available at\n\\url{https://github.com/pku0xff/CC-Riddle}.'}, {'Exploiting Channel Similarity for Accelerating Deep Convolutional Neural\n  Networks': ""To address the limitations of existing magnitude-based pruning algorithms in\ncases where model weights or activations are of large and similar magnitude, we\npropose a novel perspective to discover parameter redundancy among channels and\naccelerate deep CNNs via channel pruning. Precisely, we argue that channels\nrevealing similar feature information have functional overlap and that most\nchannels within each such similarity group can be removed without compromising\nmodel's representational power. After deriving an effective metric for\nevaluating channel similarity through probabilistic modeling, we introduce a\npruning algorithm via hierarchical clustering of channels. In particular, the\nproposed algorithm does not rely on sparsity training techniques or complex\ndata-driven optimization and can be directly applied to pre-trained models.\nExtensive experiments on benchmark datasets strongly demonstrate the superior\nacceleration performance of our approach over prior arts. On ImageNet, our\npruned ResNet-50 with 30% FLOPs reduced outperforms the baseline model.""}, {'Interpreting the Robustness of Neural NLP Models to Textual\n  Perturbations': 'Modern Natural Language Processing (NLP) models are known to be sensitive to\ninput perturbations and their performance can decrease when applied to\nreal-world, noisy data. However, it is still unclear why models are less robust\nto some perturbations than others. In this work, we test the hypothesis that\nthe extent to which a model is affected by an unseen textual perturbation\n(robustness) can be explained by the learnability of the perturbation (defined\nas how well the model learns to identify the perturbation with a small amount\nof evidence). We further give a causal justification for the learnability\nmetric. We conduct extensive experiments with four prominent NLP models --\nTextRNN, BERT, RoBERTa and XLNet -- over eight types of textual perturbations\non three datasets. We show that a model which is better at identifying a\nperturbation (higher learnability) becomes worse at ignoring such a\nperturbation at test time (lower robustness), providing empirical support for\nour hypothesis.'}, {'Merging Generated and Retrieved Knowledge for Open-Domain QA': 'Open-domain question answering (QA) systems are often built with retrieval\nmodules. However, retrieving passages from a given source is known to suffer\nfrom insufficient knowledge coverage. Alternatively, prompting large language\nmodels (LLMs) to generate contextual passages based on their parametric\nknowledge has been shown to improve QA performance. Yet, LLMs tend to\n""hallucinate"" content that conflicts with the retrieved knowledge. Based on the\nintuition that answers supported by both sources are more likely to be correct,\nwe propose COMBO, a Compatibility-Oriented knowledge Merging for Better\nOpen-domain QA framework, to effectively leverage the two sources of\ninformation. Concretely, we match LLM-generated passages with retrieved\ncounterparts into compatible pairs, based on discriminators trained with silver\ncompatibility labels. Then a Fusion-in-Decoder-based reader model handles\npassage pairs to arrive at the final answer. Experiments show that COMBO\noutperforms competitive baselines on three out of four tested open-domain QA\nbenchmarks. Further analysis reveals that our proposed framework demonstrates\ngreater efficacy in scenarios with a higher degree of knowledge conflicts.'}]","Title: Enhancing Small LMs’ Self-Correction Skills through Self-Created Critiques and Verification

Background:
Previous studies have explored how large language models (LLMs) can self-correct, improving the accuracy of their outputs. However, such capabilities are often challenging for smaller, publicly accessible models. This work aims to bridge this gap by training small, open-source language models (LMs) to self-correct their responses through the generation of critiques and verification, enhancing their reasoning abilities.

Objective:
The primary goal of this research is to investigate and implement techniques for small LMs to self-correct their responses, focusing on two main components: a critic that identifies flaws in the initial answers and a verifier that assesses the correctness of the reasoning steps leading to those answers.

Innovations:
- Proposes a SCORE pipeline (SELF-C.eyeation RE Firer and Verifier trainin.) for bootstrapping the self-correction ability of smaller LM models.
- Employs a mechanism for generating critiques using correct solutions as hints.
- Trains fine-tuned versions of the base LM, using both a prompting method and a separate regime for confidence evaluation, to improve reasoning and correction.
- Reveals the importance of inviting the base model to self-assess, which motivates it to refine and improve its initial outputs.

Methods:
- Implemented a pipeline that collects critique-correction pairs from stronger LMs to train a critic and a verifier for the smaller LM.
- Designed a few-shot prompt for the critic to generate critiques based on correct solutions as hints.
- Highlighted the effectiveness of fine-tuning for the verifier and the critic, showcasing better performance compared to verification prompts alone.
- Conduct ablation studies to refine the pipeline's architecture and data usage.

Results:
- The fine-tuned models exhibit significant improvements in final accuracy across various tasks, demonstrating self-correction capabilities when engaged in reasoning.
- The verifier shows a reasonable prediction frequency, balancing the need for correction attempts with maintaining accuracy.

Contributions:
- Reveals the practicality of using smaller, less resource-intensive models for self-correction, addressing the trade-offs between model size and reasoning ability.
- Provides a benchmark for evaluating and improving the reasoning capabilities of smaller LMs.
- Offers methods for open-source model users to enhance their model's performance through self-correction.

Applications:
This research has implications for domains where smaller, efficient models are preferred, including resource-constrained computing environments, extended-effort learning scenarios, and distributed computing platforms. The findings could impact areas like educational technology, personalized learning systems, and everyday AI applications where computational efficiency and accessibility are key considerations."
"Presently, with the assistance of advanced LLM application development
frameworks, more and more LLM-powered applications can effortlessly augment the
LLMs' knowledge with external content using the retrieval augmented generation
(RAG) technique. However, these frameworks' designs do not have sufficient
consideration of the risk of external content, thereby allowing attackers to
undermine the applications developed with these frameworks. In this paper, we
reveal a new threat to LLM-powered applications, termed retrieval poisoning,
where attackers can guide the application to yield malicious responses during
the RAG process. Specifically, through the analysis of LLM application
frameworks, attackers can craft documents visually indistinguishable from
benign ones. Despite the documents providing correct information, once they are
used as reference sources for RAG, the application is misled into generating
incorrect responses. Our preliminary experiments indicate that attackers can
mislead LLMs with an 88.33\% success rate, and achieve a 66.67\% success rate
in the real-world application, demonstrating the potential impact of retrieval
poisoning.","[{'Liquid crystal cells with ""dirty"" substrates': 'We explore liquid crystal order in a cell with a ""dirty"" substrate imposing a\nrandom surface pinning. Modeling such systems by a random-field xy-model with\nsurface heterogeneity, we find that orientational order in the\nthree-dimensional system is marginally unstable to such surface pinning. We\ncompute the Larkin length scale, and the corresponding surface and bulk\ndistortions. On longer scales we calculate correlation functions using the\nfunctional renormalization group and matching methods, finding a universal\nlogarithmic and double-logarithmic roughness in two and three dimensions,\nrespectively. For a finite thickness cell, we explore the interplay of\nhomogeneous-heterogeneous substrate pair and detail crossovers as a function of\ndisorder strength and cell thickness.'}, {'Stability and distortions of liquid crystal order in a cell with a\n  heterogeneous substrate': 'We study stability and distortions of liquid crystal nematic order in a cell\nwith a random heterogeneous substrate. Modeling this system as a bulk xy model\nwith quenched disorder confined to a surface, we find that nematic order is\nmarginally unstable to such surface pinning. We compute the length scale beyond\nwhich nematic distortions become large and calculate orientational correlation\nfunctions using the functional renormalization-group and matching methods,\nfinding universal logarithmic and double-logarithmic distortions in two and\nthree dimensions, respectively. We extend these results to a finite-thickness\nliquid crystal cell with a second homogeneous substrate, detailing crossovers\nas a function of random pinning strength and cell thickness. We conclude with\nanalysis of experimental signatures of these distortions in a conventional\ncrossed-polarizer-analyzer light microscopy.'}, {'Smectic-glass transition in a liquid crystal cell with a ""dirty""\n  substrate': 'We explore the smectic liquid crystal order in a cell with a ""dirty""\nsubstrate imposing random pinnings. Within harmonic elasticity we find a subtle\nthree-dimensional surface disorder-driven transition into a pinned\nsmectic-glass, controlled by a three-dimensional Cardy-Ostlund-like fixed line,\nakin to a super-rough phase of a two-dimensional xy model. We compute the\nassociated random substrate-driven distortions of the smectic-glass state,\nidentify the characteristic length scales on the heterogeneous substrate and in\nthe bulk, and discuss a variety of experimental signatures.'}, {'Smectic order, pinning, and phase transition in a smectic liquid crystal\n  cell with a random substrate': 'We study smectic-liquid-crystal order in a cell with a heterogeneous\nsubstrate imposing surface random positional and orientational pinnings.\nProposing a minimal random elastic model, we demonstrate that, for a thick\ncell, the smectic state without a rubbed substrate is always unstable at long\nscales and, for weak random pinning, is replaced by a smectic glass state. We\ncompute the statistics of the associated substrate-driven distortions and the\ncharacteristic smectic domain size on the heterogeneous substrate and in the\nbulk. We find that for weak disorder, the system exhibits a three-dimensional\ntemperature-controlled phase transition between a weakly and strongly pinned\nsmectic glass states akin to the Cardy-Ostlund phase transition. We explore\nexperimental implications of the predicted phenomenology and suggest that it\nprovides a plausible explanation for the experimental observations on polarized\nlight microscopy and x-ray scattering.'}, {'Designing Massive MIMO Detector via PS-ADMM approach': 'In this paper, we develop an efficient detector for massive multiple-input\nmultiple-output (MIMO) communication systems via penalty-sharing alternating\ndirection method of multipliers (PS-ADMM). Its main content are as follows:\nfirst, we formulate the MIMO detection as a maximum-likelihood optimization\nproblem with bound relaxation constraints. Then, the higher modulation signals\nare decomposed into a sum of multiple binary variables through their inherent\nstructures, by exploiting introduced binary variables as penalty functions, the\ndetection optimization model is equivalent to a nonconvex sharing minimization\nproblem. Second, a customized ADMM algorithm is presented to solve the\nformulated nonconvex optimization problem. In the implementation, all variables\ncan be solved analytically and parallelly. Third, it is proved that the\nproposed PS-ADMM algorithm converges if proper parameters are chosen.\nSimulation results demonstrate the effectiveness of the proposed approach.'}, {'Conical soliton escape into a third dimension of a surface vortex': 'We present an exact three-dimensional solitonic solution to a\nsine-Gordon-type Euler-Lagrange equation, that describes a configuration of a\nthree-dimensional vector field n constrained to a surface p-vortex, with a\nprescribed polar tilt angle on a planar substrate and escaping into the third\ndimension in the bulk. The solution is relevant to characterization of a\nschlieren texture in nematic liquid-crystal films with tangential (in-plane)\nsubstrate alignment. The solution is identical to a section of a point defect\ndiscovered many years ago by Saupe [Mol. Cryst. Liq. Cryst. 21, 211 (1973)],\nwhen latter is restricted to a surface.'}, {'Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression': 'To model categorical response variables given their covariates, we propose a\npermuted and augmented stick-breaking (paSB) construction that one-to-one maps\nthe observed categories to randomly permuted latent sticks. This new\nconstruction transforms multinomial regression into regression analysis of\nstick-specific binary random variables that are mutually independent given\ntheir covariate-dependent stick success probabilities, which are parameterized\nby the regression coefficients of their corresponding categories. The paSB\nconstruction allows transforming an arbitrary cross-entropy-loss binary\nclassifier into a Bayesian multinomial one. Specifically, we parameterize the\nnegative logarithms of the stick failure probabilities with a family of\ncovariate-dependent softplus functions to construct nonparametric Bayesian\nmultinomial softplus regression, and transform Bayesian support vector machine\n(SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression\nmodels are not only capable of providing probability estimates, quantifying\nuncertainty, increasing robustness, and producing nonlinear classification\ndecision boundaries, but also amenable to posterior simulation. Example results\ndemonstrate their attractive properties and performance.'}, {'Nonparametric Bayesian Lomax delegate racing for survival analysis with\n  competing risks': 'We propose Lomax delegate racing (LDR) to explicitly model the mechanism of\nsurvival under competing risks and to interpret how the covariates accelerate\nor decelerate the time to event. LDR explains non-monotonic covariate effects\nby racing a potentially infinite number of sub-risks, and consequently relaxes\nthe ubiquitous proportional-hazards assumption which may be too restrictive.\nMoreover, LDR is naturally able to model not only censoring, but also missing\nevent times or event types. For inference, we develop a Gibbs sampler under\ndata augmentation for moderately sized data, along with a stochastic gradient\ndescent maximum a posteriori inference algorithm for big data applications.\nIllustrative experiments are provided on both synthetic and real datasets, and\ncomparison with various benchmark algorithms for survival analysis with\ncompeting risks demonstrates distinguished performance of LDR.'}, {'On the design of Massive MIMO-QAM detector via $\\ell_2$-Box ADMM\n  approach': 'In this letter, we develop an $\\ell_2$-box maximum likelihood (ML)\nformulation for massive multiple-input multiple-output (MIMO) quadrature\namplitude modulation (QAM) signal detection and customize an alternating\ndirection method of multipliers (ADMM) algorithm to solve the nonconvex\noptimization model. In the $\\ell_2$-box ADMM implementation, all variables are\nsolved analytically. Moreover, several theoretical results related to\nconvergence, iteration complexity, and computational complexity are presented.\nSimulation results demonstrate the effectiveness of the proposed $\\ell_2$-box\nADMM detector in comparison with state-of-the-arts approaches.'}, {'Multiparty simultaneous quantum identity authentication based on\n  entanglement swapping': 'We present a multiparty simultaneous quantum identity authentication protocol\nbased on entanglement swapping. In our protocol, the multi-user can be\nauthenticated by a trusted third party simultaneously.'}]","Title: Imperceptible Retrieval Poisoning Attacks in Large Language Model-Powered Applications: Exploiting the LLM-Avatar Framework and Crafting Malicious Documents

Abstract: The widespread adoption of large language models (LLMs) has revolutionized applications in vast domains of natural language processing (NLP). However, this also endows them with an increased risk of security breaches, particularly with extensions like retrieval augmented generation (RAG), which facilitates external knowledge extraction during response generation. This study illuminatesExisting the underexplored vulnerability of LLM-powered applications to imperceptible retrieval poisoning, where malicious content can be subtly embedded in external documents to manipulate responses. We introduce a Document Crafting Algorithm, showcasing the ease with which attackers can generate subtle attack sequences and malicious documents to subvert the augmented requests generated by LLMs. Our research presents a detailed approach to this novel threat, enabling the crafting of documents that maintain human imperceptibility while sabotaging the intended responses. Evaluation demonstrates an average success rate of 88.33%, encompassing different generations and provider types of LLM models. Furthermore, demonstrating the widespread applicability of such attacks, we successfully apply this methodology to a real-world LLM-powered application, achieving an attack success rate of 66.67%. This study thus identifies a significant concern for the LLM industry, necessitating the development of robust strategies to defend against imperceptible retrieval poisoning and enhance the security of existing applications.

Keywords: Large Language Models (LLM), Imperceptible Retrieval Poisoning, LLM-Powered Applications, Security Vulnerabilities, Document Crafting"
"Retrieval-augmented language models have exhibited promising performance
across various areas of natural language processing (NLP), including
fact-critical tasks. However, due to the black-box nature of advanced large
language models (LLMs) and the non-retrieval-oriented supervision signal of
specific tasks, the training of retrieval model faces significant challenges
under the setting of black-box LLM. We propose an approach leveraging
Fine-grained Feedback with Reinforcement Retrieval (FFRR) to enhance
fact-checking on news claims by using black-box LLM. FFRR adopts a two-level
strategy to gather fine-grained feedback from the LLM, which serves as a reward
for optimizing the retrieval policy, by rating the retrieved documents based on
the non-retrieval ground truth of the task. We evaluate our model on two public
datasets for real-world news claim verification, and the results demonstrate
that FFRR achieves significant improvements over strong LLM-enabled and non-LLM
baselines.","[{'A Poisson limit theorem for Gibbs-Markov maps': ""We prove for Gibbs-Markov maps that the number of visits to a sequence of\nshrinking sets with bounded cylindrical lengths converges in distribution to a\nPoisson law. Applying to continued fractions, this result extends Doeblin's\nPoisson limit theorem.""}, {'Note on limit distribution of normalized return times and escape rate': 'In this note we discuss limit distribution of normalized return times for\nshrinking targets and draw a necessary and sufficient condition using sweep-out\nsequence in order for the limit distribution to be exponential with parameter\n$1$. The normalizing coefficients are the same as sizes of the targets.\nMoreover we study escape rate, namely the exponential decay rate of sweep-out\nsequence and prove that in $\\psi$-mixing systems for a certain class of sets\nthe escape rate is in limit proportional to the size of the set.'}, {'On the law of the iterated logarithm for continued fractions with\n  sequentially restricted partial quotients': 'We establish a law of the iterated logarithm (LIL) for the set of real\nnumbers whose $n$-th partial quotient is bigger than $\\alpha_n$, where\n$(\\alpha_n)$ is a sequence such that $\\sum 1/\\alpha_n$ is finite. This set is\nshown to have Hausdorff dimension $1/2$ in many cases and the measure in LIL is\nabsolutely continuous to the Hausdorff measure. The result is obtained as an\napplication of a strong invariance principle for unbounded observables on the\nlimit set of a sequential iterated function system.'}, {'Legal Assignments and fast EADAM with consent via classical theory of\n  stable matchings': ""Gale and Shapley's stable assignment problem has been extensively studied,\napplied, and extended. In the context of school choice, mechanisms often aim at\nfinding an assignment that is more favorable to students. We investigate two\nextensions introduced in this framework -- legal assignments and the EADAM\nalgorithm -- through the lens of classical theory of stable matchings. In any\ninstance, the set ${\\cal L}$ of legal assignments is known to contain all\nstable assignments. We prove that ${\\cal L}$ is exactly the set of stable\nassignments in another instance. Moreover, we show that essentially all\noptimization problems over ${\\cal L}$ can be solved within the same time bound\nneeded for solving it over the set of stable assignments. A key tool for this\nlatter result is an algorithm that finds the student-optimal legal assignment.\nWe then generalize our algorithm to obtain the assignment output of EADAM with\nany given set of consenting students without sacrificing the running time,\nhence largely improving in both theory and practice over known algorithms.\nLastly, we show that the set ${\\cal L}$ can be much larger than the set of\nstable matchings, connecting legal matchings with certain concepts and open\nproblems in the literature.""}, {'Affinely representable lattices, stable matchings, and choice functions': ""Birkhoff's representation theorem (Birkhoff, 1937) defines a bijection\nbetween elements of a distributive lattice and the family of upper sets of an\nassociated poset. Although not used explicitly, this result is at the backbone\nof the combinatorial algorithm by Irving et al. (1987) for maximizing a linear\nfunction over the set of stable matchings in Gale and Shapley's stable marriage\nmodel (Gale and Shapley, 1962). In this paper, we introduce a property of\ndistributive lattices, which we term as affine representability, and show its\nrole in efficiently solving linear optimization problems over the elements of a\ndistributive lattice, as well as describing the convex hull of the\ncharacteristic vectors of the lattice elements. We apply this concept to the\nstable matching model with path-independent quota-filling choice functions,\nthus giving efficient algorithms and a compact polyhedral description for this\nmodel. To the best of our knowledge, this model generalizes all models from the\nliterature for which similar results were known, and our paper is the first\nthat proposes efficient algorithms for stable matchings with choice functions,\nbeyond classical extensions of the Deferred Acceptance algorithm.""}, {'Mixed convection in a downward flow in a vertical duct with strong\n  transverse magnetic field': 'The downward flow in a vertical duct with one heated and three thermally\ninsulated walls is analyzed numerically using the two-dimensional approximation\nvalid in the asymptotic limit of an imposed strong transverse magnetic field.\nThe work is motivated by the design of liquid metal blankets with poloidal\nducts for future nuclear fusion reactors, in which the main component of the\nmagnetic field is perpendicular to the flow direction and very strong heating\nis applied at the wall facing the reaction chamber. The flow is found to be\nsteady-state or oscillating depending on the strengths of the heating and\nmagnetic field. A parametric study of the instability leading to the\noscillations is performed. It is found among other results that the flow is\nunstable and develops high-amplitude temperature oscillations at the conditions\ntypical for a fusion reactor blanket.'}, {'Towards LLM-based Fact Verification on News Claims with a Hierarchical\n  Step-by-Step Prompting Method': 'While large pre-trained language models (LLMs) have shown their impressive\ncapabilities in various NLP tasks, they are still under-explored in the\nmisinformation domain. In this paper, we examine LLMs with in-context learning\n(ICL) for news claim verification, and find that only with 4-shot demonstration\nexamples, the performance of several prompting methods can be comparable with\nprevious supervised models. To further boost performance, we introduce a\nHierarchical Step-by-Step (HiSS) prompting method which directs LLMs to\nseparate a claim into several subclaims and then verify each of them via\nmultiple questions-answering steps progressively. Experiment results on two\npublic misinformation datasets show that HiSS prompting outperforms\nstate-of-the-art fully-supervised approach and strong few-shot ICL-enabled\nbaselines.'}, {'Predicting Viral Rumors and Vulnerable Users for Infodemic Surveillance': 'In the age of the infodemic, it is crucial to have tools for effectively\nmonitoring the spread of rampant rumors that can quickly go viral, as well as\nidentifying vulnerable users who may be more susceptible to spreading such\nmisinformation. This proactive approach allows for timely preventive measures\nto be taken, mitigating the negative impact of false information on society. We\npropose a novel approach to predict viral rumors and vulnerable users using a\nunified graph neural network model. We pre-train network-based user embeddings\nand leverage a cross-attention mechanism between users and posts, together with\na community-enhanced vulnerability propagation (CVP) method to improve user and\npropagation graph representations. Furthermore, we employ two multi-task\ntraining strategies to mitigate negative transfer effects among tasks in\ndifferent settings, enhancing the overall performance of our approach. We also\nconstruct two datasets with ground-truth annotations on information virality\nand user vulnerability in rumor and non-rumor events, which are automatically\nderived from existing rumor detection datasets. Extensive evaluation results of\nour joint learning model confirm its superiority over strong baselines in all\nthree tasks: rumor detection, virality prediction, and user vulnerability\nscoring. For instance, compared to the best baselines based on the Weibo\ndataset, our model makes 3.8\\% and 3.0\\% improvements on Accuracy and MacF1 for\nrumor detection, and reduces mean squared error (MSE) by 23.9\\% and 16.5\\% for\nvirality prediction and user vulnerability scoring, respectively. Our findings\nsuggest that our approach effectively captures the correlation between rumor\nvirality and user vulnerability, leveraging this information to improve\nprediction performance and provide a valuable tool for infodemic surveillance.'}, {'A Sequence to Sequence Model for Extracting Multiple Product Name\n  Entities from Dialog': ""E-commerce voice ordering systems need to recognize multiple product name\nentities from ordering utterances. Existing voice ordering systems such as\nAmazon Alexa can capture only a single product name entity. This restrains\nusers from ordering multiple items with one utterance. In recent years,\npre-trained language models, e.g., BERT and GPT-2, have shown promising results\non NLP benchmarks like Super-GLUE. However, they can't perfectly generalize to\nthis Multiple Product Name Entity Recognition (MPNER) task due to the ambiguity\nin voice ordering utterances. To fill this research gap, we propose Entity\nTransformer (ET) neural network architectures which recognize up to 10 items in\nan utterance. In our evaluation, the best ET model (conveRT + ngram + ET) has a\nperformance improvement of 12% on our test set compared to the non-neural\nmodel, and outperforms BERT with ET as well. This helps customers finalize\ntheir shopping cart via voice dialog, which improves shopping efficiency and\nexperience.""}, {'Multi-Target, Multi-Camera Tracking by Hierarchical Clustering: Recent\n  Progress on DukeMTMC Project': 'Although many methods perform well in single camera tracking, multi-camera\ntracking remains a challenging problem with less attention. DukeMTMC is a\nlarge-scale, well-annotated multi-camera tracking benchmark which makes great\nprogress in this field. This report is dedicated to briefly introduce our\nmethod on DukeMTMC and show that simple hierarchical clustering with\nwell-trained person re-identification features can get good results on this\ndataset.'}]","**Abstract**

In an era of rapid information proliferation, the challenge of fact-checking real-world news claims escalates. This paper introduces a novel approach, Fine-grained Feedback with Reinforcement Retrieval (FFRR), designed to enhance large language models (LLMs) for this critical task. Drawing inspiration from retrieval-augmented language modeling, FFRR leverages LLMs to generate question prompts, retrieving relevant external documents through dense retrieval. Our method innovates by incorporating FFRR to query and impart feedback on the retrieved documents, managing inconsistent information and biases while minimizing time-consuming human annotation. Empirical evaluations on the RAWFC and LIAR-RAW datasets illustrate FFRR's ability to mitigate retrieval model errors, significantly improving LLM performance in fact-checking. This research contributes a powerful mechanism for infusing augmented contextual knowledge into LLM-driven fact-based reasoning, with broad implications for digital literacy, media integrity, and automated content verification platforms.

Confident in its use of dense retrieval, fine-grained feedback integration, and sensitivity optimization, FFRR represents a pivotal step towards developing more reliable and accurate fact-checking tools that integrate external information seamlessly. Its applications reach beyond current fact-checking efforts, enhancing not only human decision-making but also algorithms' performance in capturing nuanced and evolving contexts prevalent in news media. This work bridges the gap between theoretical language model advancements and practical real-world scenarios, positioning FFRR as a valuable framework for the future of information validation processes."
"Depth estimation is crucial for interpreting complex environments, especially
in areas such as autonomous vehicle navigation and robotics. Nonetheless,
obtaining accurate depth readings from event camera data remains a formidable
challenge. Event cameras operate differently from traditional digital cameras,
continuously capturing data and generating asynchronous binary spikes that
encode time, location, and light intensity. Yet, the unique sampling mechanisms
of event cameras render standard image based algorithms inadequate for
processing spike data. This necessitates the development of innovative,
spike-aware algorithms tailored for event cameras, a task compounded by the
irregularity, continuity, noise, and spatial and temporal characteristics
inherent in spiking data.Harnessing the strong generalization capabilities of
transformer neural networks for spatiotemporal data, we propose a purely
spike-driven spike transformer network for depth estimation from spiking camera
data. To address performance limitations with Spiking Neural Networks (SNN), we
introduce a novel single-stage cross-modality knowledge transfer framework
leveraging knowledge from a large vision foundational model of artificial
neural networks (ANN) (DINOv2) to enhance the performance of SNNs with limited
data. Our experimental results on both synthetic and real datasets show
substantial improvements over existing models, with notable gains in Absolute
Relative and Square Relative errors (49% and 39.77% improvements over the
benchmark model Spike-T, respectively). Besides accuracy, the proposed model
also demonstrates reduced power consumptions, a critical factor for practical
applications.","[{'Drawing complete multipartite graphs on the plane with restrictions on\n  crossings': 'We introduce the concept of NIC-planar graphs and present the full\ncharacterization of NIC-planar complete k-partite graphs.'}, {'Equitable vertex arboricity of planar graphs': 'Let $G_1$ be a planar graph such that all cycles of length at most 4 are\nindependent and let $G_2$ be a planar graph without 3-cycles and adjacent\n4-cycles. It is proved that the set of vertices of $G_1$ and $G_2$ can be\nequitably partitioned into $t$ subsets for every $t\\geq 3$ so that each subset\ninduces a forest. These results partially confirm a conjecture of Wu, Zhang and\nLi.'}, {""Hypotheses regarding Baxter's $T-Q$ relation for the periodic XYZ chain"": ""Baxter's $T-Q$ relation for the periodic spin-$\\frac12$ XYZ chain is studied.\nWe extensively perform numerical calculations for the $T-Q$ relation and the\nBethe ansatz equations. Numerical based hypotheses are then proposed to answer\nsome open questions regarding Baxter's $T-Q$ relation and the XYZ chain.""}, {'Class two 1-planar graphs with maximum degree six or seven': 'A graph is 1-planar if it can be drawn on the plane so that each edge is\ncrossed by at most one other edge. In this note we give examples of class two\n1-planar graphs with maximum degree six or seven.'}, {'List total coloring of pseudo-outerplanar graphs': 'A graph is pseudo-outerplanar if each of its blocks has an embedding in the\nplane so that the vertices lie on a fixed circle and the edges lie inside the\ndisk of this circle with each of them crossing at most one another. It is\nproved that every pseudo-outerplanar graph with maximum degree \\Delta\\geq 5 is\ntotally (\\Delta+1)-choosable.'}, {'On the Local-Global Principle for Integral Apollonian-3 Circle Packings': 'In this paper we study the integral properties of Apollonian-3 circle\npackings, which are variants of the standard Apollonian circle packings.\nSpecifically, we study the reduction theory, formulate a local-global\nconjecture, and prove a density one version of this conjecture. Along the way,\nwe prove a uniform spectral gap for congruence towers of the symmetry group.'}, {'The edge chromatic number of outer-1-planar graphs': 'A graph is outer-1-planar if it can be drawn in the plane so that all\nvertices are on the outer face and each edge is crossed at most once. In this\npaper, we completely determine the edge chromatic number of outer 1-planar\ngraphs.'}, {'The gap distribution of directions in some Schottky groups': ""We prove the existence and some properties of the limiting gap distribution\nfunctions for the directions of orbits of some infinite covolume subgroups of\n$Isom(\\mathbb{H}^2)$ in the Poincar\\'e disk.""}, {'The $\\mathcal{R}-$bounded operator families arising from the study of\n  the barotropic compressible flows with free surface': ""In this paper, we study some model problem associated to the free boundary\nvalue problem of the barotropic compressible Navier-Stokes equations in general\nsmooth domain with taking surface tension into account. To obtain the maximal\n$L_p-L_q$ regularity property of the model problem, we prove the existence of\n$\\mathcal{R}-$bounded operator families of the resolvent problem via Weis'\ntheory on operator valued Fourier multipliers.""}, {'Coupled Quintessence in a Power-Law Case and the Cosmic Coincidence\n  Problem': 'The problem of the cosmic coincidence is a longstanding puzzle. This\nconundrum may be solved by introducing a coupling between the two dark sectors.\nIn this Letter, we study a coupled quintessence scenario in which the scalar\nfield evolves in a power law potential and the mass of dark matter particles\ndepends on a power law function of $\\phi$. It is shown that this scenario has a\nstable attractor solution and can thus provide a natural solution to the cosmic\ncoincidence problem.'}]","Abstract:

Optimizing the processing of irregular continuous spike data streams for depth estimation in low light environments remains a critical challenge in computer vision. This paper introduces a novel Spiking Neural Network (SNN) architecture equipped with a hybrid Fusion Depth Estimation Head and an enhanced Knowledge Distillation scheme. The architecture successfully leverages multi-scale features to improve the representation and detail recovery of the input spike data, resulting in more accurate depth predictions.

The primary objective is to develop an efficient decoder for SNN to tackle the aforementioned challenges. Innovations consist of an updated Transformer and effective feature fusion mechanisms exploiting scale differences. This architecture is grounded in an efficient model and optimized for low power consumption, key aspects in the deployment of spiking models. Incorporating knowledge distillation into the fusion head facilitates reduced computational complexity without compromising accuracy.

The research methods involve rigorous evaluation across synthetic (DENSE) and real-world (DESC) datasets, focusing on key performance indicators such as precision, recall, and energy consumption. Extensive quantitative and qualitative comparisons illustrate the superiority of the proposed model over its counterparts.

Contributions are manifold: the architecture provides a more generalized approach to low light depth estimation, significantly reduces computational complexity, and demonstrates superior energy efficiency. From a practical standpoint, this work can lead to advancements in various applications, including surveillance, autonomous driving, and robotics, where reliable depth information under varying light conditions is crucial.

These findings promise to influence the development of efficient, low-power SNN-based systems capable of performing at par with traditional deep learning models, and contribute to the growing field of spiking neural networks."
"The goal of image-based virtual try-on is to generate an image of the target
person naturally wearing the given clothing. However, most existing methods
solely focus on the frontal try-on using the frontal clothing. When the views
of the clothing and person are significantly inconsistent, particularly when
the person's view is non-frontal, the results are unsatisfactory. To address
this challenge, we introduce Multi-View Virtual Try-ON (MV-VTON), which aims to
reconstruct the dressing results of a person from multiple views using the
given clothes. On the one hand, given that single-view clothes provide
insufficient information for MV-VTON, we instead employ two images, i.e., the
frontal and back views of the clothing, to encompass the complete view as much
as possible. On the other hand, the diffusion models that have demonstrated
superior abilities are adopted to perform our MV-VTON. In particular, we
propose a view-adaptive selection method where hard-selection and
soft-selection are applied to the global and local clothing feature extraction,
respectively. This ensures that the clothing features are roughly fit to the
person's view. Subsequently, we suggest a joint attention block to align and
fuse clothing features with person features. Additionally, we collect a MV-VTON
dataset, i.e., Multi-View Garment (MVG), in which each person has multiple
photos with diverse views and poses. Experiments show that the proposed method
not only achieves state-of-the-art results on MV-VTON task using our MVG
dataset, but also has superiority on frontal-view virtual try-on task using
VITON-HD and DressCode datasets. Codes and datasets will be publicly released
at https://github.com/hywang2002/MV-VTON .","[{'Quantitative Universality for the Largest Eigenvalue of Sample\n  Covariance Matrices': 'We prove the first explicit rate of convergence to the Tracy-Widom\ndistribution for the fluctuation of the largest eigenvalue of sample covariance\nmatrices that are not integrable. Our primary focus is matrices of type $ X^*X\n$ and the proof follows the Erd\\""{o}s-Schlein-Yau dynamical method. We use a\nrecent approach to the analysis of the Dyson Brownian motion from [5] to obtain\na quantitative error estimate for the local relaxation flow at the edge.\nTogether with a quantitative version of the Green function comparison theorem,\nthis gives the rate of convergence.\n  Combined with a result of Lee-Schnelli [26], some quantitative estimates also\nhold for more general separable sample covariance matrices $ X^* \\Sigma X $\nwith general diagonal population $ \\Sigma $.'}, {'Optimal Smoothed Analysis and Quantitative Universality for the Smallest\n  Singular Value of Random Matrices': 'The smallest singular value and condition number play important roles in\nnumerical linear algebra and the analysis of algorithms. In numerical analysis\nwith randomness, many previous works make Gaussian assumptions, which are not\ngeneral enough to reflect the arbitrariness of the input. To overcome this\ndrawback, we prove the first quantitative universality for the smallest\nsingular value and condition number of random matrices.\n  Moreover, motivated by the study of smoothed analysis that random\nperturbation makes deterministic matrices well-conditioned, we consider an\nanalog for random matrices. For a random matrix perturbed by independent\nGaussian noise, we show that this matrix quickly becomes approximately\nGaussian. In particular, we derive an optimal smoothed analysis for random\nmatrices in terms of a sharp Gaussian approximation.'}, {'Resampling Sensitivity of High-Dimensional PCA': 'The study of stability and sensitivity of statistical methods or algorithms\nwith respect to their data is an important problem in machine learning and\nstatistics. The performance of the algorithm under resampling of the data is a\nfundamental way to measure its stability and is closely related to\ngeneralization or privacy of the algorithm. In this paper, we study the\nresampling sensitivity for the principal component analysis (PCA). Given an $ n\n\\times p $ random matrix $ \\mathbf{X} $, let $ \\mathbf{X}^{[k]} $ be the matrix\nobtained from $ \\mathbf{X} $ by resampling $ k $ randomly chosen entries of $\n\\mathbf{X} $. Let $ \\mathbf{v} $ and $ \\mathbf{v}^{[k]} $ denote the principal\ncomponents of $ \\mathbf{X} $ and $ \\mathbf{X}^{[k]} $. In the proportional\ngrowth regime $ p/n \\to \\xi \\in (0,1] $, we establish the sharp threshold for\nthe sensitivity/stability transition of PCA. When $ k \\gg n^{5/3} $, the\nprincipal components $ \\mathbf{v} $ and $ \\mathbf{v}^{[k]} $ are asymptotically\northogonal. On the other hand, when $ k \\ll n^{5/3} $, the principal components\n$ \\mathbf{v} $ and $ \\mathbf{v}^{[k]} $ are asymptotically colinear. In words,\nwe show that PCA is sensitive to the input data in the sense that resampling\neven a negligible portion of the input may completely change the output.'}, {'Unsupervised Learning for Combinatorial Optimization Needs Meta-Learning': 'A general framework of unsupervised learning for combinatorial optimization\n(CO) is to train a neural network (NN) whose output gives a problem solution by\ndirectly optimizing the CO objective. Albeit with some advantages over\ntraditional solvers, the current framework optimizes an averaged performance\nover the distribution of historical problem instances, which misaligns with the\nactual goal of CO that looks for a good solution to every future encountered\ninstance. With this observation, we propose a new objective of unsupervised\nlearning for CO where the goal of learning is to search for good initialization\nfor future problem instances rather than give direct solutions. We propose a\nmeta-learning-based training pipeline for this new objective. Our method\nachieves good empirical performance. We observe that even just the initial\nsolution given by our model before fine-tuning can significantly outperform the\nbaselines under various evaluation settings including evaluation across\nmultiple datasets, and the case with big shifts in the problem scale. The\nreason we conjecture is that meta-learning-based training lets the model be\nloosely tied to each local optima for a training instance while being more\nadaptive to the changes of optimization landscapes across instances.'}, {'An Epistemic Interpretation of Tensor Disjunction': 'This paper aims to give an epistemic interpretation to the tensor disjunction\nin dependence logic, through a rather surprising connection to the so-called\nweak disjunction in Medvedev\'s early work on intermediate logic under the\nBrouwer-Heyting-Kolmogorov (BHK)-interpretation. We expose this connection in\nthe setting of inquisitive logic with tensor disjunction discussed by Ciardelli\nand Barbero (2019}, but from an epistemic perspective. More specifically, we\ntranslate the propositional formulae of inquisitive logic with tensor into\nmodal formulae in a powerful epistemic language of ""knowing how"" following the\nproposal by Wang (2021). We give a complete axiomatization of the logic of our\nfull language based on Fine\'s axiomatization of S5 modal logic with\npropositional quantifiers. Finally, we generalize the tensor operator with\nparameters $k$ and $n$, which intuitively captures the epistemic situation that\none knows $n$ potential answers to $n$ questions and is sure $k$ answers of\nthem must be correct. The original tensor disjunction is the special case when\n$k=1$ and $n=2$. We show that the generalized tensor operators do not increase\nthe expressive power of our logic, the inquisitive logic, and propositional\ndependence logic, though most of these generalized tensors are not uniformly\ndefinable in these logics, except in our dynamic epistemic logic of knowing\nhow.'}, {'Echo disappears: momentum term structure and cyclic information in\n  turnover': 'We extract cyclic information in turnover and find it can explain the\nmomentum echo. The reversal in recent month momentum is the key factor that\ncancels out the recent month momentum and excluding it makes the echo regress\nto a damped shape. Both rational and behavioral theories can explain the\nreversal. This study is the first explanation of the momentum echo in U.S.\nstock markets.'}, {'Market-level Analysis of Government-backed COVID-19 Contact Tracing Apps': ""To help curb the spread of the COVID-19 pandemic, governments and public\nhealth authorities around the world have launched a number of contact-tracing\napps. Although contact tracing apps have received extensive attentions from the\nresearch community, no existing work has characterized the users' adoption of\ncontact tracing apps from the app market level. In this work, we perform the\nfirst market-level analysis of contact tracing apps. We perform a longitudinal\nempirical study (over 4 months) of eight government-backed COVID-19 contact\ntracing apps in iOS app store. We first collect all the daily meta information\n(e.g., app updates, app rating, app comments, etc.) of these contact tracing\napps from their launch to 2020-07-31. Then we characterize them from release\npractice, app popularity, and mobile users' feedback. Our study reveals various\nissues related to contact tracing apps from the users' perspective, hoping to\nhelp improve the quality of contact tracing apps and thus achieving a high\nlevel of adoption in the population.""}, {'Inquisitive Logic as an Epistemic Logic of Knowing How': 'In this paper, we present an alternative interpretation of propositional\ninquisitive logic as an epistemic logic of knowing how. In our setting, an\ninquisitive logic formula $\\alpha$ being supported by a state is formalized as\n""knowing how to resolve $\\alpha$"" (more colloquially, ""knowing how $\\alpha$ is\ntrue"") holds on the S5 epistemic model corresponding to the state. Based on\nthis epistemic interpretation, we use a dynamic epistemic logic with both\nknow-how and know-that operators to capture the epistemic information behind\nthe innocent-looking connectives in inquisitive logic. We show that the set of\nvalid know-how formulas corresponds precisely to the inquisitive logic. The\nmain result is a complete axiomatization with intuitive axioms using the full\ndynamic epistemic language. Moreover, we show that the know-how operator and\nthe dynamic operator can both be eliminated without changing the expressivity\nover models, which is consistent with the modal translation of inquisitive\nlogic existing in the literature. We hope our framework can give an intuitive\nalternative interpretation of various concepts and technical results in\ninquisitive logic, and also provide a powerful and flexible tool to do\ninquisitive reasoning in an epistemic context.'}, {'DOLORES: Deep Contextualized Knowledge Graph Embeddings': 'We introduce a new method DOLORES for learning knowledge graph embeddings\nthat effectively captures contextual cues and dependencies among entities and\nrelations. First, we note that short paths on knowledge graphs comprising of\nchains of entities and relations can encode valuable information regarding\ntheir contextual usage. We operationalize this notion by representing knowledge\ngraphs not as a collection of triples but as a collection of entity-relation\nchains, and learn embeddings for entities and relations using deep neural\nmodels that capture such contextual usage. In particular, our model is based on\nBi-Directional LSTMs and learn deep representations of entities and relations\nfrom constructed entity-relation chains. We show that these representations can\nvery easily be incorporated into existing models to significantly advance the\nstate of the art on several knowledge graph prediction tasks like link\nprediction, triple classification, and missing relation type prediction (in\nsome cases by at least 9.5%).'}, {'NNCTC: Physical Layer Cross-Technology Communication via Neural Networks': 'Cross-technology communication(CTC) enables seamless interactions between\ndiverse wireless technologies. Most existing work is based on reversing the\ntransmission path to identify the appropriate payload to generate the waveform\nthat the target devices can recognize. However, this method suffers from many\nlimitations, including dependency on specific technologies and the necessity\nfor intricate algorithms to mitigate distortion. In this work, we present\nNNCTC, a Neural-Network-based Cross-Technology Communication framework inspired\nby the adaptability of trainable neural models in wireless communications. By\nconverting signal processing components within the CTC pipeline into neural\nmodels, the NNCTC is designed for end-to-end training without requiring labeled\ndata. This enables the NNCTC system to autonomously derive the optimal CTC\npayload, which significantly eases the development complexity and showcases the\nscalability potential for various CTC links. Particularly, we construct a CTC\nsystem from Wi-Fi to ZigBee. The NNCTC system outperforms the well-recognized\nWEBee and WIDE design in error performance, achieving an average packet\nreception rate(PRR) of 92.3% and an average symbol error rate(SER) as low as\n1.3%.'}]","Title: MV-VTON: Enhanced Virtual Try-On with Multi-view Diffusion Models

Abstract:

In an increasingly digital environment, Virtual Try-On (VTO) experiences, particularly in the context of clothing, are pivotal for online shopping convenience and customer satisfaction. Despite industry advancements, particularly in the frontal-view scenario, the challenge of multi-view VTO remains unresolved, often failing to maintain the fidelity of clothing, especially in terms of hooded items and back view detail preservation.

This paper introduces Multi-View Virtual Try-On with Diffusion Models (MV-VTON) to address this critical gap. MV-VTON emphasizes the importance of depicting garments accurately from front, back, and side views by introducing a novel method to adaptively select clothing features and fusing global and local clothing features through a joint attention block, which harmonizes with the input person's pose.

The innovation in MV-VTON includes a view-adaptive selection mechanism that enhances the relevance of specific clothing features based on the person's pose, achieving superior performance in maintaining garment fidelity across multiple views. An ablation study validates the efficacy of these components over previous conventional methods. MV-VTON demonstrates quantitative and qualitative superiority in both multi-view and frontal view virtual try-on experiments conducted on the MVG dataset and VITON-HD dataset.

The contributions of this research extend to enhancing realistic VTO experiences in both individual and professional online retail platforms, facilitating clearer item recognition and encouraging purchase intentions by significantly improving the accuracy of see-through effects in virtual environments. This advancement empowers consumers with enhanced confidence in making clothing purchases online, while efficiently providing retailers with detailed, high-fidelity product visualizations that support a personalized yet efficient shopping experience."
"The momentum distribution of particles accelerated at strong non-relativistic
shocks may be influenced by the spatial distribution of the flow speed around
the shock. This phenomenon becomes evident in the cosmic-ray modified shock,
where the particle spectrum itself determines the flow velocity profile
upstream. However, what if the flow speed is not uniform downstream as well?
Hydrodynamics indicates that its spatial variation over the length scales
involved in the acceleration of particles in supernova remnants (SNRs) could be
noticeable.} {In the present paper, we address this issue, initially following
Bell's approach to particle acceleration and then by solving the kinetic
equation. We obtained an analytical solution for the momentum distribution of
particles accelerated at the cosmic-ray modified shock with spatially variable
flow speed downstream.} {We parameterized the downstream speed profile to
illustrate its effect on two model cases, the test particle and non-linear
acceleration at the shock.The resulting particle spectrum is generally softer
in Sedov SNRs because the flow speed distribution reduces the overall shock
compression accessible to particles with higher momenta. On the other hand, the
flow structure in young SNRs could lead to harder spectra. The diffusive
properties of particles play a crucial role as they determine the distance from
the shock, and, as a consequence, the flow speed that particles encounter
downstream. We discuss the effect of the plasma velocity gradient to be
(partially) responsible for the evolution of the radio index and for the
high-energy break visible in gamma rays from some SNRs. We expect that the
effect from the gradient of the flow velocity downstream could be prominent in
regions of SNRs with higher diffusion coefficient and lower magnetic field,
i.e. where acceleration of particles is not very efficient.","[{'Approximations of the self-similar solution for blastwave in a medium\n  with power-law density variation': 'Approximations of the Sedov self-similar solution for a strong point\nexplosion in a medium with the power-law density distribution \\rho^o\\propto\nr^{-m} are reviewed and their accuracy are analyzed. Taylor approximation is\nextended to cases m\\neq 0. Two approximations of the solution are presented in\nthe Lagrangian coordinates for spherical, cylindrical and plane geometry. These\napproximations may be used for the investigation of the ionization structure of\nthe adiabatic flow, i.e., inside adiabatic supernova remnants.'}, {'Approximation for radiation power of electrons due to inverse-Compton\n  process in the black-body photon field': 'An approximation for the inverse-Compton radiation power of electrons in the\nisotropic black-body photon field is presented. The approximation allows one to\ncalculate inverse-Compton emissivity as integral over the energies of incident\nelectrons rather than over the field photon energies. Such an approach allows\nfor accurate modeling of IC emission of electrons with energy spectra being\ndifferent from power-law, in situation where the CPU resources are limited.\nHigh accuracy of this approximation allows one to use it in a wide range of\nconditions, from Thomson to extreme Klein-Nishina limits. The approach adopted\nresults also in some new analytic expressions representing known results in the\nThomson limit.'}, {'Thermal X-ray emission of the remnants of ashperical Supernova\n  explosions': 'Evolution of adiabatic remnants of an aspherical supernova explosion in\nuniform medium are considered. Thermal X-ray emission of such remnants are\ninvestigated. It is shown that integral thermal X-ray characteristics (X-ray\nluminosity and spectrum) of the objects do not allow us to reveal the assymetry\nin the explosion because these characteristics are close to their Sedov\ncounterparts. Surface distribution of X-ray emission is sensitive to anisotropy\nof the explosion and nonuniformity of the interstellar medium.'}, {'Artificial broadening of the high-energy end of electron spectrum in\n  supernova remnants': ""The observed spectrum of a supernova remnant (SNR) is a superposition of many\n``local'' spectra emitted by regions of SNRs that are under different physical\nconditions. The question remains as to whether the broadening of the\nhigh-energy end of the observed nonthermal spectrum of SNRs, like in G347.3-0.5\nand SN 1006, can be an artifact of observations or it is a consequence of the\nmicrophysics involved in the acceleration process. In this note we study the\ninfluence of parameters variations (inside the volume and over the surface of\nSNR) on the shape of the high-energy end of the synchrotron (and also inverse\nCompton) spectrum. We consider three possibilities for these parameter\nvariations: i) gradients downstream of the shock with constant maximum energy\nof the accelerated electrons and the potential variation in time of the\ninjection efficiency, ii) then we add the possibility of the maximum energy\ndepending on time, and finally iii) the possible obliquity dependences of\nmaximum energy and injection efficiency. It is shown that gradients of density\nand magnetic field strength downstream of the shock are ineffective in\nmodifying the shape of the synchrotron spectrum, even if an SNR evolves in the\nnonuniform interstellar medium and/or the injection efficiency varies in time.\nThe time dependence of the maximum energy of the electrons accelerated by the\nshock is also not able to make the observed spectrum much broader. The only\npossibility of producing considerable broadening in the spectrum is the\nvariation in the maximum energy of electrons over the surface of SNR. In such a\ncase, the obliquity dependence of the injection efficiency also affects the\nshape of the spectrum, but its role is less significant.""}, {'Supernova remnants as cosmic ray accelerators. SNR IC 443': 'We examine the hypothesis that some supernova remnants (SNRs) may be\nresponsible for some unidentified gamma-ray sources detected by EGRET\ninstrument aboard the Compton Gamma Ray Observatory. If this is the case,\ngamma-rays are produced via pion production and decay from direct inelastic\ncollisions of accelerated by SNR shock wave ultrarelativistic protons with\ntarget protons of the interstellar medium. We develop a 3-D hydrodynamical\nmodel of SNR IC 443 as a possible cosmic gamma-ray source 2EG J0618+2234. The\nderived parameters of IC 443: the explosion energy E_o=2.7*10^{50} erg, the\ninitial hydrogen number density n(0)=0.21 cm^{-3}, the mean radius R=9.6 pc and\nthe age t=4500 yr result in too low gamma-ray flux, mainly because of the low\nexplosion energy. Therefore, we investigate in detail the hydrodynamics of IC\n443 interaction with a nearby massive molecular cloud and show that the reverse\nshock wave considerably increases the cosmic ray density in the interaction\nregion. Meantime, the Rayleigh-Taylor instability of contact discontinuity\nbetween the SNR and the cloud provides an effective mixing of the containing\ncosmic ray plasma and the cloud material. We show that the resulting gamma-ray\nflux is consistent with the observational data.'}, {'Thermal X-ray composites as an effect of projection': 'A new possibility to explain the nature of thermal X-ray composites (TXCs),\ni.e. a class of supernova remnants (SNRs) with a thermal X-ray centrally-filled\nmorphology within a radio shell, as a projection effect of the 2- or\n3-dimensional shell-like SNR evolved in a nonuniform medium with scale-height\n<10 pc is proposed. Both X-ray and radio morphologies, as well as the basic\ntheoretical features of this kind of SNR and the surrounding medium, are\nconsidered. Theoretical properties of a shell-like SNR evolved at the edge of a\nmolecular cloud correspond to the observed properties of TXCs if the gradient\nof the ambient density does not lie in the projection plane and the magnetic\nfield is nearly aligned with the line of sight. So, at least a part of objects\nfrom the class may be interpreted within the framework of the considered\neffect. The proposed model suggests that SNRs with barrel-like radio and\ncentrally-brightened thermal X-ray morphologies should exist. The model allows\nus to consider TXCs as prospective sources of proton origin gamma-rays.'}, {'A New Model for the Thermal X-ray Composites and the Neutral Pion Decay\n  Gamma-Rays from Supernova Remnants': 'Recent nonthermal X-ray and gamma-ray observations, attributed to electron\nemission processes, for the first time give an experimental confirmation that\nelectrons are accelerated on SNR shocks up to the energy 10^{14} eV. We have no\ndirect observational confirmations about proton acceleration by SNR. Different\nmodels of gamma-emission from SNRs predict different emission mechanisms as\ndominating. Only decays of neutral pion created in proton-nucleon interactions\nallow us to look inside the CR nuclear component acceleration processes. A new\nmodel for the thermal X-ray composites strongly suggest that thermal X-ray peak\ninside the radio shell of SNR tells us about entering of one part of SNR shock\ninto a denser medium compared with other parts of the shell. This makes a TXCs\npromising sites for gamma-ray generation via neutral pion decays. Detailed\nconsideration of SNR-cloud interaction allows to increase an expected proton\ninduced gamma-ray flux from SNR at least on an order of magnitude, that allows\nto adjust the theoretical proton origin gamma-luminosities with observed fluxes\nat least for a few SNRs even for low density (n=10-100 cm^{-3}) cloud.'}, {'Model for Synchrotron Emission from Shell Supernova Remnants in\n  Nonuniform Interstellar Medium and Nonuniform Magnetic Field': 'Possibility to model the high energy synchrotron emission (in X- and\ngamma-rays) from supernova remnants is an important task for modern astronomy\nand astrophysics, because it may be responsible for the nonthermal X-rays and\nTeV gamma-rays observed recently from a number of SNRs. This emission allows as\nto look in the processes of particle acceleration on SNR shocks and generation\nof cosmic rays. In this paper, a model for the synchrotron emission from shell\nSNR in nonuniform interstellar medium and nonuniform magnetic field is\npresented. This model is a generalization of the model of Reynolds and\nChevalier developed for a spherical SNR in the uniform medium and uniform\nmagnetic field. The model will be used for studies on the thermal and\nnonthermal X-ray images and spectra from nonspherical SNRs in different\ninterstellar magnetic field configurations.'}, {'Hadronic gamma-ray images of Sedov supernova remnants': 'A number of modern experiments in high-energy astrophysics produce images of\nsupernova remnants (SNRs) in the TeV and GeV gamma-rays. Either relativistic\nelectrons (due to the inverse-Compton scattering) or protons (due to the pion\ndecays) may be responsible for this emission. In particular, the broad-band\nspectra of SNRs may be explained in both leptonic and hadronic scenarios.\nAnother kind of observational data, namely, images of SNRs, is an important\npart of experimental information. We present a method to model gamma-ray images\nof Sedov SNRs in uniform media and magnetic field due to hadronic emission.\nThese gamma-rays are assumed to appear as a consequence of meson decays\nproduced in inelastic collisions of accelerated protons with thermal protons\ndownstream of the shock - a model would be relevant for SNRs without firm\nconfirmations of the shock-cloud interaction, as e.g. SN1006. Distribution of\nsurface brightness of the shell-like SNR is synthesized numerically for a\nnumber of configurations. An approximate analytical formula for azimuthal and\nradial variation of hadronic gamma-ray brightness close to the shock is\nderived. The properties of images as well as the main factors determining the\nsurface brightness distribution are determined. Some conclusions which would be\nrelevant to SN1006 are discussed.'}, {'GeV light curves of young supernova remnants': 'Observational data from the Fermi Gamma-ray Space Telescope are analyzed with\na goal in mind to look for variations in gamma-ray flux from young shell-like\nsupernova remnants. Uniform methodological approach is adopted for all SNRs\nconsidered. G1.9+0.3 and Kepler SNRs are not detected. The light curves of\nCas~A and Tycho SNRs are compatible with the steady GeV flux during the recent\nten years, as also X-ray and radio fluxes. Less confident results on SN1006 and\nSN1987A are discussed.'}]","Title: DSA with non-uniform flow velocity: The downstream effect on particle acceleration in supernova remnants

Background: In conventional diffusive shock acceleration (DSA) theory, particles experience acceleration on a shock front with the assumption of uniform velocity conditions. However, numerical simulations of supernova remnants (SNRs) show that both upstream and downstream velocities can vary, challenging this idealization.

Objective: This paper investigates the downstream velocity variations in SNRs and their impact on particle acceleration. The study employs a generalized Bell approach to DSA, replacing the uniform velocity condition with a non-uniform velocity profile.

Innovations: This work introduces the use of a non-uniform velocity model within the DSA framework, offering a more realistic treatment of the particle acceleration process. 

Methods: Semi-analytical calculations combined with particle acceleration techniques were employed. The evolution of radio spectral index in young SNRs was studied, influenced by a time-dependent, non-uniform velocity profile within a sub-angle of a plane shock. 

Results: A ""downstream"" Grad-Shafranov effect due to non-uniform velocities was observed, resulting in a modified momentum distribution and spectral break, differing from classic power-law expectations.

Contributions: The findings advance DSA theory by incorporating velocity variations in shock post-acceleration regions. This provides a new framework for predicting and explaining spectral features in SNR radio observations.

Applications: Determining SNR acceleration mechanisms with this framework could aid in interpreting X-ray and radio spectra of SNRs and potentially in the search for ultra-high-energy cosmic rays.

In conclusion, the paper significantly advances our understanding of particle acceleration in SNRs by addressing the evolution in both forward and reverse shock dynamics, offering new insights for modeling the cosmic environment.

---

This abstract maps to the requested components in a succinct, focused manner, ensuring all key elements are highlighted within the 300-word limit."
"With the capabilities of understanding and executing natural language
instructions, Large language models (LLMs) can potentially act as a powerful
tool for textual data augmentation. However, the quality of augmented data
depends heavily on the augmentation instructions provided, and the
effectiveness can fluctuate across different downstream tasks. While manually
crafting and selecting instructions can offer some improvement, this approach
faces scalability and consistency issues in practice due to the diversity of
downstream tasks. In this work, we address these limitations by proposing a new
solution, which can automatically generate a large pool of augmentation
instructions and select the most suitable task-informed instructions, thereby
empowering LLMs to create high-quality augmented data for different downstream
tasks. Empirically, the proposed approach consistently generates augmented data
with better quality compared to non-LLM and LLM-based data augmentation
methods, leading to the best performance on 26 few-shot learning tasks sourced
from a wide range of application domains.","[{'MM-COVID: A Multilingual and Multimodal Data Repository for Combating\n  COVID-19 Disinformation': ""The COVID-19 epidemic is considered as the global health crisis of the whole\nsociety and the greatest challenge mankind faced since World War Two.\nUnfortunately, the fake news about COVID-19 is spreading as fast as the virus\nitself. The incorrect health measurements, anxiety, and hate speeches will have\nbad consequences on people's physical health, as well as their mental health in\nthe whole world. To help better combat the COVID-19 fake news, we propose a new\nfake news detection dataset MM-COVID(Multilingual and Multidimensional COVID-19\nFake News Data Repository). This dataset provides the multilingual fake news\nand the relevant social context. We collect 3981 pieces of fake news content\nand 7192 trustworthy information from English, Spanish, Portuguese, Hindi,\nFrench and Italian, 6 different languages. We present a detailed and\nexploratory analysis of MM-COVID from different perspectives and demonstrate\nthe utility of MM-COVID in several potential applications of COVID-19 fake news\nstudy on multilingual and social media.""}, {'Fact-Enhanced Synthetic News Generation': 'The advanced text generation methods have witnessed great success in text\nsummarization, language translation, and synthetic news generation. However,\nthese techniques can be abused to generate disinformation and fake news. To\nbetter understand the potential threats of synthetic news, we develop a new\ngeneration method FactGen to generate high-quality news content. The existing\ntext generation methods either afford limited supplementary information or lose\nconsistency between the input and output which makes the synthetic news less\ntrustworthy. To address these issues, FactGen retrieves external facts to\nenrich the output and reconstructs the input claim from the generated content\nto improve the consistency among the input and the output. Experiment results\non real-world datasets show that the generated news contents of FactGen are\nconsistent and contain rich facts. We also discuss the possible defending\nmethod to identify these synthetic news pieces if FactGen is used to generate\nsynthetic news.'}, {'Analogue Radio Over Fiber for Next-Generation RAN: Challenges and\n  Opportunities': 'The radio access network (RAN) connects the users to the core networks, where\ntypically digitised radio over fiber (D-RoF) links are employed. The data rate\nof the RAN is limited by the hardware constraints of the D-RoF-based backhaul\nand fronthaul. In order to break this bottleneck, the potential of the analogue\nradio over fiber (A-RoF) based RAN techniques are critically appraised for\nemployment in the next-generation systems, where increased-rate massive\nmultiple-input-multiple-output (massive-MIMO) and millimeter wave (mmWave)\ntechniques will be implemented. We demonstrate that huge bandwidth and\npower-consumption cost benefits may accrue upon using A-RoF for next-generation\nRANs. We provide an overview of the recent A-RoF research and a performance\ncomparison of A-RoF and D-RoF, concluding with further insights on the future\npotential of A-RoF.'}, {'Analogue Radio over Fiber aided Multi-service Communications for High\n  Speed Trains': 'High speed trains (HST) have gradually become an essential means of\ntransportation, where given our digital world, it is expected that passengers\nwill be connected all the time. More specifically, the on-board passengers\nrequire fast mobile connections, which cannot be provided by the currently\nimplemented cellular networks. Hence, in this article, we propose an analogue\nradio over fiber (A-RoF) aided multi-service network architecture for\nhigh-speed trains, in order to enhance the quality of service as well as reduce\nthe cost of the radio access network (RAN). The proposed design can\nsimultaneously support sub- 6GHz as well as milimeter wave (mmWave)\ncommunications using the same architecture. Explicitly, we design a photonics\naided beamforming technique in order to eliminate the bulky high-speed\nelectronic phase-shifters and the hostile broadband mmWave mixers while\nproviding a low-cost RAN solution. Finally, a beamforming range of 180 is\ndemonstrated with a high resolution using our proposed system.'}, {'Distributed Primal-dual Optimization for Heterogeneous Multi-agent\n  Systems': 'Heterogeneous networks comprise agents with varying capabilities in terms of\ncomputation, storage, and communication. In such settings, it is crucial to\nfactor in the operating characteristics in allowing agents to choose\nappropriate updating schemes, so as to better distribute computational tasks\nand utilize the network more efficiently. We consider the multi-agent\noptimization problem of cooperatively minimizing the sum of local strongly\nconvex objectives. We propose an asynchronous distributed primal-dual protocol,\nwhich allows for the primal update steps to be agent-dependent (an agent can\nopt between first-order or Newton updates). Our analysis introduces a unifying\nframework for such hybrid optimization scheme and establishes global linear\nconvergence in expectation, under strongly convex objectives and general agent\nactivation schemes. Numerical experiments on real life datasets attest to the\nmerits of the proposed algorithm.'}, {'GRENADE: Graph-Centric Language Model for Self-Supervised Representation\n  Learning on Text-Attributed Graphs': 'Self-supervised representation learning on text-attributed graphs, which aims\nto create expressive and generalizable representations for various downstream\ntasks, has received increasing research attention lately. However, existing\nmethods either struggle to capture the full extent of structural context\ninformation or rely on task-specific training labels, which largely hampers\ntheir effectiveness and generalizability in practice. To solve the problem of\nself-supervised representation learning on text-attributed graphs, we develop a\nnovel Graph-Centric Language model -- GRENADE. Specifically, GRENADE exploits\nthe synergistic effect of both pre-trained language model and graph neural\nnetwork by optimizing with two specialized self-supervised learning algorithms:\ngraph-centric contrastive learning and graph-centric knowledge alignment. The\nproposed graph-centric self-supervised learning algorithms effectively help\nGRENADE to capture informative textual semantics as well as structural context\ninformation on text-attributed graphs. Through extensive experiments, GRENADE\nshows its superiority over state-of-the-art methods. Implementation is\navailable at \\url{https://github.com/bigheiniu/GRENADE}.'}, {'FedADMM: A Robust Federated Deep Learning Framework with Adaptivity to\n  System Heterogeneity': 'Federated Learning (FL) is an emerging framework for distributed processing\nof large data volumes by edge devices subject to limited communication\nbandwidths, heterogeneity in data distributions and computational resources, as\nwell as privacy considerations. In this paper, we introduce a new FL protocol\ntermed FedADMM based on primal-dual optimization. The proposed method leverages\ndual variables to tackle statistical heterogeneity, and accommodates system\nheterogeneity by tolerating variable amount of work performed by clients.\nFedADMM maintains identical communication costs per round as FedAvg/Prox, and\ngeneralizes them via the augmented Lagrangian. A convergence proof is\nestablished for nonconvex objectives, under no restrictions in terms of data\ndissimilarity or number of participants per round of the algorithm. We\ndemonstrate the merits through extensive experiments on real datasets, under\nboth IID and non-IID data distributions across clients. FedADMM consistently\noutperforms all baseline methods in terms of communication efficiency, with the\nnumber of rounds needed to reach a prescribed accuracy reduced by up to 87%.\nThe algorithm effectively adapts to heterogeneous data distributions through\nthe use of dual variables, without the need for hyperparameter tuning, and its\nadvantages are more pronounced in large-scale systems.'}, {'DN-ADMM: Distributed Newton ADMM for Multi-agent Optimization': 'In a multi-agent network, we consider the problem of minimizing an objective\nfunction that is expressed as the sum of private convex and smooth functions,\nand a (possibly) non-differentiable convex regularizer. We propose a novel\ndistributed second-order method based on the framework of Alternating Direction\nMethod of Multipliers (ADMM), by invoking approximate Newton iterations to the\nprimal update corresponding to the differentiable part. In order to achieve a\ndistributed implementation, the total Hessian matrix is split into a diagonal\ncomponent (locally computable) and an off-diagonal component (that requires\ncommunication between neighboring agents). Subsequently, the Hessian inverse is\napproximated by a truncation of the Taylor expansion to $K$ terms: this amounts\nto fully distributed updates entailing $K$ distributed communication rounds. We\nestablish global linear convergence to the primal-dual optimal solution under\nthe assumption that the private functions are strongly convex and have\nLipschitz continuous gradient. Numerical experiments demonstrate the merits of\nthe approach comparatively with state-of-the-art methods.'}, {'A Communication Efficient Quasi-Newton Method for Large-scale\n  Distributed Multi-agent Optimization': 'We propose a communication efficient quasi-Newton method for large-scale\nmulti-agent convex composite optimization. We assume the setting of a network\nof agents that cooperatively solve a global minimization problem with strongly\nconvex local cost functions augmented with a non-smooth convex regularizer. By\nintroducing consensus variables, we obtain a block-diagonal Hessian and thus\neliminate the need for additional communication when approximating the\nobjective curvature information. Moreover, we reduce computational costs of\nexisting primal-dual quasi-Newton methods from $\\mathcal{O}(d^3)$ to\n$\\mathcal{O}(cd)$ by storing $c$ pairs of vectors of dimension $d$. An\nasynchronous implementation is presented that removes the need for\ncoordination. Global linear convergence rate in expectation is established, and\nwe demonstrate the merit of our algorithm numerically with real datasets.'}, {'Fuzzy-Depth Objects Grasping Based on FSG Algorithm and a Soft Robotic\n  Hand': 'Autonomous grasping is an important factor for robots physically interacting\nwith the environment and executing versatile tasks. However, a universally\napplicable, cost-effective, and rapidly deployable autonomous grasping approach\nis still limited by those target objects with fuzzy-depth information. Examples\nare transparent, specular, flat, and small objects whose depth is difficult to\nbe accurately sensed. In this work, we present a solution to those fuzzy-depth\nobjects. The framework of our approach includes two major components: one is a\nsoft robotic hand and the other one is a Fuzzy-depth Soft Grasping (FSG)\nalgorithm. The soft hand is replaceable for most existing soft hands/grippers\nwith body compliance. FSG algorithm exploits both RGB and depth images to\npredict grasps while not trying to reconstruct the whole scene. Two grasping\nprimitives are designed to further increase robustness. The proposed method\noutperforms reference baselines in unseen fuzzy-depth objects grasping\nexperiments (84% success rate).'}]","Title: Self-LLMDA: Automated Data Augmentation with Task-Informed Instructions for Large Language Models

Abstract:

In the quest for enhancing the effectiveness of large language models (LLMs), textual data augmentation emerges as a powerful approach. Primarily, existing methods have relied on domain expert-designed augmentation instructions for LLMs. Yet, numerous challenges, including the inconsistent impact of these instructions across tasks and their structure which tends to be redundant for specific downstream purposes, limit their generality and adaptability. 

This paper introduces Self-LLMDA, a novel framework that automates augmentation instruction generation and selection, bridging this gap. Self-LLMDA consists of two innovative steps. Firstly, it expands the scope of augmentation strategies by creating diverse and effective seed augmentation instructions using LLMs. Secondly, it employs a task-informed scoring model to identify and select instructions that are optimized to promote the performance of target models. 

The study was executed across a large collection of 26 types of few-shot learning tasks, using a variety of datasets. The aim was to thoroughly evaluate the effectiveness of Self-LLMDA, comparing it against common data augmentation techniques and a manually designed method (Manual-LLMDA).

The primary results indicated that the proposed Self-LLMDA consistently outperforms existing methods, demonstrating its potential to tailor augmentation instructions more effectively to the properties of different tasks. Insights from the study highlighted the importance of task-informed instruction selection in enhancing the generalization capabilities of LLMs.

In conclusion, Self-LLMDA contributes a comprehensive automation mechanism for LLM data augmentation, enabler of broader adaptability and generalization compared to traditional methods. Its applications span across various domains including natural language processing tasks such as hate speech detection, question answering, natural language inference, and phrase detection,带来更多智能化处理能力, paving ways for state-of-the-art model performance improvements in these fields."
"Large Language Models (LLMs) have shown great ability in solving traditional
natural language tasks and elementary reasoning tasks with appropriate
prompting techniques. However, their ability is still limited in solving
complicated science problems. In this work, we aim to push the upper bound of
the reasoning capability of LLMs by proposing a collaborative multi-agent,
multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs
to play different roles in a problem-solving team, and encourage different
role-play agents to collaboratively solve the target task. In particular, we
discover that applying different reasoning paths for different roles is an
effective strategy to implement few-shot prompting approaches in the
multi-agent scenarios. Empirical results demonstrate the effectiveness of the
proposed methods on two college-level science problems over competitive
baselines. Our further analysis shows the necessity of prompting LLMs to play
different roles or experts independently. We release the code at:
https://github.com/amazon-science/comm-prompt","[{'Instantaneous GNSS attitude determination: A Monte Carlo sampling\n  approach': 'A novel instantaneous GNSS ambiguity resolution approach which makes use of\nonly single-frequency carrier phase measurements for ultra-short baseline\nattitude determination is proposed. The Monte Carlo sampling method is employed\nto obtain the probability density function of ambiguities from a\nquaternion-based GNSS-attitude model and the LAMBDA method strengthened with a\nscreening mechanism is then utilized to fix the integer values. Experimental\nresults show that 100% success rate could be achieved for ultra-short\nbaselines.'}, {'Spatiotemporal information conversion machine for time-series prediction': 'Making predictions in a robust way is a difficult task only based on the\nobserved data of a nonlinear system. In this work, a neural network computing\nframework, the spatiotemporal information conversion machine (STICM), was\ndeveloped to efficiently and accurately render a multistep-ahead prediction of\na time series by employing a spatial-temporal information (STI) transformation.\nSTICM combines the advantages of both the STI equation and the temporal\nconvolutional network, which maps the high-dimensional/spatial data to the\nfuture temporal values of a target variable, thus naturally providing the\nprediction of the target variable. From the observed variables, the STICM also\ninfers the causal factors of the target variable in the sense of Granger\ncausality, which are in turn selected as effective spatial information to\nimprove the prediction robustness of time-series. The STICM was successfully\napplied to both benchmark systems and real-world datasets, all of which show\nsuperior and robust performance in multistep-ahead prediction, even when the\ndata were perturbed by noise. From both theoretical and computational\nviewpoints, the STICM has great potential in practical applications in\nartificial intelligence (AI) or as a model-free method based only on the\nobserved data, and also opens a new way to explore the observed\nhigh-dimensional data in a dynamical manner for machine learning.'}, {'Gravity Gradient Tensor Eigendecomposition for Spacecraft Positioning': 'In this Note, a new approach to spacecraft positioning based on GGT inversion\nis presented. The gravity gradient tensor is initially measured in the\ngradiometer reference frame (GRF) and then transformed to the Earth-Centered\nEarth-Fixed (ECEF) frame via attitude information as well as Earth rotation\nparameters. Matrix Eigen-Decomposition is introduced to directly translate GGT\ninto position based on the fact that the eigenvalues and eigenvectors of GGT\nare simplespecific functions of spherical coordinates of the observation\nposition. without the need of an initial position. Unlike the strategy of\ninertial navigation aiding, no prediction or first guess of the spacecraft\nposition is needed. The method makes use of the J2 gravity model, and is\nsuitable for space navigation where higher frequency terrain contributions to\nthe GGT signals can be neglected.'}, {'Low-Earth Orbit Determination from Gravity Gradient Measurements': 'An innovative orbit determination method which makes use of gravity gradients\nfor Low-Earth-Orbiting satellites is proposed. The measurement principle of\ngravity gradiometry is briefly reviewed and the sources of measurement error\nare analyzed. An adaptive hybrid least squares batch filter based on\nlinearization of the orbital equation and unscented transformation of the\nmeasurement equation is developed to estimate the orbital states and the\nmeasurement biases. The algorithm is tested with the actual flight data from\nthe European Space Agency Gravity field and steady-state Ocean Circulation\nExplorer. The orbit determination results are compared with the GPS-derived\norbits. The radial and cross-track position errors are on the order of tens of\nmeters, whereas the along-track position error is over one order of magnitude\nlarger. The gravity gradient based orbit determination method is promising for\npotential use in GPS-denied spacecraft navigation.'}, {'Autonomous Orbit Determination via Kalman Filtering of Gravity Gradients': 'Spaceborne gravity gradients are proposed in this paper to provide autonomous\norbit determination capabilities for near Earth satellites. The gravity\ngradients contain useful position information which can be extracted by\nmatching the observations with a precise gravity model. The extended Kalman\nfilter is investigated as the principal estimator. The stochastic model of\norbital motion, the measurement equation and the model configuration are\ndiscussed for the filter design. An augmented state filter is also developed to\ndeal with unknown significant measurement biases. Simulations are conducted to\nanalyze the effects of initial errors, data-sampling periods, orbital heights,\nattitude and gradiometer noise levels, and measurement biases. Results show\nthat the filter performs well with additive white noise observation errors.\nDegraded observability for the along-track position is found for the augmented\nstate filter. Real flight data from the GOCE satellite are used to test the\nalgorithm. Radial and cross-track position errors of less than 100 m have been\nachieved.'}, {'Real-time kinematic positioning of LEO satellites using a\n  single-frequency GPS receiver': 'Due to their low cost and low power consumption, single-frequency GPS\nreceivers are considered suitable for low-cost space applications such as small\nsatellite missions. Recently, requirements have emerged for real-time accurate\norbit determination at sub-meter level in order to carry out onboard geocoding\nof high-resolution imagery, open-loop operation of altimeters and radio\noccultation. This study proposes an improved real-time kinematic positioning\nmethod for LEO satellites using single-frequency receivers. The C/A code and L1\nphase are combined to eliminate ionospheric effects. The epoch-differenced\ncarrier phase measurements are utilized to acquire receiver position changes\nwhich are further used to smooth the absolute positions. A kinematic Kalman\nfilter is developed to implement kinematic orbit determination. Actual flight\ndata from China small satellite SJ-9A are used to test the navigation\nperformance. Results show that the proposed method outperforms traditional\nkinematic positioning method in terms of accuracy. A 3D position accuracy of\n0.72 m and 0.79 m has been achieved using the predicted portion of IGS\nultra-rapid products and broadcast ephemerides, respectively.'}, {'Autonomous Orbit Determination Using Epoch-Differenced Gravity Gradients\n  and Starlight Refraction': 'Autonomous orbit determination via integration of epoch-differenced gravity\ngradients and starlight refraction is proposed in this paper for\nlow-Earth-orbiting satellites operating in GPS-denied environments. The\nstarlight refrac-tion can compensate for the significant along-track position\nerror using solely gravity gradients and benefit from the integration in view\nof accuracy improvement in radial and cross-track position estimates. The\nbetween-epoch dif-ferencing of gravity gradients is employed to eliminate\nslowly varying measurement biases and noises near the orbit revolution\nfrequency. The refraction angle is directly used as measurement and its\nJacobian matrix is derived from an implicit observation equation. An\ninformation fusion filter based on sequential extended Kalman filter is\ndevel-oped for the orbit determination. Truth-model simulations are used to\ntest the performance of the algorithm and the effects of differencing intervals\nand orbital heights are analyzed. A semi-simulation study using actual gravity\ngra-dient data from the Gravity field and steady-state Ocean Circulation\nExplorer (GOCE) combined with simulated starlight refraction measurements is\nfurther conducted and a three-dimensional position accuracy of better than 100\nm is achieved.'}, {'Precise Real-Time Navigation of LEO Satellites Using a Single-Frequency\n  GPS Receiver and Ultra-Rapid Ephemerides': ""Precise (sub-meter level) real-time navigation using a space-capable\nsingle-frequency global positioning system (GPS) receiver and ultra-rapid\n(real-time) ephemerides from the international global navigation satellite\nsystems service is proposed for low-Earth-orbiting (LEO) satellites. The C/A\ncode and L1 carrier phase measurements are combined and single-differenced to\neliminate first-order ionospheric effects and receiver clock offsets. A\nrandom-walk process is employed to model the phase ambiguities in order to\nabsorb the time-varying and satellite-specific higher-order measurement errors\nas well as the GPS clock correction errors. A sequential Kalman filter which\nincorporates the known orbital dynamic model is developed to estimate orbital\nstates and phase ambiguities without matrix inversion. Real flight data from\nthe single-frequency GPS receiver onboard China's SJ-9A small satellite are\nprocessed to evaluate the orbit determination accuracy. Statistics from\ninternal orbit assessments indicate that three-dimensional accuracies of better\nthan 0.50 m and 0.55 mm/s are achieved for position and velocity, respectively.""}, {'DEFM: Delay E mbedding based Forecast Machine for Time Series\n  Forecasting by Spatiotemporal Information Transformation': ""Making accurate forecasts for a complex system is a challenge in various\npractical applications. The major difficulty in solving such a problem concerns\nnonlinear spatiotemporal dynamics with time-varying characteristics. Takens'\ndelay embedding theory provides a way to transform high-dimensional spatial\ninformation into temporal information. In this work, by combining delay\nembedding theory and deep learning techniques, we propose a novel framework,\nDelay-Embedding-based Forecast Machine (DEFM), to predict the future values of\na target variable in a self-supervised and multistep-ahead manner based on\nhigh-dimensional observations. With a three-module spatiotemporal architecture,\nthe DEFM leverages deep neural networks to effectively extract both the\nspatially and temporally associated information from the observed time series\neven with time-varying parameters or additive noise. The DEFM can accurately\npredict future information by transforming spatiotemporal information to the\ndelay embeddings of a target variable. The efficacy and precision of the DEFM\nare substantiated through applications in three spatiotemporally chaotic\nsystems: a 90-dimensional (90D) coupled Lorenz system, the Lorenz 96 system,\nand the Kuramoto-Sivashinsky (KS) equation with inhomogeneity. Additionally,\nthe performance of the DEFM is evaluated on six real-world datasets spanning\nvarious fields. Comparative experiments with five prediction methods illustrate\nthe superiority and robustness of the DEFM and show the great potential of the\nDEFM in temporal information mining and forecasting""}, {'TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes': 'Many basic indoor activities such as eating or writing are always conducted\nupon different tabletops (e.g., coffee tables, writing desks). It is\nindispensable to understanding tabletop scenes in 3D indoor scene parsing\napplications. Unfortunately, it is hard to meet this demand by directly\ndeploying data-driven algorithms, since 3D tabletop scenes are rarely available\nin current datasets. To remedy this defect, we introduce TO-Scene, a\nlarge-scale dataset focusing on tabletop scenes, which contains 20,740 scenes\nwith three variants. To acquire the data, we design an efficient and scalable\nframework, where a crowdsourcing UI is developed to transfer CAD objects from\nModelNet and ShapeNet onto tables from ScanNet, then the output tabletop scenes\nare simulated into real scans and annotated automatically.\n  Further, a tabletop-aware learning strategy is proposed for better perceiving\nthe small-sized tabletop instances. Notably, we also provide a real scanned\ntest set TO-Real to verify the practical value of TO-Scene. Experiments show\nthat the algorithms trained on TO-Scene indeed work on the realistic test data,\nand our proposed tabletop-aware learning strategy greatly improves the\nstate-of-the-art results on both 3D semantic segmentation and object detection\ntasks. Dataset and code are available at\nhttps://github.com/GAP-LAB-CUHK-SZ/TO-Scene.'}]","**Abstract**

Our research introduces the Comprehensive Multi-Agent Methods (CoMM) for addressing complex science problems using Large Language Models (LLMs). This innovation supports efficient resolution of intricate scientific challenges by bridging the gap between LLMs and domain-specific expertise. Our objective is to harness multi-agent systems in which individual agents play different roles, drawing on their unique perspectives and specialized reasoning paths. We divulge a domain expert prompting function and a debriefing function within our framework. For complex physics problems, we juxtapose physics and mathematics perspectives provided by physicist and mathematician agents, respectively. In moral scenarios, we employ an actor, a task decomposer, and a problem solver. This multifaceted approach has resulted in commendable performance improvements over current benchmarks, thereby augmenting LLM resolution capabilities for challenging problems. The versatility and application of our method have potential in enhancing scientific discovery, improving educational outcomes, and facilitating expert-level consultation in various scientific domains. Our research marks a significant stride toward leveraging multi-agent systems to optimize AI performance in scientific problem-solving.

**Keywords**: Comprehensive Multi-Agent Methods, Large Language Models, Domain-specific Expertise, Complex Science Problems, Improved Performance"
"Open Japanese large language models (LLMs) have been trained on the Japanese
portions of corpora such as CC-100, mC4, and OSCAR. However, these corpora were
not created for the quality of Japanese texts. This study builds a large
Japanese web corpus by extracting and refining text from the Common Crawl
archive (21 snapshots of approximately 63.4 billion pages crawled between 2020
and 2023). This corpus consists of approximately 312.1 billion characters
(approximately 173 million pages), which is the largest of all available
training corpora for Japanese LLMs, surpassing CC-100 (approximately 25.8
billion characters), mC4 (approximately 239.7 billion characters) and OSCAR
23.10 (approximately 74 billion characters). To confirm the quality of the
corpus, we performed continual pre-training on Llama 2 7B, 13B, 70B, Mistral 7B
v0.1, and Mixtral 8x7B Instruct as base LLMs and gained consistent (6.6-8.1
points) improvements on Japanese benchmark datasets. We also demonstrate that
the improvement on Llama 2 13B brought from the presented corpus was the
largest among those from other existing corpora.","[{'Positional Encoding to Control Output Sequence Length': 'Neural encoder-decoder models have been successful in natural language\ngeneration tasks. However, real applications of abstractive summarization must\nconsider additional constraint that a generated summary should not exceed a\ndesired length. In this paper, we propose a simple but effective extension of a\nsinusoidal positional encoding (Vaswani et al., 2017) to enable neural\nencoder-decoder model to preserves the length constraint. Unlike in previous\nstudies where that learn embeddings representing each length, the proposed\nmethod can generate a text of any length even if the target length is not\npresent in training data. The experimental results show that the proposed\nmethod can not only control the generation length but also improve the ROUGE\nscores.'}, {'Enhancing Machine Translation with Dependency-Aware Self-Attention': 'Most neural machine translation models only rely on pairs of parallel\nsentences, assuming syntactic information is automatically learned by an\nattention mechanism. In this work, we investigate different approaches to\nincorporate syntactic knowledge in the Transformer model and also propose a\nnovel, parameter-free, dependency-aware self-attention mechanism that improves\nits translation quality, especially for long sentences and in low-resource\nscenarios. We show the efficacy of each approach on WMT English-German and\nEnglish-Turkish, and WAT English-Japanese translation tasks.'}, {'Knowledge of Pretrained Language Models on Surface Information of Tokens': 'Do pretrained language models have knowledge regarding the surface\ninformation of tokens? We examined the surface information stored in word or\nsubword embeddings acquired by pretrained language models from the perspectives\nof token length, substrings, and token constitution. Additionally, we evaluated\nthe ability of models to generate knowledge regarding token surfaces. We\nfocused on 12 pretrained language models that were mainly trained on English\nand Japanese corpora. Experimental results demonstrate that pretrained language\nmodels have knowledge regarding token length and substrings but not token\nconstitution. Additionally, the results imply that there is a bottleneck on the\ndecoder side in terms of effectively utilizing acquired knowledge.'}, {'Multi-Task Learning for Cross-Lingual Abstractive Summarization': 'We present a multi-task learning framework for cross-lingual abstractive\nsummarization to augment training data. Recent studies constructed pseudo\ncross-lingual abstractive summarization data to train their neural\nencoder-decoders. Meanwhile, we introduce existing genuine data such as\ntranslation pairs and monolingual abstractive summarization data into training.\nOur proposed method, Transum, attaches a special token to the beginning of the\ninput sentence to indicate the target task. The special token enables us to\nincorporate the genuine data into the training data easily. The experimental\nresults show that Transum achieves better performance than the model trained\nwith only pseudo cross-lingual summarization data. In addition, we achieve the\ntop ROUGE score on Chinese-English and Arabic-English abstractive\nsummarization. Moreover, Transum also has a positive effect on machine\ntranslation. Experimental results indicate that Transum improves the\nperformance from the strong baseline, Transformer, in Chinese-English,\nArabic-English, and English-Japanese translation datasets.'}, {'Unsupervised Domain Adaptation for Sparse Retrieval by Filling\n  Vocabulary and Word Frequency Gaps': 'IR models using a pretrained language model significantly outperform lexical\napproaches like BM25. In particular, SPLADE, which encodes texts to sparse\nvectors, is an effective model for practical use because it shows robustness to\nout-of-domain datasets. However, SPLADE still struggles with exact matching of\nlow-frequency words in training data. In addition, domain shifts in vocabulary\nand word frequencies deteriorate the IR performance of SPLADE. Because\nsupervision data are scarce in the target domain, addressing the domain shifts\nwithout supervision data is necessary. This paper proposes an unsupervised\ndomain adaptation method by filling vocabulary and word-frequency gaps. First,\nwe expand a vocabulary and execute continual pretraining with a masked language\nmodel on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse\nvectors by inverse document frequency weights to consider the importance of\ndocuments with lowfrequency words. We conducted experiments using our method on\ndatasets with a large vocabulary gap from a source domain. We show that our\nmethod outperforms the present stateof-the-art domain adaptation method. In\naddition, our method achieves state-of-the-art results, combined with BM25.'}, {'Semantic Specialization for Knowledge-based Word Sense Disambiguation': 'A promising approach for knowledge-based Word Sense Disambiguation (WSD) is\nto select the sense whose contextualized embeddings computed for its definition\nsentence are closest to those computed for a target word in a given sentence.\nThis approach relies on the similarity of the \\textit{sense} and\n\\textit{context} embeddings computed by a pre-trained language model. We\npropose a semantic specialization for WSD where contextualized embeddings are\nadapted to the WSD task using solely lexical knowledge. The key idea is, for a\ngiven sense, to bring semantically related senses and contexts closer and send\ndifferent/unrelated senses farther away. We realize this idea as the joint\noptimization of the Attract-Repel objective for sense pairs and the\nself-training objective for context-sense pairs while controlling deviations\nfrom the original embeddings. The proposed method outperformed previous studies\nthat adapt contextualized embeddings. It achieved state-of-the-art performance\non knowledge-based WSD when combined with the reranking heuristic that uses the\nsense inventory. We found that the similarity characteristics of specialized\nembeddings conform to the key idea. We also found that the (dis)similarity of\nembeddings between the related/different/unrelated senses correlates well with\nthe performance of WSD.'}, {'Causal Reasoning through Two Layers of Cognition for Improving\n  Generalization in Visual Question Answering': 'Generalization in Visual Question Answering (VQA) requires models to answer\nquestions about images with contexts beyond the training distribution. Existing\nattempts primarily refine unimodal aspects, overlooking enhancements in\nmultimodal aspects. Besides, diverse interpretations of the input lead to\nvarious modes of answer generation, highlighting the role of causal reasoning\nbetween interpreting and answering steps in VQA. Through this lens, we propose\nCognitive pathways VQA (CopVQA) improving the multimodal predictions by\nemphasizing causal reasoning factors. CopVQA first operates a pool of pathways\nthat capture diverse causal reasoning flows through interpreting and answering\nstages. Mirroring human cognition, we decompose the responsibility of each\nstage into distinct experts and a cognition-enabled component (CC). The two CCs\nstrategically execute one expert for each stage at a time. Finally, we\nprioritize answer predictions governed by pathways involving both CCs while\ndisregarding answers produced by either CC, thereby emphasizing causal\nreasoning and supporting generalization. Our experiments on real-life and\nmedical data consistently verify that CopVQA improves VQA performance and\ngeneralization across baselines and domains. Notably, CopVQA achieves a new\nstate-of-the-art (SOTA) on PathVQA dataset and comparable accuracy to the\ncurrent SOTA on VQA-CPv2, VQAv2, and VQA RAD, with one-fourth of the model\nsize.'}, {'Reducing Sequence Length by Predicting Edit Operations with Large\n  Language Models': 'Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious tasks and gained significant attention. LLMs are also used for local\nsequence transduction tasks, including grammatical error correction (GEC) and\nformality style transfer, where most tokens in a source text are kept\nunchanged. However, the models that generate all target tokens in such tasks\nhave a tendency to simply copy the input text as is, without making needed\nchanges, because the difference between input and output texts is minimal in\nthe training data. This is also inefficient because the computational cost\ngrows quadratically with the target sequence length with Transformer. This\npaper proposes predicting edit spans for the source text for local sequence\ntransduction tasks. Representing an edit span with a position of the source\ntext and corrected tokens, we can reduce the length of the target sequence and\nthe computational cost for inference. We apply instruction tuning for LLMs on\nthe supervision data of edit spans. Experiments show that the proposed method\nachieves comparable performance to the baseline in four tasks, paraphrasing,\nformality style transfer, GEC, and text simplification, despite reducing the\nlength of the target text by as small as 21%. Furthermore, we report that the\ntask-specific fine-tuning with the proposed method achieved state-of-the-art\nperformance in the four tasks.'}, {'Controlled Generation with Prompt Insertion for Natural Language\n  Explanations in Grammatical Error Correction': ""In Grammatical Error Correction (GEC), it is crucial to ensure the user's\ncomprehension of a reason for correction. Existing studies present tokens,\nexamples, and hints as to the basis for correction but do not directly explain\nthe reasons for corrections. Although methods that use Large Language Models\n(LLMs) to provide direct explanations in natural language have been proposed\nfor various tasks, no such method exists for GEC. Generating explanations for\nGEC corrections involves aligning input and output tokens, identifying\ncorrection points, and presenting corresponding explanations consistently.\nHowever, it is not straightforward to specify a complex format to generate\nexplanations, because explicit control of generation is difficult with prompts.\nThis study introduces a method called controlled generation with Prompt\nInsertion (PI) so that LLMs can explain the reasons for corrections in natural\nlanguage. In PI, LLMs first correct the input text, and then we automatically\nextract the correction points based on the rules. The extracted correction\npoints are sequentially inserted into the LLM's explanation output as prompts,\nguiding the LLMs to generate explanations for the correction points. We also\ncreate an Explainable GEC (XGEC) dataset of correction reasons by annotating\nNUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT\nusing original prompts miss some correction points, the generation control\nusing PI can explicitly guide to describe explanations for all correction\npoints, contributing to improved performance in generating correction reasons.""}, {'Learning Semantically and Additively Compositional Distributional\n  Representations': 'This paper connects a vector-based composition model to a formal semantics,\nthe Dependency-based Compositional Semantics (DCS). We show theoretical\nevidence that the vector compositions in our model conform to the logic of DCS.\nExperimentally, we show that vector-based composition brings a strong ability\nto calculate similar phrases as similar vectors, achieving near\nstate-of-the-art on a wide range of phrase similarity tasks and relation\nclassification; meanwhile, DCS can guide building vectors for structured\nqueries that can be directly executed. We evaluate this utility on sentence\ncompletion task and report a new state-of-the-art.'}]","### Abstract

In the rapidly advancing field of artificial intelligence, our research showcases a pivotal contribution by establishing a high-quality Japanese web corpus, enhancing large language models' performance in Japanese. The objective was to build a corpus of 312 billion characters from Common Crawl archives, integrating curation techniques and filtering rules, followed by continual pre-training on prominent models like Llama 2, Mistral, and Mixtral with Japanese-focused corpora.

Innovatively, we introduced an improved normalization method to commonly quoted Japanese punctuations and efficiently deduplicated web pages, preserving a high-value 173 million page corpus. Zen modeling is featured, with this corpus achieving significant improvements on Japanese benchmarks, including a 77.36 point gain in natural language inferences via the NLI task. 

The contribution lies in our corpus's utility for refining Japanese-capable LLMs, raising the bar for Japanese language processing efficiency. Additionally, the selection and adaptation of filtering rules demonstrated the practical impact of curated data in enhancing AI model performance. As a result, this corpus serves as a catalyst for advancements in multilingual AI and promotes responsible AI for Japanese language applications.

Potential applications include enhancing AI system quality for Japanese language tasks, developing error-tolerant AI assistants, and improving automation in Japanese text processing, contributing to more sophisticated, efficient, and culturally respectful AI technologies tailored for the Japanese language landscape. Our corpus-based AI improvements have significant implications for industries, governments, and society at large, promoting inclusive AI development and global language accessibility."
"In recent years, multi-agent reinforcement learning algorithms have made
significant advancements in diverse gaming environments, leading to increased
interest in the broader application of such techniques. To address the
prevalent challenge of partial observability, communication-based algorithms
have improved cooperative performance through the sharing of numerical
embedding between agents. However, the understanding of the formation of
collaborative mechanisms is still very limited, making designing a
human-understandable communication mechanism a valuable problem to address. In
this paper, we propose a novel multi-agent reinforcement learning algorithm
that embeds large language models into agents, endowing them with the ability
to generate human-understandable verbal communication. The entire framework has
a message module and an action module. The message module is responsible for
generating and sending verbal messages to other agents, effectively enhancing
information sharing among agents. To further enhance the message module, we
employ a teacher model to generate message labels from the global view and
update the student model through Supervised Fine-Tuning (SFT). The action
module receives messages from other agents and selects actions based on current
local observations and received messages. Experiments conducted on the
Overcooked game demonstrate our method significantly enhances the learning
efficiency and performance of existing methods, while also providing an
interpretable tool for humans to understand the process of multi-agent
cooperation.","[{""Analyzing Turing's Systems via Dynamic Bifurcation Theory"": ""In this paper, we introduce a novel approach to study reaction-diffusion\nsystems -- dynamic transition theory approach developed in Ma and Wang 2015.\nThis approach generalizes Turing's classical result (linear stability analysis)\non pattern formation and cast some new insights into Turing's systems.\nSpecifically, we studied the Turing's instability and dynamic transition\nphenomenon for a Turing's system, and expressions of the critical parameters\n$L(D_{u},D_{v})$, and $D_{v}^{*}$ are derived. These two simple parameters are\nsufficient to provide us with enough information on the Turing's instability\nresult as well as the dynamic transition behavior of the system. As an\napplication, based on the method we establish in this paper, we found that the\nSchnakenberg system has two different transition types: single real eigenvalue\ntransition and double real eigenvalues transition. These transition types are\ninterpreted using phase diagrams.""}, {'Cooperative Stochastic Multi-agent Multi-armed Bandits Robust to\n  Adversarial Corruptions': 'We study the problem of stochastic bandits with adversarial corruptions in\nthe cooperative multi-agent setting, where $V$ agents interact with a common\n$K$-armed bandit problem, and each pair of agents can communicate with each\nother to expedite the learning process. In the problem, the rewards are\nindependently sampled from distributions across all agents and rounds, but they\nmay be corrupted by an adversary. Our goal is to minimize both the overall\nregret and communication cost across all agents. We first show that an additive\nterm of corruption is unavoidable for any algorithm in this problem. Then, we\npropose a new algorithm that is agnostic to the level of corruption. Our\nalgorithm not only achieves near-optimal regret in the stochastic setting, but\nalso obtains a regret with an additive term of corruption in the corrupted\nsetting, while maintaining efficient communication. The algorithm is also\napplicable for the single-agent corruption problem, and achieves a high\nprobability regret that removes the multiplicative dependence of $K$ on\ncorruption level. Our result of the single-agent case resolves an open question\nfrom Gupta et al. [2019].'}, {'SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement\n  Learning': 'Spatial information is essential in various fields. How to explicitly model\naccording to the spatial location of agents is also very important for the\nmulti-agent problem, especially when the number of agents is changing and the\nscale is enormous. Inspired by the point cloud task in computer vision, we\npropose a spatial information extraction structure for multi-agent\nreinforcement learning in this paper. Agents can effectively share the\nneighborhood and global information through a spatially encoder-decoder\nstructure. Our method follows the centralized training with decentralized\nexecution (CTDE) paradigm. In addition, our structure can be applied to various\nexisting mainstream reinforcement learning algorithms with minor modifications\nand can deal with the problem with a variable number of agents. The experiments\nin several multi-agent scenarios show that the existing methods can get\nconvincing results by adding our spatially explicit architecture.'}, {'From Explicit Communication to Tacit Cooperation:A Novel Paradigm for\n  Cooperative MARL': ""Centralized training with decentralized execution (CTDE) is a widely-used\nlearning paradigm that has achieved significant success in complex tasks.\nHowever, partial observability issues and the absence of effectively shared\nsignals between agents often limit its effectiveness in fostering cooperation.\nWhile communication can address this challenge, it simultaneously reduces the\nalgorithm's practicality. Drawing inspiration from human team cooperative\nlearning, we propose a novel paradigm that facilitates a gradual shift from\nexplicit communication to tacit cooperation. In the initial training stage, we\npromote cooperation by sharing relevant information among agents and\nconcurrently reconstructing this information using each agent's local\ntrajectory. We then combine the explicitly communicated information with the\nreconstructed information to obtain mixed information. Throughout the training\nprocess, we progressively reduce the proportion of explicitly communicated\ninformation, facilitating a seamless transition to fully decentralized\nexecution without communication. Experimental results in various scenarios\ndemonstrate that the performance of our method without communication can\napproaches or even surpasses that of QMIX and communication-based methods.""}, {'Sensitivity Analysis of Continuous-Time Systems based on Power Spectral\n  Density': 'Bode integrals of sensitivity and sensitivity-like functions along with\ncomplementary sensitivity and complementary sensitivity-like functions are\nconventionally used for describing performance limitations of a feedback\ncontrol system. In this paper, we show that in the case when the disturbance is\na wide sense stationary process the (complementary) sensitivity Bode integral\nand the (complementary) sensitivity-like Bode integral are identical. A lower\nbound of the continuous-time complementary sensitivity-like Bode integral is\nalso derived and examined with the linearized flight-path angle tracking\ncontrol problem of an F-16 aircraft.'}, {'Squeezed-field path-integral description of BCS superconductors': 'We develop a squeezed-field path-integral representation for BCS\nsuperconductors utilizing a generalized completeness relation of\nsqueezed-fermionic coherent states. We derive a Grassmann path integral of\nfermionic quasiparticles that explicitly includes the collective degrees of\nfreedom of the order-parameter dynamics governed by the classical Anderson\npseudospin model. Based on this method, we analyze the spectral function of the\nsingle-particle excitations, and show that the squeezed-field path integral for\nthe BCS Hamiltonian describes a bosonic sideband branch that corresponds to the\nHiggs amplitude mode of BCS superconductors, in addition to reproducing the\nquasiparticle and quasihole excitation branches described by the BCS mean-field\napproximation.'}, {'f-Divergence Variational Inference': ""This paper introduces the $f$-divergence variational inference ($f$-VI) that\ngeneralizes variational inference to all $f$-divergences. Initiated from\nminimizing a crafty surrogate $f$-divergence that shares the statistical\nconsistency with the $f$-divergence, the $f$-VI framework not only unifies a\nnumber of existing VI methods, e.g. Kullback-Leibler VI, R\\'{e}nyi's\n$\\alpha$-VI, and $\\chi$-VI, but offers a standardized toolkit for VI subject to\narbitrary divergences from $f$-divergence family. A general $f$-variational\nbound is derived and provides a sandwich estimate of marginal likelihood (or\nevidence). The development of the $f$-VI unfolds with a stochastic optimization\nscheme that utilizes the reparameterization trick, importance weighting and\nMonte Carlo approximation; a mean-field approximation scheme that generalizes\nthe well-known coordinate ascent variational inference (CAVI) is also proposed\nfor $f$-VI. Empirical examples, including variational autoencoders and Bayesian\nneural networks, are provided to demonstrate the effectiveness and the wide\napplicability of $f$-VI.""}, {'Fundamental Limitations of Control and Filtering in Continuous-Time\n  Systems: An Information-Theoretic Analysis': ""While information theory has been introduced to investigate and characterize\nthe control and filtering limitations for a few decades, the existing\ninformation-theoretic methods are indirect and cumbersome for analyzing the\nfundamental limitations of continuous-time systems. To answer this challenge,\nwe lift the information-theoretic analysis to continuous function spaces of\ninfinite dimensions by using Duncan's theorem or the I-MMSE relationships.\nContinuous-time control and filtering systems are modeled as an additive\nGaussian channel with or without feedback, and total information rate is\nidentified as a control and filtering trade-off metric and directly computed\nfrom the estimation error of channel input. Inequality constraints for the\ntrade-off metric are derived in a general setting and then applied to capture\nthe fundamental limitations of various control and filtering systems subject to\nlinear and nonlinear plants. For the linear systems, we show that total\ninformation rate has similar properties as some established trade-offs, e.g.,\nBode-type integrals and minimum estimation error. For the nonlinear systems, we\nprovide a direct method to compute the total information rate and its lower\nbound by the Stratonovich-Kushner equation.""}, {'Sensitivity Analysis of Continuous-Time Linear Control Systems subject\n  to Control and Measurement Noise: An Information-Theoretic Approach': 'Sensitivity of linear continuous-time control systems, subject to control and\nmeasurement noise, is analyzed by deriving the lower bounds of Bode-like\nintegrals via an information-theoretic approach. Bode integrals of four\ndifferent sensitivity-like functions are employed to gauge the control\ntrade-offs. When the signals of the control system are stationary Gaussian,\nthese four different Bode-like integrals can be represented as differences\nbetween mutual information rates. These mutual information rates and hence the\ncorresponding Bode-like integrals are proven to be bounded below by the\nunstable poles and zeros of the plant model, if the signals of the control\nsystem are wide-sense stationary.'}, {'A Simplified Approach to Analyze Complementary Sensitivity Trade-offs in\n  Continuous-Time and Discrete-Time Systems': 'A simplified approach is proposed to investigate the continuous-time and\ndiscrete-time complementary sensitivity Bode integrals (CSBIs) in this note.\nFor continuous-time feedback systems with unbounded frequency domain, the CSBI\nweighted by $1/\\omega^2$ is considered, where this simplified method reveals a\nmore explicit relationship between the value of CSBI and the structure of the\nopen-loop transfer function. With a minor modification of this method, the CSBI\nof discrete-time system is derived, and illustrative examples are provided.\nCompared with the existing results on CSBI, neither Cauchy integral theorem nor\nPoisson integral formula are used throughout the analysis, and the analytic\nconstraint on the integrand is removed.'}]","Abstract:

In this paper, we address the critical challenge of enabling agents in complex, partially observable environments to effectively coordinate and collaborate through human-understandable verbal communication. The proposed method, Verco, introduces a communication algorithm that empowers agents to exchange human-comprehensible messages, enhancing their ability to make decisions based on the information shared by teammates. 

The main objective of the research is to facilitate the generation of coordinated messages and actions among agents in Overcooked-style environments, where multidimensional cooperation is crucial. By employing large language models (LLMs) as communication modules, Verco significantly boosts decision-making efficiency compared to existing communication algorithms. 

Key innovations in Verco include the use of LoRA parameters for independent message and action policies alignment, ensuring the simultaneous development of tailored communication and action strategies. An efficient strategy-transfer framework (STF) distils complex feedback into focusable, relevant content to fine-tune LLMs for optimal performance. 

Empirical evaluations demonstrate that Verco surpasses baseline models by 29% in return, as well as Semantic-PPO/CommNet. Verco reduces episode lengths and entropy by 47%, indicating less uncertainty and better coordination in action selection. Visualizations underscore the pivotal role of verbal communication in enhancing cooperation.

Contributions of this research extend the application of LLMs in multi-agent systems, enabling more nuanced and effective communication that meets human comprehension standards. Notably, Verco's streamlined decision-making process facilitates better performance in teamwork, making it applicable in diverse sharing and collaborative tasks, such as virtual kitchen environments, resource management in emergency scenarios, or autonomous vehicle coordination. This innovative approach represents a substantial step towards the development of human-like AI collaboration systems."
"Contrastive language-audio pretraining~(CLAP) has been developed to align the
representations of audio and language, achieving remarkable performance in
retrieval and classification tasks. However, current CLAP struggles to capture
temporal information within audio and text features, presenting substantial
limitations for tasks such as audio retrieval and generation. To address this
gap, we introduce T-CLAP, a temporal-enhanced CLAP model. We use Large Language
Models~(LLMs) and mixed-up strategies to generate temporal-contrastive captions
for audio clips from extensive audio-text datasets. Subsequently, a new
temporal-focused contrastive loss is designed to fine-tune the CLAP model by
incorporating these synthetic data. We conduct comprehensive experiments and
analysis in multiple downstream tasks. T-CLAP shows improved capability in
capturing the temporal relationship of sound events and outperforms
state-of-the-art models by a significant margin.","[{'Audio2Face: Generating Speech/Face Animation from Single Audio with\n  Attention-Based Bidirectional LSTM Networks': ""We propose an end to end deep learning approach for generating real-time\nfacial animation from just audio. Specifically, our deep architecture employs\ndeep bidirectional long short-term memory network and attention mechanism to\ndiscover the latent representations of time-varying contextual information\nwithin the speech and recognize the significance of different information\ncontributed to certain face status. Therefore, our model is able to drive\ndifferent levels of facial movements at inference and automatically keep up\nwith the corresponding pitch and latent speaking style in the input audio, with\nno assumption or further human intervention. Evaluation results show that our\nmethod could not only generate accurate lip movements from audio, but also\nsuccessfully regress the speaker's time-varying facial movements.""}, {'NeuralMagicEye: Learning to See and Understand the Scene Behind an\n  Autostereogram': ""An autostereogram, a.k.a. magic eye image, is a single-image stereogram that\ncan create visual illusions of 3D scenes from 2D textures. This paper studies\nan interesting question that whether a deep CNN can be trained to recover the\ndepth behind an autostereogram and understand its content. The key to the\nautostereogram magic lies in the stereopsis - to solve such a problem, a model\nhas to learn to discover and estimate disparity from the quasi-periodic\ntextures. We show that deep CNNs embedded with disparity convolution, a novel\nconvolutional layer proposed in this paper that simulates stereopsis and\nencodes disparity, can nicely solve such a problem after being sufficiently\ntrained on a large 3D object dataset in a self-supervised fashion. We refer to\nour method as ``NeuralMagicEye''. Experiments show that our method can\naccurately recover the depth behind autostereograms with rich details and\ngradient smoothness. Experiments also show the completely different working\nmechanisms for autostereogram perception between neural networks and human\neyes. We hope this research can help people with visual impairments and those\nwho have trouble viewing autostereograms. Our code is available at\n\\url{https://jiupinjia.github.io/neuralmagiceye/}.""}, {'Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D\n  Reconstruction with Symmetry': ""Deep implicit field regression methods are effective for 3D reconstruction\nfrom single-view images. However, the impact of different sampling patterns on\nthe reconstruction quality is not well-understood. In this work, we first study\nthe effect of point set discrepancy on the network training. Based on Farthest\nPoint Sampling algorithm, we propose a sampling scheme that theoretically\nencourages better generalization performance, and results in fast convergence\nfor SGD-based optimization algorithms. Secondly, based on the reflective\nsymmetry of an object, we propose a feature fusion method that alleviates\nissues due to self-occlusions which makes it difficult to utilize local image\nfeatures. Our proposed system Ladybird is able to create high quality 3D object\nreconstructions from a single input image. We evaluate Ladybird on a large\nscale 3D dataset (ShapeNet) demonstrating highly competitive results in terms\nof Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).""}, {'Towards High-Fidelity 3D Face Reconstruction from In-the-Wild Images\n  Using Graph Convolutional Networks': '3D Morphable Model (3DMM) based methods have achieved great success in\nrecovering 3D face shapes from single-view images. However, the facial textures\nrecovered by such methods lack the fidelity as exhibited in the input images.\nRecent work demonstrates high-quality facial texture recovering with generative\nnetworks trained from a large-scale database of high-resolution UV maps of face\ntextures, which is hard to prepare and not publicly available. In this paper,\nwe introduce a method to reconstruct 3D facial shapes with high-fidelity\ntextures from single-view images in-the-wild, without the need to capture a\nlarge-scale face texture database. The main idea is to refine the initial\ntexture generated by a 3DMM based method with facial details from the input\nimage. To this end, we propose to use graph convolutional networks to\nreconstruct the detailed colors for the mesh vertices instead of reconstructing\nthe UV map. Experiments show that our method can generate high-quality results\nand outperforms state-of-the-art methods in both qualitative and quantitative\ncomparisons.'}, {'Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks': ""Face reenactment aims to animate a source face image to a different pose and\nexpression provided by a driving image. Existing approaches are either designed\nfor a specific identity, or suffer from the identity preservation problem in\nthe one-shot or few-shot scenarios. In this paper, we introduce a method for\none-shot face reenactment, which uses the reconstructed 3D meshes (i.e., the\nsource mesh and driving mesh) as guidance to learn the optical flow needed for\nthe reenacted face synthesis. Technically, we explicitly exclude the driving\nface's identity information in the reconstructed driving mesh. In this way, our\nnetwork can focus on the motion estimation for the source face without the\ninterference of driving face shape. We propose a motion net to learn the face\nmotion, which is an asymmetric autoencoder. The encoder is a graph\nconvolutional network (GCN) that learns a latent motion vector from the meshes,\nand the decoder serves to produce an optical flow image from the latent vector\nwith CNNs. Compared to previous methods using sparse keypoints to guide the\noptical flow learning, our motion net learns the optical flow directly from 3D\ndense meshes, which provide the detailed shape and pose information for the\noptical flow, so it can achieve more accurate expression and pose on the\nreenacted face. Extensive experiments show that our method can generate\nhigh-quality results and outperforms state-of-the-art methods in both\nqualitative and quantitative comparisons.""}, {'MeInGame: Create a Game Character Face from a Single Portrait': 'Many deep learning based 3D face reconstruction methods have been proposed\nrecently, however, few of them have applications in games. Current game\ncharacter customization systems either require players to manually adjust\nconsiderable face attributes to obtain the desired face, or have limited\nfreedom of facial shape and texture. In this paper, we propose an automatic\ncharacter face creation method that predicts both facial shape and texture from\na single portrait, and it can be integrated into most existing 3D games.\nAlthough 3D Morphable Face Model (3DMM) based methods can restore accurate 3D\nfaces from single images, the topology of 3DMM mesh is different from the\nmeshes used in most games. To acquire fidelity texture, existing methods\nrequire a large amount of face texture data for training, while building such\ndatasets is time-consuming and laborious. Besides, such a dataset collected\nunder laboratory conditions may not generalized well to in-the-wild situations.\nTo tackle these problems, we propose 1) a low-cost facial texture acquisition\nmethod, 2) a shape transfer algorithm that can transform the shape of a 3DMM\nmesh to games, and 3) a new pipeline for training 3D game face reconstruction\nnetworks. The proposed method not only can produce detailed and vivid game\ncharacters similar to the input portrait, but can also eliminate the influence\nof lighting and occlusions. Experiments show that our method outperforms\nstate-of-the-art methods used in games.'}, {'ZiGAN: Fine-grained Chinese Calligraphy Font Generation via a Few-shot\n  Style Transfer Approach': 'Chinese character style transfer is a very challenging problem because of the\ncomplexity of the glyph shapes or underlying structures and large numbers of\nexisted characters, when comparing with English letters. Moreover, the\nhandwriting of calligraphy masters has a more irregular stroke and is difficult\nto obtain in real-world scenarios. Recently, several GAN-based methods have\nbeen proposed for font synthesis, but some of them require numerous reference\ndata and the other part of them have cumbersome preprocessing steps to divide\nthe character into different parts to be learned and transferred separately. In\nthis paper, we propose a simple but powerful end-to-end Chinese calligraphy\nfont generation framework ZiGAN, which does not require any manual operation or\nredundant preprocessing to generate fine-grained target-style characters with\nfew-shot references. To be specific, a few paired samples from different\ncharacter styles are leveraged to attain a fine-grained correlation between\nstructures underlying different glyphs. To capture valuable style knowledge in\ntarget and strengthen the coarse-grained understanding of character content, we\nutilize multiple unpaired samples to align the feature distributions belonging\nto different character styles. By doing so, only a few target Chinese\ncalligraphy characters are needed to generated expected style transferred\ncharacters. Experiments demonstrate that our method has a state-of-the-art\ngeneralization ability in few-shot Chinese character style transfer.'}, {'Dynamic Future Net: Diversified Human Motion Generation': 'Human motion modelling is crucial in many areas such as computer graphics,\nvision and virtual reality. Acquiring high-quality skeletal motions is\ndifficult due to the need for specialized equipment and laborious manual\npost-posting, which necessitates maximizing the use of existing data to\nsynthesize new data. However, it is a challenge due to the intrinsic motion\nstochasticity of human motion dynamics, manifested in the short and long terms.\nIn the short term, there is strong randomness within a couple frames, e.g. one\nframe followed by multiple possible frames leading to different motion styles;\nwhile in the long term, there are non-deterministic action transitions. In this\npaper, we present Dynamic Future Net, a new deep learning model where we\nexplicitly focuses on the aforementioned motion stochasticity by constructing a\ngenerative model with non-trivial modelling capacity in temporal stochasticity.\nGiven limited amounts of data, our model can generate a large number of\nhigh-quality motions with arbitrary duration, and visually-convincing\nvariations in both space and time. We evaluate our model on a wide range of\nmotions and compare it with the state-of-the-art methods. Both qualitative and\nquantitative results show the superiority of our method, for its robustness,\nversatility and high-quality.'}, {'Common physical mechanism for integer and fractional quantum Hall\n  effects': 'Integer and fractional quantum Hall effects were studied with different\nphysics models and explained by different physical mechanisms. In this paper,\nthe common physical mechanism for integer and fractional quantum Hall effects\nis studied, where a new unified formulation of integer and fractional quantum\nHall effect is presented. Firstly, we introduce a 2-dimensional ideal electron\ngas model in the presence of strong magnetic field with symmetry gauge, and the\ntransverse electric filed $\\varepsilon_2$ is also introduced to balance Lorentz\nforce. Secondly, the Pauli equation is solved where the wave function and\nenergy levels is given explicitly. Thirdly, after the calculation of the\ndegeneracy density for 2-dimensional ideal electron gas system, the Hall\nresistance of the system is obtained, where the quantum Hall number $\\nu$ is\nintroduced. It is found that the new defined $\\nu$, called filling factor in\nthe literature, is related to radial quantum number n and angular quantum\nnumber $|m|$, the different $n$ and $|m|$ correspond to different $\\nu$. This\nprovides unification explaination for integer and fractional quantum Hall\neffects. It is predicated that more new cases exist of fractional quantum Hall\neffects without the concept of fractional charge.'}, {'Theoretical basis for the unification of the integer and the fractional\n  quantum Hall effects': ""This paper intends to provide a theoretical basis for the unification of the\ninteger and the fractional quantum Hall effects. Guided by concepts and\ntheories of quantum mechanics and with the solution of the Pauli equation in a\nmagnetic field under the symmetric gauge, wave functions, energy levels of\nsingle electrons, and the expectation value of electron's spatial scope are\npresented. After the quotation of non-interaction dilute gas system, the\nproduct of single electron's wave functions is used to construct wave functions\nof the N electron gas system in magnetic field. Then the expectation value of\nthe system's motion area and the electron's surface density are obtained. In\nthis way, the unification explaination of the integer and the fractional\nquantum Hall effects is formulated without the help of the concept of\nfractional charge.""}]","Title: Temporal-Enhanced Contrastive Pretraining for Improved Audio-Text Retrieval and Generation

Abstract:
As contrastive pretraining in multi-modal learning garners significant attention, this research introduces the Temporal-Enhanced Contrastive Pretraining (T-CLAP) model. The objective is to bridge the performance gap in handling temporal information, especially in audio captioning and generation tasks, to surpass existing models and enhance audio-text retrieval systems' temporal coherency (1 sentence). 

The innovation lies in augmenting CLAP, the state-of-the-art audio-text model, by enriching the sequential features in the temporal-enhanced embedding, a novel contribution (1 sentence). Key methodologies include a modified loss function focused on temporal ordering and a diverse dataset of audio and captions to improve model training. 

The primary findings demonstrate that T-CLAP outperforms previous models in text-to-audio reconstructions and temporal audio feature retrieval tasks, surpassing 50% accuracy on T-classify and achieving high R@1-R@10 metrics across benchmark datasets (2 sentences). This advancement in performance underscores the model's improved handling of temporal dynamics in audio sequences.

significantly, this research contributes to a more sophisticated method for multi-modal learning, specifically addressing the challenges of temporal information processing in audio-centric models. The findings promise improvements in downstream applications reliant on chronological audio data, such as audio description, caption synthesis, and enhanced text-based music retrieval (2 sentences).

In summary, the development of T-CLAP signifies a step forward in contrastive pretraining methodologies, particularly in optimizing models for temporal coherency in audio captions and generation, thereby enriching the fields of audio signal processing and multi-modal learning systems."
"The task of toxicity detection is still a relevant task, especially in the
context of safe and fair LMs development. Nevertheless, labeled binary toxicity
classification corpora are not available for all languages, which is
understandable given the resource-intensive nature of the annotation process.
Ukrainian, in particular, is among the languages lacking such resources. To our
knowledge, there has been no existing toxicity classification corpus in
Ukrainian. In this study, we aim to fill this gap by investigating
cross-lingual knowledge transfer techniques and creating labeled corpora by:
(i)~translating from an English corpus, (ii)~filtering toxic samples using
keywords, and (iii)~annotating with crowdsourcing. We compare LLMs prompting
and other cross-lingual transfer approaches with and without fine-tuning
offering insights into the most robust and efficient baselines.","[{'AdamR at SemEval-2023 Task 10: Solving the Class Imbalance Problem in\n  Sexism Detection with Ensemble Learning': 'The Explainable Detection of Online Sexism task presents the problem of\nexplainable sexism detection through fine-grained categorisation of sexist\ncases with three subtasks. Our team experimented with different ways to combat\nclass imbalance throughout the tasks using data augmentation and loss\nalteration techniques. We tackled the challenge by utilising ensembles of\nTransformer models trained on different datasets, which are tested to find the\nbalance between performance and interpretability. This solution ranked us in\nthe top 40\\% of teams for each of the tracks.'}, {'Detecting Text Formality: A Study of Text Classification Approaches': 'Formality is one of the important characteristics of text documents. The\nautomatic detection of the formality level of a text is potentially beneficial\nfor various natural language processing tasks. Before, two large-scale datasets\nwere introduced for multiple languages featuring formality annotation -- GYAFC\nand X-FORMAL. However, they were primarily used for the training of style\ntransfer models. At the same time, the detection of text formality on its own\nmay also be a useful application. This work proposes the first to our knowledge\nsystematic study of formality detection methods based on statistical,\nneural-based, and Transformer-based machine learning methods and delivers the\nbest-performing models for public usage. We conducted three types of\nexperiments -- monolingual, multilingual, and cross-lingual. The study shows\nthe overcome of Char BiLSTM model over Transformer-based ones for the\nmonolingual and multilingual formality classification task, while\nTransformer-based classifiers are more stable to cross-lingual knowledge\ntransfer.'}, {'Multiverse: Multilingual Evidence for Fake News Detection': 'Misleading information spreads on the Internet at an incredible speed, which\ncan lead to irreparable consequences in some cases. It is becoming essential to\ndevelop fake news detection technologies. While substantial work has been done\nin this direction, one of the limitations of the current approaches is that\nthese models are focused only on one language and do not use multilingual\ninformation. In this work, we propose Multiverse -- a new feature based on\nmultilingual evidence that can be used for fake news detection and improve\nexisting approaches. The hypothesis of the usage of cross-lingual evidence as a\nfeature for fake news detection is confirmed, firstly, by manual experiment\nbased on a set of known true and fake news. After that, we compared our fake\nnews classification system based on the proposed feature with several baselines\non two multi-domain datasets of general-topic news and one fake COVID-19 news\ndataset showing that in additional combination with linguistic features it\nyields significant improvements.'}, {'Adam-Smith at SemEval-2023 Task 4: Discovering Human Values in Arguments\n  with Ensembles of Transformer-based Models': 'This paper presents the best-performing approach alias ""Adam Smith"" for the\nSemEval-2023 Task 4: ""Identification of Human Values behind Arguments"". The\ngoal of the task was to create systems that automatically identify the values\nwithin textual arguments. We train transformer-based models until they reach\ntheir loss minimum or f1-score maximum. Ensembling the models by selecting one\nglobal decision threshold that maximizes the f1-score leads to the\nbest-performing system in the competition. Ensembling based on stacking with\nlogistic regressions shows the best performance on an additional dataset\nprovided to evaluate the robustness (""Nahj al-Balagha""). Apart from outlining\nthe submitted system, we demonstrate that the use of the large ensemble model\nis not necessary and that the system size can be significantly reduced.'}, {'MultiParaDetox: Extending Text Detoxification with Parallel Data to New\n  Languages': 'Text detoxification is a textual style transfer (TST) task where a text is\nparaphrased from a toxic surface form, e.g. featuring rude words, to the\nneutral register. Recently, text detoxification methods found their\napplications in various task such as detoxification of Large Language Models\n(LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic\nspeech combating in social networks (Deng et al., 2023; Mun et al., 2023;\nAgarwal et al., 2023). All these applications are extremely important to ensure\nsafe communication in modern digital worlds. However, the previous approaches\nfor parallel text detoxification corpora collection -- ParaDetox (Logacheva et\nal., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in\nmonolingual setup. In this work, we aim to extend ParaDetox pipeline to\nmultiple languages presenting MultiParaDetox to automate parallel\ndetoxification corpus collection for potentially any language. Then, we\nexperiment with different text detoxification models -- from unsupervised\nbaselines to LLMs and fine-tuned models on the presented parallel corpora --\nshowing the great benefit of parallel corpus presence to obtain\nstate-of-the-art text detoxification models for any language.'}, {'Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge\n  Transfer Approaches': 'Despite the extensive amount of labeled datasets in the NLP text\nclassification field, the persistent imbalance in data availability across\nvarious languages remains evident. Ukrainian, in particular, stands as a\nlanguage that still can benefit from the continued refinement of cross-lingual\nmethodologies. Due to our knowledge, there is a tremendous lack of Ukrainian\ncorpora for typical text classification tasks. In this work, we leverage the\nstate-of-the-art advances in NLP, exploring cross-lingual knowledge transfer\nmethods avoiding manual data curation: large multilingual encoders and\ntranslation systems, LLMs, and language adapters. We test the approaches on\nthree text classification tasks -- toxicity classification, formality\nclassification, and natural language inference -- providing the ""recipe"" for\nthe optimal setups.'}, {'Exploring Cross-lingual Textual Style Transfer with Large Multilingual\n  Language Models': 'Detoxification is a task of generating text in polite style while preserving\nmeaning and fluency of the original toxic text. Existing detoxification methods\nare designed to work in one exact language. This work investigates multilingual\nand cross-lingual detoxification and the behavior of large multilingual models\nlike in this setting. Unlike previous works we aim to make large language\nmodels able to perform detoxification without direct fine-tuning in given\nlanguage. Experiments show that multilingual models are capable of performing\nmultilingual style transfer. However, models are not able to perform\ncross-lingual detoxification and direct fine-tuning on exact language is\ninevitable.'}, {'Exploring Methods for Cross-lingual Text Style Transfer: The Case of\n  Text Detoxification': 'Text detoxification is the task of transferring the style of text from toxic\nto neutral. While here are approaches yielding promising results in monolingual\nsetup, e.g., (Dale et al., 2021; Hallinan et al., 2022), cross-lingual transfer\nfor this task remains a challenging open problem (Moskovskiy et al., 2022). In\nthis work, we present a large-scale study of strategies for cross-lingual text\ndetoxification -- given a parallel detoxification corpus for one language; the\ngoal is to transfer detoxification ability to another language for which we do\nnot have such a corpus. Moreover, we are the first to explore a new task where\ntext translation and detoxification are performed simultaneously, providing\nseveral strong baselines for this task. Finally, we introduce new automatic\ndetoxification evaluation metrics with higher correlations with human judgments\nthan previous benchmarks. We assess the most promising approaches also with\nmanual markup, determining the answer for the best strategy to transfer the\nknowledge of text detoxification between languages.'}, {'Methods for Detoxification of Texts for the Russian Language': 'We introduce the first study of automatic detoxification of Russian texts to\ncombat offensive language. Such a kind of textual style transfer can be used,\nfor instance, for processing toxic content in social media. While much work has\nbeen done for the English language in this field, it has never been solved for\nthe Russian language yet. We test two types of models - unsupervised approach\nbased on BERT architecture that performs local corrections and supervised\napproach based on pretrained language GPT-2 model - and compare them with\nseveral baselines. In addition, we describe evaluation setup providing training\ndatasets and metrics for automatic evaluation. The results show that the tested\napproaches can be successfully used for detoxification, although there is room\nfor improvement.'}, {'IFAN: An Explainability-Focused Interaction Framework for Humans and NLP\n  Models': ""Interpretability and human oversight are fundamental pillars of deploying\ncomplex NLP models into real-world applications. However, applying\nexplainability and human-in-the-loop methods requires technical proficiency.\nDespite existing toolkits for model understanding and analysis, options to\nintegrate human feedback are still limited. We propose IFAN, a framework for\nreal-time explanation-based interaction with NLP models. Through IFAN's\ninterface, users can provide feedback to selected model explanations, which is\nthen integrated through adapter layers to align the model with human rationale.\nWe show the system to be effective in debiasing a hate speech classifier with\nminimal impact on performance. IFAN also offers a visual admin system and API\nto manage models (and datasets) as well as control access rights. A demo is\nlive at https://ifan.ml.""}]","Title: Customized Cross-Linguistic Transfer Approaches for Ukrainian Toxicity Detection: A Novel Corpus and Method Evaluation

Abstract:
In the face of universally accessible NLP technologies, we address the gap in multilingual toxicity detection by developing the first annotated Ukrainian toxicity corpus using three approaches: data translation, keyword filtering, and crowdsourcing data annotation. We then explore three cross-lingual knowledge transfer methods for enhancing toxicity detection in under-resourced languages, establishing a benchmark for future research in customized multilingual NLP. The proposed study integrates an open-source toxicity detection framework (ukr-detect) and a unified dataset available on the Hugging Face platform, enabling researchers to leverage TypeScript and demonstrate its applicability in various NLP tasks for discerning toxic content in Ukrainian.

The objective of this study is to introduce a comprehensive and fine-tuned model for toxicity classification in Ukrainian and to investigate the effectiveness of three distinct methods for resource acquisition in multilingual NLP. Through this robust evaluation, a demonstrative framework, harnessing LLaMa, Mistral, and XLM-R models, emerges as a viable solution for tackling the toxicity detection problem across languages with differing resources.

This research makes significant contributions to the NLP community by offering a unparalleled dataset in Ukrainian, alongside the examination of various acquisition and cross-lingual transfer strategies. The exploration of these customized methodologies illuminates the novel possibilities for enhancing NBPL capabilities in resource-poor languages, thereby opening new avenues for applications in digital human rights, social media monitoring, and content moderation."
"Efficiently capturing consistent and complementary semantic features in a
multimodal conversation context is crucial for Multimodal Emotion Recognition
in Conversation (MERC). Existing methods mainly use graph structures to model
dialogue context semantic dependencies and employ Graph Neural Networks (GNN)
to capture multimodal semantic features for emotion recognition. However, these
methods are limited by some inherent characteristics of GNN, such as
over-smoothing and low-pass filtering, resulting in the inability to learn
long-distance consistency information and complementary information
efficiently. Since consistency and complementarity information correspond to
low-frequency and high-frequency information, respectively, this paper revisits
the problem of multimodal emotion recognition in conversation from the
perspective of the graph spectrum. Specifically, we propose a
Graph-Spectrum-based Multimodal Consistency and Complementary collaborative
learning framework GS-MCC. First, GS-MCC uses a sliding window to construct a
multimodal interaction graph to model conversational relationships and uses
efficient Fourier graph operators to extract long-distance high-frequency and
low-frequency information, respectively. Then, GS-MCC uses contrastive learning
to construct self-supervised signals that reflect complementarity and
consistent semantic collaboration with high and low-frequency signals, thereby
improving the ability of high and low-frequency information to reflect real
emotions. Finally, GS-MCC inputs the collaborative high and low-frequency
information into the MLP network and softmax function for emotion prediction.
Extensive experiments have proven the superiority of the GS-MCC architecture
proposed in this paper on two benchmark data sets.","[{'An Integer Linear Programming Framework for Mining Constraints from Data': 'Structured output prediction problems (e.g., sequential tagging, hierarchical\nmulti-class classification) often involve constraints over the output label\nspace. These constraints interact with the learned models to filter infeasible\nsolutions and facilitate in building an accountable system. However, although\nconstraints are useful, they are often based on hand-crafted rules. This raises\na question -- \\emph{can we mine constraints and rules from data based on a\nlearning algorithm?}\n  In this paper, we present a general framework for mining constraints from\ndata. In particular, we consider the inference in structured output prediction\nas an integer linear programming (ILP) problem. Then, given the coefficients of\nthe objective function and the corresponding solution, we mine the underlying\nconstraints by estimating the outer and inner polytopes of the feasible set. We\nverify the proposed constraint mining algorithm in various synthetic and\nreal-world applications and demonstrate that the proposed approach successfully\nidentifies the feasible set at scale.\n  In particular, we show that our approach can learn to solve 9x9 Sudoku\npuzzles and minimal spanning tree problems from examples without providing the\nunderlying rules. Our algorithm can also integrate with a neural network model\nto learn the hierarchical label structure of a multi-label classification task.\nBesides, we provide a theoretical analysis about the tightness of the polytopes\nand the reliability of the mined constraints.'}, {'InsNet: An Efficient, Flexible, and Performant Insertion-based Text\n  Generation Model': ""We propose InsNet, an expressive insertion-based text generator with\nefficient training and flexible decoding (parallel or sequential). Unlike most\nexisting insertion-based text generation works that require re-encoding of the\ncontext after each insertion operation and thus are inefficient to train,\nInsNet only requires one pass of context encoding for the entire sequence\nduring training by introducing a novel insertion-oriented position encoding and\na light-weighted slot representation strategy to enable computation sharing.\nFurthermore, we propose an algorithm InsNet-Dinic to better determine the\nparallelization of insertion operations that provides a controllable switch\nbetween parallel and sequential decoding, making it flexible to handle more\nparallelizable tasks such as machine translation with efficient decoding, or\nless parallelizable tasks such as open-domain text generation to guarantee\nhigh-quality outputs. Experiments on two lexically constrained text generation\ndatasets and three machine translation datasets demonstrate InsNet's advantages\nover previous insertion-based methods in terms of training speed, inference\nefficiency, and generation quality.""}, {'SentiBERT: A Transferable Transformer-Based Architecture for\n  Compositional Sentiment Semantics': 'We propose SentiBERT, a variant of BERT that effectively captures\ncompositional sentiment semantics. The model incorporates contextualized\nrepresentation with binary constituency parse tree to capture semantic\ncomposition. Comprehensive experiments demonstrate that SentiBERT achieves\ncompetitive performance on phrase-level sentiment classification. We further\ndemonstrate that the sentiment composition learned from the phrase-level\nannotations on SST can be transferred to other sentiment analysis tasks as well\nas related tasks, such as emotion classification tasks. Moreover, we conduct\nablation studies and design visualization methods to understand SentiBERT. We\nshow that SentiBERT is better than baseline approaches in capturing negation\nand the contrastive relation and model the compositional sentiment semantics.'}, {'Safety Guaranteed Control for Spacecraft Inspection Mission': 'This paper investigates the safety guaranteed problem in spacecraft\ninspection missions, considering multiple position obstacles and logical\nattitude forbidden zones. In order to address this issue, we propose a control\nstrategy based on control barrier functions, summarized as ""safety check on\nkinematics"" and ""velocity tracking on dynamics"" approach. The proposed approach\nemploys control barrier functions to describe the obstacles and to generate\nsafe velocities via the solution of a quadratic programming problem.\nSubsequently, we design a proportional-like controller based on the generated\nvelocity, which, despite its simplicity, can ensure safety even in the presence\nof velocity tracking errors. The stability and safety of the system are\nrigorously analyzed in this paper. Furthermore, to account for model\nuncertainties and external disturbances, we incorporate an immersion and\ninvariance-based disturbance observer in our design. Finally, numerical\nsimulations are performed to demonstrate the effectiveness of the proposed\ncontrol strategy.'}, {'Graph Information Bottleneck for Remote Sensing Segmentation': 'Remote sensing segmentation has a wide range of applications in environmental\nprotection, and urban change detection, etc. Despite the success of deep\nlearning-based remote sensing segmentation methods (e.g., CNN and Transformer),\nthey are not flexible enough to model irregular objects. In addition, existing\ngraph contrastive learning methods usually adopt the way of maximizing mutual\ninformation to keep the node representations consistent between different graph\nviews, which may cause the model to learn task-independent redundant\ninformation. To tackle the above problems, this paper treats images as graph\nstructures and introduces a simple contrastive vision GNN (SC-ViG) architecture\nfor remote sensing segmentation. Specifically, we construct a node-masked and\nedge-masked graph view to obtain an optimal graph structure representation,\nwhich can adaptively learn whether to mask nodes and edges. Furthermore, this\npaper innovatively introduces information bottleneck theory into graph\ncontrastive learning to maximize task-related information while minimizing\ntask-independent redundant information. Finally, we replace the convolutional\nmodule in UNet with the SC-ViG module to complete the segmentation and\nclassification tasks of remote sensing images. Extensive experiments on\npublicly available real datasets demonstrate that our method outperforms\nstate-of-the-art remote sensing image segmentation methods.'}, {'Target Language-Aware Constrained Inference for Cross-lingual Dependency\n  Parsing': 'Prior work on cross-lingual dependency parsing often focuses on capturing the\ncommonalities between source and target languages and overlooks the potential\nof leveraging linguistic properties of the languages to facilitate the\ntransfer. In this paper, we show that weak supervisions of linguistic knowledge\nfor the target languages can improve a cross-lingual graph-based dependency\nparser substantially. Specifically, we explore several types of corpus\nlinguistic statistics and compile them into corpus-wise constraints to guide\nthe inference process during the test time. We adapt two techniques, Lagrangian\nrelaxation and posterior regularization, to conduct inference with\ncorpus-statistics constraints. Experiments show that the Lagrangian relaxation\nand posterior regularization inference improve the performances on 15 and 17\nout of 19 target languages, respectively. The improvements are especially\nsignificant for target languages that have different word order features from\nthe source language.'}, {'Controllable Text Generation with Neurally-Decomposed Oracle': 'We propose a general and efficient framework to control auto-regressive\ngeneration models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained\nbase language model and a sequence-level boolean oracle function, we propose to\ndecompose the oracle function into token-level guidance to steer the base model\nin text generation. Specifically, the token-level guidance is approximated by a\nneural model trained with examples sampled from the base model, demanding no\nadditional auxiliary labeled data. Based on posterior regularization, we\npresent the closed-form optimal solution to incorporate the token-level\nguidance into the base model for controllable generation. We further provide a\ntheoretical analysis of how the approximation quality of NADO affects the\ncontrollable generation results. Experiments conducted on two applications: (1)\ntext generation with lexical constraints and (2) machine translation with\nformality control demonstrate that our framework efficiently guides the base\nmodel towards the given oracle while maintaining high generation quality.'}, {'Mitigating Gender Bias Amplification in Distribution by Posterior\n  Regularization': ""Advanced machine learning techniques have boosted the performance of natural\nlanguage processing. Nevertheless, recent studies, e.g., Zhao et al. (2017)\nshow that these techniques inadvertently capture the societal bias hidden in\nthe corpus and further amplify it. However, their analysis is conducted only on\nmodels' top predictions. In this paper, we investigate the gender bias\namplification issue from the distribution perspective and demonstrate that the\nbias is amplified in the view of predicted probability distribution over\nlabels. We further propose a bias mitigation approach based on posterior\nregularization. With little performance loss, our method can almost remove the\nbias amplification in the distribution. Our study sheds the light on\nunderstanding the bias amplification.""}, {'Energy cost for target control of complex networks': 'To promote the implementation of realistic control over various complex\nnetworks, recent work has been focusing on analyzing energy cost. Indeed, the\nenergy cost quantifies how much effort is required to drive the system from one\nstate to another when it is fully controllable. A fully controllable system\nmeans that the system can be driven by external inputs from any initial state\nto any final state in finite time. However, it is prohibitively expensive and\nunnecessary to confine that the system is fully controllable when we merely\nneed to accomplish the so-called target control---controlling a subnet of nodes\nchosen from the entire network. Yet, when the system is partially controllable,\nthe associated energy cost remains elusive. Here we present the minimum energy\ncost for controlling an arbitrary subset of nodes of a network. Moreover, we\nsystematically show the scaling behavior of the precise upper and lower bounds\nof the minimum energy in term of the time given to accomplish control. For\ncontrolling a given number of target nodes, we further demonstrate that the\nassociated energy over different configurations can differ by several orders of\nmagnitude. When the adjacency matrix of the network is nonsingular, we can\nsimplify the framework by just considering the induced subgraph spanned by\ntarget nodes instead of the entire network. Importantly, we find that, energy\ncost could be saved by orders of magnitude as we only need the partial\ncontrollability of the entire network. Our theoretical results are all\ncorroborated by numerical calculations, and pave the way for estimating the\nenergy cost to implement realistic target control in various applications.'}, {'CILF-CIAE: CLIP-driven Image-Language Fusion for Correcting Inverse Age\n  Estimation': 'The age estimation task aims to predict the age of an individual by analyzing\nfacial features in an image. The development of age estimation can improve the\nefficiency and accuracy of various applications (e.g., age verification and\nsecure access control, etc.). In recent years, contrastive language-image\npre-training (CLIP) has been widely used in various multimodal tasks and has\nmade some progress in the field of age estimation. However, existing CLIP-based\nage estimation methods require high memory usage (quadratic complexity) when\nglobally modeling images, and lack an error feedback mechanism to prompt the\nmodel about the quality of age prediction results. To tackle the above issues,\nwe propose a novel CLIP-driven Image-Language Fusion for Correcting Inverse Age\nEstimation (CILF-CIAE). Specifically, we first introduce the CLIP model to\nextract image features and text semantic information respectively, and map them\ninto a highly semantically aligned high-dimensional feature space. Next, we\ndesigned a new Transformer architecture (i.e., FourierFormer) to achieve\nchannel evolution and spatial interaction of images, and to fuse image and text\nsemantic information. Compared with the quadratic complexity of the attention\nmechanism, the proposed Fourierformer is of linear log complexity. To further\nnarrow the semantic gap between image and text features, we utilize an\nefficient contrastive multimodal learning module that supervises the multimodal\nfusion process of FourierFormer through contrastive loss for image-text\nmatching, thereby improving the interaction effect between different\nmodalities. Finally, we introduce reversible age estimation, which uses\nend-to-end error feedback to reduce the error rate of age predictions. Through\nextensive experiments on multiple data sets, CILF-CIAE has achieved better age\nprediction results.'}]","In the realm of conversational AI, Multimodal Emotion Recognition in Conversation (MERC) remains an unsolved challenge due to the complexity in capturing consistency and complementary semantics across text, audio, and visual modalities. This research paper presents a novel method, named Fourier Graph Operator (FGO), combined with Convergent Contrastive Learning (CCL) in the graph spectrum perspective, designed to improve MERC tasks.

Objective: Our goal is to enhance the understanding of conversational emotions by effectively leveraging both high and low-frequency signals, capturing the consistency and complementary semantics across diverse modal inputs.

Innovations: 
- We introduce FGO to leverage frequency domain signals for efficient long-distance context dependencies in graph neural networks. 
- CCL utilizes contrastive learning mechanisms to promote the collaborative processing of high- and low-frequency signals in multimodal conversations.

Methods:
- We use text embeddings from RoBERTa, audio features from openSMILE, and vision features through 3D-CNN to represent the input modalities.
- FGO in Fourier domain allows the reconstruction of MERC tasks in graphs with efficient long-distance information.
- CCL, based on LFCL (Low-Frequency Contrastive Learning) and HFCL (High-Frequency Contrastive Learning), collaboratively fuses high and low-frequency signals for enhanced emotion understanding.

Results:
Our method shows superior performance compared to several state-of-the-art baseline models when evaluated on the IEMOCAP and MELD datasets, particularly in terms of accuracy and F1 scores for specific emotions.

Contributions:
- Our approach effectively utilizes multimodal signals by focusing on both high and low-frequency dimensions, significantly improving emotion recognition accuracy.
- The utilization of spectral graph operations and CCL ensures the efficient extraction of context dependencies while mitigating the effects of over-smoothing common to GCN architectures.

Applications:
By advancing MERC, our research paves the way for more sophisticated conversational AI systems that can understand and respond to human emotions, key for applications ranging from customer service chatbots to empathetic AI companions.

The provided abstract succinctly encapsulates the research question, innovation, methods, results, contributions, and potential practical implications of the study, adhering to the 300-word limit."
"Reconstruction-based methods have been commonly used for unsupervised anomaly
detection, in which a normal image is reconstructed and compared with the given
test image to detect and locate anomalies. Recently, diffusion models have
shown promising applications for anomaly detection due to their powerful
generative ability. However, these models lack strict mathematical support for
normal image reconstruction and unexpectedly suffer from low reconstruction
quality. To address these issues, this paper proposes a novel and
highly-interpretable method named Masked Diffusion Posterior Sampling (MDPS).
In MDPS, the problem of normal image reconstruction is mathematically modeled
as multiple diffusion posterior sampling for normal images based on the devised
masked noisy observation model and the diffusion-based normal image prior under
Bayesian framework. Using a metric designed from pixel-level and
perceptual-level perspectives, MDPS can effectively compute the difference map
between each normal posterior sample and the given test image. Anomaly scores
are obtained by averaging all difference maps for multiple posterior samples.
Exhaustive experiments on MVTec and BTAD datasets demonstrate that MDPS can
achieve state-of-the-art performance in normal image reconstruction quality as
well as anomaly detection and localization.","[{'Cauchy problem for the incompressible Navier-Stokes equation with an\n  external force': 'In this paper we focus on the Cauchy problem for the incompressible\nNavier-Stokes equation with a rough external force. If the given rough external\nforce is small, we prove the local-in-time existence of this system for any\ninitial data belonging to the critical Besov space\n$\\dot{B}_{p,p}^{-1+\\frac{3}{p}}$, where $3<p<\\infty$. Moreover, We show the\nlong-time behavior of the priori global solutins constructed by us. Also, we\ngive three kinds of uniqueness results of the forced Navier-Stokes equations.'}, {'Blow-up criterion and examples of global solutions of forced\n  Navier-Stokes equations': 'In this paper we first show a blow-up criterion for solutions to the\nNavier-Stokes equations with a time-independent force by using the profile\ndecomposition method. Based on the orthogonal properties related to the\nprofiles, we give some examples of global solutions to the Navier-Stokes\nequations with a time-independent force, whose initial data are large.'}, {'Hunting for extra dimensions in the shadow of Sagittarius A*': ""Recently, Vagnozzi and Visinelli's work [Phys. Rev. D 100, 024020 (2020)]\nreveals that M87*'s shadow establishes an upper limit of $l \\lesssim 170$ AU,\nwhere $l$ is the AdS$_5$ curvature radius and 1 AU is one astronomical unit.\nThe Event Horizon Telescope, on the other hand, just captured the first image\nof the shadow of Sagittarius A* (SgrA*), the Galactic center source associated\nwith a supermassive black hole. In this paper, we are motivated to\ncomprehensively explore a new upper limit of $l$ with the shadow of SgrA*, and\nthe findings suggest that $l \\lesssim 0.097$ AU. Our results improve accuracy\nby three orders of magnitude. This is also one of the first quantifiable\nlimitations on exotic physics derived from the remarkable first image of the\nshadow of SgrA*.""}, {'Topological classes of rotating black holes': 'In this paper, we investigate the topological numbers for singly rotating\nKerr black holes in arbitrary dimensions and four-dimensional Kerr-Newman black\nhole. We show that for uncharged black holes, the rotation parameter has a\nsignificant effect on the topological number, and for rotating black holes, the\ndimension of spacetime has a remarkable effect on the topological number too.\nIn addition, we find that the topological numbers of the four-dimensional Kerr\nand Kerr-Newman black holes are the same, which seems to indicate that the\nelectric charge parameter has no effect on the topological number of rotating\nblack holes. Our current research provides more evidence that the conjecture\nput forward in Wei et al. [Phys. Rev. Lett. 129, 191101 (2022)], according to\nwhich all black hole solutions should be separated into three different\ntopological classes, is accurate, at least in the pure Einstein-Maxwell gravity\ntheory.'}, {'Classifying topology of consistent thermodynamics of the\n  four-dimensional neutral Lorentzian NUT-charged spacetimes': 'In this paper, via employing the uniformly modified form of the generalized\noff-shell Helmholtz free energy, we investigate the topological numbers for the\nfour-dimensional neutral Lorentzian Taub-NUT, Taub-NUT-AdS and Kerr-NUT\nspacetimes, and find that these solutions can also be classified into one of\nthree types of those well-known black hole solutions, which implies that these\nspacetimes should be viewed as generic black holes from the viewpoint of the\nthermodynamic topological approach.'}, {'Consistent thermodynamics and topological classes for the\n  four-dimensional Lorentzian charged Taub-NUT spacetimes': 'In this paper, we derive the consistent thermodynamics of the\nfour-dimensional Lorentzian charged Reissner-Nordstr\\""om-NUT (RN-NUT),\nKerr-Newman-NUT (KN-NUT), and RN-NUT-AdS spacetimes in the framework of the\n($\\psi-\\mathcal{N}$)-pair formalism, and then investigate their topological\nnumbers by using the uniformly modified form of the generalized off-shell\nHelmholtz free energy. We find that these solutions can be included into one of\nthree categories of those well-known black hole solutions, which implies that\nthese spacetimes should be viewed as generic black holes from the perspective\nof the topological thermodynamic defects. In addition, we demonstrate that\nalthough the existence of the NUT charge parameter seems to have no impact on\nthe topological number of the charged asymptotically locally flat spacetimes,\nit has a remarkable effect on the topological number of the charged\nasymptotically locally AdS spacetime.'}, {'Topological classes of thermodynamics of the four-dimensional static\n  accelerating black holes': 'In this paper, utilizing the generalized off shell Helmholtz free energy, we\nexplore the topological numbers of the four-dimensional static accelerating\nblack hole and its AdS extension, as well as the static charged accelerating\nblack hole and its AdS extension. Our analysis reveals a profound and\nsignificant impact of the acceleration parameter on the topological numbers\nassociated with the static black holes; and different values (nonzero) of the\nacceleration parameter do not affect the topological numbers of the\ncorresponding four-dimensional static accelerating black holes. In addition, we\ndemonstrate that the electric charge parameter has an important effect on the\ntopological number of the static neutral accelerating black holes, and the\ncosmological constant has a remarkable influence on the topological number of\nthe static accelerating black hole. Furthermore, it is interesting to observe\nthat the difference between the topological number of the asymptotically flat\nstatic accelerating black hole and that of its corresponding asymptotically\nflat static nonaccelerating black hole is always unity, and the difference\nbetween the topological number of the asymptotically AdS static accelerating\nblack hole and that of its corresponding asymptotically AdS static\nnonaccelerating black hole is always $-1$. This new observation leads us to\nconjure that it might be valid also for other accelerating black holes. Of\ncourse, this captivating conjecture requires empirical verification through\ncomprehensive investigation into the topological numbers of other accelerating\nblack holes and their corresponding usual counterparts.'}, {'Poisson metrics and Higgs bundles over noncompact Kähler manifolds': 'In this paper, we study the existence of Poisson metrics on flat vector\nbundles over noncompact Riemannian manifolds and discuss related consequence,\nspecially on the applications in Higgs bundles, towards generalizing\nCorlette-Donaldson-Hitchin-Simpson\'s nonabelian Hodge correspondence to\nnoncompact K\\""{a}hler manifolds setting.'}, {'A Survey on Monochromatic Connections of Graphs': 'The concept of monochromatic connection of graphs was introduced by Caro and\nYuster in 2011. Recently, a lot of results have been published about it. In\nthis survey, we attempt to bring together all the results that dealt with it.\nWe begin with an introduction, and then classify the results into the following\ncategories: monochromatic connection coloring of edge-version, monochromatic\nconnection coloring of vertex-version, monochromatic index, monochromatic\nconnection coloring of total-version.'}, {'Prescribing Chern scalar curvatures on noncompact manifolds': 'In this paper, we investigate the noncompact prescribed Chern scalar\ncurvature problem which reduces to solve a Kazdan-Warner type equation on\nnoncompact non-K\\""{a}hler manifolds. By introducing an analytic condition on\nnoncompact manifolds, we establish related existence results. As its another\napplication, we further give a new proof of a classical multiplicity theorem of\nW.M. Ni.'}]","**Abstract**

Background: Anomaly detection (AD) is pivotal in fields such as medical diagnosis, intelligent manufacturing, and autonomous driving. In the absence of labeled anomaly examples, unsupervised anomaly detection methods, focusing on learning from normal samples to detect anomalies, have garnered significant interest. Traditionally, reconstruction-based methods have been used for this purpose, but their effectiveness is hindered by issues such as blurring and mode collapse.

Objective: Our paper introduces Deep Directional Image Sampling (MDPS), an innovative framework that leverages Directional Marginal Distribution (DMD) for anomaly scoring from multiple normal samples derived from a single test image. 

Innovations & Methods: MDPS projects test images into DMDs, capturing distribution shifts for anomalies. Using Directional Diffusion Models (DDMs) to generate multiple normal images for a given test image, an anomaly score for the test image is calculated. Anomaly detection/localization performance is enhanced through an efficient, self-contained metric design that considers both pixel-level and perceptual-level differences, outperforming conventional techniques.

Results: Experiments on benchmark datasets like MVTec and BTAD demonstrate that MDPS surpasses state-of-the-art UAD methods in anomaly detection and localization accuracy. Particularly, when generating 16 normal images for a single test, MDPS yields the highest performances, showing improvements in AUROC scores compared to existing solutions.

Contributions: This research introduces MDPS, a novel unsupervised AD method that excels at detecting and localizing anomalies through multiple normal image sampling and efficient anomaly scoring. By theoretically grounding the use of DMDs and leveraging DDMs' effective image reconstruction and distribution coverage properties, MDPS achieves enhanced anomaly detection performance.

Applications: MDPS's superior anomaly detection capabilities make it particularly useful in real-world scenarios requiring immediate and accurate identification of anomalies, such as quality control in manufacturing, medical image analysis, and autonomous vehicle safety systems. Its efficient, self-contained nature and demonstrated improvements make it a valuable addition to the unsupervised AD toolkit."
"Automatic Speech Understanding (ASU) aims at human-like speech
interpretation, providing nuanced intent, emotion, sentiment, and content
understanding from speech and language (text) content conveyed in speech.
Typically, training a robust ASU model relies heavily on acquiring large-scale,
high-quality speech and associated transcriptions. However, it is often
challenging to collect or use speech data for training ASU due to concerns such
as privacy. To approach this setting of enabling ASU when speech (audio)
modality is missing, we propose TI-ASU, using a pre-trained text-to-speech
model to impute the missing speech. We report extensive experiments evaluating
TI-ASU on various missing scales, both multi- and single-modality settings, and
the use of LLMs. Our findings show that TI-ASU yields substantial benefits to
improve ASU in scenarios where even up to 95% of training speech is missing.
Moreover, we show that TI-ASU is adaptive to dropout training, improving model
robustness in addressing missing speech during inference.","[{'Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On\n  Federated Learning using Multiview Pseudo-Labeling': ""Speech Emotion Recognition (SER) application is frequently associated with\nprivacy concerns as it often acquires and transmits speech data at the\nclient-side to remote cloud platforms for further processing. These speech data\ncan reveal not only speech content and affective information but the speaker's\nidentity, demographic traits, and health status. Federated learning (FL) is a\ndistributed machine learning algorithm that coordinates clients to train a\nmodel collaboratively without sharing local data. This algorithm shows enormous\npotential for SER applications as sharing raw speech or speech features from a\nuser's device is vulnerable to privacy attacks. However, a major challenge in\nFL is limited availability of high-quality labeled data samples. In this work,\nwe propose a semi-supervised federated learning framework, Semi-FedSER, that\nutilizes both labeled and unlabeled data samples to address the challenge of\nlimited labeled data samples in FL. We show that our Semi-FedSER can generate\ndesired SER performance even when the local label rate l=20 using two SER\nbenchmark datasets: IEMOCAP and MSP-Improv.""}, {'Exploring Workplace Behaviors through Speaking Patterns using\n  Large-scale Multimodal Wearable Recordings: A Study of Healthcare Providers': 'Interpersonal spoken communication is central to human interaction and the\nexchange of information. Such interactive processes involve not only speech and\nspoken language but also non-verbal cues such as hand gestures, facial\nexpressions, and nonverbal vocalization, that are used to express feelings and\nprovide feedback. These multimodal communication signals carry a variety of\ninformation about the people: traits like gender and age as well as about\nphysical and psychological states and behavior. This work uses wearable\nmultimodal sensors to investigate interpersonal communication behaviors\nfocusing on speaking patterns among healthcare providers with a focus on\nnurses. We analyze longitudinal data collected from $99$ nurses in a large\nhospital setting over ten weeks. The results indicate that speaking pattern\ndifferences across shift schedules and working units. Moreover, results show\nthat speaking patterns combined with physiological measures can be used to\npredict affect measures and life satisfaction scores. The implementation of\nthis work can be accessed at https://github.com/usc-sail/tiles-audio-arousal.'}, {'Foundation Model Assisted Automatic Speech Emotion Recognition:\n  Transcribing, Annotating, and Augmenting': 'Significant advances are being made in speech emotion recognition (SER) using\ndeep learning models. Nonetheless, training SER systems remains challenging,\nrequiring both time and costly resources. Like many other machine learning\ntasks, acquiring datasets for SER requires substantial data annotation efforts,\nincluding transcription and labeling. These annotation processes present\nchallenges when attempting to scale up conventional SER systems. Recent\ndevelopments in foundational models have had a tremendous impact, giving rise\nto applications such as ChatGPT. These models have enhanced human-computer\ninteractions including bringing unique possibilities for streamlining data\ncollection in fields like SER. In this research, we explore the use of\nfoundational models to assist in automating SER from transcription and\nannotation to augmentation. Our study demonstrates that these models can\ngenerate transcriptions to enhance the performance of SER systems that rely\nsolely on speech data. Furthermore, we note that annotating emotions from\ntranscribed speech remains a challenging task. However, combining outputs from\nmultiple LLMs enhances the quality of annotations. Lastly, our findings suggest\nthe feasibility of augmenting existing speech emotion datasets by annotating\nunlabeled speech samples.'}, {'Understanding Stress, Burnout, and Behavioral Patterns in Medical\n  Residents Using Large-scale Longitudinal Wearable Recordings': 'Medical residency training is often associated with physically intense and\nemotionally demanding tasks, requiring them to engage in extended working hours\nproviding complex clinical care. Residents are hence susceptible to negative\npsychological effects, including stress and anxiety, that can lead to decreased\nwell-being, affecting them achieving desired training outcomes. Understanding\nthe daily behavioral patterns of residents can guide the researchers to\nidentify the source of stress in residency training, offering unique\nopportunities to improve residency programs. In this study, we investigate the\nworkplace behavioral patterns of 43 medical residents across different stages\nof their training, using longitudinal wearable recordings collected over a\n3-week rotation. Specifically, we explore their ambulatory patterns, the\ncomputer access, and the interactions with mentors of residents. Our analysis\nreveals that residents showed distinct working behaviors in walking movement\npatterns and computer usage compared to different years in the program.\nMoreover, we identify that interaction patterns with mentoring doctors indicate\nstress, burnout, and job satisfaction.'}, {'PEFT-SER: On the Use of Parameter Efficient Transfer Learning Approaches\n  For Speech Emotion Recognition Using Pre-trained Speech Models': 'Many recent studies have focused on fine-tuning pre-trained models for speech\nemotion recognition (SER), resulting in promising performance compared to\ntraditional methods that rely largely on low-level, knowledge-inspired acoustic\nfeatures. These pre-trained speech models learn general-purpose speech\nrepresentations using self-supervised or weakly-supervised learning objectives\nfrom large-scale datasets. Despite the significant advances made in SER through\nthe use of pre-trained architecture, fine-tuning these large pre-trained models\nfor different datasets requires saving copies of entire weight parameters,\nrendering them impractical to deploy in real-world settings. As an alternative,\nthis work explores parameter-efficient fine-tuning (PEFT) approaches for\nadapting pre-trained speech models for emotion recognition. Specifically, we\nevaluate the efficacy of adapter tuning, embedding prompt tuning, and LoRa\n(Low-rank approximation) on four popular SER testbeds. Our results reveal that\nLoRa achieves the best fine-tuning performance in emotion recognition while\nenhancing fairness and requiring only a minimal extra amount of weight\nparameters. Furthermore, our findings offer novel insights into future research\ndirections in SER, distinct from existing approaches focusing on directly\nfine-tuning the model architecture. Our code is publicly available under:\nhttps://github.com/usc-sail/peft-ser.'}, {'TrustSER: On the Trustworthiness of Fine-tuning Pre-trained Speech\n  Embeddings For Speech Emotion Recognition': 'Recent studies have explored the use of pre-trained embeddings for speech\nemotion recognition (SER), achieving comparable performance to conventional\nmethods that rely on low-level knowledge-inspired acoustic features. These\nembeddings are often generated from models trained on large-scale speech\ndatasets using self-supervised or weakly-supervised learning objectives.\nDespite the significant advancements made in SER through the use of pre-trained\nembeddings, there is a limited understanding of the trustworthiness of these\nmethods, including privacy breaches, unfair performance, vulnerability to\nadversarial attacks, and computational cost, all of which may hinder the\nreal-world deployment of these systems. In response, we introduce TrustSER, a\ngeneral framework designed to evaluate the trustworthiness of SER systems using\ndeep learning methods, with a focus on privacy, safety, fairness, and\nsustainability, offering unique insights into future research in the field of\nSER. Our code is publicly available under:\nhttps://github.com/usc-sail/trust-ser.'}, {'User-Level Differential Privacy against Attribute Inference Attack of\n  Speech Emotion Recognition in Federated Learning': 'Many existing privacy-enhanced speech emotion recognition (SER) frameworks\nfocus on perturbing the original speech data through adversarial training\nwithin a centralized machine learning setup. However, this privacy protection\nscheme can fail since the adversary can still access the perturbed data. In\nrecent years, distributed learning algorithms, especially federated learning\n(FL), have gained popularity to protect privacy in machine learning\napplications. While FL provides good intuition to safeguard privacy by keeping\nthe data on local devices, prior work has shown that privacy attacks, such as\nattribute inference attacks, are achievable for SER systems trained using FL.\nIn this work, we propose to evaluate the user-level differential privacy (UDP)\nin mitigating the privacy leaks of the SER system in FL. UDP provides\ntheoretical privacy guarantees with privacy parameters $\\epsilon$ and $\\delta$.\nOur results show that the UDP can effectively decrease attribute information\nleakage while keeping the utility of the SER system with the adversary\naccessing one model update. However, the efficacy of the UDP suffers when the\nFL system leaks more model updates to the adversary. We make the code publicly\navailable to reproduce the results in\nhttps://github.com/usc-sail/fed-ser-leakage.'}, {'Emotion-Aligned Contrastive Learning Between Images and Music': 'Traditional music search engines rely on retrieval methods that match natural\nlanguage queries with music metadata. There have been increasing efforts to\nexpand retrieval methods to consider the audio characteristics of music itself,\nusing queries of various modalities including text, video, and speech. While\nmost approaches aim to match general music semantics to the input queries, only\na few focus on affective qualities. In this work, we address the task of\nretrieving emotionally-relevant music from image queries by learning an\naffective alignment between images and music audio. Our approach focuses on\nlearning an emotion-aligned joint embedding space between images and music.\nThis embedding space is learned via emotion-supervised contrastive learning,\nusing an adapted cross-modal version of the SupCon loss. We evaluate the joint\nembeddings through cross-modal retrieval tasks (image-to-music and\nmusic-to-image) based on emotion labels. Furthermore, we investigate the\ngeneralizability of the learned music embeddings via automatic music tagging.\nOur experiments show that the proposed approach successfully aligns images and\nmusic, and that the learned embedding space is effective for cross-modal\nretrieval applications.'}, {'Unsupervised Joint Learning of Depth, Optical Flow, Ego-motion from\n  Video': ""Estimating geometric elements such as depth, camera motion, and optical flow\nfrom images is an important part of the robot's visual perception. We use a\njoint self-supervised method to estimate the three geometric elements. Depth\nnetwork, optical flow network and camera motion network are independent of each\nother but are jointly optimized during training phase. Compared with\nindependent training, joint training can make full use of the geometric\nrelationship between geometric elements and provide dynamic and static\ninformation of the scene. In this paper, we improve the joint self-supervision\nmethod from three aspects: network structure, dynamic object segmentation, and\ngeometric constraints. In terms of network structure, we apply the attention\nmechanism to the camera motion network, which helps to take advantage of the\nsimilarity of camera movement between frames. And according to attention\nmechanism in Transformer, we propose a plug-and-play convolutional attention\nmodule. In terms of dynamic object, according to the different influences of\ndynamic objects in the optical flow self-supervised framework and the\ndepth-pose self-supervised framework, we propose a threshold algorithm to\ndetect dynamic regions, and mask that in the loss function respectively. In\nterms of geometric constraints, we use traditional methods to estimate the\nfundamental matrix from the corresponding points to constrain the camera motion\nnetwork. We demonstrate the effectiveness of our method on the KITTI dataset.\nCompared with other joint self-supervised methods, our method achieves\nstate-of-the-art performance in the estimation of pose and optical flow, and\nthe depth estimation has also achieved competitive results. Code will be\navailable https://github.com/jianfenglihg/Unsupervised_geometry.""}, {'Unlocking Foundation Models for Privacy-Enhancing Speech Understanding:\n  An Early Study on Low Resource Speech Training Leveraging Label-guided\n  Synthetic Speech Content': 'Automatic Speech Understanding (ASU) leverages the power of deep learning\nmodels for accurate interpretation of human speech, leading to a wide range of\nspeech applications that enrich the human experience. However, training a\nrobust ASU model requires the curation of a large number of speech samples,\ncreating risks for privacy breaches. In this work, we investigate using\nfoundation models to assist privacy-enhancing speech computing. Unlike\nconventional works focusing primarily on data perturbation or distributed\nalgorithms, our work studies the possibilities of using pre-trained generative\nmodels to synthesize speech content as training data with just label guidance.\nWe show that zero-shot learning with training label-guided synthetic speech\ncontent remains a challenging task. On the other hand, our results demonstrate\nthat the model trained with synthetic speech samples provides an effective\ninitialization point for low-resource ASU training. This result reveals the\npotential to enhance privacy by reducing user data collection but using\nlabel-guided synthetic speech content.'}]","Title: Transformative Intent Understanding in Speech Using AI-Driven Missing Modality Incorporation 

**Background**: The pervasive use of speech analytics in applications like call centers, virtual assistants, and human-computer interaction tasks necessitates nuanced understanding and intent classification. Personalized speech data, however, is often limited due to privacy concerns and high data storage costs, fostering the development of techniques capable of handling speech absence in training.

**Objective**: The primary goal of this research is to enhance intent understanding in speech through the innovative incorporation of text modality even in cases where high percentages of the speech data are missing. 

**Innovations**: We propose the Transformer-Informed Speech Understanding (TI-ASU) framework, which imputes the missing speech modality using synthetic speech and generative AI techniques. Additionally, we introduce TI-ASU-MM, applying AI augmentation to the text modality for multimodal learning scenarios.

**Methods**: Our research encompasses synthetic speech generation, text augmentation with AI models, and a multimodal end-to-end learning framework. Specifically, TI-ASU utilizes language models and text-to-speech (TTS) synthesis to predict missing speech, while TI-ASU-MM enriches text representations using both language and speech-related content.

**Results**: Through rigorous experiments on datasets like IEMOCAP, MSP-Improv, Slurp, and SLUE-Voxceleb, we demonstrate statistically significant improvements over baseline models for intent, sentiment, and emotion classifications across complete and multimodal training conditions, especially under extreme speech loss scenarios.

**Contributions**: This work provides a state-of-the-art approach for integrating speech and text, enabling robust AI-driven speech analytics in scenarios with missing speech modality due to privacy constraints. Our techniques expand the scope of AI applications by accommodating low-data and privacy-sensitive environments.

**Applications**: The developed methodologies have broad implications for privacy-conscious industries like healthcare, finance, and customer support, where the utilization of only text data can lead to enhanced conversation analysis, improved customer service, and enhanced data privacy."
"In our pursuit of finding a zero for a monotone and Lipschitz continuous
operator $M : \R^n \rightarrow \R^n$ amidst noisy evaluations, we explore an
associated differential equation within a stochastic framework, incorporating a
correction term. We present a result establishing the existence and uniqueness
of solutions for the stochastic differential equations under examination.
Additionally, assuming that the diffusion term is square-integrable, we
demonstrate the almost sure convergence of the trajectory process $X(t)$ to a
zero of $M$ and of $\|M(X(t))\|$ to $0$ as $t \rightarrow +\infty$.
Furthermore, we provide ergodic upper bounds and ergodic convergence rates in
expectation for $\|M(X(t))\|^2$ and $\langle M(X(t), X(t)-x^*\rangle$, where
$x^*$ is an arbitrary zero of the monotone operator. Subsequently, we apply
these findings to a minimax problem. Finally, we analyze two temporal
discretizations of the continuous-time models, resulting in stochastic variants
of the Optimistic Gradient Descent Ascent and Extragradient methods,
respectively, and assess their convergence properties.","[{'On a zero duality gap result in extended monotropic programming': 'In this note we correct and improve a zero duality gap result in extended\nmonotropic programming given by Bertsekas in [1].'}, {'On extension results for n-cyclically monotone operators in reflexive\n  Banach spaces': 'In this paper we provide some extension results for n-cyclically monotone\noperators in reflexive Banach spaces by making use of the Fenchel duality. In\nthis way we give a positive answer to a question posed by Bauschke and Wang in\n[4].'}, {'Looking for appropriate qualification conditions for subdifferential\n  formulae and dual representations for convex risk measures': 'A fruitful idea, when providing subdifferential formulae and dual\nrepresentations for convex risk measures, is to make use of the conjugate\nduality theory in convex optimization. In this paper we underline the\noutstanding role played by the qualification conditions in the context of\ndifferent problem formulations in this area. We show that not only the\nmeanwhile classical generalized interiority point ones come here to bear, but\nalso a recently introduced one formulated by means of the quasi-relative\ninterior.'}, {'On the Dini-Hadamard subdifferential of the difference of two functions': 'In this paper we first provide a general formula of inclusion for the\nDini-Hadamard epsilon-subdifferential of the difference of two functions and\nshow that it becomes equality in case the functions are directionally\napproximately starshaped at a given point and a weak topological assumption is\nfulfilled. To this end we give a useful characterization of the Dini-Hadamard\nepsilon-subdifferential by means of sponges. The achieved results are employed\nin the formulation of optimality conditions via the Dini-Hadamard\nsubdifferential for cone-constrained optimization problems having the\ndifference of two functions as objective.'}, {'Error bound results for convex inequality systems via conjugate duality': 'The aim of this paper is to implement some new techniques, based on conjugate\nduality in convex optimization, for proving the existence of global error\nbounds for convex inequality systems. We deal first of all with systems\ndescribed via one convex inequality and extend the achieved results, by making\nuse of a celebrated scalarization function, to convex inequality systems\nexpressed by means of a general vector function. We also propose a second\napproach for guaranteeing the existence of global error bounds of the latter,\nwhich meanwhile sharpens the classical result of Robinson.'}, {'A double smoothing technique for solving unconstrained nondifferentiable\n  convex optimization problems': 'The aim of this paper is to develop an efficient algorithm for solving a\nclass of unconstrained nondifferentiable convex optimization problems in finite\ndimensional spaces. To this end we formulate first its Fenchel dual problem and\nregularize it in two steps into a differentiable strongly convex one with\nLipschitz continuous gradient. The doubly regularized dual problem is then\nsolved via a fast gradient method with the aim of accelerating the resulting\nconvergence scheme. The theoretical results are finally applied to an l1\nregularization problem arising in image processing.'}, {'A Douglas-Rachford type primal-dual method for solving inclusions with\n  mixtures of composite and parallel-sum type monotone operators': 'In this paper we propose two different primal-dual splitting algorithms for\nsolving inclusions involving mixtures of composite and parallel-sum type\nmonotone operators which rely on an inexact Douglas-Rachford splitting method,\nhowever applied in different underlying Hilbert spaces. Most importantly, the\nalgorithms allow to process the bounded linear operators and the set-valued\noperators occurring in the formulation of the monotone inclusion problem\nseparately at each iteration, the latter being individually accessed via their\nresolvents. The performances of the primal-dual algorithms are emphasized via\nsome numerical experiments on location and image deblurring problems.'}, {'Solving monotone inclusions involving parallel sums of linearly composed\n  maximally monotone operators': 'The aim of this article is to present two different primal-dual methods for\nsolving structured monotone inclusions involving parallel sums of compositions\nof maximally monotone operators with linear bounded operators. By employing\nsome elaborated splitting techniques, all of the operators occurring in the\nproblem formulation are processed individually via forward or backward steps.\nThe treatment of parallel sums of linearly composed maximally monotone\noperators is motivated by applications in imaging which involve first- and\nsecond-order total variation functionals, to which a special attention is\ngiven.'}, {""An inertial Tseng's type proximal algorithm for nonsmooth and nonconvex\n  optimization problems"": 'We investigate the convergence of a forward-backward-forward proximal-type\nalgorithm with inertial and memory effects when minimizing the sum of a\nnonsmooth function with a smooth one in the absence of convexity. The\nconvergence is obtained provided an appropriate regularization of the objective\nsatisfies the Kurdyka-\\L{}ojasiewicz inequality, which is for instance\nfulfilled for semi-algebraic functions.'}, {'A hybrid proximal-extragradient algorithm with inertial effects': ""We incorporate inertial terms in the hybrid proximal-extragradient algorithm\nand investigate the convergence properties of the resulting iterative scheme\ndesigned for finding the zeros of a maximally monotone operator in real Hilbert\nspaces. The convergence analysis relies on extended Fej\\'er monotonicity\ntechniques combined with the celebrated Opial Lemma. We also show that the\nclassical hybrid proximal-extragradient algorithm and the inertial versions of\nthe proximal point, the forward-backward and the forward-backward-forward\nalgorithms can be embedded in the framework of the proposed iterative scheme.""}]","Abstract:

The monotone equation M(x) = 0 plays a central role in optimization theory, with applications spanning continuous, discrete, and stochastic modeling. This article endeavors to analyze a continuous-time, stochastic differential equation (SDE) that models a continuous物流公司monotone operator M with a proposed random noise term, allowing for stochastic optimization approaches. The framework under scrutiny, termed SDE-M, leverages continuous dynamics for algorithmic developments in stochastic settings.

The objective is to rigorously investigate the existence and uniqueness of SDE-M's solution trajectory, as well as demonstrate its convergence properties, particularly in the context of noisy operator evaluations. The paper's innovations revolve around quantifying stochastic estimation error's impact on algorithmic performance, delineated through specific convergence rates in expectation.

Mathematically, the SDE-M formulation adheres to stochastic analysis methods, contending with the inherent randomness of operator evaluations. The theoretical underpinnings for existence, uniqueness, and solutions are established via a transformed regularized equation akin to a stochastic approximation. 

Results focus on exact and approximate trajectories reflecting algorithm dynamics. Within, the SDE-M model's trajectory exhibits favorable convergence properties, with error bounds presented in expectation. Pathwise and process-wise evaluations highlight the stochastic dynamics, providing insights into how randomness affects algorithmic performance.

In essence, this study contributes to the theoretical foundations of stochastic optimization methods by addressing continuous-time dynamics with non-smooth operators. It opens avenues for practical deployment, hence applicable across disciplines as diverse as financial modeling, machine learning, and signal processing.

Future research can build on these findings to improve algorithmic robustness and efficiency, drawing upon the theoretical framework presented here to enhance optimization methodologies in noisy, uncertain environments."
"The introduction of genome engineering technology has transformed biomedical
research, making it possible to make precise changes to genetic information.
However, creating an efficient gene-editing system requires a deep
understanding of CRISPR technology, and the complex experimental systems under
investigation. While Large Language Models (LLMs) have shown promise in various
tasks, they often lack specific knowledge and struggle to accurately solve
biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent
augmented with domain knowledge and external tools to automate and enhance the
design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages
the reasoning ability of LLMs to facilitate the process of selecting CRISPR
systems, designing guide RNAs, recommending cellular delivery methods, drafting
protocols, and designing validation experiments to confirm editing outcomes. We
showcase the potential of CRISPR-GPT for assisting non-expert researchers with
gene-editing experiments from scratch and validate the agent's effectiveness in
a real-world use case. Furthermore, we explore the ethical and regulatory
considerations associated with automated gene-editing design, highlighting the
need for responsible and transparent use of these tools. Our work aims to
bridge the gap between beginner biological researchers and CRISPR genome
engineering techniques, and demonstrate the potential of LLM agents in
facilitating complex biological discovery tasks.","[{'Fast Federated Learning in the Presence of Arbitrary Device\n  Unavailability': 'Federated Learning (FL) coordinates with numerous heterogeneous devices to\ncollaboratively train a shared model while preserving user privacy. Despite its\nmultiple advantages, FL faces new challenges. One challenge arises when devices\ndrop out of the training process beyond the control of the central server. In\nthis case, the convergence of popular FL algorithms such as FedAvg is severely\ninfluenced by the straggling devices. To tackle this challenge, we study\nfederated learning algorithms under arbitrary device unavailability and propose\nan algorithm named Memory-augmented Impatient Federated Averaging (MIFA). Our\nalgorithm efficiently avoids excessive latency induced by inactive devices, and\ncorrects the gradient bias using the memorized latest updates from the devices.\nWe prove that MIFA achieves minimax optimal convergence rates on non-i.i.d.\ndata for both strongly convex and non-convex smooth functions. We also provide\nan explicit characterization of the improvement over baseline algorithms\nthrough a case study, and validate the results by numerical experiments on\nreal-world datasets.'}, {'Why Do Deep Residual Networks Generalize Better than Deep Feedforward\n  Networks? -- A Neural Tangent Kernel Perspective': 'Deep residual networks (ResNets) have demonstrated better generalization\nperformance than deep feedforward networks (FFNets). However, the theory behind\nsuch a phenomenon is still largely unknown. This paper studies this fundamental\nproblem in deep learning from a so-called ""neural tangent kernel"" perspective.\nSpecifically, we first show that under proper conditions, as the width goes to\ninfinity, training deep ResNets can be viewed as learning reproducing kernel\nfunctions with some kernel function. We then compare the kernel of deep ResNets\nwith that of deep FFNets and discover that the class of functions induced by\nthe kernel of FFNets is asymptotically not learnable, as the depth goes to\ninfinity. In contrast, the class of functions induced by the kernel of ResNets\ndoes not exhibit such degeneracy. Our discovery partially justifies the\nadvantages of deep ResNets over deep FFNets in generalization abilities.\nNumerical results are provided to support our claim.'}, {'Density-dependent quark mean-field model for nuclear matter and neutron\n  stars': 'We develop a density-dependent quark mean-field (DDQMF) model to study the\nproperties of nuclear matter and neutron stars, where the coupling strength\nbetween $\\sigma$ meson and nucleon is generated by the degree of freedom of\nquarks, while other meson coupling constants are regarded as density-dependent\nones. Two values for the nucleon effective mass, $M^*_{N0}/M_N=0.556,~0.70$ at\nthe saturation density are chosen based on the consideration of the\ncore-collapse supernova simulation and finite nuclei when the meson-nucleon\ncoupling constants are fixed. We find that the equation of state (EOS) of\nnuclear matter, the symmetry energy, the mass-radius relations, and the tidal\ndeformabilities of neutron stars with larger nucleon effective mass are more\nsensitive to the skewness coefficient $J_0$. The EOSs with $M^*_{N0}/M_N=0.70$\nare softer when the skewness coefficient $J_0=-800$ MeV. However, the maximum\nmasses of the neutron star can be around $2.32M_\\odot$ with $J_0=400$ MeV\nregardless of the value of the nucleon effective mass. By manipulating the\ncoupling strength of the isovector meson to generate different slopes of\nsymmetry energy, we construct the neutron star EOSs that can satisfy the\ndifferent variables from the simultaneous mass-radius measurements of PSR\nJ0030+0451, PSR J0740+6620 by the NICER collaboration, the mass-radius\nrelations of HESS J1731-347, and the radius constraints from the\ngravitational-wave signal GW170817 in the framework of DDQMF model. At the same\ntime, most of these constructed EOSs can also satisfy the constraints of the\ntidal deformability from GW170817 event.'}, {'Score Approximation, Estimation and Distribution Recovery of Diffusion\n  Models on Low-Dimensional Data': 'Diffusion models achieve state-of-the-art performance in various generation\ntasks. However, their theoretical foundations fall far behind. This paper\nstudies score approximation, estimation, and distribution recovery of diffusion\nmodels, when data are supported on an unknown low-dimensional linear subspace.\nOur result provides sample complexity bounds for distribution estimation using\ndiffusion models. We show that with a properly chosen neural network\narchitecture, the score function can be both accurately approximated and\nefficiently estimated. Furthermore, the generated distribution based on the\nestimated score function captures the data geometric structures and converges\nto a close vicinity of the data distribution. The convergence rate depends on\nthe subspace dimension, indicating that diffusion models can circumvent the\ncurse of data ambient dimensionality.'}, {'The Hadron-Quark Crossover in Neutron Star within Gaussian Process\n  Regression Method': 'The equations of state of the neutron star at the hadron-quark crossover\nregion are interpolated with the Gaussian process regression (GPR) method,\nwhich can reduce the randomness of present interpolation schemes. The\nrelativistic mean-field (RMF) model and Nambu-Jona-Lasinio (NJL) model are\nemployed to describe the hadronic phase and quark phase, respectively. In the\nRMF model, the coupling term between $\\omega$ and $\\rho$ mesons is considered\nto control the density-dependent behaviors of symmetry energy, i.e. the slope\nof symmetry energy, $L$. Furthermore, the vector interaction between quarks is\nincluded in the NJL model to obtain the additional repulsive contributions.\nTheir coupling strengths and the crossover windows are discussed in the present\nframework under the constraints on the neutron star from gravitational wave\ndetections, massive neutron star measurements, mass-radius simultaneous\nobservation of NICER collaboration, and the neutron skin thickness of\n$^{208}$Pb from PREX-II. It is found that the slope of symmetry energy, $L$\nshould be around $50-90$ MeV and the crossover window is $(0.3,~0.6)~\\rm\nfm^{-3}$ with these observables. Furthermore, the uncertainties of neutron star\nmasses and radii in the hadron-quark crossover regions are also predicted by\nthe GPR method.'}, {'The hadronic equation of state of HESS J1731-347 from the relativistic\n  mean-field model with tensor coupling': 'A recent report has identified a central compact object (CCO) within the\nsupernova remnant HESS J1731-347, with a mass and radius of\n$M=0.77^{+0.20}_{-0.17}M{\\odot}$ and $R=10.4^{+0.86}_{-0.78}$ km, respectively.\nTo investigate this light compact star, a density-dependent relativistic\nmean-field (DDRMF) model, specifically the DDVT model, has been employed. The\nDDVT model incorporates tensor couplings of vector mesons, which {can}\nsuccessfully describe the properties of finite nuclei, such as charge radius,\nbinding energy, and spin-orbit splitting. The introduction of tensor coupling\nreduces the influence of scalar mesons and generates a softer equation of state\n(EOS) in the outer core of the neutron star. Moreover, it has been found that\nthe crust segment plays a crucial role in reproducing the mass-radius relation\nof HESS J1731-347, indicating a preference for a soft crust EOS. By\nmanipulating the coupling strength of the isovector meson in the DDVT parameter\nset, a reasonable hadronic EOS has been obtained, satisfying the constraints\nfrom the gravitational-wave signal GW170817, the simultaneous mass-radius\nmeasurements from the NICER collaboration, and the properties of finite nuclei.\nNotably, the mass-radius relations derived from this hadronic EOS also\naccurately describe the observables of HESS J1731-347. Therefore, based on our\nestimation, the CCO in HESS J1731-347 may represent the lightest known neutron\nstar.'}, {'Scaling In-Context Demonstrations with Structured Attention': 'The recent surge of large language models (LLMs) highlights their ability to\nperform in-context learning, i.e., ""learning"" to perform a task from a few\ndemonstrations in the context without any parameter updates. However, their\ncapabilities of in-context learning are limited by the model architecture: 1)\nthe use of demonstrations is constrained by a maximum sentence length due to\npositional embeddings; 2) the quadratic complexity of attention hinders users\nfrom using more demonstrations efficiently; 3) LLMs are shown to be sensitive\nto the order of the demonstrations. In this work, we tackle these challenges by\nproposing a better architectural design for in-context learning. We propose\nSAICL (Structured Attention for In-Context Learning), which replaces the\nfull-attention by a structured attention mechanism designed for in-context\nlearning, and removes unnecessary dependencies between individual\ndemonstrations, while making the model invariant to the permutation of\ndemonstrations. We evaluate SAICL in a meta-training framework and show that\nSAICL achieves comparable or better performance than full attention while\nobtaining up to 3.4x inference speed-up. SAICL also consistently outperforms a\nstrong Fusion-in-Decoder (FiD) baseline which processes each demonstration\nindependently. Finally, thanks to its linear nature, we demonstrate that SAICL\ncan easily scale to hundreds of demonstrations with continuous performance\ngains with scaling.'}, {'The possibility of the secondary object in GW190814 as a neutron star': 'A compact object was observed with a mass $2.50-2.67~M_\\odot$ by LIGO\nScientific and Virgo collaborations (LVC) in GW190814, which provides a great\nchallenge to the investigations into the supranuclear matter. To study this\nobject, properties of neutron star are systematically calculated within the\nlatest density-dependent relativistic mean-field (DDRMF) parameterizations,\nwhich are determined by the ground state properties of spherical nuclei. The\nmaximum masses of neutron star calculated by DD-MEX and DD-LZ1 sets can be\naround $2.55~M_\\odot$ with quite stiff equations of state generated by their\nstrong repulsive contributions from vector potentials at high densities. Their\nmaximum speeds of sound $c_s/c$ are smaller than $\\sqrt{0.8}$ at the center of\nneutron star and the dimensionless tidal deformabilities at $1.4~M_\\odot$ are\nless than $800$. Furthermore, the radii of $1.4 ~M_\\odot$ also satisfy the\nconstraint from the observation of mass-radius simultaneous measurements\n(NICER). Therefore, we conclude that one cannot exclude the possibility of the\nsecondary object in GW190814 as a neutron star composed of hadron matter from\nDDRMF models.'}, {'The hyperonic star in relativistic mean-field model': 'The neutron star as a supernova remnant is attracting high attention recently\ndue to the gravitation wave detection and precise measurements about its mass\nand radius. In the inner core region of the neutron star, the strangeness\ndegrees of freedom, such as the hyperons, can be present, which is also named\nas a hyperonic star. In this work, the neutron star consisting of nucleons and\nleptons, and the hyperonic star including the hyperons will be reviewed in the\nframework of the relativistic mean-field (RMF) model. The popular non-linear\nand density-dependent RMF parametrizations in the market will be adopted to\ninvestigate the role of strangeness baryons in a hyperonic star on its mass,\nradius, tidal deformability, and other properties. Finally, the magnitudes of\nthe coupling strengths between mesons and hyperons also will be discussed,\nwhich can generate the massive hyperonic star with present RMF parameter sets,\nwhen the vector coupling constants are strong.'}, {'Reward-Directed Conditional Diffusion: Provable Distribution Estimation\n  and Reward Improvement': 'We explore the methodology and theory of reward-directed generation via\nconditional diffusion models. Directed generation aims to generate samples with\ndesired properties as measured by a reward function, which has broad\napplications in generative AI, reinforcement learning, and computational\nbiology. We consider the common learning scenario where the data set consists\nof unlabeled data along with a smaller set of data with noisy reward labels.\nOur approach leverages a learned reward function on the smaller data set as a\npseudolabeler. From a theoretical standpoint, we show that this directed\ngenerator can effectively learn and sample from the reward-conditioned data\ndistribution. Additionally, our model is capable of recovering the latent\nsubspace representation of data. Moreover, we establish that the model\ngenerates a new population that moves closer to a user-specified target reward\nvalue, where the optimality gap aligns with the off-policy bandit regret in the\nfeature subspace. The improvement in rewards obtained is influenced by the\ninterplay between the strength of the reward signal, the distribution shift,\nand the cost of off-support extrapolation. We provide empirical results to\nvalidate our theory and highlight the relationship between the strength of\nextrapolation and the quality of generated samples.'}]","Title: CRISPR-GPT: Enhancing Gene-Editing Experimental Designs Through Large Language Model Integration

Abstract:

This research paper introduces CRISPR-GPT, an advanced Large Language Model (LLM)-powered system engineered to automate gene-editing experiment design. With a focus on CRISPR technologies, CRISPR-GPT addresses deceptive outputs typical of general LLMS, enabling more accurate, context-rich, and concisely tailored guidance to researchers for designing and executing experiments. Through a comprehensive evaluation process with human experts, the system demonstrated superior performance in precision, insight, and relevance compared to general-purpose LLMS.

Innovation and methods involve a meticulously designed process comprising task decomposition, interactive prompting, and Q&A functionalities, integrated within a core LLM planning engine and supplemented by a suite of external tools. This modular framework facilitates comprehensive task execution, from selecting CRISPR systems to designing optimal gene editing strategies.

The paper's main findings reveal that CRISPR-GPT effectively mitigates key limitations of general LLM outputs by providing accuracy, coherence, and the necessary depth for gene editing tasks. Contributions encompass refining the process of gene editing, enhancing efficiency, and ensuring the reliability and reproducibility of such experiments. 

CRISPR-GPT holds significant potential in expediting research and development in fields such as genetic disorders and regenerative medicine, fostering the responsible innovation and application of CRISPR technologies. This novel integration of LLMs into gene editing experimental design marks a pivotal step towards personalized and effective medical solutions."
"In this paper, we propose an effective two-stage approach named
Grounded-Dreamer to generate 3D assets that can accurately follow complex,
compositional text prompts while achieving high fidelity by using a pre-trained
multi-view diffusion model. Multi-view diffusion models, such as MVDream, have
shown to generate high-fidelity 3D assets using score distillation sampling
(SDS). However, applied naively, these methods often fail to comprehend
compositional text prompts, and may often entirely omit certain subjects or
parts. To address this issue, we first advocate leveraging text-guided 4-view
images as the bottleneck in the text-to-3D pipeline. We then introduce an
attention refocusing mechanism to encourage text-aligned 4-view image
generation, without the necessity to re-train the multi-view diffusion model or
craft a high-quality compositional 3D dataset. We further propose a hybrid
optimization strategy to encourage synergy between the SDS loss and the sparse
RGB reference images. Our method consistently outperforms previous
state-of-the-art (SOTA) methods in generating compositional 3D assets,
excelling in both quality and accuracy, and enabling diverse 3D from the same
text prompt.","[{'Moduli of Continuity for Viscosity Solutions': ""In this paper, we investigate the moduli of continuity for viscosity\nsolutions of a wide class of nonsingular quasilinear evolution equations and\nalso for the level set mean curvature flow, which is an example of singular\ndegenerate equations. We prove that the modulus of continuity is a viscosity\nsubsolution of some one dimensional equation. This work extends B. Andrews'\nrecent result on moduli of continuity for smooth spatially periodic solutions.""}, {'Modulus of Continuity Estimates for Fully Nonlinear Parabolic Equations': 'We prove that the moduli of continuity of viscosity solutions to fully\nnonlinear parabolic partial differential equations are viscosity subsolutions\nof suitable parabolic equations of one space variable. As applications, we\nobtain sharp Lipschitz bounds and gradient estimates for fully nonlinear\nparabolic equations with bounded initial data, via comparison with\none-dimensional solutions. This work extends multiple results of Andrews and\nClutterbuck for quasilinear equations to fully nonlinear equations.'}, {'Kähler surfaces with six-positive curvature operator of the second\n  kind': 'The purpose of this article is to initiate the investigation of the curvature\noperator of the second kind on K\\""ahler manifolds. The main result asserts that\na closed K\\""ahler surface with six-positive curvature operator of the second\nkind is biholomorphic to $\\mathbb{CP}^2$. It is also shown that a closed\nnon-flat K\\""ahler surface with six-nonnegative curvature operator of the second\nkind is either biholomorphic to $\\mathbb{CP}^2$ or isometric to $\\mathbb{S}^2\n\\times \\mathbb{S}^2$.'}, {'Product manifolds and the curvature operator of the second kind': 'We investigate the curvature operator of the second kind on product\nRiemannian manifolds and obtain some optimal rigidity results. For instance, we\nprove that the universal cover of an $n$-dimensional non-flat complete locally\nreducible Riemannian manifold with $(n+\\frac{n-2}{n})$-nonnegative\n(respectively, $(n+\\frac{n-2}{n})$-nonpositive) curvature operator of the\nsecond kind must be isometric to $\\mathbb{S}^{n-1}\\times \\mathbb{R}$\n(respectively, $\\mathbb{H}^{n-1}\\times \\mathbb{R}$) up to scaling. We also\nprove analogous optimal rigidity results for $\\mathbb{S}^{n_1}\\times\n\\mathbb{S}^{n_2}$ and $\\mathbb{H}^{n_1}\\times \\mathbb{H}^{n_2}$, $n_1,n_2 \\geq\n2$, among product Riemannian manifolds, as well as for $\\mathbb{CP}^{m_1}\\times\n\\mathbb{CP}^{m_2}$ and $\\mathbb{CH}^{m_1}\\times \\mathbb{CH}^{m_2}$,\n$m_1,m_2\\geq 1$, among product K\\""ahler manifolds. Our approach is pointwise\nand algebraic.'}, {'Manifolds with $4\\frac{1}{2}$-positive curvature operator of the second\n  kind': 'We show that a closed four-manifold with $4\\frac{1}{2}$-positive curvature\noperator of the second kind is diffeomorphic to a spherical space form. The\ncurvature assumption is sharp as both $\\mathbb{CP}^2$ and $\\mathbb{S}^3 \\times\n\\mathbb{S}^1$ have $4\\frac{1}{2}$-nonnegative curvature operator of the second\nkind. In higher dimensions $n\\geq 5$, we show that closed Riemannian manifolds\nwith $4\\frac{1}{2}$-positive curvature operator of the second kind are\nhomeomorphic to spherical space forms. These results are proved by showing that\n$4\\frac{1}{2}$-positive curvature operator of the second kind implies both\npositive isotropic curvature and positive Ricci curvature. Rigidity results for\n$4\\frac{1}{2}$-nonnegative curvature operator of the second kind are also\nobtained.'}, {'Manifolds with nonnegative curvature operator of the second kind': ""We investigate the curvature operator of the second kind on Riemannian\nmanifolds and prove several classification results. The first one asserts that\na closed Riemannian manifold with three-positive curvature operator of the\nsecond kind is diffeomorphic to a spherical space form, improving a recent\nresult of Cao-Gursky-Tran assuming two-positivity. The second one states that a\nclosed Riemannian manifold with three-nonnegative curvature operator of the\nsecond kind is either diffeomorphic to a spherical space form, or flat, or\nisometric to a quotient of a compact irreducible symmetric space. This settles\nthe nonnegativity part of Nishikawa's conjecture under a weaker assumption.""}, {'Kähler manifolds and the curvature operator of the second kind': 'This article aims to investigate the curvature operator of the second kind on\nK\\""ahler manifolds. The first result states that an $m$-dimensional K\\""ahler\nmanifold with $\\frac{3}{2}(m^2-1)$-nonnegative (respectively,\n$\\frac{3}{2}(m^2-1)$-nonpositive) curvature operator of the second kind must\nhave constant nonnegative (respectively, nonpositive) holomorphic sectional\ncurvature. The second result asserts that a closed $m$-dimensional K\\""ahler\nmanifold with $\\left(\\frac{3m^3-m+2}{2m}\\right)$-positive curvature operator of\nthe second kind has positive orthogonal bisectional curvature, thus being\nbiholomorphic to $\\mathbb{CP}^m$. We also prove that\n$\\left(\\frac{3m^3+2m^2-3m-2}{2m}\\right)$-positive curvature operator of the\nsecond kind implies positive orthogonal Ricci curvature. Our approach is\npointwise and algebraic.'}, {'Nonparametric hypersurfaces moving by powers of Gauss curvature': 'We study asymptotic behavior of nonparametric hypersurfaces moving by\n$\\alpha$ powers of Gauss curvature $\\alpha > 1/n$. Our work generalizes the\nresults of V. Oliker [Oli91] for $\\alpha= 1$.'}, {'Moduli of Continuity for Viscosity Solutions on Manifolds': ""We establish the estimates of modulus of continuity for viscosity solutions\nof nonlinear evolution equations on manifolds, extending previous work of B.\nAndrews and J. Clutterbuck for regular solutions on manifolds \\cite{AC3} and\nthe first author's recent work for viscosity solutions in Euclidean spaces\n\\cite{me1}.""}, {'Kähler-Ricci Shrinkers and Ancient Solutions with Nonnegative\n  Orthogonal Bisectional Curvature': 'In this paper we prove classification results for gradient shrinking Ricci\nsolitons under two invariant conditions, namely nonnegative orthogonal\nbisectional curvature and weakly PIC1, without any curvature bound. New results\non ancient solutions for the Ricci and K\\""ahler-Ricci flow are also obtained.\nThe main new feature is that no curvature upper bound is assumed.'}]","In the era of vast digital information, advancing generative models for creating photorealistic 3D objects from text descriptions has gained significant traction. Despite existing methods producing impressive results, they suffer from shortcomings like orientation inconsistency across multiple views and failure in precisely aligning objects based on text semantics. This paper introduces a novel, hierarchical text-to-3D generative framework named 'Grounded-Dreamer', addressing these challenges.

The core objective of our research is to develop a system that can accurately generate 3D models with high fidelity and support for complex text prompts, especially those describing multiple subjects in specific spatial relationships or involving animal-object interactions. To achieve this, the framework employs a two-stage iterative process, integrating diffusion models and Refocused latents to generate four diverse yet coherent reference views before optimizing these into high-resolution 3D models using NeRF-based methods.

Our innovations include a fine-grained alignment mechanism, attention refocusing loss, and synergistic reconstruction with diffusion priors, ensuring that generated objects adhere precisely to text descriptions. With our system, more compositionally complete views and better semantic alignment between the models and prompts are attained, making it a high-time-bound solution even for intricate tasks.

Compared to current state-of-the-art Text-to-3D models, 'Grounded-Dreamer' provides substantial improvements in 3D asset quality while maintaining generality across diverse text descriptions. This is particularly beneficial for industries considering digital content creation, 3D virtual environments, and interactive AI-based applications.

The research craftsmen a novel solution to a long-established but challenging problem within computer graphics and AI. With the implications of generating complex, scene-specific 3D models, the pathway opens towards advanced applications in virtual reality, gaming, e-commerce, and more. This initiative sets a new benchmark for 3D generation, boosting the capabilities of text-to-3D models to deliver higher quality, more accurate, and semantically enhanced creations."
"We consider the problem of fairly allocating a combination of divisible and
indivisible goods. While fairness criteria like envy-freeness (EF) and
proportionality (PROP) can always be achieved for divisible goods, only their
relaxed versions, such as the ''up to one'' relaxations EF1 and PROP1, can be
satisfied when the goods are indivisible. The ''up to one'' relaxations require
the fairness conditions to be satisfied provided that one good can be
completely eliminated or added in the comparison. In this work, we bridge the
gap between the two extremes and propose ''up to a fraction'' relaxations for
the allocation of mixed divisible and indivisible goods. The fraction is
determined based on the proportion of indivisible goods, which we call the
indivisibility ratio. The new concepts also introduce asymmetric conditions
that are customized for individuals with varying indivisibility ratios. We
provide both upper and lower bounds on the fractions of the modified item in
order to satisfy the fairness criterion. Our results are tight up to a constant
for EF and asymptotically tight for PROP.","[{'Parameterized Littlewood-Paley operators with variable kernels on Hardy\n  spaces and weak Hardy spaces': 'In this paper, by using the atomic decomposition theory of Hardy space and\nweak Hardy space, we discuss the boundedness of parameterized Littlewood-Paley\noperator with variable kernel on these spaces.'}, {'Invertible Convolution with Symmetric Paddings': 'We show that symmetrically padded convolution can be analytically inverted\nvia DFT. We comprehensively analyze several different symmetric and\nanti-symmetric padding modes and show that multiple cases exist where the\ninversion can be achieved. The implementation is available at\n\\url{https://github.com/prclibo/iconv_dft}.'}, {'Boundedness of parameterized Marcinkiewicz integrals with variable\n  kernels on Hardy spaces and weak Hardy spaces': 'In this paper, by using the atomic decomposition theory of Hardy space and\nweak Hardy space, the author establishes the boundedness of parameterized\nMarcinkiewicz integral with variable kernel on these spaces, under the Dini\ncondition or H\\""{o}rmander condition imposed on kernel.'}, {'Weighted Norm Inequalities for Parametric Littlewood-Paley Operators': 'In this paper, the author establishes the boundedness of parametric\nLittlewood-Paley operators from Musielak-Orlicz Hardy space to Musielak-Orlicz\nspace, or to weak Musielak-Orlicz space at the critical index. Part of these\nresults are new even for classical Hardy space of Fefferman and Stein.'}, {'A Spline-based Volumetric Data Modeling Framework and Its Applications': 'In this dissertation, we concentrate on the challenging research issue of\ndeveloping a spline-based modeling framework, which converts the conventional\ndata (e.g., surface meshes) to tensor-product trivariate splines. This\nmethodology can represent both boundary/volumetric geometry and real volumetric\nphysical attributes in a compact and continuous fashion. The regular\ntensor-product structure enables our new developed methods to be embedded into\nthe industry standard seamlessly. These properties make our techniques highly\npreferable in many physically-based applications including mechanical analysis,\nshape deformation and editing, virtual surgery training, etc.'}, {'3D Fully Convolutional Network for Vehicle Detection in Point Cloud': '2D fully convolutional network has been recently successfully applied to\nobject detection from images. In this paper, we extend the fully convolutional\nnetwork based detection techniques to 3D and apply it to point cloud data. The\nproposed approach is verified on the task of vehicle detection from lidar point\ncloud for autonomous driving. Experiments on the KITTI dataset shows a\nsignificant performance improvement over the previous point cloud based\ndetection approaches.'}, {'A Survey of Spline-based Volumetric Data Modeling Framework and Its\n  Applications': 'The rapid advances in 3D scanning and acquisition techniques have given rise\nto the explosive increase of volumetric digital models in recent years. This\ndissertation systematically trailblazes a novel volumetric modeling framework\nto represent 3D solids. The need to explore more efficient and robust 3D\nmodeling framework has gained the prominence. Although the traditional surface\nrepresentation (e.g., triangle mesh) has many attractive properties, it is\nincapable of expressing the interior space and materials. Such a serious\ndrawback overshadows many potential modeling and analysis applications.\nConsequently volumetric modeling techniques become the well-known solution to\nthis problem. Nevertheless, many unsolved research issues remain when\ndeveloping an efficient modeling paradigm for existing 3D models: complex\ngeometry (fine details and extreme concaveness), arbitrary topology,\nheterogenous materials, large-scale data storage and processing, etc.'}, {'On Enhancing Ground Surface Detection from Sparse Lidar Point Cloud': 'Ground surface detection in point cloud is widely used as a key module in\nautonomous driving systems. Different from previous approaches which are mostly\ndeveloped for lidars with high beam resolution, e.g. Velodyne HDL-64, this\npaper proposes ground detection techniques applicable to much sparser point\ncloud captured by lidars with low beam resolution, e.g. Velodyne VLP-16. The\napproach is based on the RANSAC scheme of plane fitting. Inlier verification\nfor plane hypotheses is enhanced by exploiting the point-wise tangent, which is\na local feature available to compute regardless of the density of lidar beams.\nGround surface which is not perfectly planar is fitted by multiple\n(specifically 4 in our implementation) disjoint plane regions. By assuming\nthese plane regions to be rectanglar and exploiting the integral image\ntechnique, our approach approximately finds the optimal region partition and\nplane hypotheses under the RANSAC scheme with real-time computational\ncomplexity.'}, {'An explanation for the distribution characteristics of stock returns': ""Observations indicate that the distributions of stock returns in financial\nmarkets usually do not conform to normal distributions, but rather exhibit\ncharacteristics of high peaks, fat tails and biases. In this work, we assume\nthat the effects of events or information on prices obey normal distribution,\nwhile financial markets often overreact or underreact to events or information,\nresulting in non normal distributions of stock returns. Based on the above\nassumptions, we propose a reaction function for a financial market reacting to\nevents or information, and a model based on it to describe the distribution of\nreal stock returns. Our analysis of the returns of China Securities Index 300\n(CSI 300), the Standard & Poor's 500 Index (SPX or S&P 500) and the Nikkei 225\nIndex (N225) at different time scales shows that financial markets often\nunderreact to events or information with minor impacts, overreact to events or\ninformation with relatively significant impacts, and react slightly stronger to\npositive events or information than to negative ones. In addition, differences\nin financial markets and time scales of returns can also affect the shapes of\nthe reaction functions.""}, {'Testing Business Cycle Theories: Evidence from the Great Recession': 'Empirical business cycle studies using cross-country data usually cannot\nachieve causal relationships while within-country studies mostly focus on the\nbust period. We provide the first causal investigation into the boom period of\nthe 1999-2010 U.S. cross-metropolitan business cycle. Using a novel research\ndesign, we show that credit expansion in private-label mortgages causes a\ndifferentially stronger boom (2000-2006) and bust (2007-2010) cycle in the\nhouse-related industries in the high net-export-growth areas. Most importantly,\nour unique research design enables us to perform the most comprehensive tests\non theories (hypotheses) regarding the business cycle. We show that the\nfollowing theories (hypotheses) cannot explain the cause of the 1999-2010 U.S.\nbusiness cycle: the speculative euphoria hypothesis, the real business cycle\ntheory, the collateral-driven credit cycle theory, the business uncertainty\ntheory, and the extrapolative expectation theory.'}]","**Abstract**

In the intricate landscape of fair allocation theory, inclusive of both divisible and indivisible goods, our research aims to extend traditional ""up to one"" fairness notions by introducing ""up to a fraction"" fairness concepts. Our main objective is to devise a novel algorithm that guarantees a proportional allocation (PROPα), where each entity's utility is ensured to surpass or match a predefined α-fraction compared to the best alternative item in their bundle.

We introduce PROPα, a relaxed proportional fairness criterion, to address the challenges in dividing mixed goods that feature a singular divisible component and multiple indivisibles. This approach decisively mitigates compatibility gaps between fairness and efficiency reliability when dealing with mixed goods distributions.

Utilizing a hybrid allocation system, our investigation elucidates rigorous theoretical grounds during algorithm development. We validate the existence and computational feasibility of PROPα allocations through the design of an efficient algorithm grounded in polynomial complexity.

-another key contribution is the analysis of the proposed algorithm's properties, which validates its procurement of proportional fairness while selectively allocating indivisible goods. As a demonstration, we disprove an existing outgrowth, further substantiating the necessity and superiority of the new approach over traditional methods.

The implications extend beyond theoretical confirmation, uncovering real-world applications in distribution scenarios that require equitable outcomes, particularly when the qualitative differences between divisible and indivisible goods are significant. This research significantly advances understanding in the field, presenting guiding principles for mechanisms aiming to achieve an equitable and efficient mix of assets."
"Recently, the mysterious In-Context Learning (ICL) ability exhibited by
Transformer architectures, especially in large language models (LLMs), has
sparked significant research interest. However, the resilience of Transformers'
in-context learning capabilities in the presence of noisy samples, prevalent in
both training corpora and prompt demonstrations, remains underexplored. In this
paper, inspired by prior research that studies ICL ability using simple
function classes, we take a closer look at this problem by investigating the
robustness of Transformers against noisy labels. Specifically, we first conduct
a thorough evaluation and analysis of the robustness of Transformers against
noisy labels during in-context learning and show that they exhibit notable
resilience against diverse types of noise in demonstration labels. Furthermore,
we delve deeper into this problem by exploring whether introducing noise into
the training set, akin to a form of data augmentation, enhances such robustness
during inference, and find that such noise can indeed improve the robustness of
ICL. Overall, our fruitful analysis and findings provide a comprehensive
understanding of the resilience of Transformer models against label noises
during ICL and provide valuable insights into the research on Transformers in
natural language processing. Our code is available at
https://github.com/InezYu0928/in-context-learning.","[{'Real-Time Mask Detection Based on SSD-MobileNetV2': 'After the outbreak of COVID-19, mask detection, as the most convenient and\neffective means of prevention, plays a crucial role in epidemic prevention and\ncontrol. An excellent automatic real-time mask detection system can reduce a\nlot of work pressure for relevant staff. However, by analyzing the existing\nmask detection approaches, we find that they are mostly resource-intensive and\ndo not achieve a good balance between speed and accuracy. And there is no\nperfect face mask dataset at present. In this paper, we propose a new\narchitecture for mask detection. Our system uses SSD as the mask locator and\nclassifier, and further replaces VGG-16 with MobileNetV2 to extract the\nfeatures of the image and reduce a lot of parameters. Therefore, our system can\nbe deployed on embedded devices. Transfer learning methods are used to transfer\npre-trained models from other domains to our model. Data enhancement methods in\nour system such as MixUp effectively prevent overfitting. It also effectively\nreduces the dependence on large-scale datasets. By doing experiments in\npractical scenarios, the results demonstrate that our system performed well in\nreal-time mask detection.'}, {'Many-body Localization in Clean Chains with Long-Range Interactions': 'The strong long-range interaction leads to localization in the closed quantum\nsystem without disorders. Employing the exact diagonalization method, the\nauthor numerically investigates thermalization and many-body localization in\ntranslational invariant quantum chains with finite Coulomb interactions. In the\ncomputational basis, excluding all trivial degeneracies, the\ninteraction-induced localization is well demonstrated in aspects of level\nstatistics, eigenstate expectation values, and the Anderson localization on\ngraphs constructed of the many-body basis. The nature of localization for\ngeneric eigenstates is attributed to the quasi-disorder from the power-law\ninteractions. However, due to the real-space symmetries, the long-time dynamics\nis dominated by the degenerate eigenstates and eventually reach homogeneity in\nreal space. On the other hand, the entanglement entropy exhibits the\nsize-dependence beyond the area law for the same reason, even deep in the\nlocalized state, indicating an incomplete localization in real space.'}, {'Tackling small eigen-gaps: Fine-grained eigenvector estimation and\n  inference under heteroscedastic noise': 'This paper aims to address two fundamental challenges arising in eigenvector\nestimation and inference for a low-rank matrix from noisy observations: (1) how\nto estimate an unknown eigenvector when the eigen-gap (i.e. the spacing between\nthe associated eigenvalue and the rest of the spectrum) is particularly small;\n(2) how to perform estimation and inference on linear functionals of an\neigenvector -- a sort of ""fine-grained"" statistical reasoning that goes far\nbeyond the usual $\\ell_2$ analysis. We investigate how to address these\nchallenges in a setting where the unknown $n\\times n$ matrix is symmetric and\nthe additive noise matrix contains independent (and non-symmetric) entries.\nBased on eigen-decomposition of the asymmetric data matrix, we propose\nestimation and uncertainty quantification procedures for an unknown\neigenvector, which further allow us to reason about linear functionals of an\nunknown eigenvector. The proposed procedures and the accompanying theory enjoy\nseveral important features: (1) distribution-free (i.e. prior knowledge about\nthe noise distributions is not needed); (2) adaptive to heteroscedastic noise;\n(3) minimax optimal under Gaussian noise. Along the way, we establish optimal\nprocedures to construct confidence intervals for the unknown eigenvalues. All\nthis is guaranteed even in the presence of a small eigen-gap (up to\n$O(\\sqrt{n/\\mathrm{poly}\\log (n)})$ times smaller than the requirement in prior\ntheory), which goes significantly beyond what generic matrix perturbation\ntheory has to offer.'}, {'Asymmetry Helps: Eigenvalue and Eigenvector Analyses of Asymmetrically\n  Perturbed Low-Rank Matrices': 'This paper is concerned with the interplay between statistical asymmetry and\nspectral methods. Suppose we are interested in estimating a rank-1 and\nsymmetric matrix $\\mathbf{M}^{\\star}\\in \\mathbb{R}^{n\\times n}$, yet only a\nrandomly perturbed version $\\mathbf{M}$ is observed. The noise matrix\n$\\mathbf{M}-\\mathbf{M}^{\\star}$ is composed of zero-mean independent (but not\nnecessarily homoscedastic) entries and is, therefore, not symmetric in general.\nThis might arise, for example, when we have two independent samples for each\nentry of $\\mathbf{M}^{\\star}$ and arrange them into an {\\em asymmetric} data\nmatrix $\\mathbf{M}$. The aim is to estimate the leading eigenvalue and\neigenvector of $\\mathbf{M}^{\\star}$. We demonstrate that the leading eigenvalue\nof the data matrix $\\mathbf{M}$ can be $O(\\sqrt{n})$ times more accurate --- up\nto some log factor --- than its (unadjusted) leading singular value in\neigenvalue estimation. Further, the perturbation of any linear form of the\nleading eigenvector of $\\mathbf{M}$ --- say, entrywise eigenvector perturbation\n--- is provably well-controlled. This eigen-decomposition approach is fully\nadaptive to heteroscedasticity of noise without the need of careful bias\ncorrection or any prior knowledge about the noise variance. We also provide\npartial theory for the more general rank-$r$ case. The takeaway message is\nthis: arranging the data samples in an asymmetric manner and performing\neigen-decomposition could sometimes be beneficial.'}, {'ODEs learn to walk: ODE-Net based data-driven modeling for crowd\n  dynamics': 'Predicting the behaviors of pedestrian crowds is of critical importance for a\nvariety of real-world problems. Data driven modeling, which aims to learn the\nmathematical models from observed data, is a promising tool to construct models\nthat can make accurate predictions of such systems. In this work, we present a\ndata-driven modeling approach based on the ODE-Net framework, for constructing\ncontinuous-time models of crowd dynamics. We discuss some challenging issues in\napplying the ODE-Net method to such problems, which are primarily associated\nwith the dimensionality of the underlying crowd system, and we propose to\naddress these issues by incorporating the social-force concept in the ODE-Net\nframework. Finally application examples are provided to demonstrate the\nperformance of the proposed method.'}, {'MCDIP-ADMM: Overcoming Overfitting in DIP-based CT reconstruction': 'This paper investigates the application of unsupervised learning methods for\ncomputed tomography (CT) reconstruction. To motivate our work, we review\nseveral existing priors, namely the truncated Gaussian prior, the $l_1$ prior,\nthe total variation prior, and the deep image prior (DIP). We find that DIP\noutperforms the other three priors in terms of representational capability and\nvisual performance. However, the performance of DIP deteriorates when the\nnumber of iterations exceeds a certain threshold due to overfitting. To address\nthis issue, we propose a novel method (MCDIP-ADMM) based on Multi-Code Deep\nImage Prior and plug-and-play Alternative Direction Method of Multipliers.\nSpecifically, MCDIP utilizes multiple latent codes to generate a series of\nfeature maps at an intermediate layer within a generator model. These maps are\nthen composed with trainable weights, representing the complete image prior.\nExperimental results demonstrate the superior performance of the proposed\nMCDIP-ADMM compared to three existing competitors. In the case of parallel beam\nprojection with Gaussian noise, MCDIP-ADMM achieves an average improvement of\n4.3 dB over DIP, 1.7 dB over ADMM DIP-WTV, and 1.2 dB over PnP-DIP in terms of\nPSNR. Similarly, for fan-beam projection with Poisson noise, MCDIP-ADMM\nachieves an average improvement of 3.09 dB over DIP, 1.86 dB over ADMM DIP-WTV,\nand 0.84 dB over PnP-DIP in terms of PSNR.'}, {'Collaboratively Learning Linear Models with Structured Missing Data': 'We study the problem of collaboratively learning least squares estimates for\n$m$ agents. Each agent observes a different subset of the\nfeatures$\\unicode{x2013}$e.g., containing data collected from sensors of\nvarying resolution. Our goal is to determine how to coordinate the agents in\norder to produce the best estimator for each agent. We propose a distributed,\nsemi-supervised algorithm Collab, consisting of three steps: local training,\naggregation, and distribution. Our procedure does not require communicating the\nlabeled data, making it communication efficient and useful in settings where\nthe labeled data is inaccessible. Despite this handicap, our procedure is\nnearly asymptotically local minimax optimal$\\unicode{x2013}$even among\nestimators allowed to communicate the labeled data such as imputation methods.\nWe test our method on real and synthetic data.'}, {'Dimension free ridge regression': ""Random matrix theory has become a widely useful tool in high-dimensional\nstatistics and theoretical machine learning. However, random matrix theory is\nlargely focused on the proportional asymptotics in which the number of columns\ngrows proportionally to the number of rows of the data matrix. This is not\nalways the most natural setting in statistics where columns correspond to\ncovariates and rows to samples. With the objective to move beyond the\nproportional asymptotics, we revisit ridge regression ($\\ell_2$-penalized least\nsquares) on i.i.d. data $(x_i, y_i)$, $i\\le n$, where $x_i$ is a feature vector\nand $y_i = \\beta^\\top x_i +\\epsilon_i \\in\\mathbb{R}$ is a response. We allow\nthe feature vector to be high-dimensional, or even infinite-dimensional, in\nwhich case it belongs to a separable Hilbert space, and assume either $z_i :=\n\\Sigma^{-1/2}x_i$ to have i.i.d. entries, or to satisfy a certain convex\nconcentration property. Within this setting, we establish non-asymptotic bounds\nthat approximate the bias and variance of ridge regression in terms of the bias\nand variance of an `equivalent' sequence model (a regression model with\ndiagonal design matrix). The approximation is up to multiplicative factors\nbounded by $(1\\pm \\Delta)$ for some explicitly small $\\Delta$. Previously, such\nan approximation result was known only in the proportional regime and only up\nto additive errors: in particular, it did not allow to characterize the\nbehavior of the excess risk when this converges to $0$. Our general theory\nrecovers earlier results in the proportional regime (with better error rates).\nAs a new application, we obtain a completely explicit and sharp\ncharacterization of ridge regression for Hilbert covariates with regularly\nvarying spectrum. Finally, we analyze the overparametrized near-interpolation\nsetting and obtain sharp `benign overfitting' guarantees.""}, {'Many-body delocalization with random vector potentials': 'We study the ergodic properties of excited states in a model of interacting\nfermions in quasi-one-dimensional chains subjected to a random vector\npotential. In the noninteracting limit, we show that arbitrarily small values\nof this complex off-diagonal disorder trigger localization for the whole\nspectrum; the divergence of the localization length in the single-particle\nbasis is characterized by a critical exponent $\\nu$ which depends on the energy\ndensity being investigated. When short-range interactions are included, the\nlocalization is lost, and the system is ergodic regardless of the magnitude of\ndisorder in finite chains. Our numerical results suggest a delocalization\nscheme for arbitrary small values of interactions. This finding indicates that\nthe standard scenario of the many-body localization cannot be obtained in a\nmodel with random gauge fields.'}, {'Exotic phase separation in one-dimensional hard-core boson system with\n  two- and three-body interactions': 'We investigate the ground state phase diagram of hard-core boson system with\nrepulsive two-body and attractive three-body interactions in one-dimensional\noptic lattice. When these two interactions are comparable and increasing the\nhopping rate, physically intuitive analysis indicates that there exists an\nexotic phase separation regime between the solid phase with charge density wave\norder and superfluid phase. We identify these phases and phase transitions by\nnumerically analyzing the density distribution, structure factor of\ndensity-density correlation function, three-body correlation function and von\nNeumann entropy estimator obtained by density matrix renormalization group\nmethod. These exotic phases and phase transitions are expected to be observed\nin the ultra-cold polar molecule experiments by properly tuning interaction\nparameters, which is constructive to understand the physics of ubiquitous\ninsulating-superconducting phase transitions in condensed matter systems.'}]","Title: Investigating In-Context Learning under Noisy Labels: Understanding Transformers' Adaptability

_**Abstract**_

Driven by the immense capabilities of Large Language Models (LLMs), this research endeavors to dissect the ability for Transformers to learn under conditions of noisy labels through In-Context Learning (ICL). Profiting from the extensive discourse in modern machine learning and the current prevalence of noisy, real-world data, understanding the repercussions of label imperfections on transformer performance has become paramount.

The primary goal is to characterize Transformers' robustness against noisy labels under ICL, and to explore the inherent limitations and the solutions that illuminate their proficiency in learning and adaptation. Innovatively, the investigation diverges from traditional approaches by employing simple function classes, measuring the models' efficacy in dealing with a range of label noise types, magnitudes, function complexities, and input dimensions. This multi-faceted exploration reveals the marked susceptibility of Transformers to label noise across varying training and test noise levels.

An essential contribution of this study is elucidating how the model's resilience fluctuates with the concern on model size, indicating a delicate balance between complexity and noise tolerance. Encouragingly, the outcomes suggest that introducing bias through noisy labels can fortify the Transformer's learning capability, proposing a nuanced view on the role of error in model development.

The findings generate a comprehensible framework for enhancing the interpretability, reliability, and safety of Transformers in practical applications, underscoring the necessity of considering imperfections in real-world data for effective and responsible deployment of LLMs. These insights can serve as a catalyst for developing more adaptive and resilient AI systems in the future, capable of learning effectively from imperfect information."
"Recent advancements in large language models (LLMs) have significantly
boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI
systems designed to simulate assigned personas. By harnessing multiple advanced
abilities of LLMs, including in-context learning, instruction following, and
social intelligence, RPLAs achieve a remarkable sense of human likeness and
vivid role-playing performance. RPLAs can mimic a wide range of personas,
ranging from historical figures and fictional characters to real-life
individuals. Consequently, they have catalyzed numerous AI applications, such
as emotional companions, interactive video games, personalized assistants and
copilots, and digital clones. In this paper, we conduct a comprehensive survey
of this field, illustrating the evolution and recent progress in RPLAs
integrating with cutting-edge LLM technologies. We categorize personas into
three types: 1) Demographic Persona, which leverages statistical stereotypes;
2) Character Persona, focused on well-established figures; and 3)
Individualized Persona, customized through ongoing user interactions for
personalized services. We begin by presenting a comprehensive overview of
current methodologies for RPLAs, followed by the details for each persona type,
covering corresponding data sourcing, agent construction, and evaluation.
Afterward, we discuss the fundamental risks, existing limitations, and future
prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI
applications, which reflects practical user demands that shape and drive RPLA
research. Through this work, we aim to establish a clear taxonomy of RPLA
research and applications, and facilitate future research in this critical and
ever-evolving field, and pave the way for a future where humans and RPLAs
coexist in harmony.","[{'Harnessing Knowledge and Reasoning for Human-Like Natural Language\n  Generation: A Brief Review': 'The rapid development and application of natural language generation (NLG)\ntechniques has revolutionized the field of automatic text production. However,\nthese techniques are still limited in their ability to produce human-like text\nthat is truly reasonable and informative. In this paper, we explore the\nimportance of NLG being guided by knowledge, in order to convey human-like\nreasoning through language generation. We propose ten goals for intelligent NLG\nsystems to pursue, and briefly review the achievement of NLG techniques guided\nby knowledge and reasoning. We also conclude by envisioning future directions\nand challenges in the pursuit of these goals.'}, {'Beneath Surface Similarity: Large Language Models Make Reasonable\n  Scientific Analogies after Structure Abduction': 'The vital role of analogical reasoning in human cognition allows us to grasp\nnovel concepts by linking them with familiar ones through shared relational\nstructures. Despite the attention previous research has given to word\nanalogies, this work suggests that Large Language Models (LLMs) often overlook\nthe structures that underpin these analogies, raising questions about the\nefficacy of word analogies as a measure of analogical reasoning skills akin to\nhuman cognition. In response to this, our paper introduces a task of analogical\nstructure abduction, grounded in cognitive psychology, designed to abduce\nstructures that form an analogy between two systems. In support of this task,\nwe establish a benchmark called SCAR, containing 400 scientific analogies from\n13 distinct fields, tailored for evaluating analogical reasoning with structure\nabduction. The empirical evidence underlines the continued challenges faced by\nLLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need\nfor future exploration to enhance their abilities.'}, {'Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large\n  Language Models in Knowledge Conflicts': ""By providing external information to large language models (LLMs), tool\naugmentation (including retrieval augmentation) has emerged as a promising\nsolution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the\nevidence conflicts with their parametric memory? We present the first\ncomprehensive and controlled investigation into the behavior of LLMs when\nencountering knowledge conflicts. We propose a systematic framework to elicit\nhigh-quality parametric memory from LLMs and construct the corresponding\ncounter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs. On the one\nhand, different from prior wisdom, we find that LLMs can be highly receptive to\nexternal evidence even when that conflicts with their parametric memory, given\nthat the external evidence is coherent and convincing. On the other hand, LLMs\nalso demonstrate a strong confirmation bias when the external evidence contains\nsome information that is consistent with their parametric memory, despite being\npresented with conflicting evidence at the same time. These results pose\nimportant implications that are worth careful consideration for the further\ndevelopment and deployment of tool- and retrieval-augmented LLMs. Resources are\navailable at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.""}, {'Ensuring Readability and Data-fidelity using Head-modifier Templates in\n  Deep Type Description Generation': 'A type description is a succinct noun compound which helps human and machines\nto quickly grasp the informative and distinctive information of an entity.\nEntities in most knowledge graphs (KGs) still lack such descriptions, thus\ncalling for automatic methods to supplement such information. However, existing\ngenerative methods either overlook the grammatical structure or make factual\nmistakes in generated texts. To solve these problems, we propose a\nhead-modifier template-based method to ensure the readability and data fidelity\nof generated type descriptions. We also propose a new dataset and two automatic\nmetrics for this task. Experiments show that our method improves substantially\ncompared with baselines and achieves state-of-the-art performance on both\ndatasets.'}, {'Unsupervised Editing for Counterfactual Stories': 'Creating what-if stories requires reasoning about prior statements and\npossible outcomes of the changed conditions. One can easily generate coherent\nendings under new conditions, but it would be challenging for current systems\nto do it with minimal changes to the original story. Therefore, one major\nchallenge is the trade-off between generating a logical story and rewriting\nwith minimal-edits. In this paper, we propose EDUCAT, an editing-based\nunsupervised approach for counterfactual story rewriting. EDUCAT includes a\ntarget position detection strategy based on estimating causal effects of the\nwhat-if conditions, which keeps the causal invariant parts of the story. EDUCAT\nthen generates the stories under fluency, coherence and minimal-edits\nconstraints. We also propose a new metric to alleviate the shortcomings of\ncurrent automatic metrics and better evaluate the trade-off. We evaluate EDUCAT\non a public counterfactual story rewriting benchmark. Experiments show that\nEDUCAT achieves the best trade-off over unsupervised SOTA methods according to\nboth automatic and human evaluation. The resources of EDUCAT are available at:\nhttps://github.com/jiangjiechen/EDUCAT.'}, {'Converge to the Truth: Factual Error Correction via Iterative\n  Constrained Editing': 'Given a possibly false claim sentence, how can we automatically correct it\nwith minimal editing? Existing methods either require a large number of pairs\nof false and corrected claims for supervised training or do not handle well\nerrors spanning over multiple tokens within an utterance. In this paper, we\npropose VENCE, a novel method for factual error correction (FEC) with minimal\nedits. VENCE formulates the FEC problem as iterative sampling editing actions\nwith respect to a target density function. We carefully design the target\nfunction with predicted truthfulness scores from an offline trained fact\nverification model. VENCE samples the most probable editing positions based on\nback-calculated gradients of the truthfulness score concerning input tokens and\nthe editing actions using a distantly-supervised language model (T5).\nExperiments on a public dataset show that VENCE improves the well-adopted SARI\nmetric by 5.3 (or a relative improvement of 11.8%) over the previous best\ndistantly-supervised methods.'}, {'Say What You Mean! Large Language Models Speak Too Positively about\n  Negative Commonsense Knowledge': 'Large language models (LLMs) have been widely studied for their ability to\nstore and utilize positive knowledge. However, negative knowledge, such as\n""lions don\'t live in the ocean"", is also ubiquitous in the world but rarely\nmentioned explicitly in the text. What do LLMs know about negative knowledge?\nThis work examines the ability of LLMs to negative commonsense knowledge. We\ndesign a constrained keywords-to-sentence generation task (CG) and a Boolean\nquestion-answering task (QA) to probe LLMs. Our experiments reveal that LLMs\nfrequently fail to generate valid sentences grounded in negative commonsense\nknowledge, yet they can correctly answer polar yes-or-no questions. We term\nthis phenomenon the belief conflict of LLMs. Our further analysis shows that\nstatistical shortcuts and negation reporting bias from language modeling\npre-training cause this conflict.'}, {""Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing\n  Idiomatic Translation with Language Models"": ""To translate well, machine translation (MT) systems and general-purposed\nlanguage models (LMs) need a deep understanding of both source and target\nlanguages and cultures. Therefore, idioms, with their non-compositional nature,\npose particular challenges for Transformer-based systems, as literal\ntranslations often miss the intended meaning. Traditional methods, which\nreplace idioms using existing knowledge bases (KBs), often lack scale and\ncontext awareness. Addressing these challenges, our approach prioritizes\ncontext awareness and scalability, allowing for offline storage of idioms in a\nmanageable KB size. This ensures efficient serving with smaller models and\nprovides a more comprehensive understanding of idiomatic expressions. We\nintroduce a multilingual idiom KB (IdiomKB) developed using large LMs to\naddress this. This KB facilitates better translation by smaller models, such as\nBLOOMZ (7.1B), Alpaca (7B), and InstructGPT (6.7B), by retrieving idioms'\nfigurative meanings. We present a novel, GPT-4-powered metric for human-aligned\nevaluation, demonstrating that IdiomKB considerably boosts model performance.\nHuman evaluations further validate our KB's quality.""}, {'Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and\n  Execution of LLM Agents in an Auction Arena': ""Recent advancements in Large Language Models (LLMs) showcase advanced\nreasoning, yet NLP evaluations often depend on static benchmarks. Evaluating\nthis necessitates environments that test strategic reasoning in dynamic,\ncompetitive scenarios requiring long-term planning. We introduce AucArena, a\nnovel evaluation suite that simulates auctions, a setting chosen for being\nhighly unpredictable and involving many skills related to resource and risk\nmanagement, while also being easy to evaluate. We conduct controlled\nexperiments using state-of-the-art LLMs to power bidding agents to benchmark\ntheir planning and execution skills. Our research demonstrates that LLMs, such\nas GPT-4, possess key skills for auction participation, such as budget\nmanagement and goal adherence, which improve with adaptive strategies. This\nhighlights LLMs' potential in modeling complex social interactions in\ncompetitive contexts. However, variability in LLM performance and occasional\noutperformance by simpler methods indicate opportunities for further\nadvancements in LLM design and the value of our simulation environment for\nongoing testing and refinement.""}, {'TimeArena: Shaping Efficient Multitasking Language Agents in a\n  Time-Aware Simulation': 'Despite remarkable advancements in emulating human-like behavior through\nLarge Language Models (LLMs), current textual simulations do not adequately\naddress the notion of time. To this end, we introduce TimeArena, a novel\ntextual simulated environment that incorporates complex temporal dynamics and\nconstraints that better reflect real-life planning scenarios. In TimeArena,\nagents are asked to complete multiple tasks as soon as possible, allowing for\nparallel processing to save time. We implement the dependency between actions,\nthe time duration for each action, and the occupancy of the agent and the\nobjects in the environment. TimeArena grounds to 30 real-world tasks in\ncooking, household activities, and laboratory work. We conduct extensive\nexperiments with various state-of-the-art LLMs using TimeArena. Our findings\nreveal that even the most powerful models, e.g., GPT-4, still lag behind humans\nin effective multitasking, underscoring the need for enhanced temporal\nawareness in the development of language agents.'}]","Title: Comprehensive Framework for Advancing Role-playing Language Models (RPLMs) in Personalized Environments

Abstract:

As AI applications expand, personalized Role-playing Language Models (RPLMs) - capable of customizing interactions to individual needs - become pivotal. However, their effectiveness hinges on complex factors that include personality trait integration, secure data handling, and efficient execution methods across various tasks. This research paper adopts a multidimensional approach to develop an advanced framework for enhancing personalized RPLMs thereby addressing the challenges of context-integration, ethical considerations, and real-world deployment.

Objective: To introduce a novel framework that progressively implements RPLMs to achieve seamless, personalized experiences by optimizing model personalization capabilities, ensuring secure information usage, and improving execution mechanisms. 

Innovations: The paper advances in four main areas: First, it suggests a systematic, multi-faceted approach for crafting and evaluating RPLMs across tasks like theory of mind, tool usage, and automation. Second, it focuses on memory mechanisms for sustaining continuity in interactions and for data handling integrity. Third, it applies external tools for action execution, bridging conceptual capabilities with real-world tasks. Lastly, it establishes qualitative and quantitative measures for character fidelity and personal assistant performance.

Methods: The framework encompasses methodologies献shaping RPLMs' theoretical understanding, creating character profiles, utilizing historical data, and integrating interactive tools. Performance is gauged through simulated end-user engagements, evaluating response quality, adherence to directives, and ethical dimensions always falling within legal boundaries. 

Results: Studies yield insights into how well-tailed RPLMs can cater to personalized needs, innovatively respond to tasks, and maintain data privacy, with measurable improvements in effectiveness and reliability. 

Contributions: This research makes several notable contributions. It develops a robust blueprint for retrofitting existing RPLMs to meet heightened personalization standards, bolsters security with privacy-focused solutions, and advances the application domains where RPLMs can realize exceptional command over dynamic interactions and complex task automation.

Applications: The framework offers significant potential in enhancing user-centric interfaces in diverse sectors including healthcare, accessibility, customer service, and entertainment, thereby enriching user experience through more intuitive and secure AI-driven interactions."
"Large Language Models (LLMs) have highlighted the necessity of effective
unlearning mechanisms to comply with data regulations and ethical AI practices.
LLM unlearning aims at removing undesired data influences and associated model
capabilities without compromising utility out of the scope of unlearning. While
interest in studying LLM unlearning is growing,the impact of the optimizer
choice for LLM unlearning remains under-explored. In this work, we shed light
on the significance of optimizer selection in LLM unlearning for the first
time, establishing a clear connection between {second-order optimization} and
influence unlearning (a classical approach using influence functions to update
the model for data influence removal). This insight propels us to develop a
second-order unlearning framework, termed SOUL, built upon the second-order
clipped stochastic optimization (Sophia)-based LLM training method. SOUL
extends the static, one-shot model update using influence unlearning to a
dynamic, iterative unlearning process. Our extensive experiments show that SOUL
consistently outperforms conventional first-order methods across various
unlearning tasks, models, and metrics, suggesting the promise of second-order
optimization in providing a scalable and easily implementable solution for LLM
unlearning.","[{'On the Robustness of deep learning-based MRI Reconstruction to image\n  transformations': 'Although deep learning (DL) has received much attention in accelerated\nmagnetic resonance imaging (MRI), recent studies show that tiny input\nperturbations may lead to instabilities of DL-based MRI reconstruction models.\nHowever, the approaches of robustifying these models are underdeveloped.\nCompared to image classification, it could be much more challenging to achieve\na robust MRI image reconstruction network considering its regression-based\nlearning objective, limited amount of training data, and lack of efficient\nrobustness metrics. To circumvent the above limitations, our work revisits the\nproblem of DL-based image reconstruction through the lens of robust machine\nlearning. We find a new instability source of MRI image reconstruction, i.e.,\nthe lack of reconstruction robustness against spatial transformations of an\ninput, e.g., rotation and cutout. Inspired by this new robustness metric, we\ndevelop a robustness-aware image reconstruction method that can defend against\nboth pixel-wise adversarial perturbations as well as spatial transformations.\nExtensive experiments are also conducted to demonstrate the effectiveness of\nour proposed approaches.'}, {'Text-Visual Prompting for Efficient 2D Temporal Video Grounding': ""In this paper, we study the problem of temporal video grounding (TVG), which\naims to predict the starting/ending time points of moments described by a text\nsentence within a long untrimmed video. Benefiting from fine-grained 3D visual\nfeatures, the TVG techniques have achieved remarkable progress in recent years.\nHowever, the high complexity of 3D convolutional neural networks (CNNs) makes\nextracting dense 3D visual features time-consuming, which calls for intensive\nmemory and computing resources. Towards efficient TVG, we propose a novel\ntext-visual prompting (TVP) framework, which incorporates optimized\nperturbation patterns (that we call 'prompts') into both visual inputs and\ntextual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP\nallows us to effectively co-train vision encoder and language encoder in a 2D\nTVG model and improves the performance of crossmodal feature fusion using only\nlow-complexity sparse 2D visual features. Further, we propose a\nTemporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments\non two benchmark datasets, Charades-STA and ActivityNet Captions datasets,\nempirically show that the proposed TVP significantly boosts the performance of\n2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on\nActivityNet Captions) and achieves 5x inference acceleration over TVG using 3D\nvisual features. Codes are available at Open.Intel.""}, {'On Instabilities of Conventional Multi-Coil MRI Reconstruction to Small\n  Adverserial Perturbations': 'Although deep learning (DL) has received much attention in accelerated MRI,\nrecent studies suggest small perturbations may lead to instabilities in\nDL-based reconstructions, leading to concern for their clinical application.\nHowever, these works focus on single-coil acquisitions, which is not practical.\nWe investigate instabilities caused by small adversarial attacks for multi-coil\nacquisitions. Our results suggest that, parallel imaging and multi-coil CS\nexhibit considerable instabilities against small adversarial perturbations.'}, {'Robustness-preserving Lifelong Learning via Dataset Condensation': ""Lifelong learning (LL) aims to improve a predictive model as the data source\nevolves continuously. Most work in this learning paradigm has focused on\nresolving the problem of 'catastrophic forgetting,' which refers to a notorious\ndilemma between improving model accuracy over new data and retaining accuracy\nover previous data. Yet, it is also known that machine learning (ML) models can\nbe vulnerable in the sense that tiny, adversarial input perturbations can\ndeceive the models into producing erroneous predictions. This motivates the\nresearch objective of this paper - specification of a new LL framework that can\nsalvage model robustness (against adversarial attacks) from catastrophic\nforgetting. Specifically, we propose a new memory-replay LL strategy that\nleverages modern bi-level optimization techniques to determine the 'coreset' of\nthe current data (i.e., a small amount of data to be memorized) for ease of\npreserving adversarial robustness over time. We term the resulting LL framework\n'Data-Efficient Robustness-Preserving LL' (DERPLL). The effectiveness of DERPLL\nis evaluated for class-incremental image classification using ResNet-18 over\nthe CIFAR-10 dataset. Experimental results show that DERPLL outperforms the\nconventional coreset-guided LL baseline and achieves a substantial improvement\nin both standard accuracy and robust accuracy.""}, {'TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven\n  Optimization': 'Robustness evaluation against adversarial examples has become increasingly\nimportant to unveil the trustworthiness of the prevailing deep models in\nnatural language processing (NLP). However, in contrast to the computer vision\ndomain where the first-order projected gradient descent (PGD) is used as the\nbenchmark approach to generate adversarial examples for robustness evaluation,\nthere lacks a principled first-order gradient-based robustness evaluation\nframework in NLP. The emerging optimization challenges lie in 1) the discrete\nnature of textual inputs together with the strong coupling between the\nperturbation location and the actual content, and 2) the additional constraint\nthat the perturbed text should be fluent and achieve a low perplexity under a\nlanguage model. These challenges make the development of PGD-like NLP attacks\ndifficult. To bridge the gap, we propose TextGrad, a new attack generator using\ngradient-driven optimization, supporting high-accuracy and high-quality\nassessment of adversarial robustness in NLP. Specifically, we address the\naforementioned challenges in a unified optimization framework. And we develop\nan effective convex relaxation method to co-optimize the continuously-relaxed\nsite selection and perturbation variables and leverage an effective sampling\nmethod to establish an accurate mapping from the continuous optimization\nvariables to the discrete textual perturbations. Moreover, as a first-order\nattack generation method, TextGrad can be baked into adversarial training to\nfurther improve the robustness of NLP models. Extensive experiments are\nprovided to demonstrate the effectiveness of TextGrad not only in attack\ngeneration for robustness evaluation but also in adversarial defense.'}, {'SMUG: Towards robust MRI reconstruction by smoothed unrolling': ""Although deep learning (DL) has gained much popularity for accelerated\nmagnetic resonance imaging (MRI), recent studies have shown that DL-based MRI\nreconstruction models could be oversensitive to tiny input perturbations (that\nare called 'adversarial perturbations'), which cause unstable, low-quality\nreconstructed images. This raises the question of how to design robust DL\nmethods for MRI reconstruction. To address this problem, we propose a novel\nimage reconstruction framework, termed SMOOTHED UNROLLING (SMUG), which\nadvances a deep unrolling-based MRI reconstruction model using a randomized\nsmoothing (RS)-based robust learning operation. RS, which improves the\ntolerance of a model against input noises, has been widely used in the design\nof adversarial defense for image classification. Yet, we find that the\nconventional design that applies RS to the entire DL process is ineffective for\nMRI reconstruction. We show that SMUG addresses the above issue by customizing\nthe RS operation based on the unrolling architecture of the DL-based MRI\nreconstruction model. Compared to the vanilla RS approach and several variants\nof SMUG, we show that SMUG improves the robustness of MRI reconstruction with\nrespect to a diverse set of perturbation sources, including perturbations to\nthe input measurements, different measurement sampling rates, and different\nunrolling steps. Code for SMUG will be available at\nhttps://github.com/LGM70/SMUG.""}, {'How to Robustify Black-Box ML Models? A Zeroth-Order Optimization\n  Perspective': 'The lack of adversarial robustness has been recognized as an important issue\nfor state-of-the-art machine learning (ML) models, e.g., deep neural networks\n(DNNs). Thereby, robustifying ML models against adversarial attacks is now a\nmajor focus of research. However, nearly all existing defense methods,\nparticularly for robust training, made the white-box assumption that the\ndefender has the access to the details of an ML model (or its surrogate\nalternatives if available), e.g., its architectures and parameters. Beyond\nexisting works, in this paper we aim to address the problem of black-box\ndefense: How to robustify a black-box model using just input queries and output\nfeedback? Such a problem arises in practical scenarios, where the owner of the\npredictive model is reluctant to share model information in order to preserve\nprivacy. To this end, we propose a general notion of defensive operation that\ncan be applied to black-box models, and design it through the lens of denoised\nsmoothing (DS), a first-order (FO) certified defense technique. To allow the\ndesign of merely using model queries, we further integrate DS with the\nzeroth-order (gradient-free) optimization. However, a direct implementation of\nzeroth-order (ZO) optimization suffers a high variance of gradient estimates,\nand thus leads to ineffective defense. To tackle this problem, we next propose\nto prepend an autoencoder (AE) to a given (black-box) model so that DS can be\ntrained using variance-reduced ZO optimization. We term the eventual defense as\nZO-AE-DS. In practice, we empirically show that ZO-AE- DS can achieve improved\naccuracy, certified robustness, and query complexity over existing baselines.\nAnd the effectiveness of our approach is justified under both image\nclassification and image reconstruction tasks. Codes are available at\nhttps://github.com/damon-demon/Black-Box-Defense.'}, {'CLAWSAT: Towards Both Robust and Accurate Code Models': ""We integrate contrastive learning (CL) with adversarial learning to\nco-optimize the robustness and accuracy of code models. Different from existing\nworks, we show that code obfuscation, a standard code transformation operation,\nprovides novel means to generate complementary `views' of a code that enable us\nto achieve both robust and accurate code models. To the best of our knowledge,\nthis is the first systematic study to explore and exploit the robustness and\naccuracy benefits of (multi-view) code obfuscations in code models.\nSpecifically, we first adopt adversarial codes as robustness-promoting views in\nCL at the self-supervised pre-training phase. This yields improved robustness\nand transferability for downstream tasks. Next, at the supervised fine-tuning\nstage, we show that adversarial training with a proper temporally-staggered\nschedule of adversarial code generation can further improve robustness and\naccuracy of the pre-trained code model. Built on the above two modules, we\ndevelop CLAWSAT, a novel self-supervised learning (SSL) framework for code by\nintegrating $\\underline{\\textrm{CL}}$ with $\\underline{\\textrm{a}}$dversarial\nvie$\\underline{\\textrm{w}}$s (CLAW) with $\\underline{\\textrm{s}}$taggered\n$\\underline{\\textrm{a}}$dversarial $\\underline{\\textrm{t}}$raining (SAT). On\nevaluating three downstream tasks across Python and Java, we show that CLAWSAT\nconsistently yields the best robustness and accuracy ($\\textit{e.g.}$ 11$\\%$ in\nrobustness and 6$\\%$ in accuracy on the code summarization task in Python). We\nadditionally demonstrate the effectiveness of adversarial learning in CLAW by\nanalyzing the characteristics of the loss landscape and interpretability of the\npre-trained models.""}, {'Model Sparsity Can Simplify Machine Unlearning': 'In response to recent data regulation requirements, machine unlearning (MU)\nhas emerged as a critical process to remove the influence of specific examples\nfrom a given model. Although exact unlearning can be achieved through complete\nmodel retraining using the remaining dataset, the associated computational\ncosts have driven the development of efficient, approximate unlearning\ntechniques. Moving beyond data-centric MU approaches, our study introduces a\nnovel model-based perspective: model sparsification via weight pruning, which\nis capable of reducing the gap between exact unlearning and approximate\nunlearning. We show in both theory and practice that model sparsity can boost\nthe multi-criteria unlearning performance of an approximate unlearner, closing\nthe approximation gap, while continuing to be efficient. This leads to a new MU\nparadigm, termed prune first, then unlearn, which infuses a sparse model prior\ninto the unlearning process. Building on this insight, we also develop a\nsparsity-aware unlearning method that utilizes sparsity regularization to\nenhance the training process of approximate unlearning. Extensive experiments\nshow that our proposals consistently benefit MU in various unlearning\nscenarios. A notable highlight is the 77% unlearning efficacy gain of\nfine-tuning (one of the simplest unlearning methods) when using sparsity-aware\nunlearning. Furthermore, we demonstrate the practical impact of our proposed MU\nmethods in addressing other machine learning challenges, such as defending\nagainst backdoor attacks and enhancing transfer learning. Codes are available\nat https://github.com/OPTML-Group/Unlearn-Sparse.'}, {'Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced\n  Transfer Learning': 'Massive data is often considered essential for deep learning applications,\nbut it also incurs significant computational and infrastructural costs.\nTherefore, dataset pruning (DP) has emerged as an effective way to improve data\nefficiency by identifying and removing redundant training samples without\nsacrificing performance. In this work, we aim to address the problem of DP for\ntransfer learning, i.e., how to prune a source dataset for improved pretraining\nefficiency and lossless finetuning accuracy on downstream target tasks. To our\nbest knowledge, the problem of DP for transfer learning remains open, as\nprevious studies have primarily addressed DP and transfer learning as separate\nproblems. By contrast, we establish a unified viewpoint to integrate DP with\ntransfer learning and find that existing DP methods are not suitable for the\ntransfer learning paradigm. We then propose two new DP methods, label mapping\nand feature mapping, for supervised and self-supervised pretraining settings\nrespectively, by revisiting the DP problem through the lens of source-target\ndomain mapping. Furthermore, we demonstrate the effectiveness of our approach\non numerous transfer learning tasks. We show that source data classes can be\npruned by up to 40% ~ 80% without sacrificing downstream performance, resulting\nin a significant 2 ~ 5 times speed-up during the pretraining stage. Besides,\nour proposal exhibits broad applicability and can improve other computationally\nintensive transfer learning techniques, such as adversarial pretraining. Codes\nare available at https://github.com/OPTML-Group/DP4TL.'}]","Abstract:

Context: Language models (LMs) have revolutionized the field of Natural Language Processing with applications ranging from text generation to human-like interactions. However, these models pose risks related to privacy, bias perpetuation, and misuse in various critical sectors. Unlearning LMs, aimed at removing unwanted data influences, presents a significant challenge.

Objective: This study addresses the unlearning challenge in LMs, focusing on a novel framework called Second-Order Unlearning (SOUL). The goal is to optimize the balance between removing undesired information (efficacy) and retaining the model's utility (preservation) for unrelated tasks.

Innovations: Two key innovations distinguish this study. Firstly, SOUL introduces a SO optimization strategy for unlearning LMs, enhancing efficacy and utility preservation. The framework contains algorithmic sub-frameworks for changeable unlearning problems, including Generative Algorithm (GA), Gradient Difference (GradDiff), Prediction Only (PO), and Second-Order methods for both GA and GradDiff.

Methods: The research applies SO optimization parameters to four sub-frameworks (GA, GradDiff, PO, and their SO variants) across three models (Original, Fine-tuned Input-based, and Fine-tuned LLaMA2-7B-chat) on tasks requiring unlearning, such as fictitious, model detoxification, and copyright removal. Comparative analysis evaluates the methodologies under three criteria: unlearning efficacy, utility, and model performance metrics.

Results: Quantitative and qualitative analyses validate the efficacy of SO optimization in improving unlearning outcomes, notably reducing perplexity and enhancing zero-shot accuracy. SO optimization outperforms first-order optimization across unlearning tasks, confirming the enhanced balance between unlearned efficacy and utility preservation.

Contributions: This research advances the field of LLM unlearning by benchmarking SO optimization's impact and implications. The proposed SO optimization method optimizes unlearning efficacy while preserving utility, making it a critical advancement. 

Applications: The improved unlearning techniques benefit various domains requiring confidentiality and ethical considerations in text generation and knowledge processing. By enabling more refined control over model outputs, organizations across industries can better manage and repurpose AI assets responsibly."
"Text-rich graphs, which exhibit rich textual information on nodes and edges,
are prevalent across a wide range of real-world business applications. Large
Language Models (LLMs) have demonstrated remarkable abilities in understanding
text, which also introduced the potential for more expressive modeling in
text-rich graphs. Despite these capabilities, efficiently applying LLMs to
representation learning on graphs presents significant challenges. Recently,
parameter-efficient fine-tuning methods for LLMs have enabled efficient new
task generalization with minimal time and memory consumption. Inspired by this,
we introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel
approach for efficient graph representation learning with LLMs on text-rich
graphs. Specifically, we utilize a graph neural network (GNN) to encode
structural information from neighboring nodes into a graph prompt. This prompt
is then inserted at the beginning of the text sequence. To improve the quality
of graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting
the next token in the node text. Compared with existing joint GNN and LMs, our
method directly generate the node embeddings from large language models with an
affordable fine-tuning cost. We validate our approach through comprehensive
experiments conducted on 8 different text-rich graphs, observing an average
improvement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction
evaluations. Our results demonstrate the efficacy and efficiency of our model,
showing that it can be smoothly integrated with various large language models,
including OPT, LLaMA and Falcon.","[{'Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps': 'Texts appearing in daily scenes that can be recognized by OCR (Optical\nCharacter Recognition) tools contain significant information, such as street\nname, product brand and prices. Two tasks -- text-based visual question\nanswering and text-based image captioning, with a text extension from existing\nvision-language applications, are catching on rapidly. To address these\nproblems, many sophisticated multi-modality encoding frameworks (such as\nheterogeneous graph structure) are being used. In this paper, we argue that a\nsimple attention mechanism can do the same or even better job without any bells\nand whistles. Under this mechanism, we simply split OCR token features into\nseparate visual- and linguistic-attention branches, and send them to a popular\nTransformer decoder to generate answers or captions. Surprisingly, we find this\nsimple baseline model is rather strong -- it consistently outperforms\nstate-of-the-art (SOTA) models on two popular benchmarks, TextVQA and all three\ntasks of ST-VQA, although these SOTA models use far more complex encoding\nmechanisms. Transferring it to text-based image captioning, we also surpass the\nTextCaps Challenge 2020 winner. We wish this work to set the new baseline for\nthis two OCR text related applications and to inspire new thinking of\nmulti-modality encoder design. Code is available at\nhttps://github.com/ZephyrZhuQi/ssbaseline'}, {""Chop Chop BERT: Visual Question Answering by Chopping VisualBERT's Heads"": 'Vision-and-Language (VL) pre-training has shown great potential on many\nrelated downstream tasks, such as Visual Question Answering (VQA), one of the\nmost popular problems in the VL field. All of these pre-trained models (such as\nVisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which\nextends the classical attention mechanism to multiple layers and heads. To\ninvestigate why and how these models work on VQA so well, in this paper we\nexplore the roles of individual heads and layers in Transformer models when\nhandling $12$ different types of questions. Specifically, we manually remove\n(chop) heads (or layers) from a pre-trained VisualBERT model at a time, and\ntest it on different levels of questions to record its performance. As shown in\nthe interesting echelon shape of the result matrices, experiments reveal\ndifferent heads and layers are responsible for different question types, with\nhigher-level layers activated by higher-level visual reasoning questions. Based\non this observation, we design a dynamic chopping module that can automatically\nremove heads and layers of the VisualBERT at an instance level when dealing\nwith different questions. Our dynamic chopping module can effectively reduce\nthe parameters of the original model by 50%, while only damaging the accuracy\nby less than 1% on the VQA task.'}, {'Metal Object Detection Based on Load Impedance and Input Power\n  Characteristics for High-dimensional WPT System': ""High-dimensional wireless power transfer (WPT) systems have received\nincreasing attention for charging mobile devices. With the receivers have\nhigher spatial freedom, the systems are more susceptible to the metal objects\nin the surroundings. However, conventional methods for metal object detection\n(MOD) can't satisfy the requirements of safety and stability of new systems.\nThis paper proposed a metal detection method which is more suitable for\nhigh-dimensional WPT systems. The key feature of the proposed method is\ncombining load impedance and input power characteristics curves to determine\nwhether the receiver is a metal object or a coil. And the influence of the\nmetal is discussed in the circuit and magnetic model. The method is\ntheoretically proven by the simulation calculation. And the experiments\nverified that the method can accurately detect the metal objects by setting the\nthreshold curves.""}, {'Towards Fully Intelligent Transportation through Infrastructure-Vehicle\n  Cooperative Autonomous Driving: Challenges and Opportunities': 'The infrastructure-vehicle cooperative autonomous driving approach depends on\nthe cooperation between intelligent roads and intelligent vehicles. This\napproach is not only safer but also more economical compared to the traditional\non-vehicle-only autonomous driving approach. In this paper, we introduce our\nreal-world deployment experiences of cooperative autonomous driving, and delve\ninto the details of new challenges and opportunities. Specifically, based on\nour progress towards commercial deployment, we follow a three-stage development\nroadmap of the cooperative autonomous driving approach:infrastructure-augmented\nautonomous driving (IAAD), infrastructure-guided autonomous driving (IGAD), and\ninfrastructure-planned autonomous driving (IPAD).'}, {'DSRC & C-V2X Comparison for Connected and Automated Vehicles in\n  Different Traffic Scenarios': 'Researches have been devoted to making connected and automated vehicles\n(CAVs) faster in different traffic scenarios. By using C-V2X or DSRC\ncommunication protocol, CAVs can work more effectively. In this paper, we\ncompare these two communication protocols on CAVs in three different traffic\nscenarios including ramp merging, intersection, and platoon brake. It shows\nthere is a trade-off between communication range and interval when leveraging\nC-V2X or DSRC for CAVs. The result can help support further application designs\nfor CAV autonomously choosing communication protocols in different traffic\nscenarios.'}, {'Shift-Robust GNNs: Overcoming the Limitations of Localized Graph\n  Training Data': ""There has been a recent surge of interest in designing Graph Neural Networks\n(GNNs) for semi-supervised learning tasks. Unfortunately this work has assumed\nthat the nodes labeled for use in training were selected uniformly at random\n(i.e. are an IID sample). However in many real world scenarios gathering labels\nfor graph nodes is both expensive and inherently biased -- so this assumption\ncan not be met. GNNs can suffer poor generalization when this occurs, by\noverfitting to superfluous regularities present in the training data. In this\nwork we present a method, Shift-Robust GNN (SR-GNN), designed to account for\ndistributional differences between biased training data and the graph's true\ninference distribution. SR-GNN adapts GNN models for the presence of\ndistributional shifts between the nodes which have had labels provided for\ntraining and the rest of the dataset. We illustrate the effectiveness of SR-GNN\nin a variety of experiments with biased training datasets on common GNN\nbenchmark datasets for semi-supervised learning, where we see that SR-GNN\noutperforms other GNN baselines by accuracy, eliminating at least (~40%) of the\nnegative effects introduced by biased training data. On the largest dataset we\nconsider, ogb-arxiv, we observe an 2% absolute improvement over the baseline\nand reduce 30% of the negative effects.""}, {'Semi-supervised Semantics-guided Adversarial Training for Trajectory\n  Prediction': ""Predicting the trajectories of surrounding objects is a critical task for\nself-driving vehicles and many other autonomous systems. Recent works\ndemonstrate that adversarial attacks on trajectory prediction, where small\ncrafted perturbations are introduced to history trajectories, may significantly\nmislead the prediction of future trajectories and induce unsafe planning.\nHowever, few works have addressed enhancing the robustness of this important\nsafety-critical task.In this paper, we present a novel adversarial training\nmethod for trajectory prediction. Compared with typical adversarial training on\nimage tasks, our work is challenged by more random input with rich context and\na lack of class labels. To address these challenges, we propose a method based\non a semi-supervised adversarial autoencoder, which models disentangled\nsemantic features with domain knowledge and provides additional latent labels\nfor the adversarial training. Extensive experiments with different types of\nattacks demonstrate that our Semisupervised Semantics-guided Adversarial\nTraining (SSAT) method can effectively mitigate the impact of adversarial\nattacks by up to 73% and outperform other popular defense methods. In addition,\nexperiments show that our method can significantly improve the system's robust\ngeneralization to unseen patterns of attacks. We believe that such\nsemantics-guided architecture and advancement on robust generalization is an\nimportant step for developing robust prediction models and enabling safe\ndecision-making.""}, {'CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue\n  Dataset': 'To advance multi-domain (cross-domain) dialogue modeling as well as alleviate\nthe shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first\nlarge-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It\ncontains 6K dialogue sessions and 102K utterances for 5 domains, including\nhotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains\nrich annotation of dialogue states and dialogue acts at both user and system\nsides. About 60% of the dialogues have cross-domain user goals that favor\ninter-domain dependency and encourage natural transition across domains in\nconversation. We also provide a user simulator and several benchmark models for\npipelined task-oriented dialogue systems, which will facilitate researchers to\ncompare and evaluate their models on this corpus. The large size and rich\nannotation of CrossWOZ make it suitable to investigate a variety of tasks in\ncross-domain dialogue modeling, such as dialogue state tracking, policy\nlearning, user simulation, etc.'}, {'Continual Prompt Tuning for Dialog State Tracking': ""A desirable dialog system should be able to continually learn new skills\nwithout forgetting old ones, and thereby adapt to new domains or tasks in its\nlife cycle. However, continually training a model often leads to a well-known\ncatastrophic forgetting issue. In this paper, we present Continual Prompt\nTuning, a parameter-efficient framework that not only avoids forgetting but\nalso enables knowledge transfer between tasks. To avoid forgetting, we only\nlearn and store a few prompt tokens' embeddings for each task while freezing\nthe backbone pre-trained model. To achieve bi-directional knowledge transfer\namong tasks, we propose several techniques (continual prompt initialization,\nquery fusion, and memory replay) to transfer knowledge from preceding tasks and\na memory-guided technique to transfer knowledge from subsequent tasks.\nExtensive experiments demonstrate the effectiveness and efficiency of our\nproposed method on continual learning for dialog state tracking, compared with\nstate-of-the-art baselines.""}, {'MaskPlus: Improving Mask Generation for Instance Segmentation': 'Instance segmentation is a promising yet challenging topic in computer\nvision. Recent approaches such as Mask R-CNN typically divide this problem into\ntwo parts -- a detection component and a mask generation branch, and mostly\nfocus on the improvement of the detection part. In this paper, we present an\napproach that extends Mask R-CNN with five novel optimization techniques for\nimproving the mask generation branch and reducing the conflicts between the\nmask branch and the detection component in training. These five techniques are\nindependent to each other and can be flexibly utilized in building various\ninstance segmentation architectures for increasing the overall accuracy. We\ndemonstrate the effectiveness of our approach with tests on the COCO dataset.'}]","**Abstract**

At the core of modern data systems are information networks imbued with rich textual information, such as product reviews and academic publications. The text-rich nature of these networks nuances the representation learning process, interrogating the limits of Graph Neural Networks (GNNs) and conventional language models. To address this, we introduce Graph Prompt Encoding with Fine-Tuning (GPEFT), a novel solution that integrates LLMs and GNNs for effective representation learning. GPEFT employs a parameter-efficient fine-tuning methodology, requiring merely a fraction of additional parameters—just 2%—for specific graph applications.

Our objective is to optimize the use of LLMs in graph representation learning, building upon the cascading multitask learning paradigm. To achieve this, we innovatively adapt PEFT techniques to the text-rich graphs' context, enabling efficient integration of LLMs within GNNs for improved interpretability and performance. Working within deep learning frameworks, we utilize transformer-based LLMs, leveraging their superior representation capabilities for dense text- and structure-encoded node vectors.

We demonstrate the efficacy of GPEFT through applications on text-rich graphs from domains like e-commerce (co-purchase prediction) and academia (paper recommendation). Our results showcase a notable improvement in link prediction tasks compared to state-of-the-art models, achieving nearly 16% higher accuracy. GPEFT's architecture also permits rapid adaptation to various downstream tasks, substantially enhancing the scalability and efficiency of LLM utilization in text-rich graph representation learning.

The contribution of this research lies in providing a versatile, low-cost methodology for harnessing LLMs in complex text-rich graph scenarios, with broad implications for applications ranging from recommendation systems to detection tasks. This work streamlines the integration of LLMs with GNNs, opening new frontiers in text-rich network analysis through the efficient utilization of advanced language models. 

Our results and methodology offer a step-change advancement in handling the textual intricacies of text-rich graphs, bridging the gap between the interpretability, stability, and performance often sacrificed between conventional engineering methods and the ""black box"" nature of LLMs. This innovative framework is poised to redefine how we approach and utilize large-scale text-rich networks, with significant potential impact across various domains requiring nuanced and sophisticated graph-based data analysis."
"Large Language Models are transforming NLP for a variety of tasks. However,
how LLMs perform NLP tasks for low-resource languages (LRLs) is less explored.
In line with the goals of the AmericasNLP workshop, we focus on 12 LRLs from
Brazil, 2 LRLs from Africa and 2 high-resource languages (HRLs) (e.g., English
and Brazilian Portuguese). Our results indicate that the LLMs perform worse for
the part of speech (POS) labeling of LRLs in comparison to HRLs. We explain the
reasons behind this failure and provide an error analysis through examples
observed in our data set.","[{'NaijaRC: A Multi-choice Reading Comprehension Dataset for Nigerian\n  Languages': 'In this paper, we create NaijaRC: a new multi-choice Reading Comprehension\ndataset for three native Nigeria languages that is based on high-school reading\ncomprehension examination. We provide baseline results by performing\ncross-lingual transfer using existing English RACE and Belebele training\ndataset based on a pre-trained encoder-only model. Additionally, we provide\nresults by prompting large language models (LLMs) like GPT-4.'}, {'TOKEN is a MASK: Few-shot Named Entity Recognition with Pre-trained\n  Language Models': 'Transferring knowledge from one domain to another is of practical importance\nfor many tasks in natural language processing, especially when the amount of\navailable data in the target domain is limited. In this work, we propose a\nnovel few-shot approach to domain adaptation in the context of Named Entity\nRecognition (NER). We propose a two-step approach consisting of a variable base\nmodule and a template module that leverages the knowledge captured in\npre-trained language models with the help of simple descriptive patterns. Our\napproach is simple yet versatile and can be applied in few-shot and zero-shot\nsettings. Evaluating our lightweight approach across a number of different\ndatasets shows that it can boost the performance of state-of-the-art baselines\nby 2-5% F1-score.'}, {'ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus': ""We introduce \\`{I}r\\`{o}y\\`{i}nSpeech, a new corpus influenced by the desire\nto increase the amount of high quality, contemporary Yor\\`{u}b\\'{a} speech\ndata, which can be used for both Text-to-Speech (TTS) and Automatic Speech\nRecognition (ASR) tasks. We curated about 23000 text sentences from news and\ncreative writing domains with the open license CC-BY-4.0. To encourage a\nparticipatory approach to data creation, we provide 5000 curated sentences to\nthe Mozilla Common Voice platform to crowd-source the recording and validation\nof Yor\\`{u}b\\'{a} speech data. In total, we created about 42 hours of speech\ndata recorded by 80 volunteers in-house, and 6 hours of validated recordings on\nMozilla Common Voice platform. Our TTS evaluation suggests that a\nhigh-fidelity, general domain, single-speaker Yor\\`{u}b\\'{a} voice is possible\nwith as little as 5 hours of speech. Similarly, for ASR we obtained a baseline\nword error rate (WER) of 23.8.""}, {'Preventing Author Profiling through Zero-Shot Multilingual\n  Back-Translation': 'Documents as short as a single sentence may inadvertently reveal sensitive\ninformation about their authors, including e.g. their gender or ethnicity.\nStyle transfer is an effective way of transforming texts in order to remove any\ninformation that enables author profiling. However, for a number of current\nstate-of-the-art approaches the improved privacy is accompanied by an\nundesirable drop in the down-stream utility of the transformed data. In this\npaper, we propose a simple, zero-shot way to effectively lower the risk of\nauthor profiling through multilingual back-translation using off-the-shelf\ntranslation models. We compare our models with five representative text style\ntransfer models on three datasets across different domains. Results from both\nan automatic and a human evaluation show that our approach achieves the best\noverall performance while requiring no training data. We are able to lower the\nadversarial prediction of gender and race by up to $22\\%$ while retaining\n$95\\%$ of the original utility on downstream tasks.'}, {'NollySenti: Leveraging Transfer Learning and Machine Translation for\n  Nigerian Movie Sentiment Classification': 'Africa has over 2000 indigenous languages but they are under-represented in\nNLP research due to lack of datasets. In recent years, there have been progress\nin developing labeled corpora for African languages. However, they are often\navailable in a single domain and may not generalize to other domains. In this\npaper, we focus on the task of sentiment classification for cross domain\nadaptation. We create a new dataset, NollySenti - based on the Nollywood movie\nreviews for five languages widely spoken in Nigeria (English, Hausa, Igbo,\nNigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using\nclassical machine learning methods and pre-trained language models. Leveraging\ntransfer learning, we compare the performance of cross-domain adaptation from\nTwitter domain, and cross-lingual adaptation from English language. Our\nevaluation shows that transfer from English in the same target domain leads to\nmore than 5% improvement in accuracy compared to transfer from Twitter in the\nsame language. To further mitigate the domain difference, we leverage machine\ntranslation (MT) from English to other Nigerian languages, which leads to a\nfurther improvement of 7% over cross-lingual evaluation. While MT to\nlow-resource languages are often of low quality, through human evaluation, we\nshow that most of the translated sentences preserve the sentiment of the\noriginal English reviews.'}, {'How good are Large Language Models on African Languages?': 'Recent advancements in natural language processing have led to the\nproliferation of large language models (LLMs). These models have been shown to\nyield good performance, using in-context learning, even on tasks and languages\nthey are not trained on. However, their performance on African languages is\nlargely understudied relative to high-resource languages. We present an\nanalysis of four popular large language models (mT0, Aya, LLaMa 2, and GPT-4)\non six tasks (topic classification, sentiment classification, machine\ntranslation, summarization, question answering, and named entity recognition)\nacross 60 African languages, spanning different language families and\ngeographical regions. Our results suggest that all LLMs produce lower\nperformance for African languages, and there is a large gap in performance\ncompared to high-resource languages (such as English) for most tasks. We find\nthat GPT-4 has an average to good performance on classification tasks, yet its\nperformance on generative tasks such as machine translation and summarization\nis significantly lacking. Surprisingly, we find that mT0 had the best overall\nperformance for cross-lingual QA, better than the state-of-the-art supervised\nmodel (i.e. fine-tuned mT5) and GPT-4 on African languages. Similarly, we find\nthe recent Aya model to have comparable result to mT0 in almost all tasks\nexcept for topic classification where it outperform mT0. Overall, LLaMa 2\nshowed the worst performance, which we believe is due to its English and\ncode-centric~(around 98%) pre-training corpus. Our findings confirms that\nperformance on African languages continues to remain a hurdle for the current\nLLMs, underscoring the need for additional efforts to close this gap.'}, {'BibleTTS: a large, high-fidelity, multilingual, and uniquely African\n  speech corpus': 'BibleTTS is a large, high-quality, open speech dataset for ten languages\nspoken in Sub-Saharan Africa. The corpus contains up to 86 hours of aligned,\nstudio quality 48kHz single speaker recordings per language, enabling the\ndevelopment of high-quality text-to-speech models. The ten languages\nrepresented are: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu,\nLingala, Luganda, Luo, and Yoruba. This corpus is a derivative work of Bible\nrecordings made and released by the Open.Bible project from Biblica. We have\naligned, cleaned, and filtered the original recordings, and additionally\nhand-checked a subset of the alignments for each language. We present results\nfor text-to-speech models with Coqui TTS. The data is released under a\ncommercial-friendly CC-BY-SA license.'}, {'ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for\n  Angolan Language Model': 'In recent years, the development of pre-trained language models (PLMs) has\ngained momentum, showcasing their capacity to transcend linguistic barriers and\nfacilitate knowledge transfer across diverse languages. However, this progress\nhas predominantly bypassed the inclusion of very-low resource languages,\ncreating a notable void in the multilingual landscape. This paper addresses\nthis gap by introducing four tailored PLMs specifically finetuned for Angolan\nlanguages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In\nthis paper, we survey the role of informed embedding initialization and\nsynthetic data in enhancing the performance of MAFT models in downstream tasks.\nWe improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA\n(an effective embedding initialization) by 12.3 and 3.8 points respectively.'}, {'Privacy Guarantees for De-identifying Text Transformations': 'Machine Learning approaches to Natural Language Processing tasks benefit from\na comprehensive collection of real-life user data. At the same time, there is a\nclear need for protecting the privacy of the users whose data is collected and\nprocessed. For text collections, such as, e.g., transcripts of voice\ninteractions or patient records, replacing sensitive parts with benign\nalternatives can provide de-identification. However, how much privacy is\nactually guaranteed by such text transformations, and are the resulting texts\nstill useful for machine learning? In this paper, we derive formal privacy\nguarantees for general text transformation-based de-identification methods on\nthe basis of Differential Privacy. We also measure the effect that different\nways of masking private information in dialog transcripts have on a subsequent\nmachine learning task. To this end, we formulate different masking strategies\nand compare their privacy-utility trade-offs. In particular, we compare a\nsimple redact approach with more sophisticated word-by-word replacement using\ndeep learning models on multiple natural language understanding tasks like\nnamed entity recognition, intent detection, and dialog act classification. We\nfind that only word-by-word replacement is robust against performance drops in\nvarious tasks.'}, {'Estimating Community Feedback Effect on Topic Choice in Social Media\n  with Predictive Modeling': ""Social media users post content on various topics. A defining feature of\nsocial media is that other users can provide feedback -- called community\nfeedback -- to their content in the form of comments, replies, and retweets. We\nhypothesize that the amount of received feedback influences the choice of\ntopics on which a social media user posts. However, it is challenging to test\nthis hypothesis as user heterogeneity and external confounders complicate\nmeasuring the feedback effect. Here, we investigate this hypothesis with a\npredictive approach based on an interpretable model of an author's decision to\ncontinue the topic of their previous post. We explore the confounding factors,\nincluding author's topic preferences and unobserved external factors such as\nnews and social events, by optimizing the predictive accuracy. This approach\nenables us to identify which users are susceptible to community feedback.\nOverall, we find that 33\\% and 14\\% of active users in Reddit and Twitter,\nrespectively, are influenced by community feedback. The model suggests that\nthis feedback alters the probability of topic continuation up to 14\\%,\ndepending on the user and the amount of feedback.""}]","Abstract:

In the context of the global linguistic diversity challenge, this research paper evaluates the potential of Large Language Models (LLMs) in processing part-of-speech (POS) tagging for numerous low-resource languages, with a focus on the linguistic scene in South America. Despite the growing advancements in LLM research, more than 6,800 languages lag behind in terms of integrated language technologies, underscoring the necessity for novel methodologies enabling cross-lingual annotation capabilities.

The primary objective of this study is to assess how well leading LLMs, such as GPT-4 and Cross-lingual Transfer models like XLM-R, perform on POS tagging for 12 low-resource languages (LRLs) in Brazil and contrasting African languages compared to high-resource languages (HRLs) English and Brazilian Portuguese. The research highlights the scarcity of benchmark datasets and large monolingual data for LRLs, expanding our understanding on the efficacy of zero-shot and transfer learning.

Methodologically, we leverage universal dependencies (UD) alongside seven publicly available Bible corpora for each language, employing GPT-4 for prompting tasks and adaption techniques for improved linguistic context. Notably, our work addresses a significant gap in the literature by featuring the first evaluation of LLMs on Brazilian LRLs, demonstrating their respective limitations and potential for adaptation to low-resource environments.

Our key findings indicate that although zero-shot approaches yield suboptimal results for LRLs, enabling systems to annotate POS tags with a performance slightly over 30% supports the feasibility of cross-lingual annotation with LLMs. Moreover, the experimental improvements by +3 to +12 points through language adaptation shows promising trends for enhancing LLM capabilities with limited multilingual resources.

By exploring the adaptivity of LLMs in low-resource settings, this research contributes to the development of more inclusive and globally applicable AI technologies. The innovative use of Bible corpora and the comparison between LRL and HRL showcases the potential for adapting LLMs to less shovelable languages, laying the groundwork for future advancements in multilingual NLP. This study emphasizes the need for further development and support in linguistic data and methods for low-resource languages, enabling more equitable access to AI advancements across diverse linguistic communities."
"The growing demand for Large Language Models (LLMs) across diverse
applications has prompted a paradigm shift in the design of deep learning
serving systems. Deploying LLMs, especially in multi-tenant environments,
presents considerable challenges due to their high computational and memory
demands. We present BlockLLM, a serving system that exploits the potential of
sharing components among fine-tuned LLM models to offer an efficient and
flexible solution for LLM workloads. BlockLLM partitions the models into
finer-grained blocks to enable the reuse of model components and independent
provisioning to improve the computation efficiency. BlockLLM consists of an
offline block zoo, for storing the blocks, and an online system to serve the
requests through chains of blocks. It offers multi-fold flexibility: (1)
Adaptive assembly of block chains on-the-fly is achieved with the help of
equivalence evaluation among blocks in the zoo. (2) We enable per-block batch
size and configure best-effort KV cache coordination at individual block level.
(3) We adopt speculative execution and locality-aware block placement to
mitigate the communication costs from dynamic block resource allocation. Our
evaluation demonstrates that BlockLLM reduces memory and storage footprints and
improves computation efficiency, outperforming existing serving approach in
95\%ile latency and GPU utilization by 33.5\% and 20.1\%, respectively.","[{'On the generalized multiplicities of maximal minors and sub-maximal\n  pfaffians': 'Let $S=\\mathbb{C}[x_{ij}]$ be a polynomial ring of $m\\times n$ generic\nvariables (resp. a polynomial ring of $(2n+1) \\times (2n+1)$ skew-symmetric\nvariables) over $\\mathbb{C}$ and let $I$ (resp. Pf) be the determinantal ideal\nof maximal minors (resp. sub-maximal pfaffians) of $S$. Using the\nrepresentation theoretic techniques introduced in the work of Raicu et al, we\nstudy the asymptotic behavior of the length of the local cohomology module of\ndeterminantal and pfaffian thickenings for suitable choices of cohomological\ndegrees. This asymptotic behavior is also defined as a notion of multiplicty.\nWe show that the multiplicity in our setting coincides with the degrees of\nGrassmannian and Orthogonal Grassmannian.'}, {'A note on the multiplicities of the determinantal thickenings of maximal\n  minors': 'Let $S=\\mathbb{C}[x_{ij}]$ be a polynomial ring of $m\\times n$ variables over\n$\\mathbb{C}$ and let $I$ be the determinantal ideal of maximal minors of $S$.\nUsing the representation theoretic techniques introduced in arXiv:1305.1719,\narXiv:1309.0617 and arXiv:1611.00415, we prove the existence of the generalized\n$j$-multiplicities $\\epsilon^j(I)$ defined by Dao and Monta\\~{n}o in\narXiv:1705.05033. We will also give a closed formula of $\\epsilon^j(I)$, which\ngeneralized the results in arXiv:1308.0582 and arXiv:1912.02917 in the maximal\nminors case.'}, {'Singularities of generic linkage via Frobenius powers': 'Let $I$ be an equidimensional ideal of a ring polynomial $R$ over\n$\\mathbb{C}$ and let $J$ be its generic linkage. We prove that there is a\nuniform bound of the difference between the F-pure thresholds of $I_p$ and\n$J_p$ via the generalized Frobenius powers of ideals. This provides evidence\nthat the F-pure threshold of an equidimensional ideal $I$ is less than that of\nits generic linkage. As a corollary we recover a result on log canonical\nthresholds of generic linkage by Niu.'}, {'Three-dimensional exponential sums under constant perturbation': ""In this paper, by generalizing Bombieri and Iwaniec's double large sieve\ninequality, we obtain an estimation on a type of three-dimensional exponential\nsums with constant perturbation. As an application of our estimation, we obtain\nan asymptotic formula for a sum involving the Mangoldt function and the\nintegral part function. This type of sum was originally studied by\nBordelles-Dai-Heyman-Pan-Shparlinski.""}, {'Local cohomology and Segre products': 'We prove a K\\""{u}nneth formula for local cohomology of a Segr\\\'{e} product of\ngraded modules supported in a Segr\\\'{e} product of ideals. In order to apply\nour formula to the study of cohomological dimension, we also investigate\nasymptotic behaviors of Eulerian graded $\\scr{D}$-modules.'}, {'Socle degrees for local cohomology modules of thickenings of maximal\n  minors and sub-maximal Pfaffians': 'Let $S$ be the polynomial ring on the space of non-square generic matrices or\nthe space of odd-sized skew-symmetric matrices, and let $I$ be the\ndeterminantal ideal of maximal minors or $\\operatorname{Pf}$ the ideal of\nsub-maximal Pfaffians, respectively. Using desingularizations and\nrepresentation theory of the general linear group we expand upon work of\nRaicu--Weyman--Witt to determine the $S$-module structures of\n$\\operatorname{Ext}^j_S(S/I^t, S)$ and\n$\\operatorname{Ext}^j_S(S/\\operatorname{Pf}^t, S)$, from which we get the\ndegrees of generators of these $\\operatorname{Ext}$ modules. As a consequence,\nvia graded local duality we answer a question of Wenliang Zhang on the socle\ndegrees of local cohomology modules of the form $H^j_\\mathfrak{m}(S/I^t)$.'}, {'A New Result on Packing Unit Squares into a Large Square': 'In their 2009 note: \\emph{Packing equal squares into a large square}, Chung\nand Graham proved that the uncovered area of a large square of side length $x$\nis $O\\left(x^{(3+\\sqrt{2})/7}\\log x\\right)$ after maximum number of\nnon-overlapping unit squares are packed into it, which improved the earlier\nresults of Erd\\H{o}s-Graham, Roth-Vaughan, and Karabash-Soifer. Here we further\nimprove the result to $O(x^{5/8})$ that also helps to improve the bound for the\ndual problem: finding the minimum number of unit squares needed for covering\nthe large square, from $x^2+O\\left(x^{(3+\\sqrt{2})/7}\\log x\\right)$ to\n$x^2+O(x^{5/8})$.'}, {'Three-way noiseless signal splitting in a parametric amplifier with\n  quantum correlation': 'We demonstrate that a phase-insensitive parametric amplifier, coupled to a\nquantum correlated source, can be used as a quantum information tap for\nnoiseless three-way signal splitting. We find that the output signals are\namplified noiselessly in two of the three output ports while the other can more\nor less keep its original input size without adding noise. This scheme is able\nto cascade and scales up for efficient information distribution in an optical\nnetwork. Furthermore, we find this scheme satisfies the criteria for a\nnon-ideal quantum non-demolition (QND) measurement and thus can serve as a QND\nmeasurement device. With two readouts correlated to the input, we find this\nscheme also satisfies the criterion for sequential QND measurement.'}, {'Power Minimization for Wireless Backhaul Based Ultra-Dense Cache-enabled\n  C-RAN': 'This correspondence paper investigates joint design of small base station\n(SBS) clustering, multicast beamforming for access and backhaul links, as well\nas frequency allocation in backhaul transmission to minimize the total power\nconsumption for wireless backhaul based ultra-dense cache-enabled cloud radio\naccess network (C-RAN). To solve this nontrivial problem, we develop a\nlow-complexity algorithm, which is a combination of smoothed ${\\ell\n_0}{\\text{-norm}}$ approximation and convex-concave procedure. Simulation\nresults show that the proposed algorithm converges fast and greatly reduces the\nbackhaul traffic.'}, {'Accelerating Distributed MoE Training and Inference with Lina': 'Scaling model parameters improves model quality at the price of high\ncomputation overhead. Sparsely activated models, usually in the form of Mixture\nof Experts (MoE) architecture, have sub-linear scaling of computation cost with\nmodel size, thus providing opportunities to train and serve a larger model at\nlower cost than their dense counterparts. However, distributed MoE training and\ninference is inefficient, mainly due to the interleaved all-to-all\ncommunication during model computation. This paper makes two main\ncontributions. First, we systematically analyze all-to-all overhead in\ndistributed MoE and present the main causes for it to be the bottleneck in\ntraining and inference, respectively. Second, we design and build Lina to\naddress the all-to-all bottleneck head-on. Lina opportunistically prioritizes\nall-to-all over the concurrent allreduce whenever feasible using tensor\npartitioning, so all-to-all and training step time is improved. Lina further\nexploits the inherent pattern of expert selection to dynamically schedule\nresources during inference, so that the transfer size and bandwidth of\nall-to-all across devices are balanced amid the highly skewed expert popularity\nin practice. Experiments on an A100 GPU testbed show that Lina reduces the\ntraining step time by up to 1.73x and reduces the 95%ile inference time by an\naverage of 1.63x over the state-of-the-art systems.'}]","Abstract

We present BlockLLM, a method that partitions large language models (LLMs) into blocks to optimize the serving of diverse applications while minimizing redundant resource utilization. The main objective of this research is to enhance the efficiency of LLM serving in multi-application environments.

This paper innovates by proposing a pipeline-based approach, speculative execution for expedited inference, and a block-granularity provisioning technique for dynamic resource allocation. Leveraging block-level adaptive serving, BlockLLM reduces redundant parameter duplication and optimizes GPU utilization by allowing more frequent scaling of individual blocks.

Intricately developed, BlockLLM facilitates algorithmic scheduling for optimal resource allocation and includes an architectural design for嵊ting blocks. Through an empirical evaluation, we demonstrate that BlockLLM significantly lowers latency, boosts throughput, and cuts down communication costs in parallel processing. The research contributes to a versatile and efficient mechanism for serving complex applications, making state-of-the-art LLMs more accessible and efficient for real-world deployment.

We envision BlockLLM's potential to impact multiple domains, including but not limited to search engines, chatbots, and content generation, showcasing its promise to streamline large-scale LLM operations and applications. The method thus offers advanced solutions for optimizing large-scale AI infrastructure while enhancing scalability and resource management in LLM-based applications."
"In the burgeoning field of large language models (LLMs), the assessment of
fundamental knowledge remains a critical challenge, particularly for models
tailored to Chinese language and culture. This paper introduces FoundaBench, a
pioneering benchmark designed to rigorously evaluate the fundamental knowledge
capabilities of Chinese LLMs. FoundaBench encompasses a diverse array of 3354
multiple-choice questions across common sense and K-12 educational subjects,
meticulously curated to reflect the breadth and depth of everyday and academic
knowledge. We present an extensive evaluation of 12 state-of-the-art LLMs using
FoundaBench, employing both traditional assessment methods and our CircularEval
protocol to mitigate potential biases in model responses. Our results highlight
the superior performance of models pre-trained on Chinese corpora, and reveal a
significant disparity between models' reasoning and memory recall capabilities.
The insights gleaned from FoundaBench evaluations set a new standard for
understanding the fundamental knowledge of LLMs, providing a robust framework
for future advancements in the field.","[{'Observation of a ""Ridge"" correlation structure in high multiplicity\n  proton-proton collisions: A brief review': 'This paper briefly reviews the striking experimental observation of a\nridge-like dihadron correlation structure in high multiplicity proton-proton\ncollisions at the Large Hadron Collider (LHC). Recent progress of both\nexperimental and theoretical efforts on understanding the physical origin of\nthe novel effect is reviewed. Outlook on future direction of possible new\nstudies is discussed.'}, {'Collective flow from AA, pA to pp collisions - Toward a unified paradigm': 'I give an overview of the latest development in understanding collective\nphenomena in high-multiplicity hadronic final state from relativistic\nnucleus-nucleus, proton-nucleus and proton-proton collisions. Upon reviewing\nthe experimental data and confronting them with theoretical models, a unified\nparadigm in describing the observed collectivity across all hadronic collision\nsystems is emerging. Potential future paths toward addressing key open\nquestions, especially on collectivity in small systems (pp, pA), are discussed.'}, {'Partial Differential Chow Forms and a Type of Partial Differential Chow\n  varieties': 'We first present an intersection theory of partial differential varieties\nwith quasi-generic differential hypersurfaces. Then based on the generic\nintersection theory, we define the partial differential Chow form for an\nirreducible partial differential variety $V$ of Kolchin polynomial\n$\\omega_V(t)=(d+1){t+m\\choose m}-{t+m-s\\choose m}$. And we establish for the\npartial differential Chow form most of the basic properties of the ordinary\ndifferential Chow form. Furthermore, we prove the existence of a type of\npartial differential Chow varieties.'}, {'Chiral algebra from worldsheet': 'The chiral algebra of a 4D $N\\geq2$ superconformal field theory is a vertex\noperator algebra generated by the Schur subsector of the 4D theory and its\nrigid (yet rich) structure has been useful in constraining and classifying 4D\nN=2 SCFTs. We study how the chiral algebra arises from the worldsheet\nperspective. In the worldsheet CFT dual of 4D N=4 SYM at the free point, we\nextract the subsector that corresponds to the spacetime Schur operators at\ngeneric coupling, and show how they generate the chiral algebra. The result can\nbe easily generalized to 4D N=2 superconformal field theories that arise as\norbifolds of 4D N=4 SYM.'}, {'Non-Supersymmetric Attractors in Symmetric Coset Spaces': 'We present a method of constructing generic single-centered and\nmulti-centered extremal black hole solutions in a large class of 4D N=2\nsupergravities coupled to vector-multiplets with cubic prepotentials. The\nmethod is applicable to models for which the 3D moduli spaces obtained via\nc*-map are symmetric coset spaces. The attractor solutions are generated by\ncertain nilpotent elements in the coset algebra. We present explicit\ncomputations in 4D N=2 supergravity coupled to one vector-multiplet, whose 3D\nmoduli space is the symmetric coset space G_{2(2)}/SL(2,R)^2. The\nnon-supersymmetric multi-centered black holes in this model are found to lack\nthe intricate moduli space of bound configurations that are typical of the\nsupersymmetric case.'}, {'Gluing affine Yangians with bi-fundamentals': 'The affine Yangian of $\\mathfrak{gl}_1$ is isomorphic to the universal\nenveloping algebra of $\\mathcal{W}_{1+\\infty}$ and can serve as a building\nblock in the construction of new vertex operator algebras. In [1], a\ntwo-parameter family generalization of $\\mathcal{N}=2$ supersymmetric\n$\\mathcal{W}_{\\infty}$ algebra was constructed by ""gluing"" two affine Yangians\nof $\\mathfrak{gl}_1$ using operators that transform as $(\\square,\n\\overline{\\square})$ and $(\\overline{\\square}, \\square)$ w.r.t. the two affine\nYangians. In this paper we realize a similar (but non-isomorphic) two-parameter\ngluing construction where the gluing operators transform as $(\\square,\n\\square)$ and $(\\overline{\\square}, \\overline{\\square})$ w.r.t. the two affine\nYangians. The corresponding representation space consists of pairs of plane\npartitions connected by a common leg whose cross-section takes the shape of\nYoung diagrams, offering a more transparent geometric picture than the previous\nconstruction.'}, {'Adapting Blockchain Technology for Scientific Computing': 'Blockchain stores information into a chain of ""blocks"", whose integrity is\nusually guaranteed by Proof of Work (PoW). In many blockchain applications\n(including cryptocurrencies), users compete with each other to win the\nownership of the blocks, a process commonly referred as ""mining"". Mining\nactivities consume huge amount of power, while the outcome appears to be\nuseless besides validating a block. Here we discuss the requirements of\ndesigning a new PoW algorithm. We also propose a PoW scheme to help solve\nhigh-dimension, non-linear optimization problems. Simulation experiments of\nblockchains generated by three miners solved an instance of Traveling Salesman\nProblem (TSP), a well-known NP-hard problem. The revised scheme enables us to\naddress difficult scientific questions as a byproduct of mining.'}, {'Learning UI Navigation through Demonstrations composed of Macro Actions': 'We have developed a framework to reliably build agents capable of UI\nnavigation. The state space is simplified from raw-pixels to a set of UI\nelements extracted from screen understanding, such as OCR and icon detection.\nThe action space is restricted to the UI elements plus a few global actions.\nActions can be customized for tasks and each action is a sequence of basic\noperations conditioned on status checks. With such a design, we are able to\ntrain DQfD and BC agents with a small number of demonstration episodes. We\npropose demo augmentation that significantly reduces the required number of\nhuman demonstrations. We made a customization of DQfD to allow demos collected\non screenshots to facilitate the demo coverage of rare cases. Demos are only\ncollected for the failed cases during the evaluation of the previous version of\nthe agent. With 10s of iterations looping over evaluation, demo collection, and\ntraining, the agent reaches a 98.7\\% success rate on the search task in an\nenvironment of 80+ apps and websites where initial states and viewing\nparameters are randomized.'}, {'Quiver algebras and their representations for arbitrary quivers': 'The quiver Yangians were originally defined for the quiver and superpotential\nfrom string theory on general toric Calabi-Yau threefolds, and serve as BPS\nalgebras of these systems. Their characters reproduce the unrefined BPS\nindices, which correspond to classical Donaldson-Thomas (DT) invariants. We\ngeneralize this construction in two directions. First, we show that this\ndefinition extends to arbitrary quivers with potentials. Second, we explain how\nto define the characters to incorporate the refined BPS indices, which\ncorrespond to motivic DT invariants. We focus on two main classes of quivers:\nthe BPS quivers of 4D $N=2$ theories and the quivers from the knot-quiver\ncorrespondence. The entire construction allows for straightforward\ngeneralizations to trigonometric, elliptic, and generalized cohomologies.'}, {'A Development Calculus for Specifications': ""A first order inference system, called R-calculus, is defined to develop the\nspecifications. It is used to eliminate the laws which is not consistent with\nthe user's requirements. The R-calculus consists of the structural rules, an\naxiom, a cut rule, and the rules for logical connectives. Some examples are\ngiven to demonstrate the usage of the R-calculus. The properties about\nreachability and completeness of the R-calculus are formally defined and are\nproved.""}]","### Abstract

The continuous advance in large language models (LLMs) prompts a necessity for refined frameworks that can encapsulate and evaluate their fundamental knowledge abilities fully, particularly in Chinese contexts. This research emphasizes the creation of a benchmark, titled ""FoundaBench,"" that harnesses psychostatistical methods to dissect and evaluate the fundamental knowledge strength of LLMs. Accompanied by meticulous methodologies, computational improvements, and scrutiny of quality, reliability, and validity assurance, ""FoundaBench"" prioritizes a taxonomy merging common sense, life knowledge, cultural insights, social dynamics, scientific and technological understanding, artistic appreciation, and recreational awareness, enriched with multiple-choice questions and捌 multiple contexts. The benchmark evaluates a total of 12 models across varying parameter scales, tracing or outperforming human knowledge benchmarks. The study therefore fulfills a critical gap in language model evaluation, boosts transparency in assessment standards, accommodates a diverse array of applicational needs through comprehensive datasets, and upholds robust evaluation techniques to manifest the inner workings and competence of state-of-the-art LLMs. This innovative endeavor underscores the pivotal role of ""FoundaBench"" in shaping the future of benchmarking system design for LLMs, aiming to enhance the reliability, efficiency, and generalizability of future models deployed in a multitude of sectors, ranging from education to advanced AI applications and beyond."
"With the proliferation of large language models (LLMs), the comprehensive
alignment of such models across multiple tasks has emerged as a critical area
of research. Existing alignment methodologies primarily address single task,
such as multi-turn dialogue, coding, mathematical problem-solving, and tool
usage. However, AI-driven products that leverage language models usually
necessitate a fusion of these abilities to function effectively in real-world
scenarios. Moreover, the considerable computational resources required for
proper alignment of LLMs underscore the need for a more robust, efficient, and
encompassing approach to multi-task alignment, ensuring improved generative
performance. In response to these challenges, we introduce a novel technique
termed Mixture-of-Instructions (MoI), which employs a strategy of instruction
concatenation combined with diverse system prompts to boost the alignment
efficiency of language models. We have also compiled a diverse set of seven
benchmark datasets to rigorously evaluate the alignment efficacy of the
MoI-enhanced language model. Our methodology was applied to the open-source
Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This
enhanced model demonstrates significant advancements in generative capabilities
across coding, mathematics, and tool use tasks.","[{'What is Meant by AGI? On the Definition of Artificial General\n  Intelligence': ""This paper aims to establish a consensus on AGI's definition. General\nintelligence refers to the adaptation to open environments according to certain\nprinciples using limited resources. It emphasizes that adaptation or learning\nis an indispensable property of intelligence, and places the controversial part\nwithin the principles of intelligence, which can be described from different\nperspectives.""}, {'A Brain-Inspired Sequence Learning Model based on a Logic': 'Sequence learning is an essential aspect of intelligence. In Artificial\nIntelligence, sequence prediction task is usually used to test a sequence\nlearning model. In this paper, a model of sequence learning, which is\ninterpretable through Non-Axiomatic Logic, is designed and tested. The learning\nmechanism is composed of three steps, hypothesizing, revising, and recycling,\nwhich enable the model to work under the Assumption of Insufficient Knowledge\nand Resources. Synthetic datasets for sequence prediction task are generated to\ntest the capacity of the model. The results show that the model works well\nwithin different levels of difficulty. In addition, since the model adopts\nconcept-centered representation, it theoretically does not suffer from\ncatastrophic forgetting, and the practical results also support this property.\nThis paper shows the potential of learning sequences in a logical way.'}, {'Artificial Open World for Evaluating AGI: a Conceptual Design': ""How to evaluate Artificial General Intelligence (AGI) is a critical problem\nthat is discussed and unsolved for a long period. In the research of narrow AI,\nthis seems not a severe problem, since researchers in that field focus on some\nspecific problems as well as one or some aspects of cognition, and the criteria\nfor evaluation are explicitly defined. By contrast, an AGI agent should solve\nproblems that are never-encountered by both agents and developers. However,\nonce a developer tests and debugs the agent with a problem, the\nnever-encountered problem becomes the encountered problem, as a result, the\nproblem is solved by the developers to some extent, exploiting their\nexperience, rather than the agents. This conflict, as we call the trap of\ndevelopers' experience, leads to that this kind of problems is probably hard to\nbecome an acknowledged criterion. In this paper, we propose an evaluation\nmethod named Artificial Open World, aiming to jump out of the trap. The\nintuition is that most of the experience in the actual world should not be\nnecessary to be applied to the artificial world, and the world should be open\nin some sense, such that developers are unable to perceive the world and solve\nproblems by themselves before testing, though after that they are allowed to\ncheck all the data. The world is generated in a similar way as the actual\nworld, and a general form of problems is proposed. A metric is proposed aiming\nto quantify the progress of research. This paper describes the conceptual\ndesign of the Artificial Open World, though the formalization and the\nimplementation are left to the future.""}, {'Simulation of Non-inductive Vector Control of Permanent Magnet\n  Synchronous Motor Based on Sliding Mode Observer': ""Permanent magnet synchronous motors (PMSM) are widely used due to their\nnumerous benefits. It is critical to get rotor position and speed information\nin order to operate the motor accurately. Sensorless control techniques have\nemerged as a popular study area both at home and overseas. The sliding mode\nobserver (SMO) may indirectly detect rotor position and has the benefits of\neasy implementation and efficient algorithms. In this study, a mathematical\nmodel for sensorless control of a PMSM is developed using SMO, vector control,\nand other techniques. With a surface-mounted PMSM as the study object, a\nmathematical model for sensorless control of PMSM is developed. PMSM's sliding\nmode observer model is built in the matlab/simulink environment. Experiments\ndemonstrate that the system can track the rotor position and speed of the motor\nprecisely and fulfill the requirements of sensorless vector control of PMSM.""}, {'Efficient Teacher: Semi-Supervised Object Detection for YOLOv5': 'Semi-Supervised Object Detection (SSOD) has been successful in improving the\nperformance of both R-CNN series and anchor-free detectors. However, one-stage\nanchor-based detectors lack the structure to generate high-quality or flexible\npseudo labels, leading to serious inconsistency problems in SSOD. In this\npaper, we propose the Efficient Teacher framework for scalable and effective\none-stage anchor-based SSOD training, consisting of Dense Detector, Pseudo\nLabel Assigner, and Epoch Adaptor. Dense Detector is a baseline model that\nextends RetinaNet with dense sampling techniques inspired by YOLOv5. The\nEfficient Teacher framework introduces a novel pseudo label assignment\nmechanism, named Pseudo Label Assigner, which makes more refined use of pseudo\nlabels from Dense Detector. Epoch Adaptor is a method that enables a stable and\nefficient end-to-end semi-supervised training schedule for Dense Detector. The\nPseudo Label Assigner prevents the occurrence of bias caused by a large number\nof low-quality pseudo labels that may interfere with the Dense Detector during\nthe student-teacher mutual learning mechanism, and the Epoch Adaptor utilizes\ndomain and distribution adaptation to allow Dense Detector to learn globally\ndistributed consistent features, making the training independent of the\nproportion of labeled data. Our experiments show that the Efficient Teacher\nframework achieves state-of-the-art results on VOC, COCO-standard, and\nCOCO-additional using fewer FLOPs than previous methods. To the best of our\nknowledge, this is the first attempt to apply Semi-Supervised Object Detection\nto YOLOv5.Code is available:\nhttps://github.com/AlibabaResearch/efficientteacher'}, {'Three Mechanisms of Weight Decay Regularization': 'Weight decay is one of the standard tricks in the neural network toolbox, but\nthe reasons for its regularization effect are poorly understood, and recent\nresults have cast doubt on the traditional interpretation in terms of $L_2$\nregularization. Literal weight decay has been shown to outperform $L_2$\nregularization for optimizers for which they differ. We empirically investigate\nweight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a\nvariety of network architectures. We identify three distinct mechanisms by\nwhich weight decay exerts a regularization effect, depending on the particular\noptimization algorithm and architecture: (1) increasing the effective learning\nrate, (2) approximately regularizing the input-output Jacobian norm, and (3)\nreducing the effective damping coefficient for second-order optimization. Our\nresults provide insight into how to improve the regularization of neural\nnetworks.'}, {'Automated DevOps Pipeline Generation for Code Repositories using Large\n  Language Models': 'Automating software development processes through the orchestration of GitHub\nAction workflows has revolutionized the efficiency and agility of software\ndelivery pipelines. This paper presents a detailed investigation into the use\nof Large Language Models (LLMs) specifically, GPT 3.5 and GPT 4 to generate and\nevaluate GitHub Action workflows for DevOps tasks. Our methodology involves\ndata collection from public GitHub repositories, prompt engineering for LLM\nutilization, and evaluation metrics encompassing exact match scores, BLEU\nscores, and a novel DevOps Aware score. The research scrutinizes the\nproficiency of GPT 3.5 and GPT 4 in generating GitHub workflows, while\nassessing the influence of various prompt elements in constructing the most\nefficient pipeline. Results indicate substantial advancements in GPT 4,\nparticularly in DevOps awareness and syntax correctness. The research\nintroduces a GitHub App built on Probot, empowering users to automate workflow\ngeneration within GitHub ecosystem. This study contributes insights into the\nevolving landscape of AI-driven automation in DevOps practices.'}, {'Accurate Polygonal Mapping of Buildings in Satellite Imagery': 'This paper studies the problem of polygonal mapping of buildings by tackling\nthe issue of mask reversibility that leads to a notable performance gap between\nthe predicted masks and polygons from the learning-based methods. We addressed\nsuch an issue by exploiting the hierarchical supervision (of bottom-level\nvertices, mid-level line segments and the high-level regional masks) and\nproposed a novel interaction mechanism of feature embedding sourced from\ndifferent levels of supervision signals to obtain reversible building masks for\npolygonal mapping of buildings. As a result, we show that the learned\nreversible building masks take all the merits of the advances of deep\nconvolutional neural networks for high-performing polygonal mapping of\nbuildings. In the experiments, we evaluated our method on the two public\nbenchmarks of AICrowd and Inria. On the AICrowd dataset, our proposed method\nobtains unanimous improvements on the metrics of AP, APboundary and PoLiS. For\nthe Inria dataset, our proposed method also obtains very competitive results on\nthe metrics of IoU and Accuracy. The models and source code are available at\nhttps://github.com/SarahwXU.'}, {'Ques-Chain: an Ethereum Based E-Voting System': 'Ethereum is an open-source, public, blockchain-based distributed computing\nplatform and operating system featuring smart contract functionality. In this\npaper, we proposed an Ethereum based eletronic voting (e-voting) protocol,\nQues-Chain, which can ensure the authentication can be done without hurting\nconfidentiality and the anonymity can be protected without problems of scams at\nthe same time. Furthermore, the authors considered the wider usages Ques-Chain\ncan be applied on, pointing out that it is able to process all kinds of\nmessages and can be used in all fields with similar needs.'}, {'4D Attention-based Neural Network for EEG Emotion Recognition': 'Electroencephalograph (EEG) emotion recognition is a significant task in the\nbrain-computer interface field. Although many deep learning methods are\nproposed recently, it is still challenging to make full use of the information\ncontained in different domains of EEG signals. In this paper, we present a\nnovel method, called four-dimensional attention-based neural network (4D-aNN)\nfor EEG emotion recognition. First, raw EEG signals are transformed into 4D\nspatial-spectral-temporal representations. Then, the proposed 4D-aNN adopts\nspectral and spatial attention mechanisms to adaptively assign the weights of\ndifferent brain regions and frequency bands, and a convolutional neural network\n(CNN) is utilized to deal with the spectral and spatial information of the 4D\nrepresentations. Moreover, a temporal attention mechanism is integrated into a\nbidirectional Long Short-Term Memory (LSTM) to explore temporal dependencies of\nthe 4D representations. Our model achieves state-of-the-art performance on the\nSEED dataset under intra-subject splitting. The experimental results have shown\nthe effectiveness of the attention mechanisms in different domains for EEG\nemotion recognition.'}]","Abstract

Context and Importance:  
Against the backdrop of the rapid advancements in large language models (LLMs), limitations in addressing specific computational tasks, such as code generation, have emerged as a focus for research. This challenge is particularly acute in handling tasks requiring nuanced understanding of programming concepts and algorithms, often evinced in programming contests and benchmarks. This research addresses this bottleneck by leveraging significant improvements in fine-tuning techniques to enable specialized skill acquisition in LLMs.

Objective:  
The main objective of this study is to enhance large language models as problem solvers by introducing a method for aligning models with specific computational tasks, namely code generation and mathematical reasoning, through structured fine-tuning using multiple instructions (MoI).

Innovations:  
A novel, method termed ""MoI"" is introduced, which integrates multiple instructions into a single system prompt for fine-tuning. This approach encourages models to incorporate domain-specific strategies, leading to improved performance in computational tasks. Additionally, a method to modify system prompts during the fine-tuning phase significantly enhances alignment with the target computational tasks.

Methods:  
An initial model (Qwen-7B-chat) is selected as the base. Under the MoI framework, this base model is fine-tuned using various system prompts tailored for code generation and mathematical reasoning tasks. The learning process is optimized using the DeepSpeed strategy for efficient GPU utilization. The fine-tuned models are rigorously evaluated across several benchmarks including HumanEval, MBPP, GSM8K, and MATH to measure improvements in performance metrics.

Results:  
The implementation of MoI and system prompt modifications demonstrated superior performance compared to existing models in code generation and mathematical reasoning tasks, showcasing significant enhancements in computational capabilities of the large language models. Specifically, the fine-tuned models were able to solve problems more efficiently and accurately, highlighting the effectiveness of model-specific alignment through structured fine-tuning.

Contributions:  
This research pioneers a method that enhances LLMs' performance in computational tasks by directly influencing the model's decision-making process through system prompt modification and MoI approach. It significantly advances the field of artificial intelligence by enabling large language models to exhibit enhanced skills in problem-solving domains typically beyond their standard capacity. This opens a new avenue for further advancements in educational technologies, code generation, and the automation of complex reasoning tasks.

Applications:  
The findings have broad applicability in educational software development, enhancing the capabilities of AI in educational tools, and automating problem-solving processes in computational fields. This research can facilitate the creation of more engaging and effective computational learning platforms and can potentially lead to breakthroughs in the automation of software development, code generation, and tutoring systems for programming nuances and mathematical challenges."
"The current use of large language models (LLMs) for zero-shot document
ranking follows one of two ways: 1) prompt-based re-ranking methods, which
require no further training but are feasible for only re-ranking a handful of
candidate documents due to the associated computational costs; and 2)
unsupervised contrastive trained dense retrieval methods, which can retrieve
relevant documents from the entire corpus but require a large amount of paired
text data for contrastive training. In this paper, we propose PromptReps, which
combines the advantages of both categories: no need for training and the
ability to retrieve from the whole corpus. Our method only requires prompts to
guide an LLM to generate query and document representations for effective
document retrieval. Specifically, we prompt the LLMs to represent a given text
using a single word, and then use the last token's hidden states and the
corresponding logits associated to the prediction of the next token to
construct a hybrid document retrieval system. The retrieval system harnesses
both dense text embedding and sparse bag-of-words representations given by the
LLM. Our experimental evaluation on the BEIR zero-shot document retrieval
datasets illustrates that this simple prompt-based LLM retrieval method can
achieve a similar or higher retrieval effectiveness than state-of-the-art LLM
embedding methods that are trained with large amounts of unsupervised data,
especially when using a larger LLM.","[{'Dealing with Typos for BERT-based Passage Retrieval and Ranking': 'Passage retrieval and ranking is a key task in open-domain question answering\nand information retrieval. Current effective approaches mostly rely on\npre-trained deep language model-based retrievers and rankers. These methods\nhave been shown to effectively model the semantic matching between queries and\npassages, also in presence of keyword mismatch, i.e. passages that are relevant\nto a query but do not contain important query keywords. In this paper we\nconsider the Dense Retriever (DR), a passage retrieval method, and the BERT\nre-ranker, a popular passage re-ranking method. In this context, we formally\ninvestigate how these models respond and adapt to a specific type of keyword\nmismatch -- that caused by keyword typos occurring in queries. Through\nempirical investigation, we find that typos can lead to a significant drop in\nretrieval and ranking effectiveness. We then propose a simple typos-aware\ntraining framework for DR and BERT re-ranker to address this issue. Our\nexperimental results on the MS MARCO passage ranking dataset show that, with\nour proposed typos-aware training, DR and BERT re-ranker can become robust to\ntypos in queries, resulting in significantly improved effectiveness compared to\nmodels trained without appropriately accounting for typos.'}, {'Fast Passage Re-ranking with Contextualized Exact Term Matching and\n  Efficient Passage Expansion': 'BERT-based information retrieval models are expensive, in both time (query\nlatency) and computational resources (energy, hardware cost), making many of\nthese models impractical especially under resource constraints. The reliance on\na query encoder that only performs tokenization and on the pre-processing of\npassage representations at indexing, has allowed the recently proposed TILDE\nmethod to overcome the high query latency issue typical of BERT-based models.\nThis however is at the expense of a lower effectiveness compared to other\nBERT-based re-rankers and dense retrievers. In addition, the original TILDE\nmethod is characterised by indexes with a very high memory footprint, as it\nexpands each passage into the size of the BERT vocabulary. In this paper, we\npropose TILDEv2, a new model that stems from the original TILDE but that\naddresses its limitations. TILDEv2 relies on contextualized exact term matching\nwith expanded passages. This requires to only store in the index the score of\ntokens that appear in the expanded passages (rather than all the vocabulary),\nthus producing indexes that are 99% smaller than those of TILDE. This matching\nmechanism also improves ranking effectiveness by 24%, without adding to the\nquery latency. This makes TILDEv2 the state-of-the-art passage re-ranking\nmethod for CPU-only environments, capable of maintaining query latency below\n100ms on commodity hardware.'}, {'Asyncval: A Toolkit for Asynchronously Validating Dense Retriever\n  Checkpoints during Training': 'The process of model checkpoint validation refers to the evaluation of the\nperformance of a model checkpoint executed on a held-out portion of the\ntraining data while learning the hyperparameters of the model, and is used to\navoid over-fitting and determine when the model has converged so as to stop\ntraining. A simple and efficient strategy to validate deep learning checkpoints\nis the addition of validation loops to execute during training. However, the\nvalidation of dense retrievers (DR) checkpoints is not as trivial -- and the\naddition of validation loops is not efficient. This is because, in order to\naccurately evaluate the performance of a DR checkpoint, the whole document\ncorpus needs to be encoded into vectors using the current checkpoint before any\nactual retrieval operation for checkpoint validation can be performed. This\ncorpus encoding process can be very time-consuming if the document corpus\ncontains millions of documents (e.g., 8.8m for MS MARCO and 21m for Natural\nQuestions). Thus, a naive use of validation loops during training will\nsignificantly increase training time. To address this issue, in this demo\npaper, we propose Asyncval: a Python-based toolkit for efficiently validating\nDR checkpoints during training. Instead of pausing the training loop for\nvalidating DR checkpoints, Asyncval decouples the validation loop from the\ntraining loop, uses another GPU to automatically validate new DR checkpoints\nand thus permits to perform validation asynchronously from training. Asyncval\nalso implements a range of different corpus subset sampling strategies for\nvalidating DR checkpoints; these strategies allow to further speed up the\nvalidation process. We provide an investigation of these methods in terms of\ntheir impact on validation time and validation fidelity. Asyncval is made\navailable as an open-source project at https://github.com/ielab/asyncval.'}, {'CharacterBERT and Self-Teaching for Improving the Robustness of Dense\n  Retrievers on Queries with Typos': ""Current dense retrievers are not robust to out-of-domain and outlier queries,\ni.e. their effectiveness on these queries is much poorer than what one would\nexpect. In this paper, we consider a specific instance of such queries: queries\nthat contain typos. We show that a small character level perturbation in\nqueries (as caused by typos) highly impacts the effectiveness of dense\nretrievers. We then demonstrate that the root cause of this resides in the\ninput tokenization strategy employed by BERT. In BERT, tokenization is\nperformed using the BERT's WordPiece tokenizer and we show that a token with a\ntypo will significantly change the token distributions obtained after\ntokenization. This distribution change translates to changes in the input\nembeddings passed to the BERT-based query encoder of dense retrievers. We then\nturn our attention to devising dense retriever methods that are robust to such\nqueries with typos, while still being as performant as previous methods on\nqueries without typos. For this, we use CharacterBERT as the backbone encoder\nand an efficient yet effective training method, called Self-Teaching (ST), that\ndistills knowledge from queries without typos into the queries with typos.\nExperimental results show that CharacterBERT in combination with ST achieves\nsignificantly higher effectiveness on queries with typos compared to previous\nmethods. Along with these results and the open-sourced implementation of the\nmethods, we also provide a new passage retrieval dataset consisting of\nreal-world queries with typos and associated relevance assessments on the MS\nMARCO corpus, thus supporting the research community in the investigation of\neffective and robust dense retrievers. Code, experimental results and dataset\nare made available at https://github.com/ielab/CharacterBERT-DR.""}, {'A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking\n  with Large Language Models': 'We propose a novel zero-shot document ranking approach based on Large\nLanguage Models (LLMs): the Setwise prompting approach. Our approach\ncomplements existing prompting approaches for LLM-based zero-shot ranking:\nPointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative\nevaluation within a consistent experimental framework and considering factors\nlike model size, token consumption, latency, among others, we show that\nexisting approaches are inherently characterised by trade-offs between\neffectiveness and efficiency. We find that while Pointwise approaches score\nhigh on efficiency, they suffer from poor effectiveness. Conversely, Pairwise\napproaches demonstrate superior effectiveness but incur high computational\noverhead. Our Setwise approach, instead, reduces the number of LLM inferences\nand the amount of prompt token consumption during the ranking procedure,\ncompared to previous methods. This significantly improves the efficiency of\nLLM-based zero-shot ranking, while also retaining high zero-shot ranking\neffectiveness. We make our code and results publicly available at\n\\url{https://github.com/ielab/llm-rankers}.'}, {'Reinforcement Online Learning to Rank with Unbiased Reward Shaping': ""Online learning to rank (OLTR) aims to learn a ranker directly from implicit\nfeedback derived from users' interactions, such as clicks. Clicks however are a\nbiased signal: specifically, top-ranked documents are likely to attract more\nclicks than documents down the ranking (position bias). In this paper, we\npropose a novel learning algorithm for OLTR that uses reinforcement learning to\noptimize rankers: Reinforcement Online Learning to Rank (ROLTR). In ROLTR, the\ngradients of the ranker are estimated based on the rewards assigned to clicked\nand unclicked documents. In order to de-bias the users' position bias contained\nin the reward signals, we introduce unbiased reward shaping functions that\nexploit inverse propensity scoring for clicked and unclicked documents. The\nfact that our method can also model unclicked documents provides a further\nadvantage in that less users interactions are required to effectively train a\nranker, thus providing gains in efficiency. Empirical evaluation on standard\nOLTR datasets shows that ROLTR achieves state-of-the-art performance, and\nprovides significantly better user experience than other OLTR approaches. To\nfacilitate the reproducibility of our experiments, we make all experiment code\navailable at https://github.com/ielab/OLTR.""}, {'Implicit Feedback for Dense Passage Retrieval: A Counterfactual Approach': 'In this paper we study how to effectively exploit implicit feedback in Dense\nRetrievers (DRs). We consider the specific case in which click data from a\nhistoric click log is available as implicit feedback. We then exploit such\nhistoric implicit interactions to improve the effectiveness of a DR. A key\nchallenge that we study is the effect that biases in the click signal, such as\nposition bias, have on the DRs. To overcome the problems associated with the\npresence of such bias, we propose the Counterfactual Rocchio (CoRocchio)\nalgorithm for exploiting implicit feedback in Dense Retrievers. We demonstrate\nboth theoretically and empirically that dense query representations learnt with\nCoRocchio are unbiased with respect to position bias and lead to higher\nretrieval effectiveness. We make available the implementations of the proposed\nmethods and the experimental framework, along with all results at\nhttps://github.com/ielab/Counterfactual-DR.'}, {'Exploring the Representation Power of SPLADE Models': 'The SPLADE (SParse Lexical AnD Expansion) model is a highly effective\napproach to learned sparse retrieval, where documents are represented by term\nimpact scores derived from large language models. During training, SPLADE\napplies regularization to ensure postings lists are kept sparse -- with the aim\nof mimicking the properties of natural term distributions -- allowing efficient\nand effective lexical matching and ranking. However, we hypothesize that SPLADE\nmay encode additional signals into common postings lists to further improve\neffectiveness. To explore this idea, we perform a number of empirical analyses\nwhere we re-train SPLADE with different, controlled vocabularies and measure\nhow effective it is at ranking passages. Our findings suggest that SPLADE can\neffectively encode useful ranking signals in documents even when the vocabulary\nis constrained to terms that are not traditionally useful for ranking, such as\nstopwords or even random words.'}, {'Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial\n  Retrieval with Neural Rankers and Large Language Models': ""We describe team ielab from CSIRO and The University of Queensland's approach\nto the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers\nbut to utilise Large Language Models to overcome the issue of lack of training\ndata for such rankers. Specifically, we employ ChatGPT to generate relevant\npatient descriptions for randomly selected clinical trials from the corpus.\nThis synthetic dataset, combined with human-annotated training data from\nprevious years, is used to train both dense and sparse retrievers based on\nPubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the\nsystem. To further enhance the effectiveness of our approach, we prompting\nGPT-4 as a TREC annotator to provide judgments on our run files. These\njudgments are subsequently employed to re-rank the results. This architecture\ntightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large\nLanguage Models, demonstrating a new approach to clinical trial retrieval.""}, {'Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval\n  Systems': 'The emergence of Vec2Text -- a method for text embedding inversion -- has\nraised serious privacy concerns for dense retrieval systems which use text\nembeddings, such as those offered by OpenAI and Cohere. This threat comes from\nthe ability for a malicious attacker with access to embeddings to reconstruct\nthe original text. In this paper, we investigate various factors related to\nembedding models that may impact text recoverability via Vec2Text. We explore\nfactors such as distance metrics, pooling functions, bottleneck pre-training,\ntraining with noise addition, embedding quantization, and embedding dimensions,\nwhich were not considered in the original Vec2Text paper. Through a\ncomprehensive analysis of these factors, our objective is to gain a deeper\nunderstanding of the key elements that affect the trade-offs between the text\nrecoverability and retrieval effectiveness of dense retrieval systems, offering\ninsights for practitioners designing privacy-aware dense retrieval systems. We\nalso propose a simple embedding transformation fix that guarantees equal\nranking effectiveness while mitigating the recoverability risk. Overall, this\nstudy reveals that Vec2Text could pose a threat to current dense retrieval\nsystems, but there are some effective methods to patch such systems.'}]","Title: PromptReps: Leveraging Large Language Models for Document Retrieval Without Extra Training

Abstract: Grounded in the transformative impact of Large Language Models (LLMs) on natural language understanding and generation, this research introduces PromptReps, a novel method to utilize LLMs for robust first-stage document retrieval without additional training. Our core innovation lies in prompting LLMs with designed queries in a LLM-efficient manner to generate both dense and sparse embeddings simultaneously, which are then integrated into efficient search indices for fast retrieval. These indices support both fast sparse and dense retrieval processes, offering improved precision, especially in large scale document collections.

Key contributions include an end-to-end prompting framework that significantly improves retrieval effectiveness compared to state-of-the-art methods without incurring the cost of extra training steps. Specifically, PromptReps outperforms LLM2Vec, BM25, and E5-PTlarge on multiple benchmarks, demonstrating enhanced performance across various IR tasks. 

Empowering LLMs with intuitive guidance enables their standardization across diverse domains, tasks, and even multilingual settings, highlighting the stronger role for original prompting techniques rather than relying on further pre-training. This approach provides a cost-effective and scalable solution for large-scale document indexing and retrieval, with potential applications in search engines, information retrieval systems, and broader content management platforms. Moreover, PromptReps opens avenues for further research in developing more sophisticated LLM-based retrieval systems through integration with unsupervised contrastive pre-training strategies on paired text data and synthetic datasets."
"Developing effective biomedical retrieval models is important for excelling
at knowledge-intensive biomedical tasks but still challenging due to the
deficiency of sufficient publicly annotated biomedical data and computational
resources. We present BMRetriever, a series of dense retrievers for enhancing
biomedical retrieval via unsupervised pre-training on large biomedical corpora,
followed by instruction fine-tuning on a combination of labeled datasets and
synthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify
BMRetriever's efficacy on various biomedical applications. BMRetriever also
exhibits strong parameter efficiency, with the 410M variant outperforming
baselines up to 11.7 times larger, and the 2B variant matching the performance
of models with over 5B parameters. The training data and model checkpoints are
released at \url{https://huggingface.co/BMRetriever} to ensure transparency,
reproducibility, and application to new domains.","[{'Estimating Social Influence Using Latent Space Adjusted Approach in R': ""Social influence, sometimes referred to as spillover or contagion, have been\nextensively studied in various empirical social network research. However,\nthere are various estimation challenges in identifying social influence\neffects, as they are often entangled with other factors, such as homophily in\nthe selection process, the individual's preference for the same social\nsettings, etc. Methods currently available either do not solve these problems\nor require strong assumptions. Recent works by Xu 2018 and others show that a\nlatent-space adjusted approach based on the latent space model has potential to\ndisentangle the influence from other processes, and the simulation evidence\nshows the approach performs better than other state-of-the-art approaches in\nterms of recovering the true social influence effect when there is an\nunobserved trait co-determining influence and selection. In this paper we\nillustrate how latent space adjusted approach accounts for bias in the\nestimation of the social influence effect, and demonstrate how this approach\ncan be implemented to estimate various social influence models with an\nempirical example in R.""}, {'A hybrid molecular dynamics/atomic-scale finite element method for\n  quasi-static atomistic simulations at finite temperature': 'In this paper, a hybrid quasi-static atomistic simulation method at finite\ntemperature is developed, which combines the advantages of MD for thermal\nequilibrium and atomic-scale finite element method (AFEM) for efficient\nequilibration. Some temperature effects are embedded in static AFEM simulation\nby applying the virtual and equivalent thermal disturbance forces extracted\nfrom MD. Alternatively performing MD and AFEM can quickly obtain a series of\nthermodynamic equilibrium configurations such that a quasi-static process is\nmodeled. Moreover, a stirring-accelerated MD/AFEM fast relaxation approach is\nproposed, in which the atomic forces and velocities are randomly exchanged to\nartificially accelerate the ""slow processes"" such as mechanical wave\npropagation and thermal diffusion. The efficiency of the proposed methods is\ndemonstrated by numerical examples on single wall carbon nanotubes.'}, {'Kinetic Energy-Based Temperature Computation in Non-Equilibrium\n  Molecular Dynamics Simulation': 'The average kinetic energy is widely used to characterize temperature in\nmolecular dynamics (MD) simulation. In this letter, the applicability of three\ntypes of average kinetic energy as measures of temperature is investigated,\ni.e., the total kinetic energy, kinetic energy without the centroid translation\npart, and thermal disturbance kinetic energy. Our MD simulations indicate that\ndefinitions of temperature based on the kinetic energy including rigid\ntranslational or rotational motion may yield unrealistic results. In contrast,\nthe thermal disturbance kinetic energy has wider applicability to temperature\ncomputation in non-equilibrium molecular dynamics simulation. If small samples\nneed to be used for local temperature, then a calibration approach is proposed\nto eliminate the sample-size dependence of the average disturbance kinetic\nenergy.'}, {'Scalable hierarchical parallel algorithm for the solution of super\n  large-scale sparse linear equations': 'The parallel linear equations solver capable of effectively using 1000+\nprocessors becomes the bottleneck of large-scale implicit engineering\nsimulations. In this paper, we present a new hierarchical parallel\nmaster-slave-structural iterative algorithm for the solution of super\nlarge-scale sparse linear equations in distributed memory computer cluster.\nThrough alternatively performing global equilibrium computation and local\nrelaxation, our proposed algorithm will reach the specific accuracy requirement\nin a few of iterative steps. Moreover, each set/slave-processor majorly\ncommunicate with its nearest neighbors, and the transferring data between\nsets/slave-processors and master is always far below the set-neighbor\ncommunication. The corresponding algorithm for implicit finite element analysis\nhas been implemented based on MPI library, and a super large 2-dimension square\nsystem of triangle-lattice truss structure under random static loads is\nsimulated with over one billion degrees of freedom and up to 2001 processors on\n""Exploration 100"" cluster in Tsinghua University. The numerical experiments\ndemonstrate that this algorithm has excellent parallel efficiency and high\nscalability, and it may have broad application in other implicit simulations.'}, {'Sequential Labeling with online Deep Learning': 'Deep learning has attracted great attention recently and yielded the state of\nthe art performance in dimension reduction and classification problems.\nHowever, it cannot effectively handle the structured output prediction, e.g.\nsequential labeling. In this paper, we propose a deep learning structure, which\ncan learn discriminative features for sequential labeling problems. More\nspecifically, we add the inter-relationship between labels in our deep learning\nstructure, in order to incorporate the context information from the sequential\ndata. Thus, our model is more powerful than linear Conditional Random Fields\n(CRFs) because the objective function learns latent non-linear features so that\ntarget labeling can be better predicted. We pretrain the deep structure with\nstacked restricted Boltzmann machines (RBMs) for feature learning and optimize\nour objective function with online learning algorithm, a mixture of perceptron\ntraining and stochastic gradient descent. We test our model on different\nchallenge tasks, and show that our model outperforms significantly over the\ncompletive baselines.'}, {'Context-aware Active Multi-Step Reinforcement Learning': 'Reinforcement learning has attracted great attention recently, especially\npolicy gradient algorithms, which have been demonstrated on challenging\ndecision making and control tasks. In this paper, we propose an active\nmulti-step TD algorithm with adaptive stepsizes to learn actor and critic.\nSpecifically, our model consists of two components: active stepsize learning\nand adaptive multi-step TD algorithm. Firstly, we divide the time horizon into\nchunks and actively select state and action inside each chunk. Then given the\nselected samples, we propose the adaptive multi-step TD, which generalizes\nTD($\\lambda$), but adaptively switch on/off the backups from future returns of\ndifferent steps. Particularly, the adaptive multi-step TD introduces a\ncontext-aware mechanism, here a binary classifier, which decides whether or not\nto turn on its future backups based on the context changes. Thus, our model is\nkind of combination of active learning and multi-step TD algorithm, which has\nthe capacity for learning off-policy without the need of importance sampling.\nWe evaluate our approach on both discrete and continuous space tasks in an\noff-policy setting respectively, and demonstrate competitive results compared\nto other reinforcement learning baselines.'}, {'MetaHistoSeg: A Python Framework for Meta Learning in Histopathology\n  Image Segmentation': 'Few-shot learning is a standard practice in most deep learning based\nhistopathology image segmentation, given the relatively low number of digitized\nslides that are generally available. While many models have been developed for\ndomain specific histopathology image segmentation, cross-domain generalization\nremains a key challenge for properly validating models. Here, tooling and\ndatasets to benchmark model performance across histopathological domains are\nlacking. To address this limitation, we introduce MetaHistoSeg - a Python\nframework that implements unique scenarios in both meta learning and instance\nbased transfer learning. Designed for easy extension to customized datasets and\ntask sampling schemes, the framework empowers researchers with the ability of\nrapid model design and experimentation. We also curate a histopathology meta\ndataset - a benchmark dataset for training and validating models on\nout-of-distribution performance across a range of cancer types. In experiments\nwe showcase the usage of MetaHistoSeg with the meta dataset and find that both\nmeta-learning and instance based transfer learning deliver comparable results\non average, but in some cases tasks can greatly benefit from one over the\nother.'}, {'A Discrete Time Markov Chain Model for High Throughput Bidirectional\n  Fano Decoders': 'The bidirectional Fano algorithm (BFA) can achieve at least two times\ndecoding throughput compared to the conventional unidirectional Fano algorithm\n(UFA). In this paper, bidirectional Fano decoding is examined from the queuing\ntheory perspective. A Discrete Time Markov Chain (DTMC) is employed to model\nthe BFA decoder with a finite input buffer. The relationship between the input\ndata rate, the input buffer size and the clock speed of the BFA decoder is\nestablished. The DTMC based modelling can be used in designing a high\nthroughput parallel BFA decoding system. It is shown that there is a tradeoff\nbetween the number of BFA decoders and the input buffer size, and an optimal\ninput buffer size can be chosen to minimize the hardware complexity for a\ntarget decoding throughput in designing a high throughput parallel BFA decoding\nsystem.'}, {'Random Forests for Metric Learning with Implicit Pairwise Position\n  Dependence': 'Metric learning makes it plausible to learn distances for complex\ndistributions of data from labeled data. However, to date, most metric learning\nmethods are based on a single Mahalanobis metric, which cannot handle\nheterogeneous data well. Those that learn multiple metrics throughout the space\nhave demonstrated superior accuracy, but at the cost of computational\nefficiency. Here, we take a new angle to the metric learning problem and learn\na single metric that is able to implicitly adapt its distance function\nthroughout the feature space. This metric adaptation is accomplished by using a\nrandom forest-based classifier to underpin the distance function and\nincorporate both absolute pairwise position and standard relative position into\nthe representation. We have implemented and tested our method against state of\nthe art global and multi-metric methods on a variety of data sets. Overall, the\nproposed method outperforms both types of methods in terms of accuracy\n(consistently ranked first) and is an order of magnitude faster than state of\nthe art multi-metric methods (16x faster in the worst case).'}, {'Asymptotic Correlation Structure of Discounted Incurred But Not Reported\n  Claims under Fractional Poisson Arrival Process': 'This paper studies the joint moments of a compound discounted renewal process\nobserved at different times with each arrival removed from the system after a\nrandom delay. This process can be used to describe the aggregate (discounted)\nIncurred But Not Reported claims in insurance and also the total number of\ncustomers in an infinite server queue. It is shown that the joint moments can\nbe obtained recursively in terms of the renewal density, from which the\ncovariance and correlation structures are derived. In particular, the\nfractional Poisson process defined via the renewal approach is also considered.\nFurthermore, the asymptotic behaviour of covariance and correlation coefficient\nof the aforementioned quantities is analyzed as the time horizon goes to\ninfinity. Special attention is paid to the cases of exponential and Pareto\ndelays.'}]","Title: BMRETRIEVER: A Large-Scale Biomedical Knowledge Retrieval Framework Enabling Large Language Models

**Background**: 
Across biomedicine and the life sciences, effective information retrieval from large datasets, documents, and patient records is essential for enhancing the performance of AI models. However, up-to-date domain knowledge poses a significant challenge when relying on BERT-series models. The current paper pioneers a novel large-scale retrieval framework, BMRETRIEVER, designed specifically for biomedical text retrieval, embodied with dense embedding capabilities, capable of augmenting the powers of large language models (LLMs).

**Objective**: 
The primary aim of this research is to develop and demonstrate BMRETRIEVER, a dense retrieval framework that transforms biomedical text into dense representations to perform targeted retrieval. It contrasts unsupervised and supervised contrastive learning paradigms with instruction fine-tuning, all aimed at relieving models from the reliance on proprietary data, privacy concerns, and domain-specific architectural decisions.

**Innovations**:
BMRETRIEVER introduces a two-stage pre-training framework for unsupervised contrastive learning on large-scale biomedical query-passage pairs. This framework facilitates productive knowledge acquisition through a conjunction of synthetic data generation and prompt-engineering. By contrast, instruction fine-tuning leverages a diverse set of labeled tasks for a systematic adaptation to domains like question-answering, categorization, and entailment.

**Methods**:
The solution employs a combination of unsupervised contrastive learning with unsupervised anthropogenic text and supervised contrastive learning on representative biomedical datasets. A notable component is the prompt-engineering technique to align pre-trained models with downstream biomedical tasks. Additionally, BMRETRIEVER is designed to be scalable, applicable to models like GPT-410M, -1B, -2B, -7B, for both pre-training and fine-tuning, showcasing scalability and performance across a range of configurations.

**Results**:
The findings indicate that BMRETRIEVER significantly outperforms state-of-the-art retrieval techniques like BM25, Contriever, Dragon, SciMult, SPECTER 2.0, etc., on five popular biomedical tasks. Notably, it surpasses many fully supervised models when using the unsupervised pre-trained models directly.

**Contributions**:
BMRETRIEVER introduces specialized pre-training and fine-tuning paradigms for biomedical text retrieval, proposes a dual use of synthetic data generation and instruction fine-tuning, and ensures scalability to accommodate larger model architectures. Moreover, it bypasses privacy concerns surrounding proprietary data sources.

**Applications**:
This framework enables the refining and enhancement of LLMs, specifically for tasks in biomedicine, recursive patient record querying, and personalized treatment recommendations, to foster innovation in medical research, patient care, and clinical decision making. BMRETRIEVER sets a new benchmark in biomedical retrieval that can significantly influence the current landscape and future applications of large language models in healthcare."
"U-Nets are among the most widely used architectures in computer vision,
renowned for their exceptional performance in applications such as image
segmentation, denoising, and diffusion modeling. However, a theoretical
explanation of the U-Net architecture design has not yet been fully
established.
  This paper introduces a novel interpretation of the U-Net architecture by
studying certain generative hierarchical models, which are tree-structured
graphical models extensively utilized in both language and image domains. With
their encoder-decoder structure, long skip connections, and pooling and
up-sampling layers, we demonstrate how U-Nets can naturally implement the
belief propagation denoising algorithm in such generative hierarchical models,
thereby efficiently approximating the denoising functions. This leads to an
efficient sample complexity bound for learning the denoising function using
U-Nets within these models. Additionally, we discuss the broader implications
of these findings for diffusion models in generative hierarchical models. We
also demonstrate that the conventional architecture of convolutional neural
networks (ConvNets) is ideally suited for classification tasks within these
models. This offers a unified view of the roles of ConvNets and U-Nets,
highlighting the versatility of generative hierarchical models in modeling
complex data distributions across language and image domains.","[{'A Novel Location Free Link Prediction in Multiplex Social Networks': 'In recent decades, the emergence of social networks has enabled internet\nservice providers (e.g., Facebook, Twitter and Uber) to achieve great\ncommercial success. Link prediction is recognized as a common practice to build\nthe topology of social networks and keep them evolving. Conventionally, link\nprediction methods are dependent of location information of users, which\nsuffers from information leakage from time to time. To deal with this problem,\ncompanies of smart devices (e.g., Apple Inc.) keeps tightening their privacy\npolicy, impeding internet service providers from acquiring location\ninformation. Therefore, it is of great importance to design location free link\nprediction methods, while the accuracy still preserves. In this study, a novel\nlocation free link prediction method is proposed for complex social networks.\nExperiments on real datasets show that the precision of our location free link\nprediction method increases by 10 percent.'}, {'Plausible deniability for privacy-preserving data synthesis': 'In the field of privacy protection, publishing complete data (especially\nhigh-dimensional data sets) is one of the most challenging problems. The common\nencryption technology can not deal with the attacker to take differential\nattack to obtain sensitive information, while the existing differential privacy\nprotection algorithm model takes a long time for high-dimensional calculation\nand needs to add noise to reduce data accuracy, which is not suitable for\nhigh-dimensional large data sets. In view of this situation, this paper designs\na complete data synthesis scheme to protect data privacy around the concept of\n""plausible denial"". Firstly, the paper provides the theoretical support for the\ndifference between ""plausible data"" and ""plausible data"". In the process of\nscheme designing, this paper decomposes the scheme design into construction\ndata synthesis module and privacy test module, then designs algorithm models\nfor them respectively and realizes the function of privacy protection. When\nevaluating the feasibility of the scheme, the paper selects the Results of the\n2013 community census in the United States as the high-dimensional data set,\nuses the simulation program that is based on Python to test and analyzes the\nefficiency and reliability of the data synthesis scheme. This portion focuses\non the evaluation of the privacy protection effectiveness of the scheme.'}, {'Deep Networks as Denoising Algorithms: Sample-Efficient Learning of\n  Diffusion Models in High-Dimensional Graphical Models': 'We investigate the approximation efficiency of score functions by deep neural\nnetworks in diffusion-based generative modeling. While existing approximation\ntheories utilize the smoothness of score functions, they suffer from the curse\nof dimensionality for intrinsically high-dimensional data. This limitation is\npronounced in graphical models such as Markov random fields, common for image\ndistributions, where the approximation efficiency of score functions remains\nunestablished.\n  To address this, we observe score functions can often be well-approximated in\ngraphical models through variational inference denoising algorithms.\nFurthermore, these algorithms are amenable to efficient neural network\nrepresentation. We demonstrate this in examples of graphical models, including\nIsing models, conditional Ising models, restricted Boltzmann machines, and\nsparse encoding models. Combined with off-the-shelf discretization error bounds\nfor diffusion-based sampling, we provide an efficient sample complexity bound\nfor diffusion-based generative modeling when the score function is learned by\ndeep neural networks.'}, {'Sample-Efficient Learning of Correlated Equilibria in Extensive-Form\n  Games': 'Imperfect-Information Extensive-Form Games (IIEFGs) is a prevalent model for\nreal-world games involving imperfect information and sequential plays. The\nExtensive-Form Correlated Equilibrium (EFCE) has been proposed as a natural\nsolution concept for multi-player general-sum IIEFGs. However, existing\nalgorithms for finding an EFCE require full feedback from the game, and it\nremains open how to efficiently learn the EFCE in the more challenging bandit\nfeedback setting where the game can only be learned by observations from\nrepeated playing.\n  This paper presents the first sample-efficient algorithm for learning the\nEFCE from bandit feedback. We begin by proposing $K$-EFCE -- a more generalized\ndefinition that allows players to observe and deviate from the recommended\nactions for $K$ times. The $K$-EFCE includes the EFCE as a special case at\n$K=1$, and is an increasingly stricter notion of equilibrium as $K$ increases.\nWe then design an uncoupled no-regret algorithm that finds an\n$\\varepsilon$-approximate $K$-EFCE within\n$\\widetilde{\\mathcal{O}}(\\max_{i}X_iA_i^{K}/\\varepsilon^2)$ iterations in the\nfull feedback setting, where $X_i$ and $A_i$ are the number of information sets\nand actions for the $i$-th player. Our algorithm works by minimizing a\nwide-range regret at each information set that takes into account all possible\nrecommendation histories. Finally, we design a sample-based variant of our\nalgorithm that learns an $\\varepsilon$-approximate $K$-EFCE within\n$\\widetilde{\\mathcal{O}}(\\max_{i}X_iA_i^{K+1}/\\varepsilon^2)$ episodes of play\nin the bandit feedback setting. When specialized to $K=1$, this gives the first\nsample-efficient algorithm for learning EFCE from bandit feedback.'}, {'When Can We Learn General-Sum Markov Games with a Large Number of\n  Players Sample-Efficiently?': 'Multi-agent reinforcement learning has made substantial empirical progresses\nin solving games with a large number of players. However, theoretically, the\nbest known sample complexity for finding a Nash equilibrium in general-sum\ngames scales exponentially in the number of players due to the size of the\njoint action space, and there is a matching exponential lower bound. This paper\ninvestigates what learning goals admit better sample complexities in the\nsetting of $m$-player general-sum Markov games with $H$ steps, $S$ states, and\n$A_i$ actions per player. First, we design algorithms for learning an\n$\\epsilon$-Coarse Correlated Equilibrium (CCE) in\n$\\widetilde{\\mathcal{O}}(H^5S\\max_{i\\le m} A_i / \\epsilon^2)$ episodes, and an\n$\\epsilon$-Correlated Equilibrium (CE) in\n$\\widetilde{\\mathcal{O}}(H^6S\\max_{i\\le m} A_i^2 / \\epsilon^2)$ episodes. This\nis the first line of results for learning CCE and CE with sample complexities\npolynomial in $\\max_{i\\le m} A_i$. Our algorithm for learning CE integrates an\nadversarial bandit subroutine which minimizes a weighted swap regret, along\nwith several novel designs in the outer loop. Second, we consider the important\nspecial case of Markov Potential Games, and design an algorithm that learns an\n$\\epsilon$-approximate Nash equilibrium within\n$\\widetilde{\\mathcal{O}}(S\\sum_{i\\le m} A_i / \\epsilon^3)$ episodes (when only\nhighlighting the dependence on $S$, $A_i$, and $\\epsilon$), which only depends\nlinearly in $\\sum_{i\\le m} A_i$ and significantly improves over existing\nefficient algorithm in the $\\epsilon$ dependence. Overall, our results shed\nlight on what equilibria or structural assumptions on the game may enable\nsample-efficient learning with many players.'}, {'Analysis of Sequential Quadratic Programming through the Lens of\n  Riemannian Optimization': 'We prove that a ""first-order"" Sequential Quadratic Programming (SQP)\nalgorithm for equality constrained optimization has local linear convergence\nwith rate $(1-1/\\kappa_R)^k$, where $\\kappa_R$ is the condition number of the\nRiemannian Hessian, and global convergence with rate $k^{-1/4}$. Our analysis\nbuilds on insights from Riemannian optimization -- we show that the SQP and\nRiemannian gradient methods have nearly identical behavior near the constraint\nmanifold, which could be of broader interest for understanding constrained\noptimization.'}, {'The generalization error of random features regression: Precise\n  asymptotics and double descent curve': ""Deep learning methods operate in regimes that defy the traditional\nstatistical mindset. Neural network architectures often contain more parameters\nthan training samples, and are so rich that they can interpolate the observed\nlabels, even if the latter are replaced by pure noise. Despite their huge\ncomplexity, the same architectures achieve small generalization error on real\ndata.\n  This phenomenon has been rationalized in terms of a so-called `double\ndescent' curve. As the model complexity increases, the test error follows the\nusual U-shaped curve at the beginning, first decreasing and then peaking around\nthe interpolation threshold (when the model achieves vanishing training error).\nHowever, it descends again as model complexity exceeds this threshold. The\nglobal minimum of the test error is found above the interpolation threshold,\noften in the extreme overparametrization regime in which the number of\nparameters is much larger than the number of samples. Far from being a peculiar\nproperty of deep neural networks, elements of this behavior have been\ndemonstrated in much simpler settings, including linear regression with random\ncovariates.\n  In this paper we consider the problem of learning an unknown function over\nthe $d$-dimensional sphere $\\mathbb S^{d-1}$, from $n$ i.i.d. samples\n$(\\boldsymbol x_i, y_i)\\in \\mathbb S^{d-1} \\times \\mathbb R$, $i\\le n$. We\nperform ridge regression on $N$ random features of the form $\\sigma(\\boldsymbol\nw_a^{\\mathsf T} \\boldsymbol x)$, $a\\le N$. This can be equivalently described\nas a two-layers neural network with random first-layer weights. We compute the\nprecise asymptotics of the test error, in the limit $N,n,d\\to \\infty$ with\n$N/d$ and $n/d$ fixed. This provides the first analytically tractable model\nthat captures all the features of the double descent phenomenon without\nassuming ad hoc misspecification structures.""}, {'On a molecular based Q-tensor model for liquid crystals with density\n  variations': ""In this article, we study the new Q-tensor model previously derived from\nOnsager's molecular theory by Han \\textit{et al.} [Arch. Rational Mech. Anal.,\n215.3 (2014), pp. 741-809] for static liquid crystal modeling. Taking density\nand Q-tensor as order parameters, the new Q-tensor model not only characterizes\nimportant phases while capturing density variation effects, but also remains\ncomputationally tractable and efficient. We report the results of two numerical\napplications of the model, namely the isotropic--nematic--smectic-A--smectic-C\nphase transitions and the isotropic--nematic interface problem, in which\ndensity variations are indispensable. Meanwhile, we show the connections of the\nnew Q-tensor model with classical models including generalized Landau-de Gennes\nmodels, generalized McMillan models, and the Chen-Lubensky model. The new\nQ-tensor model is the pivot and an appropriate trade-off between the classical\nmodels in three scales.""}, {'Learning with convolution and pooling operations in kernel methods': 'Recent empirical work has shown that hierarchical convolutional kernels\ninspired by convolutional neural networks (CNNs) significantly improve the\nperformance of kernel methods in image classification tasks. A widely accepted\nexplanation for their success is that these architectures encode hypothesis\nclasses that are suitable for natural images. However, understanding the\nprecise interplay between approximation and generalization in convolutional\narchitectures remains a challenge. In this paper, we consider the stylized\nsetting of covariates (image pixels) uniformly distributed on the hypercube,\nand characterize exactly the RKHS of kernels composed of single layers of\nconvolution, pooling, and downsampling operations. We use this characterization\nto compute sharp asymptotics of the generalization error for any given function\nin high-dimension. In particular, we quantify the gain in sample complexity\nbrought by enforcing locality with the convolution operation and approximate\ntranslation invariance with average pooling. Notably, these results provide a\nprecise description of how convolution and pooling operations trade off\napproximation with generalization power in one layer convolutional kernels.'}, {'Efficient Phi-Regret Minimization in Extensive-Form Games via Online\n  Mirror Descent': 'A conceptually appealing approach for learning Extensive-Form Games (EFGs) is\nto convert them to Normal-Form Games (NFGs). This approach enables us to\ndirectly translate state-of-the-art techniques and analyses in NFGs to learning\nEFGs, but typically suffers from computational intractability due to the\nexponential blow-up of the game size introduced by the conversion. In this\npaper, we address this problem in natural and important setups for the\n\\emph{$\\Phi$-Hedge} algorithm -- A generic algorithm capable of learning a\nlarge class of equilibria for NFGs. We show that $\\Phi$-Hedge can be directly\nused to learn Nash Equilibria (zero-sum settings), Normal-Form Coarse\nCorrelated Equilibria (NFCCE), and Extensive-Form Correlated Equilibria (EFCE)\nin EFGs. We prove that, in those settings, the \\emph{$\\Phi$-Hedge} algorithms\nare equivalent to standard Online Mirror Descent (OMD) algorithms for EFGs with\nsuitable dilated regularizers, and run in polynomial time. This new connection\nfurther allows us to design and analyze a new class of OMD algorithms based on\nmodifying its log-partition function. In particular, we design an improved\nalgorithm with balancing techniques that achieves a sharp\n$\\widetilde{\\mathcal{O}}(\\sqrt{XAT})$ EFCE-regret under bandit-feedback in an\nEFG with $X$ information sets, $A$ actions, and $T$ episodes. To our best\nknowledge, this is the first such rate and matches the information-theoretic\nlower bound.'}]","Title: Neural Network Approximation of Hierarchical Generative Models for Sampling and Denoising

Abstract:

In this paper, we innovate the theoretical foundation for developing neural network architectures within the realm of hierarchical generative models (GHMs). Our central objective is elucidating how GHMs derived from belief propagation algorithms can be outstandingly well approximated by Convolutional Neural Networks (ConvNets) and U-Nets, a result seldom investigated before. We also establish novel insight into text nesting in language modelling.

Using belief propagation as a basis, we present approximation theorems for both the Bayes classifier and the Bayes denoiser for GHMs with non-negative and normalized ψ functions. We show that ConvNets are capable of closely mimicking this optimal classifier, validating their use in GHM contexts. Furthermore, we demonstrate the underlying encoder-decoder, skip connections, pooling and upsampling structure inherent in ConvNets' design parallels the structure within a U-Net, designed for image denoising. These findings lead to a favorable sample complexity bound for U-Nets learning the Bayes denoiser within GHMs.

For classification and diffusion tasks, we introduce a specialized neural network architecture, U-Nets, showing parallelism with belief propagation's downward and upward processes. This enables efficient structural components in U-Nets to reflect code structure nesting in language, substantiating our theoretical claims.

We contribute an enhanced theoretical framework that facilitates neural network design for complex tasks while utilizing simplistic models in the GHM category. This not only simplifies the utilized architectures but also provides precise bounds for sample complexity and classifier approximability. In essence, this work serves as a foundational step for expanding neural network application scope in treelike structures, with profound implications for sampling, classification, and denoising tasks.

Our research opens a novel pathway for leveraging GHMs for tasks involving complex data structures. This has significant applications in computer vision, natural language processing, and the broader domain of machine learning, with potential for enhancing task efficiency and accuracy."
"Multi-domain recommendation and multi-task recommendation have demonstrated
their effectiveness in leveraging common information from different domains and
objectives for comprehensive user modeling. Nonetheless, the practical
recommendation usually faces multiple domains and tasks simultaneously, which
cannot be well-addressed by current methods. To this end, we introduce M3oE, an
adaptive multi-domain multi-task mixture-of-experts recommendation framework.
M3oE integrates multi-domain information, maps knowledge across domains and
tasks, and optimizes multiple objectives. We leverage three mixture-of-experts
modules to learn common, domain-aspect, and task-aspect user preferences
respectively to address the complex dependencies among multiple domains and
tasks in a disentangled manner. Additionally, we design a two-level fusion
mechanism for precise control over feature extraction and fusion across diverse
domains and tasks. The framework's adaptability is further enhanced by applying
AutoML technique, which allows dynamic structure optimization. To the best of
the authors' knowledge, our M3oE is the first effort to solve multi-domain
multi-task recommendation self-adaptively. Extensive experiments on two
benchmark datasets against diverse baselines demonstrate M3oE's superior
performance. The implementation code is available to ensure reproducibility.","[{'Pattern-Division Multiplexing for Multi-User Continuous-Aperture MIMO': 'In recent years, thanks to the advances in meta-materials, the concept of\ncontinuous-aperture MIMO (CAP-MIMO) is reinvestigated to achieve improved\ncommunication performance with limited antenna apertures. Unlike the classical\nMIMO composed of discrete antennas, CAP-MIMO has a quasi-continuous antenna\nsurface, which is expected to generate any current distribution (i.e., pattern)\nand induce controllable spatial electromagnetic (EM) waves. In this way, the\ninformation is directly modulated on the EM waves, which makes it promising to\napproach the ultimate capacity of finite apertures. The pattern design is the\nkey factor to determine the communication performance of CAP-MIMO, but it has\nnot been well studied in the literature. In this paper, we develop\npattern-division multiplexing (PDM) to design the patterns for CAP-MIMO.\nSpecifically, we first study and model a typical multi-user CAP-MIMO system,\nwhich allows us to formulate the sum-rate maximization problem. Then, we\ndevelop a general PDM technique to transform the design of the continuous\npattern functions to the design of their projection lengths on finite\northogonal bases, which can overcome the challenge of functional programming.\nUtilizing PDM, we further propose a block coordinate descent (BCD) based\npattern design scheme to solve the formulated sum-rate maximization problem.\nSimulation results show that, the sum-rate achieved by the proposed scheme is\nhigher than that achieved by benchmark schemes, which demonstrates the\neffectiveness of the developed PDM for CAP-MIMO.'}, {'Capacity Improvement in Wideband Reconfigurable Intelligent\n  Surface-Aided Cell-Free Network': 'Thanks to the strong ability against the inter-cell interference, cell-free\nnetwork has been considered as a promising technique to improve the network\ncapacity of future wireless systems. However, for further capacity enhancement,\nit requires to deploy more base stations (BSs) with high cost and power\nconsumption. To address the issue, inspired by the recently proposed technique\ncalled reconfigurable intelligent surface (RIS), we propose the concept of\nRIS-aided cell-free network to improve the network capacity with low cost and\npower consumption. Then, for the proposed RIS-aided cell-free network in the\ntypical wideband scenario, we formulate the joint precoding design problem at\nthe BSs and RISs to maximize the network capacity. Due to the non-convexity and\nhigh complexity of the formulated problem, we develop an alternating\noptimization algorithm to solve this challenging problem. Note that most of the\nconsidered scenarios in existing works are special cases of the general\nscenario in this paper, and the proposed joint precoding framework can also\nserve as a general solution to maximize the capacity in most of existing\nRIS-aided scenarios. Finally, simulation results verify that, compared with the\nconventional cell-free network, the network capacity of the proposed scheme can\nbe improved significantly.'}, {'Pattern-Division Multiplexing for Continuous-Aperture MIMO': 'In recent years, continuous-aperture multiple-input multiple-output\n(CAP-MIMO) is reinvestigated to achieve improved communication performance with\nlimited antenna apertures. Unlike the classical MIMO composed of discrete\nantennas, CAP-MIMO has a continuous antenna surface, which is expected to\ngenerate any current distribution (i.e., pattern) and induce controllable\nspatial electromagnetic waves. In this way, the information can be modulated on\nthe electromagnetic waves, which makes it promising to approach the ultimate\ncapacity of finite apertures. The pattern design for CAP-MIMO is the key factor\nto determine the communication performance, but it has not been well studied in\nthe literature. In this paper, we propose the pattern-division multiplexing to\ndesign the patterns for CAP-MIMO. Specifically, we first derive the system\nmodel of a typical multi-user CAP-MIMO system, which allows us to formulate the\nsum-rate maximization problem. Then, we propose a general pattern-division\nmultiplexing technique to transform the design of continuous pattern functions\nto the design of their projection lengths on finite orthogonal bases. Based on\nthis technique, we further propose a pattern design scheme to solve the\nformulated sum-rate maximization problem. Simulation results show that, the\nsum-rate achieved by the proposed scheme is about 260% higher than that\nachieved by the benchmark scheme.'}, {'Reconfigurable Intelligent Surfaces for 6G: Nine Fundamental Issues and\n  One Critical Problem': ""Thanks to the recent advances in metamaterials, reconfigurable intelligent\nsurface (RIS) has emerged as a promising technology for future 6G wireless\ncommunications. Benefiting from its high array gain, low cost, and low power\nconsumption, RISs are expected to greatly enlarge signal coverage, improve\nsystem capacity, and increase energy efficiency. In this article, we\nsystematically overview the emerging RIS technology with the focus on its key\nbasics, nine fundamental issues, and one critical problem. Specifically, we\nfirst explain the RIS basics, including its working principles, hardware\nstructures, and potential benefits for communications. Based on these basics,\nnine fundamental issues of RISs, such as ``What's the differences between RISs\nand massive MIMO?'' and ``Is RIS really intelligent?'', are explicitly\naddressed to elaborate its technical features, distinguish it from existing\ntechnologies, and clarify some misunderstandings in the literature. Then, one\ncritical problem of RISs is revealed that, due to the ``multiplicative fading''\neffect, existing passive RISs can hardly achieve visible performance gains in\nmany communication scenarios with strong direct links. To address this critical\nproblem, a potential solution called active RISs is introduced, and its\neffectiveness is demonstrated by numerical simulations.""}, {'A joint precoding framework for wideband reconfigurable intelligent\n  surface-aided cell-free network': 'Thanks to the strong ability against the inter-cell interference, cell-free\nnetwork is considered as a promising technique to improve network capacity.\nHowever, further capacity improvement requires to deploy more base stations\n(BSs) with high cost and power consumption. To address this issue, inspired by\nthe recently developed reconfigurable intelligent surface (RIS) technique, we\npropose the concept of RIS-aided cell-free network to improve the capacity with\nlow cost and power consumption. The key idea is to replace some of the required\nBSs by low-cost and energy-efficient RISs. Then, in a wideband RIS-aided\ncell-free network, we formulate the problem of joint precoding design at BSs\nand RISs to maximize the network capacity. Due to the non-convexity and high\ncomplexity of the formulated problem, we develop an alternating optimization\nframework to solve this challenging problem. In particular, we decouple this\nproblem via fractional programming, and solve the subproblems alternatively.\nNote that most of the scenarios considered in existing works are special cases\nof the general scenario studied in this paper, and the proposed joint precoding\nframework can serve as a general solution to maximize the capacity in most\nexisting RIS-aided scenarios. Finally, simulation results demonstrate that,\ncompared with the conventional cell-free network, the network capacity under\nthe proposed scheme can be improved significantly.'}, {'Bifurcation and chaos for a new model of trigonal centrifugal governor\n  with nonsmooth control': 'The flywheel ball and hexagonal structures in the design of the classical\ncentrifugal governor systems lead to both modeling and analytical difficulties.\nIn the present paper, a new trigonal centrifugal governor is proposed in an\nattempt to overcome both of these difficulties by introducing the radical\nnonlinearity and nonsmooth control strategy in the simple and clear formula.\nThe nonlinear dynamical behaviors of this new model are investigated for both\nthe autonomous and the non-autonomous cases. The three equations of motion of\nthe TCG are presented based on Euler-Lagrange equation and the theorem of\nangular momentum. The velocity, nonlinear restoring force and nonsmooth torque\nsurfaces are plotted to display the complex relationship of parameter change\ndependence. Secondely, the equilibrium bifurcation and stability analysis for\nautonomous system are investigated to show the pitchork bifurcation phenomena\nand the saddle-focus point respectively. It is found that the system bears\nsignificant similarities to the Duffing system with mono-stable and bi-stable\ncharacteristic. Finally, the three-dimensional Melnikov method is defined and\nemployed to obtained the analytical chaotic thresholds for non-autonomous\ncentrifugal governor system, and numerical results of chaotic behaviors verify\nthe proposed theoretical criteria of the non-autonomous system. The\nexperimental studies are carried out to validate the theoretical and numerical\nresults.'}, {'Nonlinear energy harvesting system with multiple stability': 'The nonlinear energy harvesting systems of the forced vibration with an\nelectron-mechanical coupling are widely used to capture ambient vibration\nenergy and convert mechanical energy into electrical energy. However, the\nnonlinear response mechanism of the friction induced vibration (FIV) energy\nharvesting system with multiple stability and stick-slip motion is still\nunclear. In the current paper, a novel nonlinear energy harvesting model with\nmultiple stability of single-, double- and triple-well potential is proposed\nbased on V-shaped structure spring and the belt conveying system. The dynamic\nequations for the energy harvesting system with multiple stability and\nself-excited friction are established by using Euler-Lagrangian equations.\nSecondly, the nonlinear restoring force, friction force, and potential energy\nsurfaces for static characteristics of the energy harvesting system are\nobtained to show the nonlinear varying stiffness, multiple equilibrium points,\ndiscontinuous behaviors and multiple well response. Then, the equilibrium\nsurface of bifurcation sets of the autonomous system is given to show the\nthird-order quasi zero stiffness (QZS3), fifth-order quasi zero stiffness\n(QZS5), double well (DW) and triple well (TW). Furthermore, the response\namplitudes of charge, current, voltage and power of the forced\nelectron-mechanical coupled vibration system for QZS3, QZS5, DW and TW are\nanalyzed by using the numerically solution. Finally, a prototype of FIV energy\nharvesting system is manufactured and the experimental system is setup. The\nexperimental work of static restoring force, damping force and electrical\noutput are well agreeable with the numerical results, which testified the\nproposed FIV energy harvesting model.'}, {'Nonlinear vibration of a dipteran flight robot system with rotational\n  geometric nonlinearity': 'The dipteran flight mechanism of the insects is commonly used to design the\nnonlinear flight robot system. However, the dynamic response of the click\nmechanism of the nonlinear robot system with multiple stability still unclear.\nIn this paper, a novel dipteran robot model with click mechanism proposed based\non the multiple stability of snap-through buckling. The motion of equation of\nthe nonlinear flight robot system is obtained by using the Euler-Lagrange\nequation. The nonlinear potential energy, the elastic force, equilibrium\nbifurcation, as well as equilibrium stability are investigated to show the\nmultiple stability characteristics. The transient sets of bifurcation and\npersistent set of regions in the system parameter plane and the corresponding\nphase portraits are obtained with multiple stability of single and double well\nbehaviors. Then, the periodic free vibration response are defined by the\nanalytical solution of three kinds of elliptical functions, as well as the\namplitude frequency responses are investigated by numerical integration. Based\non the topological equivalent method, the chaotic thresholds of the homo-clinic\norbits for the chaotic vibration of harmonic forced robot system are derived to\nshow the chaotic parametric condition. Finally, the prototype of nonlinear\nflapping robot is manufactured and the experimental system is setup. The\nnonlinear static moment of force curves, periodic response and dynamic flight\nvibration of dipteran robot system are carried out. It is shown that the test\nresults are agree well with the theoretical analysis and numerical simulation.\nThose result have the potential application for the structure design of the\nefficient flight robot.'}, {'Contextual Local Explanation for Black Box Classifiers': 'We introduce a new model-agnostic explanation technique which explains the\nprediction of any classifier called CLE. CLE gives an faithful and\ninterpretable explanation to the prediction, by approximating the model locally\nusing an interpretable model. We demonstrate the flexibility of CLE by\nexplaining different models for text, tabular and image classification, and the\nfidelity of it by doing simulated user experiments.'}, {'SparCAssist: A Model Risk Assessment Assistant Based on Sparse Generated\n  Counterfactuals': ""We introduce SparcAssist, a general-purpose risk assessment tool for the\nmachine learning models trained for language tasks. It evaluates models' risk\nby inspecting their behavior on counterfactuals, namely out-of-distribution\ninstances generated based on the given data instance. The counterfactuals are\ngenerated by replacing tokens in rational subsequences identified by ExPred,\nwhile the replacements are retrieved using HotFlip or\nMasked-Language-Model-based algorithms. The main purpose of our system is to\nhelp the human annotators to assess the model's risk on deployment. The\ncounterfactual instances generated during the assessment are the by-product and\ncan be used to train more robust NLP models in the future.""}]","ABSTRACT

Understanding and accurately predicting user preferences for multiple domains and tasks has emerged as an essential component of recommendation systems. This paper introduces M3oE, a Multi-Domain Multi-Task (MDMT) Mixture-of-Experts recommendation framework, addressing the challenges of underperforming in different domains and tasks simultaneously. The core innovation of M3oE lies in its self-adaptive two-level fusion mechanism, which disentangles domain and task effects and unifies the experts' outputs, aligning closely with data and task-specific characteristics. Experimental results on MovieLens and KuaiRand-Pure datasets vividly display M3oE’s superior performance and adaptability across various settings compared to state-of-the-art methods for MDC and MTT recommendations. By leveraging distinct abilities across domains and tasks, M3oE shows a remarkable reduction in prediction errors and enhances the satisfactory outcomes in multi-scenario applications. This comprehensive framework demonstrates substantial contributions, advancing the state of the art in MDMT recommender systems and opening promising future prospects for practical deployment in tech-driven industries."
"Despite the surprisingly high intelligence exhibited by Large Language Models
(LLMs), we are somehow intimidated to fully deploy them into real-life
applications considering their black-box nature. Concept-based explanations
arise as a promising avenue for explaining what the LLMs have learned, making
them more transparent to humans. However, current evaluations for concepts tend
to be heuristic and non-deterministic, e.g. case study or human evaluation,
hindering the development of the field. To bridge the gap, we approach
concept-based explanation evaluation via faithfulness and readability. We first
introduce a formal definition of concept generalizable to diverse concept-based
explanations. Based on this, we quantify faithfulness via the difference in the
output upon perturbation. We then provide an automatic measure for readability,
by measuring the coherence of patterns that maximally activate a concept. This
measure serves as a cost-effective and reliable substitute for human
evaluation. Finally, based on measurement theory, we describe a meta-evaluation
method for evaluating the above measures via reliability and validity, which
can be generalized to other tasks as well. Extensive experimental analysis has
been conducted to validate and inform the selection of concept evaluation
measures.","[{'Existence of normalized solutions for fractional coupled Hartree-Fock\n  type system': 'In this paper, we consider the existence of solutions for the following\nfractional coupled Hartree-Fock type system \\begin{align*}\n\\left\\{\\begin{aligned} &(-\\Delta)^s u+V_1(x)u+\\lambda_1u=\\mu_1(I_{\\alpha}\\star\n|u|^p)|u|^{p-2}u+\\beta(I_{\\alpha}\\star |v|^r)|u|^{r-2}u\\\\ &(-\\Delta)^s\nv+V_2(x)v+\\lambda_2v=\\mu_2(I_{\\alpha}\\star\n|v|^q)|v|^{q-2}v+\\beta(I_{\\alpha}\\star |u|^r)|v|^{r-2}v \\end{aligned}\n\\right.~\\quad x\\in\\mathbb{R}^N, \\end{align*} under the constraint\n\\begin{align*} \\int_{\\mathbb{R}^N}|u|^2=a^2,~\\int_{\\mathbb{R}^N}|v|^2=b^2.\n\\end{align*} where\n$s\\in(0,1),~N\\ge3,~\\mu_1>0,~\\mu_2>0,~\\beta>0,~\\alpha\\in(0,N),~1+\\frac{\\alpha}{N}<p,~q,~r<\\frac{N+\\alpha}{N-2s}$\nand $I_{\\alpha}(x)=|x|^{\\alpha-N}$. Under some restrictions of $N,\\alpha,p,q$\nand $r$, we give the positivity of normalized solutions for $p,q,r\\le\n1+\\frac{\\alpha+2s}{N}$.'}, {'Limits of Sums for Binomial and Eulerian Numbers and their Associated\n  Distributions': 'We provide a unified, probabilistic approach using renewal theory to derive\nsome novel limits of sums for the normalized binomial coefficients and for the\nnormalized Eulerian numbers. We also investigate some corresponding results for\ntheir associated distributions -- the binomial distributions for the binomial\ncoefficients and the Irwin-Hall distributions (uniform B-splines) for the\nEulerian numbers.'}, {'Learning Asymmetric and Local Features in Multi-Dimensional Data through\n  Wavelets with Recursive Partitioning': 'Effective learning of asymmetric and local features in images and other data\nobserved on multi-dimensional grids is a challenging objective critical for a\nwide range of image processing applications involving biomedical and natural\nimages. It requires methods that are sensitive to local details while fast\nenough to handle massive numbers of images of ever increasing sizes. We\nintroduce a probabilistic model-based framework that achieves these objectives\nby incorporating adaptivity into discrete wavelet transforms (DWT) through\nBayesian hierarchical modeling, thereby allowing wavelet bases to adapt to the\ngeometric structure of the data while maintaining the high computational\nscalability of wavelet methods---linear in the sample size (e.g., the\nresolution of an image). We derive a recursive representation of the Bayesian\nposterior model which leads to an exact message passing algorithm to complete\nlearning and inference. While our framework is applicable to a range of\nproblems including multi-dimensional signal processing, compression, and\nstructural learning, we illustrate its work and evaluate its performance in the\ncontext of image reconstruction using real images from the ImageNet database,\ntwo widely used benchmark datasets, and a dataset from retinal optical\ncoherence tomography and compare its performance to state-of-the-art methods\nbased on basis transforms and deep learning.'}, {'Bayesian Detection of Image Boundaries': 'Detecting boundary of an image based on noisy observations is a fundamental\nproblem of image processing and image segmentation. For a $d$-dimensional image\n($d = 2, 3, \\ldots$), the boundary can often be described by a closed smooth\n$(d - 1)$-dimensional manifold. In this paper, we propose a nonparametric\nBayesian approach based on priors indexed by $\\mathbb{S}^{d - 1}$, the unit\nsphere in $\\mathbb{R}^d$. We derive optimal posterior contraction rates using\nGaussian processes or finite random series priors using basis functions such as\ntrigonometric polynomials for 2-dimensional images and spherical harmonics for\n3-dimensional images. For 2-dimensional images, we show a rescaled squared\nexponential Gaussian process on $\\mathbb{S}^1$ achieves four goals of\nguaranteed geometric restriction, (nearly) minimax rate optimal and adaptive to\nthe smoothness level, convenient for joint inference and computationally\nefficient. We conduct an extensive study of its reproducing kernel Hilbert\nspace, which may be of interest by its own and can also be used in other\ncontexts. Simulations confirm excellent performance of the proposed method and\nindicate its robustness under model misspecification at least under the\nsimulated settings.'}, {'End to End Generative Meta Curriculum Learning For Medical Data\n  Augmentation': ""Current medical image synthetic augmentation techniques rely on intensive use\nof generative adversarial networks (GANs). However, the nature of GAN\narchitecture leads to heavy computational resources to produce synthetic images\nand the augmentation process requires multiple stages to complete. To address\nthese challenges, we introduce a novel generative meta curriculum learning\nmethod that trains the task-specific model (student) end-to-end with only one\nadditional teacher model. The teacher learns to generate curriculum to feed\ninto the student model for data augmentation and guides the student to improve\nperformance in a meta-learning style. In contrast to the generator and\ndiscriminator in GAN, which compete with each other, the teacher and student\ncollaborate to improve the student's performance on the target tasks. Extensive\nexperiments on the histopathology datasets show that leveraging our framework\nresults in significant and consistent improvements in classification\nperformance.""}, {'Experimental realization of state transfer by quantum walks with two\n  coins': 'Quantum state transfer between different sites is a significant problem for\nquantum networks and quantum computers. By selecting quantum walks with two\ncoins as the basic model and two coin spaces as the communication carriers, we\nsuccessfully implement quantum state transfer on various graphs (EPL,\n\\textbf{124} (2018) 60009) \\cite{Shang_2019}. Here, we demonstrate the\nexperimental implementation of this scheme using IBM quantum experience\nplatform. In particular, we show the transfer of Bell state, GHZ state and W\nstate on complete graph on the quantum device. Also, we observe that our\nprotocol has high fidelity by preforming quantum state tomography.'}, {'Evolution of Quantum Resources in Quantum-walk-based Search Algorithm': 'Quantum walk is fundamental to designing many quantum algorithms. Here we\nconsider the effects of quantum coherence and quantum entanglement for the\nquantum walk search on the complete bipartite graph. First, we numerically show\nthe complementary relationship between the success probability and the two\nquantum resources (quantum coherence and quantum entanglement). We also provide\ntheoretical analysis in the asymptotic scenarios. At last, we discuss the role\nplayed by generalized depolarizing noises and find that it would influence the\ndynamics of success probability and quantum coherence sharply, which is\ndemonstrated by theoretical derivation and numerical simulation.'}, {'Standardization of multivariate Gaussian mixture models and background\n  adjustment of PET images in brain oncology': 'In brain oncology, it is routine to evaluate the progress or remission of the\ndisease based on the differences between a pre-treatment and a post-treatment\nPositron Emission Tomography (PET) scan. Background adjustment is necessary to\nreduce confounding by tissue-dependent changes not related to the disease. When\nmodeling the voxel intensities for the two scans as a bivariate Gaussian\nmixture, background adjustment translates into standardizing the mixture at\neach voxel, while tumor lesions present themselves as outliers to be detected.\nIn this paper, we address the question of how to standardize the mixture to a\nstandard multivariate normal distribution, so that the outliers (i.e., tumor\nlesions) can be detected using a statistical test. We show theoretically and\nnumerically that the tail distribution of the standardized scores is favorably\nclose to standard normal in a wide range of scenarios while being conservative\nat the tails, validating voxelwise hypothesis testing based on standardized\nscores. To address standardization in spatially heterogeneous image data, we\npropose a spatial and robust multivariate expectation-maximization (EM)\nalgorithm, where prior class membership probabilities are provided by\ntransformation of spatial probability template maps and the estimation of the\nclass mean and covariances are robust to outliers. Simulations in both\nunivariate and bivariate cases suggest that standardized scores with soft\nassignment have tail probabilities that are either very close to or more\nconservative than standard normal. The proposed methods are applied to a real\ndata set from a PET phantom experiment, yet they are generic and can be used in\nother contexts.'}, {'Scalable Double Regularization for 3D Nano-CT Reconstruction': 'Nano-CT (computerized tomography) has emerged as a non-destructive\nhigh-resolution cross-sectional imaging technique to effectively study the\nsub-$\\mu$m pore structure of shale, which is of fundamental importance to the\nevaluation and development of shale oil and gas. Nano-CT poses unique\nchallenges to the inverse problem of reconstructing the 3D structure due to the\nlower signal-to-noise ratio (than Micro-CT) at the nano-scale, increased\nsensitivity to the misaligned geometry caused by the movement of object\nmanipulator, limited sample size, and a larger volume of data at higher\nresolution. In this paper, we propose a scalable double regularization (SDR)\nmethod to utilize the entire dataset for simultaneous 3D structural\nreconstruction across slices through total variation regularization within\nslices and $L_1$ regularization between adjacent slices. SDR allows information\nborrowing both within and between slices, contrasting with the traditional\nmethods that usually build on slice by slice reconstruction. We develop a\nscalable and memory-efficient algorithm by exploiting the systematic sparsity\nand consistent geometry induced by such Nano-CT data. We illustrate the\nproposed method using synthetic data and two Nano-CT imaging datasets of\nJiulaodong (JLD) shale and Longmaxi (LMX) shale acquired in the Sichuan Basin.\nThese numerical experiments show that the proposed method substantially\noutperforms selected alternatives both visually and quantitatively.'}, {'BayesBD: An R Package for Bayesian Inference on Image Boundaries': 'We present the BayesBD package providing Bayesian inference for boundaries of\nnoisy images. The BayesBD package implements flexible Gaussian process priors\nindexed by the circle to recover the boundary in a binary or Gaussian noised\nimage, with the benefits of guaranteed geometric restrictions on the estimated\nboundary, (nearly) minimax optimal and smoothness adaptive convergence rates,\nand convenient joint inferences under certain assumptions. The core sampling\ntasks for our model have linear complexity, and our implementation in c++ using\npackages Rcpp and RcppArmadillo is computationally efficient. Users can access\nthe full functionality of the package in both Rgui and the corresponding shiny\napplication. Additionally, the package includes numerous utility functions to\naid users in data preparation and analysis of results. We compare BayesBD with\nselected existing packages using both simulations and real data applications,\nand demonstrate the excellent performance and flexibility of BayesBD even when\nthe observation contains complicated structural information that may violate\nits assumptions.'}]","Artificial Intelligence (AI) increasingly powers numerous aspects of society, yet the mechanisms driving AI decision-making remain opaque, presenting challenges in practical contexts like performance enhancement, security, and interpretability. To address this, Explainable Artificial Intelligence (XAI) has emerged, aiming to illuminate the internal workings of machine learning models, especially pre-trained language models. We introduce novel evaluation measures for concept-based explanations, crucial for understanding and interpreting AI outputs. Our contribution lies in formulating measurable ""faithfulness"" and ""readability,"" providing a rigorous framework for evaluating these explanations. 

Critically, we propose concept perturbation to test faithfulness by measuring output differences before and after concept changes. We then assessed twelve explanation methods across faithfulness and readability dimensions, proposing a suite of Constructs like Intrinsically-Represented Concept (IN-EmbDist, IN-EmbCos), Externally-Represented Concept (OUT-EmbDist, OUT-EmbCos), and User-specific Concept (IN-UCI, IN-Umass) to probe AI model understanding and explanations.

Innovatively, our evaluation measures quantify the faithfulness and readability against diverse explanation methods, ensuring explanatory models are both accurate (faithful) and understandable (readable). Empirical results showed that our measures were significantly more effective than existing approaches, delivering comparable performance with higher efficiency.

Our work advances the practical application of XAI, enabling users to gain more trust and leverage from AI systems. By providing a standardized, quantitative evaluation, our study opens avenues for improving the explainability of AI models, enhancing human-AI collaboration in critical domains such as healthcare, finance, and legal.

In summary, the paper introduces a pioneering approach to systematically assess concept-based explanations in XAI, offering a comprehensive toolkit that combines faithfulness, readability, and efficiency – making it an essential read for researchers and practitioners in AI, machine learning, and AI ethics."
"The intermolecular potential plays crucial roles in real-fluid interactions
away from the ideal-gas equilibrium, such as supercritical fluid, high-enthalpy
fluid, plasma interactions, etc. We propose a Boltzmann-weighted
Full-dimensional (BWF) potential model for real-fluid computations. It includes
diverse intermolecular interactions so as to determine the potential well,
molecular diameter, dipole moment, polarizability of species without
introducing bath gases, allowing more accurate descriptions of potential
surfaces with more potential parameters. The anisotropy and temperature
dependence of potential parameters are also considered by applying the
Boltzmann weighting on all orientations. Through the high-level
Symmetry-Adapted Perturbation Theory calculations, full-dimensional potential
energy surface datasets are obtained in 432 orientations for each species.
Subsequently, the Boltzmann-weighted Full-dimensional potential parameters are
derived by training the dataset exceeding 5*106 data, including nonpolar and
polar molecules, radicals, long-chain molecules, and ions. These BWF transport
properties calculated by the BWF potential have been compared against the
Lennard-Jones transport properties as well as experimental viscosity, mass
diffusivity, and thermal conductivity coefficients. It shows discrepancies of
viscosity coefficients within 1% and 5% for nonpolar and polar molecules,
respectively. Furthermore, this potential model is applied to study radicals,
long-chain molecules, and ions, for which the experimental data is rarely
accessed in high accuracy. It indicates significant prediction improvements of
complex interactions between various particles. The new transport properties
are also embedded to predict the laminar flame speeds and the flame extinction
limits of methane, dimethyl ether, and n-heptane at elevated pressures,
confirming its predictivity and effectiveness.","[{'Drawing complete multipartite graphs on the plane with restrictions on\n  crossings': 'We introduce the concept of NIC-planar graphs and present the full\ncharacterization of NIC-planar complete k-partite graphs.'}, {'Equitable vertex arboricity of planar graphs': 'Let $G_1$ be a planar graph such that all cycles of length at most 4 are\nindependent and let $G_2$ be a planar graph without 3-cycles and adjacent\n4-cycles. It is proved that the set of vertices of $G_1$ and $G_2$ can be\nequitably partitioned into $t$ subsets for every $t\\geq 3$ so that each subset\ninduces a forest. These results partially confirm a conjecture of Wu, Zhang and\nLi.'}, {""Hypotheses regarding Baxter's $T-Q$ relation for the periodic XYZ chain"": ""Baxter's $T-Q$ relation for the periodic spin-$\\frac12$ XYZ chain is studied.\nWe extensively perform numerical calculations for the $T-Q$ relation and the\nBethe ansatz equations. Numerical based hypotheses are then proposed to answer\nsome open questions regarding Baxter's $T-Q$ relation and the XYZ chain.""}, {'Class two 1-planar graphs with maximum degree six or seven': 'A graph is 1-planar if it can be drawn on the plane so that each edge is\ncrossed by at most one other edge. In this note we give examples of class two\n1-planar graphs with maximum degree six or seven.'}, {'List total coloring of pseudo-outerplanar graphs': 'A graph is pseudo-outerplanar if each of its blocks has an embedding in the\nplane so that the vertices lie on a fixed circle and the edges lie inside the\ndisk of this circle with each of them crossing at most one another. It is\nproved that every pseudo-outerplanar graph with maximum degree \\Delta\\geq 5 is\ntotally (\\Delta+1)-choosable.'}, {'On the Local-Global Principle for Integral Apollonian-3 Circle Packings': 'In this paper we study the integral properties of Apollonian-3 circle\npackings, which are variants of the standard Apollonian circle packings.\nSpecifically, we study the reduction theory, formulate a local-global\nconjecture, and prove a density one version of this conjecture. Along the way,\nwe prove a uniform spectral gap for congruence towers of the symmetry group.'}, {'The edge chromatic number of outer-1-planar graphs': 'A graph is outer-1-planar if it can be drawn in the plane so that all\nvertices are on the outer face and each edge is crossed at most once. In this\npaper, we completely determine the edge chromatic number of outer 1-planar\ngraphs.'}, {'The gap distribution of directions in some Schottky groups': ""We prove the existence and some properties of the limiting gap distribution\nfunctions for the directions of orbits of some infinite covolume subgroups of\n$Isom(\\mathbb{H}^2)$ in the Poincar\\'e disk.""}, {'The $\\mathcal{R}-$bounded operator families arising from the study of\n  the barotropic compressible flows with free surface': ""In this paper, we study some model problem associated to the free boundary\nvalue problem of the barotropic compressible Navier-Stokes equations in general\nsmooth domain with taking surface tension into account. To obtain the maximal\n$L_p-L_q$ regularity property of the model problem, we prove the existence of\n$\\mathcal{R}-$bounded operator families of the resolvent problem via Weis'\ntheory on operator valued Fourier multipliers.""}, {'Coupled Quintessence in a Power-Law Case and the Cosmic Coincidence\n  Problem': 'The problem of the cosmic coincidence is a longstanding puzzle. This\nconundrum may be solved by introducing a coupling between the two dark sectors.\nIn this Letter, we study a coupled quintessence scenario in which the scalar\nfield evolves in a power law potential and the mass of dark matter particles\ndepends on a power law function of $\\phi$. It is shown that this scenario has a\nstable attractor solution and can thus provide a natural solution to the cosmic\ncoincidence problem.'}]","Title: Comprehensive Real-Fluid Potential Model for Accurate Thermophysical Property Predictions in Extreme Environments

Abstract:

Against the backdrop of rapid advancements in combustion engine and chemical process technologies, accurate prediction of thermophysical properties within the bounds of extreme temperature and pressure conditions is vital for enhancing operational efficiency and safety. This research introduces an innovative Boltzmann-weighted Full-dimensional Potential (BWF) model, which utilizes the former to develop precise transport coefficients to supplement the latter. The study positions itself as a pioneering comprehensive approach to account for the intricate electrostatic, induction, and dispersion interactions, thereby significantly improving upon traditional Lennard-Jones potential models.

The BWF model, directly derived from the Enskog theory, represents an advancement by complementing basic potential models with an extensive suite of interaction corrections. This results in remarkable improvement down to 1-600 atmospheres in pressures and 300-2400 Kelvins in temperatures, showcasing superior predictive accuracy for both non-polar and polar gases, radicals, long-chain molecules, and ions.

The methodology involves stringent validation against a range of experimental data, including flame speed measurements and flame extinction limits for various hydrocarbons and mixtures. The model’s accuracy manifests in its superior agreement with experimental flame speeds, with deviations contrary to other Lennard-Jones-based methods.

Contributing to the field, the BWF model introduces a novel modeling strategy based on the Boltzmann-weighted Full-dimensional Potential, which comprehensively evaluates complex intermolecular interactions, thereby enriching computational chemistry and chemical engineering models. This advancements holds significant implications for enhancing environmental sustainability and industrial productivity by providing a robust theoretical foundation for understanding complex chemical processes under extreme conditions.


This research not only delivers a superior tool for predicting thermophysical properties, but also broadens the horizon for both academic inquiry into intermolecular interactions and practical applications in energy efficiency and safety measures."
"As Large Language Models (LLMs) have become more advanced, they have outpaced
our abilities to accurately evaluate their quality. Not only is finding data to
adequately probe particular model properties difficult, but evaluating the
correctness of a model's freeform generation alone is a challenge. To address
this, many evaluations now rely on using LLMs themselves as judges to score the
quality of outputs from other LLMs. Evaluations most commonly use a single
large model like GPT4. While this method has grown in popularity, it is costly,
has been shown to introduce intramodel bias, and in this work, we find that
very large models are often unnecessary. We propose instead to evaluate models
using a Panel of LLm evaluators (PoLL). Across three distinct judge settings
and spanning six different datasets, we find that using a PoLL composed of a
larger number of smaller models outperforms a single large judge, exhibits less
intra-model bias due to its composition of disjoint model families, and does so
while being over seven times less expensive.","[{'Simultaneously Linking Entities and Extracting Relations from Biomedical\n  Text Without Mention-level Supervision': 'Understanding the meaning of text often involves reasoning about entities and\ntheir relationships. This requires identifying textual mentions of entities,\nlinking them to a canonical concept, and discerning their relationships. These\ntasks are nearly always viewed as separate components within a pipeline, each\nrequiring a distinct model and training data. While relation extraction can\noften be trained with readily available weak or distant supervision, entity\nlinkers typically require expensive mention-level supervision -- which is not\navailable in many domains. Instead, we propose a model which is trained to\nsimultaneously produce entity linking and relation decisions while requiring no\nmention-level annotations. This approach avoids cascading errors that arise\nfrom pipelined methods and more accurately predicts entity relationships from\ntext. We show that our model outperforms a state-of-the art entity linking and\nrelation extraction pipeline on two biomedical datasets and can drastically\nimprove the overall recall of the system.'}, {'Faithful to the Document or to the World? Mitigating Hallucinations via\n  Entity-linked Knowledge in Abstractive Summarization': ""Despite recent advances in abstractive summarization, current summarization\nsystems still suffer from content hallucinations where models generate text\nthat is either irrelevant or contradictory to the source document. However,\nprior work has been predicated on the assumption that any generated facts not\nappearing explicitly in the source are undesired hallucinations. Methods have\nbeen proposed to address this scenario by ultimately improving `faithfulness'\nto the source document, but in reality, there is a large portion of entities in\nthe gold reference targets that are not directly in the source. In this work,\nwe show that these entities are not aberrations, but they instead require\nutilizing external world knowledge to infer reasoning paths from entities in\nthe source. We show that by utilizing an external knowledge base, we can\nimprove the faithfulness of summaries without simply making them more\nextractive, and additionally, we show that external knowledge bases linked from\nthe source can benefit the factuality of generated summaries.""}, {'To Adapt or to Annotate: Challenges and Interventions for Domain\n  Adaptation in Open-Domain Question Answering': 'Recent advances in open-domain question answering (ODQA) have demonstrated\nimpressive accuracy on standard Wikipedia style benchmarks. However, it is less\nclear how robust these models are and how well they perform when applied to\nreal-world applications in drastically different domains. While there has been\nsome work investigating how well ODQA models perform when tested for\nout-of-domain (OOD) generalization, these studies have been conducted only\nunder conservative shifts in data distribution and typically focus on a single\ncomponent (ie. retrieval) rather than an end-to-end system. In response, we\npropose a more realistic and challenging domain shift evaluation setting and,\nthrough extensive experiments, study end-to-end model performance. We find that\nnot only do models fail to generalize, but high retrieval scores often still\nyield poor answer prediction accuracy. We then categorize different types of\nshifts and propose techniques that, when presented with a new dataset, predict\nif intervention methods are likely to be successful. Finally, using insights\nfrom this analysis, we propose and evaluate several intervention methods which\nimprove end-to-end answer F1 score by up to 24 points.'}, {'Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive\n  Autoencoders': 'We introduce deep inside-outside recursive autoencoders (DIORA), a\nfully-unsupervised method for discovering syntax that simultaneously learns\nrepresentations for constituents within the induced tree. Our approach predicts\neach word in an input sentence conditioned on the rest of the sentence and uses\ninside-outside dynamic programming to consider all possible binary trees over\nthe sentence. At test time the CKY algorithm extracts the highest scoring\nparse. DIORA achieves a new state-of-the-art F1 in unsupervised binary\nconstituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI.'}, {'Reasoning Over Virtual Knowledge Bases With Open Predicate Relations': 'We present the Open Predicate Query Language (OPQL); a method for\nconstructing a virtual KB (VKB) trained entirely from text. Large Knowledge\nBases (KBs) are indispensable for a wide-range of industry applications such as\nquestion answering and recommendation. Typically, KBs encode world knowledge in\na structured, readily accessible form derived from laborious human annotation\nefforts. Unfortunately, while they are extremely high precision, KBs are\ninevitably highly incomplete and automated methods for enriching them are far\ntoo inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set\nof relation mentions in a way that naturally enables reasoning and can be\ntrained without any structured supervision. We demonstrate that OPQL\noutperforms prior VKB methods on two different KB reasoning tasks and,\nadditionally, can be used as an external memory integrated into a language\nmodel (OPQL-LM) leading to improvements on two open-domain question answering\ntasks.'}, {'Facts as Experts: Adaptable and Interpretable Neural Memory over\n  Symbolic Knowledge': 'Massive language models are the core of modern NLP modeling and have been\nshown to encode impressive amounts of commonsense and factual information.\nHowever, that knowledge exists only within the latent parameters of the model,\ninaccessible to inspection and interpretation, and even worse, factual\ninformation memorized from the training corpora is likely to become stale as\nthe world changes. Knowledge stored as parameters will also inevitably exhibit\nall of the biases inherent in the source materials. To address these problems,\nwe develop a neural language model that includes an explicit interface between\nsymbolically interpretable factual information and subsymbolic neural\nknowledge. We show that this model dramatically improves performance on two\nknowledge-intensive question-answering tasks. More interestingly, the model can\nbe updated without re-training by manipulating its symbolic representations. In\nparticular this model allows us to add new facts and overwrite existing ones in\nways that are not possible for earlier models.'}, {'QA Is the New KR: Question-Answer Pairs as Knowledge Bases': 'In this position paper, we propose a new approach to generating a type of\nknowledge base (KB) from text, based on question generation and entity linking.\nWe argue that the proposed type of KB has many of the key advantages of a\ntraditional symbolic KB: in particular, it consists of small modular\ncomponents, which can be combined compositionally to answer complex queries,\nincluding relational queries and queries involving ""multi-hop"" inferences.\nHowever, unlike a traditional KB, this information store is well-aligned with\ncommon user information needs.'}, {'Multilingual Fact Linking': 'Knowledge-intensive NLP tasks can benefit from linking natural language text\nwith facts from a Knowledge Graph (KG). Although facts themselves are\nlanguage-agnostic, the fact labels (i.e., language-specific representation of\nthe fact) in the KG are often present only in a few languages. This makes it\nchallenging to link KG facts to sentences in languages other than the limited\nset of languages. To address this problem, we introduce the task of\nMultilingual Fact Linking (MFL) where the goal is to link fact expressed in a\nsentence to corresponding fact in the KG, even when the fact label in the KG is\nnot available in the language of the sentence. To facilitate research in this\narea, we present a new evaluation dataset, IndicLink. This dataset contains\n11,293 linked WikiData facts and 6,429 sentences spanning English and six\nIndian languages. We propose a Retrieval+Generation model, ReFCoG, that can\nscale to millions of KG facts by combining Dual Encoder based retrieval with a\nSeq2Seq based generation model which is constrained to output only valid KG\nfacts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in\nPrecision@1. In spite of this gain, the model achieves an overall score of\n52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink\ndata are available at https://github.com/SaiKeshav/mfl'}, {'Augmenting Pre-trained Language Models with QA-Memory for Open-Domain\n  Question Answering': 'Retrieval augmented language models have recently become the standard for\nknowledge intensive tasks. Rather than relying purely on latent semantics\nwithin the parameters of large neural models, these methods enlist a\nsemi-parametric memory to encode an index of knowledge for the model to\nretrieve over. Most prior work has employed text passages as the unit of\nknowledge, which has high coverage at the cost of interpretability,\ncontrollability, and efficiency. The opposite properties arise in other methods\nwhich have instead relied on knowledge base (KB) facts. At the same time, more\nrecent work has demonstrated the effectiveness of storing and retrieving from\nan index of Q-A pairs derived from text \\citep{lewis2021paq}. This approach\nyields a high coverage knowledge representation that maintains KB-like\nproperties due to its representations being more atomic units of information.\nIn this work we push this line of research further by proposing a\nquestion-answer augmented encoder-decoder model and accompanying pretraining\nstrategy. This yields an end-to-end system that not only outperforms prior QA\nretrieval methods on single-hop QA tasks but also enables compositional\nreasoning, as demonstrated by strong performance on two multi-hop QA datasets.\nTogether, these methods improve the ability to interpret and control the model\nwhile narrowing the performance gap with passage retrieval systems.'}, {'MuRAG: Multimodal Retrieval-Augmented Generator for Open Question\n  Answering over Images and Text': 'While language Models store a massive amount of world knowledge implicitly in\ntheir parameters, even very large models often fail to encode information about\nrare entities and events, while incurring huge computational costs. Recently,\nretrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated\nworld knowledge into language generation by leveraging an external\nnon-parametric index and have demonstrated impressive performance with\nconstrained model sizes. However, these methods are restricted to retrieving\nonly textual knowledge, neglecting the ubiquitous amount of knowledge in other\nmodalities like images -- much of which contains information not covered by any\ntext. To address this limitation, we propose the first Multimodal\nRetrieval-Augmented Transformer (MuRAG), which accesses an external\nnon-parametric multimodal memory to augment language generation. MuRAG is\npre-trained with a mixture of large-scale image-text and text-only corpora\nusing a joint contrastive and generative loss. We perform experiments on two\ndifferent datasets that require retrieving and reasoning over both images and\ntext to answer a given query: WebQA, and MultimodalQA. Our results show that\nMuRAG achieves state-of-the-art accuracy, outperforming existing models by\n10-20\\% absolute on both datasets and under both distractor and full-wiki\nsettings.'}]","### Abstract

In the rapidly evolving field of language model (LM) evaluation, concerns about intra-model biases and the high cost associated with using large, universally capable models emerge. This paper addresses these issues by proposing a novel evaluation method called Panel of LLM Evaluators (PoLL), which integrates decisions from multiple diversified models to enhance bias reduction and improve cost-effectiveness. 

**Objective**: The main goal is to introduce a systematic approach that leverages ensemble models to mitigate biases inherent in using a single judge, particularly the tendency of large models to prefer their own generations.

**Innovations**: The PoLL method is designed for efficient and unbiased evaluation by aggregating assessments from various model families, each with unique strengths for different tasks. Instead of relying on a single large, capable model as a judge, PoLL combines the judgements of smaller, efficient models via innovative voting algorithms—max pooling or average pooling.

**Methods**: In the conducting of experiments, three models from different families (Command R, GPT-3.5, and Haiku) are incorporated into the PoLL panel. Two voting functions are experimented with—max voting for binary decision tasks and average pooling for ordinal scoring scales. This setup allows for accurate evaluation while minimizing the impact of model-specific preferences.

**Results**: PoLL shows statistically significant correlations with human judgements, surpassing the performance of individual large models, particularly evident in scenarios where GPT-4, identified as a weak judge in previous studies, demonstrates high variance due to bias. Across various dataset experiments, including natural questions, multi-hop question answering, and chatbot interactions, PoLL provides more consistent and less biased evaluation scores.

**Contributions**: This research contributes novel insights into overcoming the limitations of current evaluation methods, specifically focusing on reducing intra-model bias and optimizing costs beyond the capabilities of a single-preferred model. By utilizing a panel approach, the study enhances the reliability and accuracy of LLM evaluation, promoting a more fair, efficient, and inclusive assessment process.

**Applications**: PoLL has potential implications for various natural language processing applications, from refining model outputs in real-time interactions to systematically evaluating model generalization across diverse tasks. The method addresses critical challenges in the machine learning community, fostering the development of evaluation standards that are fair, reliable, and economically viable."
"We give a new algorithm for learning mixtures of $k$ Gaussians (with identity
covariance in $\mathbb{R}^n$) to TV error $\varepsilon$, with quasi-polynomial
($O(n^{\text{poly log}\left(\frac{n+k}{\varepsilon}\right)})$) time and sample
complexity, under a minimum weight assumption. Unlike previous approaches, most
of which are algebraic in nature, our approach is analytic and relies on the
framework of diffusion models. Diffusion models are a modern paradigm for
generative modeling, which typically rely on learning the score function
(gradient log-pdf) along a process transforming a pure noise distribution, in
our case a Gaussian, to the data distribution. Despite their dazzling
performance in tasks such as image generation, there are few end-to-end
theoretical guarantees that they can efficiently learn nontrivial families of
distributions; we give some of the first such guarantees. We proceed by
deriving higher-order Gaussian noise sensitivity bounds for the score functions
for a Gaussian mixture to show that that they can be inductively learned using
piecewise polynomial regression (up to poly-logarithmic degree), and combine
this with known convergence results for diffusion models. Our results extend to
continuous mixtures of Gaussians where the mixing distribution is supported on
a union of $k$ balls of constant radius. In particular, this applies to the
case of Gaussian convolutions of distributions on low-dimensional manifolds, or
more generally sets with small covering number.","[{'Optimal algorithms for group distributionally robust optimization and\n  beyond': 'Distributionally robust optimization (DRO) can improve the robustness and\nfairness of learning methods. In this paper, we devise stochastic algorithms\nfor a class of DRO problems including group DRO, subpopulation fairness, and\nempirical conditional value at risk (CVaR) optimization. Our new algorithms\nachieve faster convergence rates than existing algorithms for multiple DRO\nsettings. We also provide a new information-theoretic lower bound that implies\nour bounds are tight for group DRO. Empirically, too, our algorithms outperform\nknown methods'}, {'A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm\n  under Smoothness and Isoperimetry': 'We study the mixing time of Metropolis-Adjusted Langevin algorithm (MALA) for\nsampling a target density on $\\mathbb{R}^d$. We assume that the target density\nsatisfies $\\psi_\\mu$-isoperimetry and that the operator norm and trace of its\nHessian are bounded by $L$ and $\\Upsilon$ respectively. Our main result\nestablishes that, from a warm start, to achieve $\\epsilon$-total variation\ndistance to the target density, MALA mixes in\n$O\\left(\\frac{(L\\Upsilon)^{\\frac12}}{\\psi_\\mu^2}\n\\log\\left(\\frac{1}{\\epsilon}\\right)\\right)$ iterations. Notably, this result\nholds beyond the log-concave sampling setting and the mixing time depends on\nonly $\\Upsilon$ rather than its upper bound $L d$. In the $m$-strongly\nlogconcave and $L$-log-smooth sampling setting, our bound recovers the previous\nminimax mixing bound of MALA~\\cite{wu2021minimax}.'}, {'Projection-Free Online Convex Optimization via Efficient Newton\n  Iterations': 'This paper presents new projection-free algorithms for Online Convex\nOptimization (OCO) over a convex domain $\\mathcal{K} \\subset \\mathbb{R}^d$.\nClassical OCO algorithms (such as Online Gradient Descent) typically need to\nperform Euclidean projections onto the convex set $\\cK$ to ensure feasibility\nof their iterates. Alternative algorithms, such as those based on the\nFrank-Wolfe method, swap potentially-expensive Euclidean projections onto\n$\\mathcal{K}$ for linear optimization over $\\mathcal{K}$. However, such\nalgorithms have a sub-optimal regret in OCO compared to projection-based\nalgorithms. In this paper, we look at a third type of algorithms that output\napproximate Newton iterates using a self-concordant barrier for the set of\ninterest. The use of a self-concordant barrier automatically ensures\nfeasibility without the need for projections. However, the computation of the\nNewton iterates requires a matrix inverse, which can still be expensive. As our\nmain contribution, we show how the stability of the Newton iterates can be\nleveraged to compute the inverse Hessian only a vanishing fraction of the\nrounds, leading to a new efficient projection-free OCO algorithm with a\nstate-of-the-art regret bound.'}, {'Non-submodular Function Maximization subject to a Matroid Constraint,\n  with Applications': 'The standard greedy algorithm has been recently shown to enjoy approximation\nguarantees for constrained non-submodular nondecreasing set function\nmaximization. While these recent results allow to better characterize the\nempirical success of the greedy algorithm, they are only applicable to simple\ncardinality constraints. In this paper, we study the problem of maximizing a\nnon-submodular nondecreasing set function subject to a general matroid\nconstraint. We first show that the standard greedy algorithm offers an\napproximation factor of $\\frac{0.4 {\\gamma}^{2}}{\\sqrt{\\gamma r} + 1}$, where\n$\\gamma$ is the submodularity ratio of the function and $r$ is the rank of the\nmatroid. Then, we show that the same greedy algorithm offers a constant\napproximation factor of $(1 + 1/(1-\\alpha))^{-1}$, where $\\alpha$ is the\ngeneralized curvature of the function. In addition, we demonstrate that these\napproximation guarantees are applicable to several real-world applications in\nwhich the submodularity ratio and the generalized curvature can be bounded.\nFinally, we show that our greedy algorithm does achieve a competitive\nperformance in practice using a variety of experiments on synthetic and\nreal-world data.'}, {'When does Metropolized Hamiltonian Monte Carlo provably outperform\n  Metropolis-adjusted Langevin algorithm?': 'We analyze the mixing time of Metropolized Hamiltonian Monte Carlo (HMC) with\nthe leapfrog integrator to sample from a distribution on $\\mathbb{R}^d$ whose\nlog-density is smooth, has Lipschitz Hessian in Frobenius norm and satisfies\nisoperimetry. We bound the gradient complexity to reach $\\epsilon$ error in\ntotal variation distance from a warm start by $\\tilde\nO(d^{1/4}\\text{polylog}(1/\\epsilon))$ and demonstrate the benefit of choosing\nthe number of leapfrog steps to be larger than 1. To surpass previous analysis\non Metropolis-adjusted Langevin algorithm (MALA) that has\n$\\tilde{O}(d^{1/2}\\text{polylog}(1/\\epsilon))$ dimension dependency in Wu et\nal. (2022), we reveal a key feature in our proof that the joint distribution of\nthe location and velocity variables of the discretization of the continuous HMC\ndynamics stays approximately invariant. This key feature, when shown via\ninduction over the number of leapfrog steps, enables us to obtain estimates on\nmoments of various quantities that appear in the acceptance rate control of\nMetropolized HMC. Moreover, to deal with another bottleneck on the HMC proposal\ndistribution overlap control in the literature, we provide a new approach to\nupper bound the Kullback-Leibler divergence between push-forwards of the\nGaussian distribution through HMC dynamics initialized at two different points.\nNotably, our analysis does not require log-concavity or independence of the\nmarginals, and only relies on an isoperimetric inequality. To illustrate the\napplicability of our result, several examples of natural functions that fall\ninto our framework are discussed.'}, {'Quasi-Newton Steps for Efficient Online Exp-Concave Optimization': 'The aim of this paper is to design computationally-efficient and optimal\nalgorithms for the online and stochastic exp-concave optimization settings.\nTypical algorithms for these settings, such as the Online Newton Step (ONS),\ncan guarantee a $O(d\\ln T)$ bound on their regret after $T$ rounds, where $d$\nis the dimension of the feasible set. However, such algorithms perform\nso-called generalized projections whenever their iterates step outside the\nfeasible set. Such generalized projections require $\\Omega(d^3)$ arithmetic\noperations even for simple sets such a Euclidean ball, making the total runtime\nof ONS of order $d^3 T$ after $T$ rounds, in the worst-case. In this paper, we\nside-step generalized projections by using a self-concordant barrier as a\nregularizer to compute the Newton steps. This ensures that the iterates are\nalways within the feasible set without requiring projections. This approach\nstill requires the computation of the inverse of the Hessian of the barrier at\nevery step. However, using the stability properties of the Newton steps, we\nshow that the inverse of the Hessians can be efficiently approximated via\nTaylor expansions for most rounds, resulting in a $O(d^2 T +d^\\omega \\sqrt{T})$\ntotal computational complexity, where $\\omega$ is the exponent of matrix\nmultiplication. In the stochastic setting, we show that this translates into a\n$O(d^3/\\epsilon)$ computational complexity for finding an $\\epsilon$-suboptimal\npoint, answering an open question by Koren 2013. We first show these new\nresults for the simple case where the feasible set is a Euclidean ball. Then,\nto move to general convex set, we use a reduction to Online Convex Optimization\nover the Euclidean ball. Our final algorithm can be viewed as a more efficient\nversion of ONS.'}, {'Information Theoretic Bounds on Optimal Worst-case Error in Binary\n  Mixture Identification': 'Identification of latent binary sequences from a pool of noisy observations\nhas a wide range of applications in both statistical learning and population\ngenetics. Each observed sequence is the result of passing one of the latent\nmother-sequences through a binary symmetric channel, which makes this\nconfiguration analogous to a special case of Bernoulli Mixture Models. This\npaper aims to attain an asymptotically tight upper-bound on the error of\nMaximum Likelihood mixture identification in such problems. The obtained\nresults demonstrate fundamental guarantees on the inference accuracy of the\noptimal estimator. To this end, we set out to find the closest pair of discrete\ndistributions with respect to the Chernoff Information measure. We provide a\nnovel technique to lower bound the Chernoff Information in an efficient way. We\nalso show that a drastic phase transition occurs at noise level 0.25. Our\nfindings reveal that the identification problem becomes much harder as the\nnoise probability exceeds this threshold.'}, {'Testing Determinantal Point Processes': 'Determinantal point processes (DPPs) are popular probabilistic models of\ndiversity. In this paper, we investigate DPPs from a new perspective: property\ntesting of distributions. Given sample access to an unknown distribution $q$\nover the subsets of a ground set, we aim to distinguish whether $q$ is a DPP\ndistribution, or $\\epsilon$-far from all DPP distributions in\n$\\ell_1$-distance. In this work, we propose the first algorithm for testing\nDPPs. Furthermore, we establish a matching lower bound on the sample complexity\nof DPP testing. This lower bound also extends to showing a new hardness result\nfor the problem of testing the more general class of log-submodular\ndistributions.'}, {'Sampling with Barriers: Faster Mixing via Lewis Weights': 'We analyze Riemannian Hamiltonian Monte Carlo (RHMC) for sampling a polytope\ndefined by $m$ inequalities in $\\R^n$ endowed with the metric defined by the\nHessian of a convex barrier function. The advantage of RHMC over Euclidean\nmethods such as the ball walk, hit-and-run and the Dikin walk is in its ability\nto take longer steps. However, in all previous work, the mixing rate has a\nlinear dependence on the number of inequalities. We introduce a hybrid of the\nLewis weights barrier and the standard logarithmic barrier and prove that the\nmixing rate for the corresponding RHMC is bounded by $\\tilde\nO(m^{1/3}n^{4/3})$, improving on the previous best bound of $\\tilde\nO(mn^{2/3})$ (based on the log barrier). This continues the general parallels\nbetween optimization and sampling, with the latter typically leading to new\ntools and more refined analysis. To prove our main results, we have to\novercomes several challenges relating to the smoothness of Hamiltonian curves\nand the self-concordance properties of the barrier. In the process, we give a\ngeneral framework for the analysis of Markov chains on Riemannian manifolds,\nderive new smoothness bounds on Hamiltonian curves, a central topic of\ncomparison geometry, and extend self-concordance to the infinity norm, which\ngives sharper bounds; these properties appear to be of independent interest.'}, {'Convergence of the Riemannian Langevin Algorithm': 'We study the Riemannian Langevin Algorithm for the problem of sampling from a\ndistribution with density $\\nu$ with respect to the natural measure on a\nmanifold with metric $g$. We assume that the target density satisfies a\nlog-Sobolev inequality with respect to the metric and prove that the manifold\ngeneralization of the Unadjusted Langevin Algorithm converges rapidly to $\\nu$\nfor Hessian manifolds. This allows us to reduce the problem of sampling\nnon-smooth (constrained) densities in ${\\bf R}^n$ to sampling smooth densities\nover appropriate manifolds, while needing access only to the gradient of the\nlog-density, and this, in turn, to sampling from the natural Brownian motion on\nthe manifold. Our main analytic tools are (1) an extension of self-concordance\nto manifolds, and (2) a stochastic approach to bounding smoothness on\nmanifolds. A special case of our approach is sampling isoperimetric densities\nrestricted to polytopes by using the metric defined by the logarithmic barrier.'}]","Title: Quasi-Polynomial Time Learning Algorithms for Gaussian Mixtures via Diﬀusion Models and Piecewise Polynomial Regression

**Background**: The problem of learning mixtures of Gaussians from independent samples is a fundamental task in unsupervised learning with wide-ranging applications in clustering, image analysis, and anomaly detection. Despite rich theoretical results, efficient and scalable algorithms have presented significant challenges, particularly for mixtures in high dimensions.

**Objective**: The primary objective is to devise a computationally efficient method, namely a quasi-polynomial time algorithm, for learning the parameters of a mixture of Gaussians under general conditions without requiring separation assumptions. 

**Innovations**: This paper introduces two key innovations:
- **Polynomial Approximations by Diﬀusion Models**: Utilizes a connection between denoising generative models like reverse Brownian motion and pseudo-maximization for constructing score functions that approximate the unknown mixture distributions. 
- **Piecewise Polynomial Regression**: DEVELOPS a low-degree polynomial approximation for the score functions using the Hermite series and noise stability conditions, relying on prior computational learning theory.

**Methods**: Drawing from convergent diﬀusion models theory, the authors design a scheme for estimating mixture component distributions and a novel regression framework for approximating piecewise polynomials as score functions of the Gaussians. This involves a series of Gaussian noise stabilization assumptions to guide approximation accuracy.

**Results**: The developed techniques enable the recovery of parameters for k Gaussian components in polynomial time, specifically achieving quasi-polynomial time complexity for given precision requirements. Properties like polynomial dependence on dimension, logarithms of sample size, and separation gap significantly simplify over prior approaches.

**Contributions**: The contributions span both statistical learning theory and computational statistics, offering:
- A theoretically sound, efficient, and scalable algorithm for gaussian mixtures learning.
- A new application of diﬀusion models to distributions learning, leveraging insights from continuous-time stochastic processes for approximation.
- Progress at the intersection of optimization, probability, and learning, advancing practical implementations and insights into scalable mixture learning.

**Applications**: The research can be applied to various domains requiring scalable unsupervised learning, such as:
- Image segmentation and clustering tasks in computer vision.
- Anomaly detection in financial data or network diagnostics.
- Clustering in genomics and bioinformatics, aiding in the understanding of complex biological systems.

This paper advances the field through novel methodologies and applications, addressing the critical need for efficient learning algorithms for high-dimensional mixed distributions."
"We study the problem of learning mixtures of $k$ Gaussians in $d$ dimensions.
We make no separation assumptions on the underlying mixture components: we only
require that the covariance matrices have bounded condition number and that the
means and covariances lie in a ball of bounded radius. We give an algorithm
that draws $d^{\mathrm{poly}(k/\varepsilon)}$ samples from the target mixture,
runs in sample-polynomial time, and constructs a sampler whose output
distribution is $\varepsilon$-far from the unknown mixture in total variation.
Prior works for this problem either (i) required exponential runtime in the
dimension $d$, (ii) placed strong assumptions on the instance (e.g., spherical
covariances or clusterability), or (iii) had doubly exponential dependence on
the number of components $k$.
  Our approach departs from commonly used techniques for this problem like the
method of moments. Instead, we leverage a recently developed reduction, based
on diffusion models, from distribution learning to a supervised learning task
called score matching. We give an algorithm for the latter by proving a
structural result showing that the score function of a Gaussian mixture can be
approximated by a piecewise-polynomial function, and there is an efficient
algorithm for finding it. To our knowledge, this is the first example of
diffusion models achieving a state-of-the-art theoretical guarantee for an
unsupervised learning task.","[{'On the Rank Number of Grid Graphs': 'A vertex k-ranking is a labeling of the vertices of a graph with integers\nfrom 1 to k so any path connecting two vertices with the same label will pass\nthrough a vertex with a greater label. The rank number of a graph is defined to\nbe the minimum possible k for which a k-ranking exists for that graph. For mxn\ngrid graphs, the rank number has been found only for m<4. In this paper, we\ndetermine its for m=4 and improve its upper bound for general grids.\nFurthermore, we improve lower bounds on the rank numbers for square and\ntriangle grid graphs from logarithmic to linear. These new lower bounds are key\nto characterizing the rank number for general grids, and our results have\napplications in optimizing VLSI circuit design and parallel processing, search,\nand scheduling.'}, {'Cellular Automata to More Efficiently Compute the Collatz Map': 'The Collatz, or 3x+1, Conjecture claims that for every positive integer n,\nthere exists some k such that T^k(n)=1, where T is the Collatz map. We present\nthree cellular automata (CA) that transform the global problem of mimicking the\nCollatz map in bases 2, 3, and 4 into a local one of transforming the digits of\niterates. The CAs streamline computation first by bypassing calculation of\ncertain parts of trajectories: the binary CA bypasses division by two\naltogether. In addition, they allow for multiple trajectories to be calculated\nsimultaneously, representing both a significant improvement upon existing\nsequential methods of computing the Collatz map and a demonstration of the\nefficacy of using a massively parallel approach with cellular automata to\ntackle iterative problems like the Collatz Conjecture.'}, {'Basis Collapse for Holographic Algorithms Over All Domain Sizes': 'The theory of holographic algorithms introduced by Valiant represents a novel\napproach to achieving polynomial-time algorithms for seemingly intractable\ncounting problems via a reduction to counting planar perfect matchings and a\nlinear change of basis. Two fundamental parameters in holographic algorithms\nare the \\emph{domain size} and the \\emph{basis size}. Roughly, the domain size\nis the range of colors involved in the counting problem at hand (e.g. counting\ngraph $k$-colorings is a problem over domain size $k$), while the basis size\n$\\ell$ captures the dimensionality of the representation of those colors. A\nmajor open problem has been: for a given $k$, what is the smallest $\\ell$ for\nwhich any holographic algorithm for a problem over domain size $k$ ""collapses\nto"" (can be simulated by) a holographic algorithm with basis size $\\ell$? Cai\nand Lu showed in 2008 that over domain size 2, basis size 1 suffices, opening\nthe door to an extensive line of work on the structural theory of holographic\nalgorithms over the Boolean domain. Cai and Fu later showed for signatures of\nfull rank that over domain sizes 3 and 4, basis sizes 1 and 2, respectively,\nsuffice, and they conjectured that over domain size $k$ there is a collapse to\nbasis size $\\lfloor\\log_2 k\\rfloor$. In this work, we resolve this conjecture\nin the affirmative for signatures of full rank for all $k$.'}, {'A faster and simpler algorithm for learning shallow networks': 'We revisit the well-studied problem of learning a linear combination of $k$\nReLU activations given labeled examples drawn from the standard $d$-dimensional\nGaussian measure. Chen et al. [CDG+23] recently gave the first algorithm for\nthis problem to run in $\\text{poly}(d,1/\\varepsilon)$ time when $k = O(1)$,\nwhere $\\varepsilon$ is the target error. More precisely, their algorithm runs\nin time $(d/\\varepsilon)^{\\mathrm{quasipoly}(k)}$ and learns over multiple\nstages. Here we show that a much simpler one-stage version of their algorithm\nsuffices, and moreover its runtime is only $(d/\\varepsilon)^{O(k^2)}$.'}, {'Algorithmic Foundations for the Diffraction Limit': 'For more than a century and a half it has been widely-believed (but was never\nrigorously shown) that the physics of diffraction imposes certain fundamental\nlimits on the resolution of an optical system. However our understanding of\nwhat exactly can and cannot be resolved has never risen above heuristic\narguments which, even worse, appear contradictory. In this work we remedy this\ngap by studying the diffraction limit as a statistical inverse problem and,\nbased on connections to provable algorithms for learning mixture models, we\nrigorously prove upper and lower bounds on the statistical and algorithmic\ncomplexity needed to resolve closely spaced point sources. In particular we\nshow that there is a phase transition where the sample complexity goes from\npolynomial to exponential. Surprisingly, we show that this does not occur at\nthe Abbe limit, which has long been presumed to be the true diffraction limit.'}, {'Linear Programming Bounds for Randomly Sampling Colorings': ""Here we study the problem of sampling random proper colorings of a bounded\ndegree graph. Let $k$ be the number of colors and let $d$ be the maximum\ndegree. In 1999, Vigoda showed that the Glauber dynamics is rapidly mixing for\nany $k > \\frac{11}{6} d$. It turns out that there is a natural barrier at\n$\\frac{11}{6}$, below which there is no one-step coupling that is contractive,\neven for the flip dynamics.\n  We use linear programming and duality arguments to guide our construction of\na better coupling. We fully characterize the obstructions to going beyond\n$\\frac{11}{6}$. These examples turn out to be quite brittle, and even starting\nfrom one, they are likely to break apart before the flip dynamics changes the\ndistance between two neighboring colorings. We use this intuition to design a\nvariable length coupling that shows that the Glauber dynamics is rapidly mixing\nfor any $k\\ge \\left(\\frac{11}{6} - \\epsilon_0\\right)d$ where $\\epsilon_0 \\geq\n9.4 \\cdot 10^{-5}$. This is the first improvement to Vigoda's analysis that\nholds for general graphs.""}, {'Learning Polynomials of Few Relevant Dimensions': 'Polynomial regression is a basic primitive in learning and statistics. In its\nmost basic form the goal is to fit a degree $d$ polynomial to a response\nvariable $y$ in terms of an $n$-dimensional input vector $x$. This is extremely\nwell-studied with many applications and has sample and runtime complexity\n$\\Theta(n^d)$. Can one achieve better runtime if the intrinsic dimension of the\ndata is much smaller than the ambient dimension $n$? Concretely, we are given\nsamples $(x,y)$ where $y$ is a degree at most $d$ polynomial in an unknown\n$r$-dimensional projection (the relevant dimensions) of $x$. This can be seen\nboth as a generalization of phase retrieval and as a special case of learning\nmulti-index models where the link function is an unknown low-degree polynomial.\nNote that without distributional assumptions, this is at least as hard as junta\nlearning.\n  In this work we consider the important case where the covariates are\nGaussian. We give an algorithm that learns the polynomial within accuracy\n$\\epsilon$ with sample complexity that is roughly $N = O_{r,d}(n\n\\log^2(1/\\epsilon) (\\log n)^d)$ and runtime $O_{r,d}(N n^2)$. Prior to our\nwork, no such results were known even for the case of $r=1$. We introduce a new\nfiltered PCA approach to get a warm start for the true subspace and use\ngeodesic SGD to boost to arbitrary accuracy; our techniques may be of\nindependent interest, especially for problems dealing with subspace recovery or\nanalyzing SGD on manifolds.'}, {'Efficient Pauli channel estimation with logarithmic quantum memory': 'Here we revisit one of the prototypical tasks for characterizing the\nstructure of noise in quantum devices: estimating every eigenvalue of an\n$n$-qubit Pauli noise channel to error $\\epsilon$. Prior work (Chen et al.,\n2022) proved no-go theorems for this task in the practical regime where one has\na limited amount of quantum memory, e.g. any protocol with $\\le 0.99n$ ancilla\nqubits of quantum memory must make exponentially many measurements, provided it\nis non-concatenating. Such protocols can only interact with the channel by\nrepeatedly preparing a state, passing it through the channel, and measuring\nimmediately afterward.\n  This left open a natural question: does the lower bound hold even for general\nprotocols, i.e. ones which chain together many queries to the channel,\ninterleaved with arbitrary data-processing channels, before measuring?\nSurprisingly, in this work we show the opposite: there is a protocol that can\nestimate the eigenvalues of a Pauli channel to error $\\epsilon$ using only\n$O(\\log n/\\epsilon^2)$ ancilla qubits and $\\tilde{O}(n^2/\\epsilon^2)$\nmeasurements. In contrast, we show that any protocol with zero ancilla, even a\nconcatenating one, must make $\\Omega(2^n/\\epsilon^2)$ measurements, which is\ntight.\n  Our results imply, to our knowledge, the first quantum learning task where\nlogarithmically many qubits of quantum memory suffice for an exponential\nstatistical advantage.'}, {'Provably learning a multi-head attention layer': 'The multi-head attention layer is one of the key components of the\ntransformer architecture that sets it apart from traditional feed-forward\nmodels. Given a sequence length $k$, attention matrices\n$\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and\nprojection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times\nd}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to\n\\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional\ntokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq\n\\sum^m_{i=1}\n\\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$.\nIn this work, we initiate the study of provably learning a multi-head attention\nlayer from random examples and give the first nontrivial upper and lower bounds\nfor this problem:\n  - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain\nnon-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns\n$F$ to small error given random labeled examples drawn uniformly from $\\{\\pm\n1\\}^{k\\times d}$.\n  - We prove computational lower bounds showing that in the worst case,\nexponential dependence on $m$ is unavoidable.\n  We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in\nlarge language models, though our techniques naturally extend to standard\ncontinuous settings, e.g. Gaussian. Our algorithm, which is centered around\nusing examples to sculpt a convex body containing the unknown parameters, is a\nsignificant departure from existing provable algorithms for learning\nfeedforward networks, which predominantly exploit algebraic and rotation\ninvariance properties of the Gaussian distribution. In contrast, our analysis\nis more flexible as it primarily relies on various upper and lower tail bounds\nfor the input distribution and ""slices"" thereof.'}, {'Beyond the Low-Degree Algorithm: Mixtures of Subcubes and Their\n  Applications': 'We introduce the problem of learning mixtures of $k$ subcubes over\n$\\{0,1\\}^n$, which contains many classic learning theory problems as a special\ncase (and is itself a special case of others). We give a surprising $n^{O(\\log\nk)}$-time learning algorithm based on higher-order multilinear moments. It is\nnot possible to learn the parameters because the same distribution can be\nrepresented by quite different models. Instead, we develop a framework for\nreasoning about how multilinear moments can pinpoint essential features of the\nmixture, like the number of components.\n  We also give applications of our algorithm to learning decision trees with\nstochastic transitions (which also capture interesting scenarios where the\ntransitions are deterministic but there are latent variables). Using our\nalgorithm for learning mixtures of subcubes, we can approximate the Bayes\noptimal classifier within additive error $\\epsilon$ on $k$-leaf decision trees\nwith at most $s$ stochastic transitions on any root-to-leaf path in $n^{O(s +\n\\log k)}\\cdot\\text{poly}(1/\\epsilon)$ time. In this stochastic setting, the\nclassic Occam algorithms for learning decision trees with zero stochastic\ntransitions break down, while the low-degree algorithm of Linial et al.\ninherently has a quasipolynomial dependence on $1/\\epsilon$.\n  In contrast, as we will show, mixtures of $k$ subcubes are uniquely\ndetermined by their degree $2 \\log k$ moments and hence provide a useful\nabstraction for simultaneously achieving the polynomial dependence on\n$1/\\epsilon$ of the classic Occam algorithms for decision trees and the\nflexibility of the low-degree algorithm in being able to accommodate stochastic\ntransitions. Using our multilinear moment techniques, we also give the first\nimproved upper and lower bounds since the work of Feldman et al. for the\nrelated but harder problem of learning mixtures of binary product\ndistributions.'}]","Abstract:

This research paper introduces the effective unsupervised learning of Gaussian mixture models (GMMs) via a denoising diffusion process, offering the first improved theoretical guarantees in this domain. The main objective is to elucidate under what conditions a piecewise polynomial approximation can accurately approximate the score function of a well-conditioned GMM. By leveraging a novel clustering method and polynomial approximation, our algorithm efficiently learns the parameters of a GMM. This innovative framework simplifies the score function using a partition derived from a polynomial approximation, allowing effective learning without prior knowledge of the model parameters. The methodology relies on a reduction from score matching to polynomial regression, offering computational efficiency. Through theoretical analysis, we demonstrate that our approach significantly reduces computational complexity, making it more practical for high-dimensional data. The developed algorithm leads to notable improvements in learning GMMs, setting a benchmark for unsupervised learning tasks. The outcomes have direct applications in statistical modeling, clustering, signal processing, and machine learning, unlocking potentially transformative advancements in data analysis."
"To overcome the sim-to-real gap in reinforcement learning (RL), learned
policies must maintain robustness against environmental uncertainties. While
robust RL has been widely studied in single-agent regimes, in multi-agent
environments, the problem remains understudied -- despite the fact that the
problems posed by environmental uncertainties are often exacerbated by
strategic interactions. This work focuses on learning in distributionally
robust Markov games (RMGs), a robust variant of standard Markov games, wherein
each agent aims to learn a policy that maximizes its own worst-case performance
when the deployed environment deviates within its own prescribed uncertainty
set. This results in a set of robust equilibrium strategies for all agents that
align with classic notions of game-theoretic equilibria. Assuming a
non-adaptive sampling mechanism from a generative model, we propose a
sample-efficient model-based algorithm (DRNVI) with finite-sample complexity
guarantees for learning robust variants of various notions of game-theoretic
equilibria. We also establish an information-theoretic lower bound for solving
RMGs, which confirms the near-optimal sample complexity of DRNVI with respect
to problem-dependent factors such as the size of the state space, the target
accuracy, and the horizon length.","[{'Manifold Gradient Descent Solves Multi-Channel Sparse Blind\n  Deconvolution Provably and Efficiently': 'Multi-channel sparse blind deconvolution, or convolutional sparse coding,\nrefers to the problem of learning an unknown filter by observing its circulant\nconvolutions with multiple input signals that are sparse. This problem finds\nnumerous applications in signal processing, computer vision, and inverse\nproblems. However, it is challenging to learn the filter efficiently due to the\nbilinear structure of the observations with the respect to the unknown filter\nand inputs, as well as the sparsity constraint. In this paper, we propose a\nnovel approach based on nonconvex optimization over the sphere manifold by\nminimizing a smooth surrogate of the sparsity-promoting loss function. It is\ndemonstrated that manifold gradient descent with random initializations will\nprovably recover the filter, up to scaling and shift ambiguity, as soon as the\nnumber of observations is sufficiently large under an appropriate random data\nmodel. Numerical experiments are provided to illustrate the performance of the\nproposed method with comparisons to existing ones.'}, {'Distributionally Robust Model-Based Offline Reinforcement Learning with\n  Near-Optimal Sample Complexity': 'This paper concerns the central issues of model robustness and sample\nefficiency in offline reinforcement learning (RL), which aims to learn to\nperform decision making from history data without active exploration. Due to\nuncertainties and variabilities of the environment, it is critical to learn a\nrobust policy -- with as few samples as possible -- that performs well even\nwhen the deployed environment deviates from the nominal one used to collect the\nhistory dataset. We consider a distributionally robust formulation of offline\nRL, focusing on tabular robust Markov decision processes with an uncertainty\nset specified by the Kullback-Leibler divergence in both finite-horizon and\ninfinite-horizon settings. To combat with sample scarcity, a model-based\nalgorithm that combines distributionally robust value iteration with the\nprinciple of pessimism in the face of uncertainty is proposed, by penalizing\nthe robust value estimates with a carefully designed data-driven penalty term.\nUnder a mild and tailored assumption of the history dataset that measures\ndistribution shift without requiring full coverage of the state-action space,\nwe establish the finite-sample complexity of the proposed algorithms. We\nfurther develop an information-theoretic lower bound, which suggests that\nlearning RMDPs is at least as hard as the standard MDPs when the uncertainty\nlevel is sufficient small, and corroborates the tightness of our upper bound up\nto polynomial factors of the (effective) horizon length for a range of\nuncertainty levels. To the best our knowledge, this provides the first provably\nnear-optimal robust offline RL algorithm that learns under model uncertainty\nand partial coverage.'}, {'Micro Hand Gesture Recognition System Using Ultrasonic Active Sensing': ""In this paper, we propose a micro hand gesture recognition system and methods\nusing ultrasonic active sensing. This system uses micro dynamic hand gestures\nfor recognition to achieve human-computer interaction (HCI). The implemented\nsystem, called hand-ultrasonic gesture (HUG), consists of ultrasonic active\nsensing, pulsed radar signal processing, and time-sequence pattern recognition\nby machine learning. We adopt lower frequency (300 kHz) ultrasonic active\nsensing to obtain high resolution range-Doppler image features. Using high\nquality sequential range-Doppler features, we propose a state-transition-based\nhidden Markov model for gesture recognition. This method achieves a recognition\naccuracy of nearly 90\\% by using symbolized range-Doppler features and\nsignificantly reduces the computational complexity and power consumption.\nFurthermore, to achieve higher classification accuracy, we utilize an\nend-to-end neural network model and obtain a recognition accuracy of 96.32\\%.\nIn addition to offline analysis, a real-time prototype is released to verify\nour method's potential for application in the real world.""}, {'Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free\n  Reinforcement Learning': 'Achieving sample efficiency in online episodic reinforcement learning (RL)\nrequires optimally balancing exploration and exploitation. When it comes to a\nfinite-horizon episodic Markov decision process with $S$ states, $A$ actions\nand horizon length $H$, substantial progress has been achieved towards\ncharacterizing the minimax-optimal regret, which scales on the order of\n$\\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples.\nWhile several competing solution paradigms have been proposed to minimize\nregret, they are either memory-inefficient, or fall short of optimality unless\nthe sample size exceeds an enormous threshold (e.g., $S^6A^4\n\\,\\mathrm{poly}(H)$ for existing model-free methods).\n  To overcome such a large sample size barrier to efficient RL, we design a\nnovel model-free algorithm, with space complexity $O(SAH)$, that achieves\nnear-optimal regret as soon as the sample size exceeds the order of\n$SA\\,\\mathrm{poly}(H)$. In terms of this sample size requirement (also referred\nto the initial burn-in cost), our method improves -- by at least a factor of\n$S^5A^3$ -- upon any prior memory-efficient algorithm that is asymptotically\nregret-optimal. Leveraging the recently introduced variance reduction strategy\n(also called {\\em reference-advantage decomposition}), the proposed algorithm\nemploys an {\\em early-settled} reference update rule, with the aid of two\nQ-learning sequences with upper and lower confidence bounds. The design\nprinciple of our early-settled variance reduction method might be of\nindependent interest to other RL settings that involve intricate\nexploration-exploitation trade-offs.'}, {'Sample Complexity of Offline Distributionally Robust Linear Markov\n  Decision Processes': 'In offline reinforcement learning (RL), the absence of active exploration\ncalls for attention on the model robustness to tackle the sim-to-real gap,\nwhere the discrepancy between the simulated and deployed environments can\nsignificantly undermine the performance of the learned policy. To endow the\nlearned policy with robustness in a sample-efficient manner in the presence of\nhigh-dimensional state-action space, this paper considers the sample complexity\nof distributionally robust linear Markov decision processes (MDPs) with an\nuncertainty set characterized by the total variation distance using offline\ndata. We develop a pessimistic model-based algorithm and establish its sample\ncomplexity bound under minimal data coverage assumptions, which outperforms\nprior art by at least $\\widetilde{O}(d)$, where $d$ is the feature dimension.\nWe further improve the performance guarantee of the proposed algorithm by\nincorporating a carefully-designed variance estimator.'}, {'Seeing is not Believing: Robust Reinforcement Learning against Spurious\n  Correlation': 'Robustness has been extensively studied in reinforcement learning (RL) to\nhandle various forms of uncertainty such as random perturbations, rare events,\nand malicious attacks. In this work, we consider one critical type of\nrobustness against spurious correlation, where different portions of the state\ndo not have correlations induced by unobserved confounders. These spurious\ncorrelations are ubiquitous in real-world tasks, for instance, a self-driving\ncar usually observes heavy traffic in the daytime and light traffic at night\ndue to unobservable human activity. A model that learns such useless or even\nharmful correlation could catastrophically fail when the confounder in the test\ncase deviates from the training one. Although motivated, enabling robustness\nagainst spurious correlation poses significant challenges since the uncertainty\nset, shaped by the unobserved confounder and causal structure, is difficult to\ncharacterize and identify. Existing robust algorithms that assume simple and\nunstructured uncertainty sets are therefore inadequate to address this\nchallenge. To solve this issue, we propose Robust State-Confounded Markov\nDecision Processes (RSC-MDPs) and theoretically demonstrate its superiority in\navoiding learning spurious correlations compared with other robust RL\ncounterparts. We also design an empirical algorithm to learn the robust optimal\npolicy for RSC-MDPs, which outperforms all baselines in eight realistic\nself-driving and manipulation tasks.'}, {'Federated Offline Reinforcement Learning: Collaborative Single-Policy\n  Coverage Suffices': 'Offline reinforcement learning (RL), which seeks to learn an optimal policy\nusing offline data, has garnered significant interest due to its potential in\ncritical applications where online data collection is infeasible or expensive.\nThis work explores the benefit of federated learning for offline RL, aiming at\ncollaboratively leveraging offline datasets at multiple agents. Focusing on\nfinite-horizon episodic tabular Markov decision processes (MDPs), we design\nFedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for\nfederated offline RL. FedLCB-Q updates local Q-functions at agents with novel\nlearning rate schedules and aggregates them at a central server using\nimportance averaging and a carefully designed pessimistic penalty term. Our\nsample complexity analysis reveals that, with appropriately chosen parameters\nand synchronization schedules, FedLCB-Q achieves linear speedup in terms of the\nnumber of agents without requiring high-quality datasets at individual agents,\nas long as the local datasets collectively cover the state-action space visited\nby the optimal policy, highlighting the power of collaboration in the federated\nsetting. In fact, the sample complexity almost matches that of the single-agent\ncounterpart, as if all the data are stored at a central location, up to\npolynomial factors of the horizon length. Furthermore, FedLCB-Q is\ncommunication-efficient, where the number of communication rounds is only\nlinear with respect to the horizon length up to logarithmic factors.'}, {'Pessimistic Q-Learning for Offline Reinforcement Learning: Towards\n  Optimal Sample Complexity': 'Offline or batch reinforcement learning seeks to learn a near-optimal policy\nusing history data without active exploration of the environment. To counter\nthe insufficient coverage and sample scarcity of many offline datasets, the\nprinciple of pessimism has been recently introduced to mitigate high bias of\nthe estimated values. While pessimistic variants of model-based algorithms\n(e.g., value iteration with lower confidence bounds) have been theoretically\ninvestigated, their model-free counterparts -- which do not require explicit\nmodel estimation -- have not been adequately studied, especially in terms of\nsample efficiency. To address this inadequacy, we study a pessimistic variant\nof Q-learning in the context of finite-horizon Markov decision processes, and\ncharacterize its sample complexity under the single-policy concentrability\nassumption which does not require the full coverage of the state-action space.\nIn addition, a variance-reduced pessimistic Q-learning algorithm is proposed to\nachieve near-optimal sample complexity. Altogether, this work highlights the\nefficiency of model-free algorithms in offline RL when used in conjunction with\npessimism and variance reduction.'}, {'Offline Reinforcement Learning with On-Policy Q-Function Regularization': 'The core challenge of offline reinforcement learning (RL) is dealing with the\n(potentially catastrophic) extrapolation error induced by the distribution\nshift between the history dataset and the desired policy. A large portion of\nprior work tackles this challenge by implicitly/explicitly regularizing the\nlearning policy towards the behavior policy, which is hard to estimate reliably\nin practice. In this work, we propose to regularize towards the Q-function of\nthe behavior policy instead of the behavior policy itself, under the premise\nthat the Q-function can be estimated more reliably and easily by a SARSA-style\nestimate and handles the extrapolation error more straightforwardly. We propose\ntwo algorithms taking advantage of the estimated Q-function through\nregularizations, and demonstrate they exhibit strong performance on the D4RL\nbenchmarks.'}, {'The Curious Price of Distributional Robustness in Reinforcement Learning\n  with a Generative Model': 'This paper investigates model robustness in reinforcement learning (RL) to\nreduce the sim-to-real gap in practice. We adopt the framework of\ndistributionally robust Markov decision processes (RMDPs), aimed at learning a\npolicy that optimizes the worst-case performance when the deployed environment\nfalls within a prescribed uncertainty set around the nominal MDP. Despite\nrecent efforts, the sample complexity of RMDPs remained mostly unsettled\nregardless of the uncertainty set in use. It was unclear if distributional\nrobustness bears any statistical consequences when benchmarked against standard\nRL. Assuming access to a generative model that draws samples based on the\nnominal MDP, we characterize the sample complexity of RMDPs when the\nuncertainty set is specified via either the total variation (TV) distance or\n$\\chi^2$ divergence. The algorithm studied here is a model-based method called\n{\\em distributionally robust value iteration}, which is shown to be\nnear-optimal for the full range of uncertainty levels. Somewhat surprisingly,\nour results uncover that RMDPs are not necessarily easier or harder to learn\nthan standard MDPs. The statistical consequence incurred by the robustness\nrequirement depends heavily on the size and shape of the uncertainty set: in\nthe case w.r.t.~the TV distance, the minimax sample complexity of RMDPs is\nalways smaller than that of standard MDPs; in the case w.r.t.~the $\\chi^2$\ndivergence, the sample complexity of RMDPs can often far exceed the standard\nMDP counterpart.'}]","Title: Robust Markov Games: Algorithms, Complexity, and Applications

Abstract:
The rise of multi-agent systems in diverse real-world domains emphasizes the need for algorithms that facilitate efficient, stable interactions among agents. Multi-agent reinforcement learning (MARL) stands as a promising solution through a joint decision-making framework, yet its promise is constrained by uncertainties arising from environmental perturbations, model mismatches, and adversaries. This paper introduces robust Markov games (RMGs), a versatile framework that aims to enhance the robustness of MARL by accounting for such uncertainties.

Objective:
The primary goal is to develop efficient algorithms for learning robust equilibria under model uncertainties in RMGs, addressing the inherent computational complexity. This advancement facilitates the practical deployment of MARL in unpredictable, adversarial environments.

Innovations:
We propose statistical robustness-aware reinforcement learning methods, like Distributionally Robust Nash Value Iteration (DR-NVI), to tackle uncertainties in environments and model parameters. This framework effectively deals with complex game-theoretical interactions and nonlinear rewards, improving upon traditional MARL algorithms.

Methods:
NMVI, a technique that integrates robust Q-function estimation, is utilized to modify the standard Nash value iteration process. We prove theoretical guarantees for the upper bound of learning robust Nash equilibria through DR-NVI, supported by lower bound analysis.

Results:
Our theoretical findings corroborate with empirical validations, establishing DR-NVI as an efficient and robust algorithm in RMGs. Simulations demonstrate a significant enhancement in stability and performance over standard MARL algorithms when environmental uncertainties are present.

Contributions:
This research introduces a novel and practical approach to accounting for uncertainties in multi-agent systems, making it a significant contribution to the field of MARL. The presented algorithmic advancements and theoretical insights pave the way for more reliable and resilient MARL applications across various domains.

Applications:
Robust MARL can dramatically impact sectors such as autonomous systems, cybersecurity, economics, and urban planning, enabling more intelligent and adaptive solutions in scenarios where uncertainty is inherent. The robust framework proposed in this paper is particularly valuable in enabling the deployment of MARL beyond controlled environments, to real-world settings that require robust decision-making capabilities."
"In the classical Reinforcement Learning from Human Feedback (RLHF) framework,
Proximal Policy Optimization (PPO) is employed to learn from sparse,
sentence-level rewards -- a challenging scenario in traditional deep
reinforcement learning. Despite the great successes of PPO in the alignment of
state-of-the-art closed-source large language models (LLMs), its open-source
implementation is still largely sub-optimal, as widely reported by numerous
research studies. To address these issues, we introduce a framework that models
RLHF problems as a Markov decision process (MDP), enabling the capture of
fine-grained token-wise information. Furthermore, we provide theoretical
insights that demonstrate the superiority of our MDP framework over the
previous sentence-level bandit formulation. Under this framework, we introduce
an algorithm, dubbed as Reinforced Token Optimization (\texttt{RTO}), which
learns the token-wise reward function from preference data and performs policy
optimization based on this learned token-wise reward signal. Theoretically,
\texttt{RTO} is proven to have the capability of finding the near-optimal
policy sample-efficiently. For its practical implementation, \texttt{RTO}
innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,
originally derived from sparse sentence rewards, surprisingly provides us with
a token-wise characterization of response quality, which is seamlessly
incorporated into our subsequent PPO training stage. Extensive real-world
alignment experiments verify the effectiveness of the proposed approach.","[{'A Theoretical Analysis of Optimistic Proximal Policy Optimization in\n  Linear Markov Decision Processes': 'The proximal policy optimization (PPO) algorithm stands as one of the most\nprosperous methods in the field of reinforcement learning (RL). Despite its\nsuccess, the theoretical understanding of PPO remains deficient. Specifically,\nit is unclear whether PPO or its optimistic variants can effectively solve\nlinear Markov decision processes (MDPs), which are arguably the simplest models\nin RL with function approximation. To bridge this gap, we propose an optimistic\nvariant of PPO for episodic adversarial linear MDPs with full-information\nfeedback, and establish a $\\tilde{\\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret for\nit. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of each\nepisode, and $K$ is the number of episodes. Compared with existing policy-based\nalgorithms, we achieve the state-of-the-art regret bound in both stochastic\nlinear MDPs and adversarial linear MDPs with full information. Additionally,\nour algorithm design features a novel multi-batched updating mechanism and the\ntheoretical analysis utilizes a new covering number argument of value and\npolicy classes, which might be of independent interest.'}, {'Rethinking Model-based, Policy-based, and Value-based Reinforcement\n  Learning via the Lens of Representation Complexity': 'Reinforcement Learning (RL) encompasses diverse paradigms, including\nmodel-based RL, policy-based RL, and value-based RL, each tailored to\napproximate the model, optimal policy, and optimal value function,\nrespectively. This work investigates the potential hierarchy of representation\ncomplexity -- the complexity of functions to be represented -- among these RL\nparadigms. We first demonstrate that, for a broad class of Markov decision\nprocesses (MDPs), the model can be represented by constant-depth circuits with\npolynomial size or Multi-Layer Perceptrons (MLPs) with constant layers and\npolynomial hidden dimension. However, the representation of the optimal policy\nand optimal value proves to be $\\mathsf{NP}$-complete and unattainable by\nconstant-layer MLPs with polynomial size. This demonstrates a significant\nrepresentation complexity gap between model-based RL and model-free RL, which\nincludes policy-based RL and value-based RL. To further explore the\nrepresentation complexity hierarchy between policy-based RL and value-based RL,\nwe introduce another general class of MDPs where both the model and optimal\npolicy can be represented by constant-depth circuits with polynomial size or\nconstant-layer MLPs with polynomial size. In contrast, representing the optimal\nvalue is $\\mathsf{P}$-complete and intractable via a constant-layer MLP with\npolynomial hidden dimension. This accentuates the intricate representation\ncomplexity associated with value-based RL compared to policy-based RL. In\nsummary, we unveil a potential representation complexity hierarchy within RL --\nrepresenting the model emerges as the easiest task, followed by the optimal\npolicy, while representing the optimal value function presents the most\nintricate challenge.'}, {'Optimistic Policy Optimization is Provably Efficient in Non-stationary\n  MDPs': 'We study episodic reinforcement learning (RL) in non-stationary linear kernel\nMarkov decision processes (MDPs). In this setting, both the reward function and\nthe transition kernel are linear with respect to the given feature maps and are\nallowed to vary over time, as long as their respective parameter variations do\nnot exceed certain variation budgets. We propose the\n$\\underline{\\text{p}}$eriodically $\\underline{\\text{r}}$estarted\n$\\underline{\\text{o}}$ptimistic $\\underline{\\text{p}}$olicy\n$\\underline{\\text{o}}$ptimization algorithm (PROPO), which is an optimistic\npolicy optimization algorithm with linear function approximation. PROPO\nfeatures two mechanisms: sliding-window-based policy evaluation and\nperiodic-restart-based policy improvement, which are tailored for policy\noptimization in a non-stationary environment. In addition, only utilizing the\ntechnique of sliding window, we propose a value-iteration algorithm. We\nestablish dynamic upper bounds for the proposed methods and a matching minimax\nlower bound which shows the (near-) optimality of the proposed methods. To our\nbest knowledge, PROPO is the first provably efficient policy optimization\nalgorithm that handles non-stationarity.'}, {'Distributionally Robust Reinforcement Learning with Interactive Data\n  Collection: Fundamental Hardness and Near-Optimal Algorithm': 'The sim-to-real gap, which represents the disparity between training and\ntesting environments, poses a significant challenge in reinforcement learning\n(RL). A promising approach to addressing this challenge is distributionally\nrobust RL, often framed as a robust Markov decision process (RMDP). In this\nframework, the objective is to find a robust policy that achieves good\nperformance under the worst-case scenario among all environments within a\npre-specified uncertainty set centered around the training environment. Unlike\nprevious work, which relies on a generative model or a pre-collected offline\ndataset enjoying good coverage of the deployment environment, we tackle robust\nRL via interactive data collection, where the learner interacts with the\ntraining environment only and refines the policy through trial and error. In\nthis robust RL paradigm, two main challenges emerge: managing distributional\nrobustness while striking a balance between exploration and exploitation during\ndata collection. Initially, we establish that sample-efficient learning without\nadditional assumptions is unattainable owing to the curse of support shift;\ni.e., the potential disjointedness of the distributional supports between the\ntraining and testing environments. To circumvent such a hardness result, we\nintroduce the vanishing minimal value assumption to RMDPs with a\ntotal-variation (TV) distance robust set, postulating that the minimal value of\nthe optimal robust value function is zero. We prove that such an assumption\neffectively eliminates the support shift issue for RMDPs with a TV distance\nrobust set, and present an algorithm with a provable sample complexity\nguarantee. Our work makes the initial step to uncovering the inherent\ndifficulty of robust RL via interactive data collection and sufficient\nconditions for designing a sample-efficient algorithm accompanied by sharp\nsample complexity analysis.'}, {'Double Pessimism is Provably Efficient for Distributionally Robust\n  Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage': 'In this paper, we study distributionally robust offline reinforcement\nlearning (robust offline RL), which seeks to find an optimal policy purely from\nan offline dataset that can perform well in perturbed environments. In\nspecific, we propose a generic algorithm framework called Doubly Pessimistic\nModel-based Policy Optimization ($P^2MPO$), which features a novel combination\nof a flexible model estimation subroutine and a doubly pessimistic policy\noptimization step. Notably, the double pessimism principle is crucial to\novercome the distributional shifts incurred by (i) the mismatch between the\nbehavior policy and the target policies; and (ii) the perturbation of the\nnominal model. Under certain accuracy conditions on the model estimation\nsubroutine, we prove that $P^2MPO$ is sample-efficient with robust partial\ncoverage data, which only requires the offline data to have good coverage of\nthe distributions induced by the optimal robust policy and the perturbed models\naround the nominal model.\n  By tailoring specific model estimation subroutines for concrete examples of\nRMDPs, including tabular RMDPs, factored RMDPs, kernel and neural RMDPs, we\nprove that $P^2MPO$ enjoys a $\\tilde{\\mathcal{O}}(n^{-1/2})$ convergence rate,\nwhere $n$ is the dataset size. We highlight that all these examples, except\ntabular RMDPs, are first identified and proven tractable by this work.\nFurthermore, we continue our study of robust offline RL in the robust Markov\ngames (RMGs). By extending the double pessimism principle identified for\nsingle-agent RMDPs, we propose another algorithm framework that can efficiently\nfind the robust Nash equilibria among players using only robust unilateral\n(partial) coverage data. To our best knowledge, this work proposes the first\ngeneral learning principle -- double pessimism -- for robust offline RL and\nshows that it is provably efficient with general function approximation.'}, {'Towards Robust Offline Reinforcement Learning under Diverse Data\n  Corruption': ""Offline reinforcement learning (RL) presents a promising approach for\nlearning reinforced policies from offline datasets without the need for costly\nor unsafe interactions with the environment. However, datasets collected by\nhumans in real-world environments are often noisy and may even be maliciously\ncorrupted, which can significantly degrade the performance of offline RL. In\nthis work, we first investigate the performance of current offline RL\nalgorithms under comprehensive data corruption, including states, actions,\nrewards, and dynamics. Our extensive experiments reveal that implicit\nQ-learning (IQL) demonstrates remarkable resilience to data corruption among\nvarious offline RL algorithms. Furthermore, we conduct both empirical and\ntheoretical analyses to understand IQL's robust performance, identifying its\nsupervised policy learning scheme as the key factor. Despite its relative\nrobustness, IQL still suffers from heavy-tail targets of Q functions under\ndynamics corruption. To tackle this challenge, we draw inspiration from robust\nstatistics to employ the Huber loss to handle the heavy-tailedness and utilize\nquantile estimators to balance penalization for corrupted data and learning\nstability. By incorporating these simple yet effective modifications into IQL,\nwe propose a more robust offline RL approach named Robust IQL (RIQL). Extensive\nexperiments demonstrate that RIQL exhibits highly robust performance when\nsubjected to diverse data corruption scenarios.""}, {'Breaking the Moments Condition Barrier: No-Regret Algorithm for Bandits\n  with Super Heavy-Tailed Payoffs': 'Despite a large amount of effort in dealing with heavy-tailed error in\nmachine learning, little is known when moments of the error can become\nnon-existential: the random noise $\\eta$ satisfies Pr$\\left[|\\eta| > |y|\\right]\n\\le 1/|y|^{\\alpha}$ for some $\\alpha > 0$. We make the first attempt to\nactively handle such super heavy-tailed noise in bandit learning problems: We\npropose a novel robust statistical estimator, mean of medians, which estimates\na random variable by computing the empirical mean of a sequence of empirical\nmedians. We then present a generic reductionist algorithmic framework for\nsolving bandit learning problems (including multi-armed and linear bandit\nproblem): the mean of medians estimator can be applied to nearly any bandit\nlearning algorithm as a black-box filtering for its reward signals and obtain\nsimilar regret bound as if the reward is sub-Gaussian. We show that the regret\nbound is near-optimal even with very heavy-tailed noise. We also empirically\ndemonstrate the effectiveness of the proposed algorithm, which further\ncorroborates our theoretical results.'}, {'Provable Sim-to-real Transfer in Continuous Domain with Partial\n  Observations': 'Sim-to-real transfer trains RL agents in the simulated environments and then\ndeploys them in the real world. Sim-to-real transfer has been widely used in\npractice because it is often cheaper, safer and much faster to collect samples\nin simulation than in the real world. Despite the empirical success of the\nsim-to-real transfer, its theoretical foundation is much less understood. In\nthis paper, we study the sim-to-real transfer in continuous domain with partial\nobservations, where the simulated environments and real-world environments are\nmodeled by linear quadratic Gaussian (LQG) systems. We show that a popular\nrobust adversarial training algorithm is capable of learning a policy from the\nsimulated environment that is competitive to the optimal policy in the\nreal-world environment. To achieve our results, we design a new algorithm for\ninfinite-horizon average-cost LQGs and establish a regret bound that depends on\nthe intrinsic complexity of the model class. Our algorithm crucially relies on\na novel history clipping scheme, which might be of independent interest.'}, {'Sample-efficient Learning of Infinite-horizon Average-reward MDPs with\n  General Function Approximation': 'We study infinite-horizon average-reward Markov decision processes (AMDPs) in\nthe context of general function approximation. Specifically, we propose a novel\nalgorithmic framework named Local-fitted Optimization with OPtimism (LOOP),\nwhich incorporates both model-based and value-based incarnations. In\nparticular, LOOP features a novel construction of confidence sets and a\nlow-switching policy updating scheme, which are tailored to the average-reward\nand function approximation setting. Moreover, for AMDPs, we propose a novel\ncomplexity measure -- average-reward generalized eluder coefficient (AGEC) --\nwhich captures the challenge of exploration in AMDPs with general function\napproximation. Such a complexity measure encompasses almost all previously\nknown tractable AMDP models, such as linear AMDPs and linear mixture AMDPs, and\nalso includes newly identified cases such as kernel AMDPs and AMDPs with\nBellman eluder dimensions. Using AGEC, we prove that LOOP achieves a sublinear\n$\\tilde{\\mathcal{O}}(\\mathrm{poly}(d, \\mathrm{sp}(V^*)) \\sqrt{T\\beta} )$\nregret, where $d$ and $\\beta$ correspond to AGEC and log-covering number of the\nhypothesis class respectively, $\\mathrm{sp}(V^*)$ is the span of the optimal\nstate bias function, $T$ denotes the number of steps, and $\\tilde{\\mathcal{O}}\n(\\cdot) $ omits logarithmic factors. When specialized to concrete AMDP models,\nour regret bounds are comparable to those established by the existing\nalgorithms designed specifically for these special cases. To the best of our\nknowledge, this paper presents the first comprehensive theoretical framework\ncapable of handling nearly all AMDPs.'}, {'Can Reinforcement Learning Find Stackelberg-Nash Equilibria in\n  General-Sum Markov Games with Myopic Followers?': ""We study multi-player general-sum Markov games with one of the players\ndesignated as the leader and the other players regarded as followers. In\nparticular, we focus on the class of games where the followers are myopic,\ni.e., they aim to maximize their instantaneous rewards. For such a game, our\ngoal is to find a Stackelberg-Nash equilibrium (SNE), which is a policy pair\n$(\\pi^*, \\nu^*)$ such that (i) $\\pi^*$ is the optimal policy for the leader\nwhen the followers always play their best response, and (ii) $\\nu^*$ is the\nbest response policy of the followers, which is a Nash equilibrium of the\nfollowers' game induced by $\\pi^*$. We develop sample-efficient reinforcement\nlearning (RL) algorithms for solving for an SNE in both online and offline\nsettings. Our algorithms are optimistic and pessimistic variants of\nleast-squares value iteration, and they are readily able to incorporate\nfunction approximation tools in the setting of large state spaces. Furthermore,\nfor the case with linear function approximation, we prove that our algorithms\nachieve sublinear regret and suboptimality under online and offline setups\nrespectively. To the best of our knowledge, we establish the first provably\nefficient RL algorithms for solving for SNEs in general-sum Markov games with\nmyopic followers.""}]","Title: Token-wise Reward Optimization for Language Model Alignment: A Deep Reinforcement Learning Perspective

Abstract:

Context: Recent advancements in large language models have fueled the development of research towards refining these models for specific tasks and enhancing alignment with human preferences. This highlights the critical need for robust methodologies to leverage preferences in the training of language models.

Objective: The primary goal of this research is to systematically explore and optimize language model alignment through the lens of deep reinforcement learning (DRL). Specifically, we aim to improve the policy optimization phase of the reward-based self-training framework (RLHF) by refining the reward computation for effective alignment.

Innovations: 
This work introduces a novel token-wise reward optimization (RTO) scheme within the RLHF framework. We concurrently derive improvements to established methods like PPO while also proposing a comprehensive analysis for token-level rewards computation, advancing the state-of-the-art models in aligning against human preference signals. 

Methods:
Utilizing Pythia-2.8B models as the backbone, we implement our RTO algorithm to derive efficient token-wise rewards. This process is tightly integrated with PPO to enhance model alignment and evaluation. We further devise experiments, drawing on a helpful subset of the Anthropic database, to thoroughly test and validate the performance of our proposed RTO.

Results:
The token-wise rewards implemented via our RTO method consistently surpass baseline models in terms of overall alignment and are notably superior to the conventional sentence-level rewards computed with PPO. New evaluations on models like GPT-4 showcase the superior performance of RTO-trained models across various tasks.

Contributions:
The key contributions herein involve proposing an innovative token-wise reward computation along with an advanced reward optimization scheme tailored for language model alignment. These advancements provide a significant step forward in aligning large language models and invite future research in enhancing RLHF frameworks.

Applications:
This research has broad implications for the development of user-centric language models, enabling more natural and intuitive interactions in chatbots, interactive assistants, and personalized content generation tools. Enhanced alignment and optimization techniques contribute to the overall user experience and satisfaction across various SAAS products.

This structured approach to optimizing token-wise rewards showcases a pivotal step in refining the RLHF framework, promising to elevate the capabilities and utility of large language models for various applications."
"The SuperCLUE-Fin (SC-Fin) benchmark is a pioneering evaluation framework
tailored for Chinese-native financial large language models (FLMs). It assesses
FLMs across six financial application domains and twenty-five specialized
tasks, encompassing theoretical knowledge and practical applications such as
compliance, risk management, and investment analysis. Using multi-turn,
open-ended conversations that mimic real-life scenarios, SC-Fin measures models
on a range of criteria, including accurate financial understanding, logical
reasoning, clarity, computational efficiency, business acumen, risk perception,
and compliance with Chinese regulations.
  In a rigorous evaluation involving over a thousand questions, SC-Fin
identifies a performance hierarchy where domestic models like GLM-4 and
MoonShot-v1-128k outperform others with an A-grade, highlighting the potential
for further development in transforming theoretical knowledge into pragmatic
financial solutions. This benchmark serves as a critical tool for refining FLMs
in the Chinese context, directing improvements in financial knowledge
databases, standardizing financial interpretations, and promoting models that
prioritize compliance, risk management, and secure practices.
  We create a contextually relevant and comprehensive benchmark that drives the
development of AI in the Chinese financial sector. SC-Fin facilitates the
advancement and responsible deployment of FLMs, offering valuable insights for
enhancing model performance and usability for both individual and institutional
users in the Chinese market..~\footnote{Our benchmark can be found at
\url{https://www.CLUEbenchmarks.com}}.","[{'Sobolev Inequalities in Spacelike Submanifolds of Minkowski Space': 'We follow the method of ABP estimate in \\cite{brendle2021} and apply it to\nspacelike submanifolds in $\\mathbb R^{n,1}$. We then obtain Michael-Simon type\ninequalities. Surprisingly, our investigation leads to a Sobolev inequality\nwithout a mean curvature term, provided the hypersurface is mean convex.'}, {'Computational Analysis of Control Systems Using Dynamic Optimization': 'Several concepts on the measure of observability, reachability, and\nrobustness are defined and illustrated for both linear and nonlinear control\nsystems. Defined by using computational dynamic optimization, these concepts\nare applicable to a wide spectrum of problems. Some questions addressed include\nthe observability based on user-information, the determination of strong\nobservability vs. weak observability, partial observability of complex systems,\nthe computation of $L^2$-gain for nonlinear control systems, and the measure of\nreachability in the presence of state constraints. Examples on dynamic systems\ndefined by both ordinary and partial differential equations are shown.'}, {'Sparsity-Based Kalman Filters for Data Assimilation': 'Several variations of the Kalman filter algorithm, such as the extended\nKalman filter (EKF) and the unscented Kalman filter (UKF), are widely used in\nscience and engineering applications. In this paper, we introduce two\nalgorithms of sparsity-based Kalman filters, namely the sparse UKF and the\nprogressive EKF. The filters are designed specifically for problems with very\nhigh dimensions. Different from various types of ensemble Kalman filters\n(EnKFs) in which the error covariance is approximated using a set of dense\nensemble vectors, the algorithms developed in this paper are based on sparse\nmatrix approximations of error covariance. The new algorithms enjoy several\nadvantages. The error covariance has full rank without being limited by a set\nof ensembles. In addition to the estimated states, the algorithms provide\nupdated error covariance for the next assimilation cycle. The sparsity of error\ncovariance significantly reduces the required memory size for the numerical\ncomputation. In addition, the granularity of the sparse error covariance can be\nadjusted to optimize the parallelization of the algorithms.'}, {'Mean Square Capacity of Power Constrained Fading Channels with Causal\n  Encoders and Decoders': 'This paper is concerned with the mean square stabilization problem of\ndiscrete-time LTI systems over a power constrained fading channel. Different\nfrom existing research works, the channel considered in this paper suffers from\nboth fading and additive noises. We allow any form of causal channel\nencoders/decoders, unlike linear encoders/decoders commonly studied in the\nliterature. Sufficient conditions and necessary conditions for the mean square\nstabilizability are given in terms of channel parameters such as transmission\npower and fading and additive noise statistics in relation to the unstable\neigenvalues of the open-loop system matrix. The corresponding mean square\ncapacity of the power constrained fading channel under causal encoders/decoders\nis given. It is proved that this mean square capacity is smaller than the\ncorresponding Shannon channel capacity. In the end, numerical examples are\npresented, which demonstrate that the causal encoders/decoders render less\nrestrictive stabilizability conditions than those under linear\nencoders/decoders studied in the existing works.'}, {'Mean Square Stabilization of Vector LTI Systems over Power Constrained\n  Lossy Channels': 'This paper studies the mean square stabilization problem of vector LTI\nsystems over power constrained lossy channels. The communication channel is\nwith packet dropouts, additive noises and input power constraints. To overcome\nthe difficulty of optimally allocating channel resources among different\nsub-dynamics, schedulers are designed with time division multiplexing of\nchannels. An adaptive TDMA (Time Division Multiple Access) scheduler is\nproposed first, which is shown to be able to achieve a larger stabilizability\nregion than the conventional TDMA scheduler, and is optimal under some special\ncases. In particular, for two-dimensional systems, an optimal scheduler is\ndesigned, which provides the necessary and sufficient condition for mean square\nstabilization.'}, {'Remote State Estimation with Stochastic Event-triggered Sensor Schedule\n  in the Presence of Packet Drops': 'This paper studies the remote state estimation problem of linear\ntime-invariant systems with stochastic event-triggered sensor schedules in the\npresence of packet drops between the sensor and the estimator. It is shown that\nthe system state conditioned on the available information at the estimator side\nis Gaussian mixture distributed. Minimum mean square error (MMSE) estimators\nare subsequently derived for both open-loop and closed-loop schedules. Since\nthe optimal estimators require exponentially increasing computation and memory,\nsub-optimal estimators to reduce the computational complexities are further\nprovided. In the end, simulations are conducted to illustrate the performance\nof the optimal and sub-optimal estimators.'}, {'Distributed Consensus over Markovian Packet Loss Channels': 'This paper studies the consensusability problem of multi-agent systems\n(MASs), where agents communicate with each other through Markovian packet loss\nchannels. We try to determine conditions under which there exists a linear\ndistributed consensus controller such that the MAS can achieve mean square\nconsensus. We first provide a necessary and sufficient consensus condition for\nMASs with single input and i.i.d.\\ channel losses, which complements existing\nresults. Then we proceed to study the case with identical Markovian packet\nlosses. A necessary and sufficient consensus condition is firstly derived based\non the stability of Markov jump linear systems. Then a numerically verifiable\nconsensus criterion in terms of the feasibility of linear matrix inequalities\n(LMIs) is proposed. Furthermore, analytic sufficient conditions and necessary\nconditions for mean square consensusability are provided for general MASs. The\ncase with nonidentical packet loss is studied subsequently. The necessary and\nsufficient consensus condition and a sufficient consensus condition in terms of\nLMIs are proposed. In the end, numerical simulations are conducted to verify\nthe derived results.'}, {'CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language\n  Model': 'In this paper, we introduce the Chinese corpus from CLUE organization,\nCLUECorpus2020, a large-scale corpus that can be used directly for\nself-supervised learning such as pre-training of a language model, or language\ngeneration. It has 100G raw corpus with 35 billion Chinese characters, which is\nretrieved from Common Crawl. To better understand this corpus, we conduct\nlanguage understanding experiments on both small and large scale, and results\nshow that the models trained on this corpus can achieve excellent performance\non Chinese. We release a new Chinese vocabulary with a size of 8K, which is\nonly one-third of the vocabulary size used in Chinese Bert released by Google.\nIt saves computational cost and memory while works as good as original\nvocabulary. We also release both large and tiny versions of the pre-trained\nmodel on this corpus. The former achieves the state-of-the-art result, and the\nlatter retains most precision while accelerating training and prediction speed\nfor eight times compared to Bert-base. To facilitate future work on\nself-supervised learning on Chinese, we release our dataset, new vocabulary,\ncodes, and pre-trained models on Github.'}, {'Neural Energy Casimir Control for Port-Hamiltonian Systems': 'The energy Casimir method is an effective controller design approach to\nstabilize port-Hamiltonian systems at a desired equilibrium. However, its\napplication relies on the availability of suitable Casimir and Lyapunov\nfunctions, whose computation are generally intractable. In this paper, we\npropose a neural network-based framework to learn these functions. We show how\nto achieve equilibrium assignment by adding suitable regularization terms in\nthe training cost. We also propose a parameterization of Casimir functions for\nreducing the training complexity. Moreover, the distance between the\nequilibrium of the learned Lyapunov function and the desired equilibrium is\nanalyzed, which indicates that for small suboptimality gaps, the distance\ndecreases linearly with respect to the training loss. Our methods are backed up\nby simulations on a pendulum system.'}, {'Finite-sample-based Spectral Radius Estimation and Stabilizability Test\n  for Networked Control Systems': 'In the analysis and control of discrete-time linear time-invariant systems,\nthe spectral radius of the system state matrix plays an essential role.\nUsually, it is assumed that system matrices are known, from which the spectral\nradius can be directly computed. Instead, we consider the setting where the\nsystem is affected by process noise, and one has only finitely many samples of\nsystem input and state measurements. We provide two methods for estimating the\nspectral radius and derive error bounds that hold with high probability.\nMoreover, we show how to use the derived results to test stabilizability for\nnetworked control systems (NCSs) with lossy channels when only finitely many\nsamples of the system input, state, and packet drop sequence are available.'}]","Title: SuperCLUE-Fin: A Comprehensive Benchmarking Framework for Chinese Financial Model Evaluation

Abstract:

As artificial intelligence (AI) technologies advance, there is increasing demand for AI models capable of handling complex financial tasks relevant to the Chinese market, necessitating a novel benchmark framework. This paper introduces SuperCLUE-Fin, a benchmark designed to evaluate the comprehensive competencies of large language models in financial domains. 

The main goal is to assess domestic and international models in dealing with the ever-evolving and complex financial scenarios specific to China, ensuring they can fulfill the needs of both individual investors and financial institutions. SuperCLUE-Fin introduces new models such as GLM-4, MoonShot-v1-128K, GPT-4, MACError, and ERNIE Bot 4.0 to the market for benchmarking purposes.

The methods involve an all-around, multi-angle evaluation system with six contextual task dimensions: financial fundamentals, information extraction and analysis, mathematical calculation, logical analysis, compliance and risk management, and investment research and advice. The adoption of open multi-round interactive quality assurance enhances the realness of the models' performance.

Through rigorous evaluation, the results established benchmarks for AI financial models, pinpointing strengths and weaknesses, and guiding advancements in financial AI. The contributions are substantial, providing policymakers, financial industry players, and AI developers with critical insights into the current status of AI in Chinese finance and aiding in the direction for future innovations.

Potential implications include improving AI models' effectiveness in real-world financial decision-making, enhancing trust and safety in financial AI applications, and facilitating better accessibility and efficiency in the lucrative Chinese financial markets, benefiting both consumers and the economy at large. This work serves as a crucial stepping stone for fostering an AI-driven, innovative financial ecosystem."
"Knowledge graphs (KGs), which store an extensive number of relational facts
(head, relation, tail), serve various applications. While many downstream tasks
highly rely on the expressive modeling and predictive embedding of KGs, most of
the current KG representation learning methods, where each entity is embedded
as a vector in the Euclidean space and each relation is embedded as a
transformation, follow an entity ranking protocol. On one hand, such an
embedding design cannot capture many-to-many relations. On the other hand, in
many retrieval cases, the users wish to get an exact set of answers without any
ranking, especially when the results are expected to be precise, e.g., which
genes cause an illness. Such scenarios are commonly referred to as ""set
retrieval"". This work presents a pioneering study on the KG set retrieval
problem. We show that the set retrieval highly depends on expressive modeling
of many-to-many relations, and propose a new KG embedding model SpherE to
address this problem. SpherE is based on rotational embedding methods, but each
entity is embedded as a sphere instead of a vector. While inheriting the high
interpretability of rotational-based models, our SpherE can more expressively
model one-to-many, many-to-one, and many-to-many relations. Through extensive
experiments, we show that our SpherE can well address the set retrieval problem
while still having a good predictive ability to infer missing facts. The code
is available at https://github.com/Violet24K/SpherE.","[{'The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic\n  Parrots and Hallucination': 'With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our\nwhole society, rapidly altering the way we think, create and live. For\ninstance, the GPT integration in Bing has altered our approach to online\nsearching. While nascent LLMs have many advantages, new legal and ethical risks\nare also emerging, stemming in particular from stochastic parrots and\nhallucination. The EU is the first and foremost jurisdiction that has focused\non the regulation of AI models. However, the risks posed by the new LLMs are\nlikely to be underestimated by the emerging EU regulatory paradigm. Therefore,\nthis correspondence warns that the European AI regulatory paradigm must evolve\nfurther to mitigate such risks.'}, {'DiffuRec: A Diffusion Model for Sequential Recommendation': ""Mainstream solutions to Sequential Recommendation (SR) represent items with\nfixed vectors. These vectors have limited capability in capturing items' latent\naspects and users' diverse preferences. As a new generative paradigm, Diffusion\nmodels have achieved excellent performance in areas like computer vision and\nnatural language processing. To our understanding, its unique merit in\nrepresentation generation well fits the problem setting of sequential\nrecommendation. In this paper, we make the very first attempt to adapt\nDiffusion model to SR and propose DiffuRec, for item representation\nconstruction and uncertainty injection. Rather than modeling item\nrepresentations as fixed vectors, we represent them as distributions in\nDiffuRec, which reflect user's multiple interests and item's various aspects\nadaptively. In diffusion phase, DiffuRec corrupts the target item embedding\ninto a Gaussian distribution via noise adding, which is further applied for\nsequential item distribution representation generation and uncertainty\ninjection. Afterward, the item representation is fed into an Approximator for\ntarget item representation reconstruction. In reverse phase, based on user's\nhistorical interaction behaviors, we reverse a Gaussian noise into the target\nitem representation, then apply a rounding operation for target item\nprediction. Experiments over four datasets show that DiffuRec outperforms\nstrong baselines by a large margin.""}, {'A Kullback-Leibler Divergence-based Distributionally Robust Optimization\n  Model for Heat Pump Day-ahead Operational Schedule in Distribution Networks': 'For its high coefficient of performance and zero local emissions, the heat\npump (HP) has recently become popular in North Europe and China. However, the\nintegration of HPs may aggravate the daily peak-valley gap in distribution\nnetworks significantly.'}, {'Improved Lyman Alpha Tomography using Optimized Reconstruction with\n  Constraints on Absorption (ORCA)': 'In this work, we propose an improved approach to reconstruct the\nthree-dimensional intergalactic medium from observed Lyman-$\\alpha$ forest\nabsorption features. We present our new method, the Optimized Reconstruction\nwith Constraints on Absorption (ORCA), which outperforms the current baseline\nWiener Filter (WF) when tested on mock Lyman Alpha forest data generated from\nhydrodynamical simulations. We find that both reconstructed flux errors and\ncosmic web classification improve substantially with ORCA, equivalent to\n30-40\\% additional sight-lines with the standard WF. We use this method to\nidentify and classify extremal objects, i.e. voids and (proto)-clusters, and\nfind improved reconstruction across all summary statistics explored. We apply\nORCA to existing Lyman Alpha forest data from the COSMOS Lyman Alpha Mapping\nand Tomography Observations (CLAMATO) Survey and compare it to the WF\nreconstruction.'}, {'Dynamic Local Feature Aggregation for Learning on Point Clouds': 'Existing point cloud learning methods aggregate features from neighbouring\npoints relying on constructing graph in the spatial domain, which results in\nfeature update for each point based on spatially-fixed neighbours throughout\nlayers. In this paper, we propose a dynamic feature aggregation (DFA) method\nthat can transfer information by constructing local graphs in the feature\ndomain without spatial constraints. By finding k-nearest neighbors in the\nfeature domain, we perform relative position encoding and semantic feature\nencoding to explore latent position and feature similarity information,\nrespectively, so that rich local features can be learned. At the same time, we\nalso learn low-dimensional global features from the original point cloud for\nenhancing feature representation. Between DFA layers, we dynamically update the\nconstructed local graph structure, so that we can learn richer information,\nwhich greatly improves adaptability and efficiency. We demonstrate the\nsuperiority of our method by conducting extensive experiments on point cloud\nclassification and segmentation tasks. Implementation code is available:\nhttps://github.com/jiamang/DFA.'}, {'Robust and efficient verification of graph states in blind\n  measurement-based quantum computation': 'Blind quantum computation (BQC) is a secure quantum computation method that\nprotects the privacy of clients. Measurement-based quantum computation (MBQC)\nis a promising approach for realizing BQC. To obtain reliable results in blind\nMBQC, it is crucial to verify whether the resource graph states are accurately\nprepared in the adversarial scenario. However, previous verification protocols\nfor this task are too resource consuming or noise susceptible to be applied in\npractice. Here, we propose a robust and efficient protocol for verifying\narbitrary graph states with any prime local dimension in the adversarial\nscenario, which leads to a robust and efficient protocol for verifying the\nresource state in blind MBQC. Our protocol requires only local Pauli\nmeasurements and is thus easy to realize with current technologies.\nNevertheless, it can achieve the optimal scaling behaviors with respect to the\nsystem size and the target precision as quantified by the infidelity and\nsignificance level, which has never been achieved before. Notably, our protocol\ncan exponentially enhance the scaling behavior with the significance level.'}, {'Allocating Mixed Goods with Customized Fairness and Indivisibility Ratio': ""We consider the problem of fairly allocating a combination of divisible and\nindivisible goods. While fairness criteria like envy-freeness (EF) and\nproportionality (PROP) can always be achieved for divisible goods, only their\nrelaxed versions, such as the ''up to one'' relaxations EF1 and PROP1, can be\nsatisfied when the goods are indivisible. The ''up to one'' relaxations require\nthe fairness conditions to be satisfied provided that one good can be\ncompletely eliminated or added in the comparison. In this work, we bridge the\ngap between the two extremes and propose ''up to a fraction'' relaxations for\nthe allocation of mixed divisible and indivisible goods. The fraction is\ndetermined based on the proportion of indivisible goods, which we call the\nindivisibility ratio. The new concepts also introduce asymmetric conditions\nthat are customized for individuals with varying indivisibility ratios. We\nprovide both upper and lower bounds on the fractions of the modified item in\norder to satisfy the fairness criterion. Our results are tight up to a constant\nfor EF and asymptotically tight for PROP.""}, {'Implementation of generalized measurements on a qudit via quantum walks': 'Quantum measurements play a fundamental role in quantum mechanics and quantum\ninformation processing, but it is not easy to implement generalized\nmeasurements, the most powerful measurements allowed by quantum mechanics. Here\nwe propose a simple recipe for implementing generalized measurements on a qudit\nvia quantum walks. With this recipe, any discrete quantum measurement can be\nimplemented via a one-dimensional discrete quantum walk; the number of steps is\nonly two times the number of measurement outcomes. As an illustration, we\npresent a unified solution for implementing arbitrary symmetric informationally\ncomplete measurements in dimension 3.'}, {'Sequentially Optimal Pricing under Informational Robustness': ""A seller sells an object over time but is uncertain how the buyer learns\ntheir willingness-to-pay. We consider informational robustness under\n\\textit{limited commitment}, where the seller offers a price \\textit{each\nperiod} to maximize continuation profit against worst-case information arrival.\nOur formulation maintains dynamic consistency by considering the worst case\n\\textit{sequentially}. Under general conditions, we characterize an essentially\nunique equilibrium where the buyer does not delay to learn more later.\nFurthermore, we identify a condition that ensures the equilibrium price path is\n``reinforcing,'' so even dynamically inconsistent information arrival would not\nlower the seller's payoff below the equilibrium level.""}, {'Generalized matrix nearness problems': 'We show that the global minimum solution of $\\lVert A - BXC \\rVert$ can be\nfound in closed-form with singular value decompositions and generalized\nsingular value decompositions for a variety of constraints on $X$ involving\nrank, norm, symmetry, two-sided product, and prescribed eigenvalue. This\nextends the solution of Friedland--Torokhti for the generalized\nrank-constrained approximation problem to other constraints as well as provides\nan alternative solution for rank constraint in terms of singular value\ndecompositions. For more complicated constraints on $X$ involving structures\nsuch as Toeplitz, Hankel, circulant, nonnegativity, stochasticity, positive\nsemidefiniteness, prescribed eigenvector, etc, we prove that a simple iterative\nmethod is linearly and globally convergent to the global minimum solution.'}]","Title: SpherE: Universal Sphere Entities for Knowledge Graphs 

Abstract:

Strengthened by the burgeoning of complex data, Knowledge Graph Embedding (KGE) has blossomed into a powerful representation technique. Currently undermining its broad application is the unavailability of a method that most directly models entities as 'universe objects'. In seeking a solution, this research introduces SpherE, a pioneering entity embedding method. Aiming to enhance model interpretability, SpherE reforms Euclidean vectors into 2D, 3D spheres and kD spheres. This novel approach significantly expands model expressivity, encompassing many unique relation properties (symmetry, anti-symmetry, inversion and others). Upon evaluating against benchmark methods like RotatE and HousE models, experimental results substantiate the effectiveness of SpherE, demonstrating superior performance across diverse aspects like retrieval accuracy and relation expressiveness. Moreover, this study establishes the Knowledge Graph Set Retrieval problem, providing a stringent benchmark that showcases the innovative capabilities of SpherE. This work underscores the importance of universal entity modeling in enhancing performance, interpretability, and applicability in knowledge graph domains. The methodology and findings hold significant implications for a range of fields such as biomedical knowledge querying, enhancing natural language processing, and enriching semantic search functionalities. 

Keywords: Knowledge Graph Embedding (KGE), SpherE Model, Universality of Entities, Relation Expressiveness, Knowledge Graph Retrieval."
"In this paper, we establish a benchmark for table visual question answering,
referred to as the TableVQA-Bench, derived from pre-existing table
question-answering (QA) and table structure recognition datasets. It is
important to note that existing datasets have not incorporated images or QA
pairs, which are two crucial components of TableVQA. As such, the primary
objective of this paper is to obtain these necessary components. Specifically,
images are sourced either through the application of a \textit{stylesheet} or
by employing the proposed table rendering system. QA pairs are generated by
exploiting the large language model (LLM) where the input is a text-formatted
table. Ultimately, the completed TableVQA-Bench comprises 1,500 QA pairs. We
comprehensively compare the performance of various multi-modal large language
models (MLLMs) on TableVQA-Bench. GPT-4V achieves the highest accuracy among
commercial and open-sourced MLLMs from our experiments. Moreover, we discover
that the number of vision queries plays a significant role in TableVQA
performance. To further analyze the capabilities of MLLMs in comparison to
their LLM backbones, we investigate by presenting image-formatted tables to
MLLMs and text-formatted tables to LLMs, respectively. Our findings suggest
that processing visual inputs is more challenging than text inputs, as
evidenced by the lower performance of MLLMs, despite generally requiring higher
computational costs than LLMs. The proposed TableVQA-Bench and evaluation codes
are available at
\href{https://github.com/naver-ai/tablevqabench}{https://github.com/naver-ai/tablevqabench}.","[{'Multi-modal Text Recognition Networks: Interactive Enhancements between\n  Visual and Semantic Features': 'Linguistic knowledge has brought great benefits to scene text recognition by\nproviding semantics to refine character sequences. However, since linguistic\nknowledge has been applied individually on the output sequence, previous\nmethods have not fully utilized the semantics to understand visual clues for\ntext recognition. This paper introduces a novel method, called Multi-modAl Text\nRecognition Network (MATRN), that enables interactions between visual and\nsemantic features for better recognition performances. Specifically, MATRN\nidentifies visual and semantic feature pairs and encodes spatial information\ninto semantic features. Based on the spatial encoding, visual and semantic\nfeatures are enhanced by referring to related features in the other modality.\nFurthermore, MATRN stimulates combining semantic features into visual features\nby hiding visual clues related to the character in the training phase. Our\nexperiments demonstrate that MATRN achieves state-of-the-art performances on\nseven benchmarks with large margins, while naive combinations of two modalities\nshow less-effective improvements. Further ablative studies prove the\neffectiveness of our proposed components. Our implementation is available at\nhttps://github.com/wp03052/MATRN.'}, {'On Web-based Visual Corpus Construction for Visual Document\n  Understanding': 'In recent years, research on visual document understanding (VDU) has grown\nsignificantly, with a particular emphasis on the development of self-supervised\nlearning methods. However, one of the significant challenges faced in this\nfield is the limited availability of publicly accessible visual corpora or\nextensive collections of images with detailed text annotations, particularly\nfor non-Latin or resource-scarce languages. To address this challenge, we\npropose Web-based Visual Corpus Builder (Webvicob), a dataset generator engine\ncapable of constructing large-scale, multilingual visual corpora from raw\nWikipedia HTML dumps. Our experiments demonstrate that the data generated by\nWebvicob can be used to train robust VDU models that perform well on various\ndownstream tasks, such as DocVQA and post-OCR parsing. Furthermore, when using\na dataset of 1 million images generated by Webvicob, we observed an improvement\nof over 13% on the DocVQA Task 3 compared to a dataset of 11 million images\nfrom the IIT-CDIP. The implementation of our engine is publicly available on\nhttps://github.com/clovaai/webvicob'}, {'Towards Unified Scene Text Spotting based on Sequence Generation': 'Sequence generation models have recently made significant progress in\nunifying various vision tasks. Although some auto-regressive models have\ndemonstrated promising results in end-to-end text spotting, they use specific\ndetection formats while ignoring various text shapes and are limited in the\nmaximum number of text instances that can be detected. To overcome these\nlimitations, we propose a UNIfied scene Text Spotter, called UNITS. Our model\nunifies various detection formats, including quadrilaterals and polygons,\nallowing it to detect text in arbitrary shapes. Additionally, we apply\nstarting-point prompting to enable the model to extract texts from an arbitrary\nstarting point, thereby extracting more texts beyond the number of instances it\nwas trained on. Experimental results demonstrate that our method achieves\ncompetitive performance compared to state-of-the-art methods. Further analysis\nshows that UNITS can extract a larger number of texts than it was trained on.\nWe provide the code for our method at https://github.com/clovaai/units.'}, {'A New Convolutional Network-in-Network Structure and Its Applications in\n  Skin Detection, Semantic Segmentation, and Artifact Reduction': 'The inception network has been shown to provide good performance on image\nclassification problems, but there are not much evidences that it is also\neffective for the image restoration or pixel-wise labeling problems. For image\nrestoration problems, the pooling is generally not used because the decimated\nfeatures are not helpful for the reconstruction of an image as the output.\nMoreover, most deep learning architectures for the restoration problems do not\nuse dense prediction that need lots of training parameters. From these\nobservations, for enjoying the performance of inception-like structure on the\nimage based problems we propose a new convolutional network-in-network\nstructure. The proposed network can be considered a modification of inception\nstructure where pool projection and pooling layer are removed for maintaining\nthe entire feature map size, and a larger kernel filter is added instead.\nProposed network greatly reduces the number of parameters on account of removed\ndense prediction and pooling, which is an advantage, but may also reduce the\nreceptive field in each layer. Hence, we add a larger kernel than the original\ninception structure for not increasing the depth of layers. The proposed\nstructure is applied to typical image-to-image learning problems, i.e., the\nproblems where the size of input and output are same such as skin detection,\nsemantic segmentation, and compression artifacts reduction. Extensive\nexperiments show that the proposed network brings comparable or better results\nthan the state-of-the-art convolutional neural networks for these problems.'}, {'SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text\n  Recognition Models': 'For successful scene text recognition (STR) models, synthetic text image\ngenerators have alleviated the lack of annotated text images from the real\nworld. Specifically, they generate multiple text images with diverse\nbackgrounds, font styles, and text shapes and enable STR models to learn visual\npatterns that might not be accessible from manually annotated data. In this\npaper, we introduce a new synthetic text image generator, SynthTIGER, by\nanalyzing techniques used for text image synthesis and integrating effective\nones under a single algorithm. Moreover, we propose two techniques that\nalleviate the long-tail problem in length and character distributions of\ntraining data. In our experiments, SynthTIGER achieves better STR performance\nthan the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST).\nOur ablation study demonstrates the benefits of using sub-components of\nSynthTIGER and the guideline on generating synthetic text images for STR\nmodels. Our implementation is publicly available at\nhttps://github.com/clovaai/synthtiger.'}, {'SCOB: Universal Text Understanding via Character-wise Supervised\n  Contrastive Learning with Online Text Rendering for Bridging Domain Gap': 'Inspired by the great success of language model (LM)-based pre-training,\nrecent studies in visual document understanding have explored LM-based\npre-training methods for modeling text within document images. Among them,\npre-training that reads all text from an image has shown promise, but often\nexhibits instability and even fails when applied to broader domains, such as\nthose involving both visual documents and scene text images. This is a\nsubstantial limitation for real-world scenarios, where the processing of text\nimage inputs in diverse domains is essential. In this paper, we investigate\neffective pre-training tasks in the broader domains and also propose a novel\npre-training method called SCOB that leverages character-wise supervised\ncontrastive learning with online text rendering to effectively pre-train\ndocument and scene text domains by bridging the domain gap. Moreover, SCOB\nenables weakly supervised learning, significantly reducing annotation costs.\nExtensive benchmarks demonstrate that SCOB generally improves vanilla\npre-training methods and achieves comparable performance to state-of-the-art\nmethods. Our findings suggest that SCOB can be served generally and effectively\nfor read-type pre-training methods. The code will be available at\nhttps://github.com/naver-ai/scob.'}, {'RewriteNet: Reliable Scene Text Editing with Implicit Decomposition of\n  Text Contents and Styles': 'Scene text editing (STE), which converts a text in a scene image into the\ndesired text while preserving an original style, is a challenging task due to a\ncomplex intervention between text and style. In this paper, we propose a novel\nSTE model, referred to as RewriteNet, that decomposes text images into content\nand style features and re-writes a text in the original image. Specifically,\nRewriteNet implicitly distinguishes the content from the style by introducing\nscene text recognition. Additionally, independent of the exact supervisions\nwith synthetic examples, we propose a self-supervised training scheme for\nunlabeled real-world images, which bridges the domain gap between synthetic and\nreal data. Our experiments present that RewriteNet achieves better generation\nperformances than other comparisons. Further analysis proves the feature\ndecomposition of RewriteNet and demonstrates the reliability and robustness\nthrough diverse experiments. Our implementation is publicly available at\n\\url{https://github.com/clovaai/rewritenet}'}, {'Visually-Situated Natural Language Understanding with Contrastive\n  Reading Model and Frozen Large Language Models': 'Recent advances in Large Language Models (LLMs) have stimulated a surge of\nresearch aimed at extending their applications to the visual domain. While\nthese models exhibit promise in generating abstract image captions and\nfacilitating natural conversations, their performance on text-rich images still\nrequires improvement. In this paper, we introduce Contrastive Reading Model\n(Cream), a novel neural architecture designed to enhance the language-image\nunderstanding capability of LLMs by capturing intricate details that are often\noverlooked in existing methods. Cream combines vision and auxiliary encoders,\nfortified by a contrastive feature alignment technique, to achieve a more\neffective comprehension of language information in visually situated contexts\nwithin the images. Our approach bridges the gap between vision and language\nunderstanding, paving the way for the development of more sophisticated\nDocument Intelligence Assistants. Through rigorous evaluations across diverse\nvisually-situated language understanding tasks that demand reasoning\ncapabilities, we demonstrate the compelling performance of Cream, positioning\nit as a prominent model in the field of visual document understanding. We\nprovide our codebase and newly-generated datasets at\nhttps://github.com/naver-ai/cream .'}, {'DEER: Detection-agnostic End-to-End Recognizer for Scene Text Spotting': 'Recent end-to-end scene text spotters have achieved great improvement in\nrecognizing arbitrary-shaped text instances. Common approaches for text\nspotting use region of interest pooling or segmentation masks to restrict\nfeatures to single text instances. However, this makes it hard for the\nrecognizer to decode correct sequences when the detection is not accurate i.e.\none or more characters are cropped out. Considering that it is hard to\naccurately decide word boundaries with only the detector, we propose a novel\nDetection-agnostic End-to-End Recognizer, DEER, framework. The proposed method\nreduces the tight dependency between detection and recognition modules by\nbridging them with a single reference point for each text instance, instead of\nusing detected regions. The proposed method allows the decoder to recognize the\ntexts that are indicated by the reference point, with features from the whole\nimage. Since only a single point is required to recognize the text, the\nproposed method enables text spotting without an arbitrarily-shaped detector or\nbounding polygon annotations. Experimental results present that the proposed\nmethod achieves competitive results on regular and arbitrarily-shaped text\nspotting benchmarks. Further analysis shows that DEER is robust to the\ndetection errors. The code and dataset will be publicly available.'}, {'Transfer Learning from Synthetic to Real-Noise Denoising with Adaptive\n  Instance Normalization': 'Real-noise denoising is a challenging task because the statistics of\nreal-noise do not follow the normal distribution, and they are also spatially\nand temporally changing. In order to cope with various and complex real-noise,\nwe propose a well-generalized denoising architecture and a transfer learning\nscheme. Specifically, we adopt an adaptive instance normalization to build a\ndenoiser, which can regularize the feature map and prevent the network from\noverfitting to the training set. We also introduce a transfer learning scheme\nthat transfers knowledge learned from synthetic-noise data to the real-noise\ndenoiser. From the proposed transfer learning, the synthetic-noise denoiser can\nlearn general features from various synthetic-noise data, and the real-noise\ndenoiser can learn the real-noise characteristics from real data. From the\nexperiments, we find that the proposed denoising method has great\ngeneralization ability, such that our network trained with synthetic-noise\nachieves the best performance for Darmstadt Noise Dataset (DND) among the\nmethods from published papers. We can also see that the proposed transfer\nlearning scheme robustly works for real-noise images through the learning with\na very small number of labeled data.'}]","**Abstract**

**Background**: The rapid growth of tabular data in various domains, including finance, opens new avenues for querying. However, the lack of standardized benchmarks for ""table-based visual question answering (TSR)"" poses a challenge for researchers and developers alike. 

**Objective**: The study aims to design and populate a benchmark dataset, TableVQA-Bench, to facilitate research on TSR and to foster advancements in AI models.

**Innovations**: The development of TableVQA-Bench comprises three core datasets: VWTQ, VTabFact, and FinTabNetQA, created by extracting from Wikipedia, TabFact, and FinTabNet, and incorporating novel human-generated questions to enhance robustness. This effort promotes a unified performance evaluation mechanism for TSR.

**Methods**: The benchmark spans diverse table domains capturing four key modalities: images, text (HTML), and respective QA pairs. For table acquisition, the framework leverages existing resources and introduces new synthetic designs (VWTQ-Syn, FinTabNetQA-Rev) through human expertise and AI power.

**Results**: The evaluation showcases a range of TSR models' performance, involving multi-modal and 2-stage inference approaches. Models like Gemini-Pro and GPT-4 demonstrate varying degrees of proficiency, revealing insights into model limitations and avenues for improvement.

**Contributions**: The TableVQA-Bench introduces a comprehensive, scalable benchmark for TSR, offering researchers and practitioners a standardized platform to test, compare, and develop TSR AI models. 

**Applications**: The benchmark facilitates advancements in AI for visual computing, particularly in BI tools, educational software, and specialized query systems that interact with extensive tabular data across different sectors.

This research not only paves the way for precise machine understanding of tabular data but also stimulates further exploration and innovation in AI systems designed for querying and manipulating structured visual data."
"This work designs and analyzes a novel set of algorithms for multi-agent
reinforcement learning (MARL) based on the principle of information-directed
sampling (IDS). These algorithms draw inspiration from foundational concepts in
information theory, and are proven to be sample efficient in MARL settings such
as two-player zero-sum Markov games (MGs) and multi-player general-sum MGs. For
episodic two-player zero-sum MGs, we present three sample-efficient algorithms
for learning Nash equilibrium. The basic algorithm, referred to as MAIDS,
employs an asymmetric learning structure where the max-player first solves a
minimax optimization problem based on the joint information ratio of the joint
policy, and the min-player then minimizes the marginal information ratio with
the max-player's policy fixed. Theoretical analyses show that it achieves a
Bayesian regret of tilde{O}(sqrt{K}) for K episodes. To reduce the
computational load of MAIDS, we develop an improved algorithm called Reg-MAIDS,
which has the same Bayesian regret bound while enjoying less computational
complexity. Moreover, by leveraging the flexibility of IDS principle in
choosing the learning target, we propose two methods for constructing
compressed environments based on rate-distortion theory, upon which we develop
an algorithm Compressed-MAIDS wherein the learning target is a compressed
environment. Finally, we extend Reg-MAIDS to multi-player general-sum MGs and
prove that it can learn either the Nash equilibrium or coarse correlated
equilibrium in a sample efficient manner.","[{'Computationally Efficient Covert Communication': ""In this paper, we design the first computationally efficient codes for\nsimultaneously reliable and covert communication over Binary Symmetric Channels\n(BSCs). Our setting is as follows: a transmitter Alice wishes to potentially\nreliably transmit a message to a receiver Bob, while ensuring that the\ntransmission taking place is covert with respect to an eavesdropper Willie (who\nhears Alice's transmission over a noisier BSC). Prior works show that Alice can\nreliably and covertly transmit O(\\sqrt{n}) bits over n channel uses without any\nshared secret between Alice and Bob. One drawback of prior works is that the\ncomputational complexity of the codes designed scales as 2^{\\Theta(\\sqrt{n})}.\nIn this work we provide the first computationally tractable codes with provable\nguarantees on both reliability and covertness, while simultaneously achieving\nthe best known throughput for the problem.""}, {'Covert Identification over Binary-Input Discrete Memoryless Channels': 'This paper considers the covert identification problem in which a sender aims\nto reliably convey an identification (ID) message to a set of receivers via a\nbinary-input discrete memoryless channel (BDMC), and simultaneously to\nguarantee that the communication is covert with respect to a warden who\nmonitors the communication via another independent BDMC. We prove a square-root\nlaw for the covert identification problem. This states that an ID message of\nsize \\exp(\\exp(\\Theta(\\sqrt{n}))) can be transmitted over n channel uses. We\nthen characterize the exact pre-constant in the \\Theta(.) notation. This\nconstant is referred to as the covert identification capacity. We show that it\nequals the recently developed covert capacity in the standard covert\ncommunication problem, and somewhat surprisingly, the covert identification\ncapacity can be achieved without any shared key between the sender and\nreceivers. The achievability proof relies on a random coding argument with\npulse-position modulation (PPM), coupled with a second stage which performs\ncode refinements. The converse proof relies on an expurgation argument as well\nas results for channel resolvability with stringent input constraints.'}, {'Covert Communication With Mismatched Decoders': 'This paper considers the problem of covert communication with mismatched\ndecoding, in which a sender wishes to reliably communicate with a receiver\nwhose decoder is fixed and possibly sub-optimal, and simultaneously to ensure\nthat the communication is covert with respect to a warden. We present\nsingle-letter lower and upper bounds on the information-theoretically optimal\nthroughput as a function of the given decoding metric, channel laws, and the\ndesired level of covertness. These bounds match for a variety of scenarios of\ninterest, such as (i) when the channel between the sender and receiver is a\nbinary-input binary-output channel, and (ii) when the decoding metric is\nparticularized to the so-called erasures-only metric. The lower bound is\nobtained based on a modified random coding union bound with pulse position\nmodulation (PPM) codebooks, coupled with a non-standard expurgation argument.\nThe proof of the upper bound relies on a non-trivial combination of analytical\ntechniques for the problems of covert communication and mismatched decoding.'}, {'Covert Communication over Adversarially Jammed Channels': ""Suppose that a transmitter Alice potentially wishes to communicate with a\nreceiver Bob over an adversarially jammed binary channel. An active adversary\nJames eavesdrops on their communication over a binary symmetric channel\n(BSC(q)), and may maliciously flip (up to) a certain fraction p of their\ntransmitted bits based on his observations. We consider a setting where the\ncommunication must be simultaneously covert as well as reliable, i.e., James\nshould be unable to accurately distinguish whether or not Alice is\ncommunicating, while Bob should be able to correctly recover Alice's message\nwith high probability regardless of the adversarial jamming strategy. We show\nthat, unlike the setting with passive adversaries, covert communication against\nactive adversaries requires Alice and Bob to have a shared key (of length at\nleast Omega(log n)) even when Bob has a better channel than James. We present\nlower and upper bounds on the information-theoretically optimal throughput as a\nfunction of the channel parameters, the desired level of covertness, and the\namount of shared key available. These bounds match for a wide range of\nparameters of interest. We also develop a computationally efficient coding\nscheme (based on concatenated codes) when the amount of shared key available is\n$\\Omega(\\sqrt{n} \\log n)$, and further show that this scheme can be implemented\nwith much less amount of shared key when the adversary is assumed to be\ncomputationally bounded.""}, {'Exact Recovery in the General Hypergraph Stochastic Block Model': 'This paper investigates fundamental limits of exact recovery in the general\nd-uniform hypergraph stochastic block model (d-HSBM), wherein n nodes are\npartitioned into k disjoint communities with relative sizes (p1,..., pk). Each\nsubset of nodes with cardinality d is generated independently as an order-d\nhyperedge with a certain probability that depends on the ground-truth\ncommunities that the d nodes belong to. The goal is to exactly recover the k\nhidden communities based on the observed hypergraph. We show that there exists\na sharp threshold such that exact recovery is achievable above the threshold\nand impossible below the threshold (apart from a small regime of parameters\nthat will be specified precisely). This threshold is represented in terms of a\nquantity which we term as the generalized Chernoff-Hellinger divergence between\ncommunities. Our result for this general model recovers prior results for the\nstandard SBM and d-HSBM with two symmetric communities as special cases. En\nroute to proving our achievability results, we develop a polynomial-time\ntwo-stage algorithm that meets the threshold. The first stage adopts a certain\nhypergraph spectral clustering method to obtain a coarse estimate of\ncommunities, and the second stage refines each node individually via local\nrefinement steps to ensure exact recovery.'}, {'Matrix Completion with Hypergraphs:Sharp Thresholds and Efficient\n  Algorithms': ""This paper considers the problem of completing a rating matrix based on\nsub-sampled matrix entries as well as observed social graphs and hypergraphs.\nWe show that there exists a \\emph{sharp threshold} on the sample probability\nfor the task of exactly completing the rating matrix -- the task is achievable\nwhen the sample probability is above the threshold, and is impossible otherwise\n-- demonstrating a phase transition phenomenon. The threshold can be expressed\nas a function of the ``quality'' of hypergraphs, enabling us to \\emph{quantify}\nthe amount of reduction in sample probability due to the exploitation of\nhypergraphs. This also highlights the usefulness of hypergraphs in the matrix\ncompletion problem. En route to discovering the sharp threshold, we develop a\ncomputationally efficient matrix completion algorithm that effectively exploits\nthe observed graphs and hypergraphs. Theoretical analyses show that our\nalgorithm succeeds with high probability as long as the sample probability\nexceeds the aforementioned threshold, and this theoretical result is further\nvalidated by synthetic experiments. Moreover, our experiments on a real social\nnetwork dataset (with both graphs and hypergraphs) show that our algorithm\noutperforms other state-of-the-art matrix completion algorithms.""}, {'Stealthy Communication over Adversarially Jammed Multipath Networks': 'We consider the problem of stealthy communication over a multipath network in\nthe presence of an active adversary. The multipath network consists of multiple\nparallel noiseless links, and the adversary is able to eavesdrop and jam a\nsubset of links. We consider two types of jamming---erasure jamming and\noverwrite jamming. We require the communication to be both stealthy and\nreliable, i.e., the adversary should be unable to detect whether or not\nmeaningful communication is taking place, while the legitimate receiver should\nreconstruct any potential messages from the transmitter with high probability\nsimultaneously. We provide inner bounds on the stealthy capacities under both\nadversarial erasure and adversarial overwrite jamming.'}, {'Community Detection and Matrix Completion with Social and Item\n  Similarity Graphs': 'We consider the problem of recovering a binary rating matrix as well as\nclusters of users and items based on a partially observed matrix together with\nside-information in the form of social and item similarity graphs. These two\ngraphs are both generated according to the celebrated stochastic block model\n(SBM). We develop lower and upper bounds on sample complexity that match for\nvarious scenarios. Our information-theoretic results quantify the benefits of\nthe availability of the social and item similarity graphs. Further analysis\nreveals that under certain scenarios, the social and item similarity graphs\nproduce an interesting synergistic effect. This means that observing two graphs\nis strictly better than observing just one in terms of reducing the sample\ncomplexity.'}, {'Community Detection in the Multi-View Stochastic Block Model': 'This paper considers the problem of community detection on multiple\npotentially correlated graphs from an information-theoretical perspective. We\nfirst put forth a random graph model, called the multi-view stochastic block\nmodel (MVSBM), designed to generate correlated graphs on the same set of nodes\n(with cardinality $n$). The $n$ nodes are partitioned into two disjoint\ncommunities of equal size. The presence or absence of edges in the graphs for\neach pair of nodes depends on whether the two nodes belong to the same\ncommunity or not. The objective for the learner is to recover the hidden\ncommunities with observed graphs. Our technical contributions are two-fold: (i)\nWe establish an information-theoretic upper bound (Theorem~1) showing that\nexact recovery of community is achievable when the model parameters of MVSBM\nexceed a certain threshold. (ii) Conversely, we derive an information-theoretic\nlower bound (Theorem~2) showing that when the model parameters of MVSBM fall\nbelow the aforementioned threshold, then for any estimator, the expected number\nof misclassified nodes will always be greater than one. Our results for the\nMVSBM recover several prior results for community detection in the standard SBM\nas well as in multiple independent SBMs as special cases.'}, {'Comparison among dimensionality reduction techniques based on Random\n  Projection for cancer classification': 'Random Projection (RP) technique has been widely applied in many scenarios\nbecause it can reduce high-dimensional features into low-dimensional space\nwithin short time and meet the need of real-time analysis of massive data.\nThere is an urgent need of dimensionality reduction with fast increase of big\ngenomics data. However, the performance of RP is usually lower. We attempt to\nimprove classification accuracy of RP through combining other reduction\ndimension methods such as Principle Component Analysis (PCA), Linear\nDiscriminant Analysis (LDA), and Feature Selection (FS). We compared\nclassification accuracy and running time of different combination methods on\nthree microarray datasets and a simulation dataset. Experimental results show a\nremarkable improvement of 14.77% in classification accuracy of FS followed by\nRP compared to RP on BC-TCGA dataset. LDA followed by RP also helps RP to yield\na more discriminative subspace with an increase of 13.65% on classification\naccuracy on the same dataset. FS followed by RP outperforms other combination\nmethods in classification accuracy on most of the datasets.'}]","Title: Towards Sample-Efficient Compressed Reinforcement Learning in Multiagent Systems via the Information-Driven Strategies (IDS) Principle

Abstract:
In the burgeoning field of multiagent reinforcement learning (MARL), finding efficient and informative learning strategies poses a significant challenge. This paper introduces a novel approach that leverages the Information-Driven Strategies (IDS) principle, advancing traditional methods in sample-efficient MARL. The main objective is to design algorithms with information-theoretic foundations that guarantee Bayesian regret bounds. 

Novelties are twofold. Firstly, the paper introduces a Bayes-optimal learning algorithm inspired by state-of-the-art Bayesian bandit techniques, enhancing existing MARL algorithms with signal-based state abstraction. Secondly, it extends this algorithm to general-sum multiagent games (MGs) by proposing a new concept called the joint information ratio. This metric integrates the ideas of obtaining exploitability and learning information about the environment, optimizing the trade-off between exploration and exploitation through compressed environments.

Methods involve constructing a mean environment that utilizes information-theoretic techniques to distill a compressed version of the original game. The joint information ratio guides sample allocation, mitigating sub-optimality by focusing on both efficient information gathering and exploiting knowable beneficial actions. The implementation in both zero-sum and general-sum game settings showcases the algorithm's versatility and effectiveness.

The results highlight the improved sample-efficiency of our method in discovering Nash equilibria and correlated equilibria, reducing Bayesian regret compared to prior work. This advancement is achieved by intelligently utilizing available information and directly targeting the acquisition of useful data. 

Contributions include novel strategies for information-driven learning algorithms in MARL, the incorporation of Bayesian methods to achieve optimal learning under uncertainty, and a practical application to the design of efficient MARL systems adaptable to various domains, such as collaborative or competitive robotics, strategic games in economics, and networked systems. By providing a theoretical framework and practical algorithms for information-driven learning in multiagent settings, this research advances the field's understanding of how to efficiently learn optimal strategies in complex, dynamic environments."
"Offline Reinforcement Learning (RL) has shown promising results in learning a
task-specific policy from a fixed dataset. However, successful offline RL often
relies heavily on the coverage and quality of the given dataset. In scenarios
where the dataset for a specific task is limited, a natural approach is to
improve offline RL with datasets from other tasks, namely, to conduct
Multi-Task Data Sharing (MTDS). Nevertheless, directly sharing datasets from
other tasks exacerbates the distribution shift in offline RL. In this paper, we
propose an uncertainty-based MTDS approach that shares the entire dataset
without data selection. Given ensemble-based uncertainty quantification, we
perform pessimistic value iteration on the shared offline dataset, which
provides a unified framework for single- and multi-task offline RL. We further
provide theoretical analysis, which shows that the optimality gap of our method
is only related to the expected data coverage of the shared dataset, thus
resolving the distribution shift issue in data sharing. Empirically, we release
an MTDS benchmark and collect datasets from three challenging domains. The
experimental results show our algorithm outperforms the previous
state-of-the-art methods in challenging MTDS problems. See
https://github.com/Baichenjia/UTDS for the datasets and code.","[{'Privileged Knowledge Distillation for Sim-to-Real Policy Generalization': 'Reinforcement Learning (RL) has recently achieved remarkable success in\nrobotic control. However, most RL methods operate in simulated environments\nwhere privileged knowledge (e.g., dynamics, surroundings, terrains) is readily\navailable. Conversely, in real-world scenarios, robot agents usually rely\nsolely on local states (e.g., proprioceptive feedback of robot joints) to\nselect actions, leading to a significant sim-to-real gap. Existing methods\naddress this gap by either gradually reducing the reliance on privileged\nknowledge or performing a two-stage policy imitation. However, we argue that\nthese methods are limited in their ability to fully leverage the privileged\nknowledge, resulting in suboptimal performance. In this paper, we propose a\nnovel single-stage privileged knowledge distillation method called the\nHistorical Information Bottleneck (HIB) to narrow the sim-to-real gap. In\nparticular, HIB learns a privileged knowledge representation from historical\ntrajectories by capturing the underlying changeable dynamic information.\nTheoretical analysis shows that the learned privileged knowledge representation\nhelps reduce the value discrepancy between the oracle and learned policies.\nEmpirical experiments on both simulated and real-world tasks demonstrate that\nHIB yields improved generalizability compared to previous methods.'}, {'Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty\n  and Smoothness': 'To obtain a near-optimal policy with fewer interactions in Reinforcement\nLearning (RL), a promising approach involves the combination of offline RL,\nwhich enhances sample efficiency by leveraging offline datasets, and online RL,\nwhich explores informative transitions by interacting with the environment.\nOffline-to-Online (O2O) RL provides a paradigm for improving an offline trained\nagent within limited online interactions. However, due to the significant\ndistribution shift between online experiences and offline data, most offline RL\nalgorithms suffer from performance drops and fail to achieve stable policy\nimprovement in O2O adaptation. To address this problem, we propose the Robust\nOffline-to-Online (RO2O) algorithm, designed to enhance offline policies\nthrough uncertainty and smoothness, and to mitigate the performance drop in\nonline adaptation. Specifically, RO2O incorporates Q-ensemble for uncertainty\npenalty and adversarial samples for policy and value smoothness, which enable\nRO2O to maintain a consistent learning procedure in online adaptation without\nrequiring special changes to the learning objective. Theoretical analyses in\nlinear MDPs demonstrate that the uncertainty and smoothness lead to a tighter\noptimality bound in O2O against distribution shift. Experimental results\nillustrate the superiority of RO2O in facilitating stable offline-to-online\nlearning and achieving significant improvement with limited online\ninteractions.'}, {'Regularized Conditional Diffusion Model for Multi-Task Preference\n  Alignment': 'Sequential decision-making is desired to align with human intents and exhibit\nversatility across various tasks. Previous methods formulate it as a\nconditional generation process, utilizing return-conditioned diffusion models\nto directly model trajectory distributions. Nevertheless, the\nreturn-conditioned paradigm relies on pre-defined reward functions, facing\nchallenges when applied in multi-task settings characterized by varying reward\nfunctions (versatility) and showing limited controllability concerning human\npreferences (alignment). In this work, we adopt multi-task preferences as a\nunified condition for both single- and multi-task decision-making, and propose\npreference representations aligned with preference labels. The learned\nrepresentations are used to guide the conditional generation process of\ndiffusion models, and we introduce an auxiliary objective to maximize the\nmutual information between representations and corresponding generated\ntrajectories, improving alignment between trajectories and preferences.\nExtensive experiments in D4RL and Meta-World demonstrate that our method\npresents favorable performance in single- and multi-task scenarios, and\nexhibits superior alignment with preferences.'}, {'Diverse Randomized Value Functions: A Provably Pessimistic Approach for\n  Offline Reinforcement Learning': 'Offline Reinforcement Learning (RL) faces distributional shift and unreliable\nvalue estimation, especially for out-of-distribution (OOD) actions. To address\nthis, existing uncertainty-based methods penalize the value function with\nuncertainty quantification and demand numerous ensemble networks, posing\ncomputational challenges and suboptimal outcomes. In this paper, we introduce a\nnovel strategy employing diverse randomized value functions to estimate the\nposterior distribution of $Q$-values. It provides robust uncertainty\nquantification and estimates lower confidence bounds (LCB) of $Q$-values. By\napplying moderate value penalties for OOD actions, our method fosters a\nprovably pessimistic approach. We also emphasize on diversity within randomized\nvalue functions and enhance efficiency by introducing a diversity\nregularization method, reducing the requisite number of networks. These modules\nlead to reliable value estimation and efficient policy learning from offline\ndata. Theoretical analysis shows that our method recovers the provably\nefficient LCB-penalty under linear MDP assumptions. Extensive empirical results\nalso demonstrate that our proposed method significantly outperforms baseline\nmethods in terms of performance and parametric efficiency.'}, {'Contrastive UCB: Provably Efficient Contrastive Self-Supervised Learning\n  in Online Reinforcement Learning': 'In view of its power in extracting feature representation, contrastive\nself-supervised learning has been successfully integrated into the practice of\n(deep) reinforcement learning (RL), leading to efficient policy learning in\nvarious applications. Despite its tremendous empirical successes, the\nunderstanding of contrastive learning for RL remains elusive. To narrow such a\ngap, we study how RL can be empowered by contrastive learning in a class of\nMarkov decision processes (MDPs) and Markov games (MGs) with low-rank\ntransitions. For both models, we propose to extract the correct feature\nrepresentations of the low-rank model by minimizing a contrastive loss.\nMoreover, under the online setting, we propose novel upper confidence bound\n(UCB)-type algorithms that incorporate such a contrastive loss with online RL\nalgorithms for MDPs or MGs. We further theoretically prove that our algorithm\nrecovers the true representations and simultaneously achieves sample efficiency\nin learning the optimal policy and Nash equilibrium in MDPs and MGs. We also\nprovide empirical studies to demonstrate the efficacy of the UCB-based\ncontrastive learning method for RL. To the best of our knowledge, we provide\nthe first provably efficient online RL algorithm that incorporates contrastive\nlearning for representation learning. Our codes are available at\nhttps://github.com/Baichenjia/Contrastive-UCB.'}, {'Principled Exploration via Optimistic Bootstrapping and Backward\n  Induction': 'One principled approach for provably efficient exploration is incorporating\nthe upper confidence bound (UCB) into the value function as a bonus. However,\nUCB is specified to deal with linear and tabular settings and is incompatible\nwith Deep Reinforcement Learning (DRL). In this paper, we propose a principled\nexploration method for DRL through Optimistic Bootstrapping and Backward\nInduction (OB2I). OB2I constructs a general-purpose UCB-bonus through\nnon-parametric bootstrap in DRL. The UCB-bonus estimates the epistemic\nuncertainty of state-action pairs for optimistic exploration. We build\ntheoretical connections between the proposed UCB-bonus and the LSVI-UCB in a\nlinear setting. We propagate future uncertainty in a time-consistent manner\nthrough episodic backward update, which exploits the theoretical advantage and\nempirically improves the sample-efficiency. Our experiments in the MNIST maze\nand Atari suite suggest that OB2I outperforms several state-of-the-art\nexploration approaches.'}, {'Dynamic Bottleneck for Robust Self-Supervised Exploration': 'Exploration methods based on pseudo-count of transitions or curiosity of\ndynamics have achieved promising results in solving reinforcement learning with\nsparse rewards. However, such methods are usually sensitive to environmental\ndynamics-irrelevant information, e.g., white-noise. To handle such\ndynamics-irrelevant information, we propose a Dynamic Bottleneck (DB) model,\nwhich attains a dynamics-relevant representation based on the\ninformation-bottleneck principle. Based on the DB model, we further propose\nDB-bonus, which encourages the agent to explore state-action pairs with high\ninformation gain. We establish theoretical connections between the proposed\nDB-bonus, the upper confidence bound (UCB) for linear case, and the visiting\ncount for tabular case. We evaluate the proposed method on Atari suits with\ndynamics-irrelevant noises. Our experiments show that exploration with DB bonus\noutperforms several state-of-the-art exploration methods in noisy environments.'}, {'Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement\n  Learning': 'Offline Reinforcement Learning (RL) aims to learn policies from previously\ncollected datasets without exploring the environment. Directly applying\noff-policy algorithms to offline RL usually fails due to the extrapolation\nerror caused by the out-of-distribution (OOD) actions. Previous methods tackle\nsuch problem by penalizing the Q-values of OOD actions or constraining the\ntrained policy to be close to the behavior policy. Nevertheless, such methods\ntypically prevent the generalization of value functions beyond the offline data\nand also lack precise characterization of OOD data. In this paper, we propose\nPessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven\noffline algorithm without explicit policy constraints. Specifically, PBRL\nconducts uncertainty quantification via the disagreement of bootstrapped\nQ-functions, and performs pessimistic updates by penalizing the value function\nbased on the estimated uncertainty. To tackle the extrapolating error, we\nfurther propose a novel OOD sampling method. We show that such OOD sampling and\npessimistic bootstrapping yields provable uncertainty quantifier in linear\nMDPs, thus providing the theoretical underpinning for PBRL. Extensive\nexperiments on D4RL benchmark show that PBRL has better performance compared to\nthe state-of-the-art algorithms.'}, {'Variational Dynamic for Self-Supervised Exploration in Deep\n  Reinforcement Learning': 'Efficient exploration remains a challenging problem in reinforcement\nlearning, especially for tasks where extrinsic rewards from environments are\nsparse or even totally disregarded. Significant advances based on intrinsic\nmotivation show promising results in simple environments but often get stuck in\nenvironments with multimodal and stochastic dynamics. In this work, we propose\na variational dynamic model based on the conditional variational inference to\nmodel the multimodality and stochasticity. We consider the environmental\nstate-action transition as a conditional generative process by generating the\nnext-state prediction under the condition of the current state, action, and\nlatent variable, which provides a better understanding of the dynamics and\nleads a better performance in exploration. We derive an upper bound of the\nnegative log-likelihood of the environmental transition and use such an upper\nbound as the intrinsic reward for exploration, which allows the agent to learn\nskills by self-supervised exploration without observing extrinsic rewards. We\nevaluate the proposed method on several image-based simulation tasks and a real\nrobotic manipulating task. Our method outperforms several state-of-the-art\nenvironment model-based exploration approaches.'}, {'RORL: Robust Offline Reinforcement Learning via Conservative Smoothing': 'Offline reinforcement learning (RL) provides a promising direction to exploit\nmassive amount of offline data for complex decision-making tasks. Due to the\ndistribution shift issue, current offline RL algorithms are generally designed\nto be conservative in value estimation and action selection. However, such\nconservatism can impair the robustness of learned policies when encountering\nobservation deviation under realistic conditions, such as sensor errors and\nadversarial attacks. To trade off robustness and conservatism, we propose\nRobust Offline Reinforcement Learning (RORL) with a novel conservative\nsmoothing technique. In RORL, we explicitly introduce regularization on the\npolicy and the value function for states near the dataset, as well as\nadditional conservative value estimation on these states. Theoretically, we\nshow RORL enjoys a tighter suboptimality bound than recent theoretical results\nin linear MDPs. We demonstrate that RORL can achieve state-of-the-art\nperformance on the general offline RL benchmark and is considerably robust to\nadversarial observation perturbations.'}]","Title: The Enhanced Prognostic Capability of Uncertainty Quantification in Data-Sharing Offline Reinforcement Learning

Abstract:
In the ambitious field of offline reinforcement learning (RL), where an agent is tasked with optimizing policies based on a static dataset without further interaction, efficient uncertainty quantification is pivotal. Given the risks associated with off-policy distributional shifts, this paper introduces Uncertainty Quantification for Data Sharing Offline RL (UTDS), a method that enhances policy inference by managing heuristic uncertainty through pessimistic updates during training. This encapsulates an innovative way to improve upon existing data-sharing techniques. UTDS leverages a multi-ensemble of Q-value updated networks to gauge uncertainty, with an objective function formulated to penalize estimated Q-values based on their inferred variability. 

The study underscores UTDS's superiority over the conventional CDS (Conservative Distributional Sharing) method, specifically addressing the challenge of how to effectively share and utilize diverse datasets to improve policy learning. UTDS's adaptability ensures it reduces policy divergence while optimizing learned policies for a central task, particularly for datasets of lower quality, enabling better performance across a range of synthetic domains and settings. 

UTDS boasts the ability to mitigate the growing complexity often associated with dataset sizes and the associated diverse behaviors for multi-task domains, utilizing consensus between tasks to guide learning towards more stable and effective data selection for policy optimization rather than a single most conservative policy. 

The findings indicate UTDS exhibits faster learning convergence, a smaller reduction in expected policy performance when transitioning between tasks, and executes more effective exploration of varied states and actions, thereby laying the foundation for an advanced technique in offline RL. 

Given the increasing demand for AI-driven systems across multiple industries to leverage historical data for learning competitive strategies, UTDS' advancements in improving the robustness and efficiency of offline RL could lead to the development of more adaptable AI systems capable of real-time or static, data-driven decision-making across complex environments."
"Decoding non-invasive brain recordings is crucial for advancing our
understanding of human cognition, yet faces challenges from individual
differences and complex neural signal representations. Traditional methods
require custom models and extensive trials, and lack interpretability in visual
reconstruction tasks. Our framework integrating integrates 3D brain structures
with visual semantics by Vision Transformer 3D. The unified feature extractor
aligns fMRI features with multiple levels of visual embeddings efficiently,
removing the need for individual-specific models and allowing extraction from
single-trial data. This extractor consolidates multi-level visual features into
one network, simplifying integration with Large Language Models (LLMs).
Additionally, we have enhanced the fMRI dataset with various fMRI-image related
textual data to support multimodal large model development. The integration
with LLMs enhances decoding capabilities, enabling tasks like brain captioning,
question-answering, detailed descriptions, complex reasoning, and visual
reconstruction. Our approach not only shows superior performance across these
tasks but also precisely identifies and manipulates language-based concepts
within brain signals, enhancing interpretability and providing deeper neural
process insights. These advances significantly broaden non-invasive brain
decoding applicability in neuroscience and human-computer interaction, setting
the stage for advanced brain-computer interfaces and cognitive models.","[{'Exploiting High Performance Spiking Neural Networks with Efficient\n  Spiking Patterns': 'Spiking Neural Networks (SNNs) use discrete spike sequences to transmit\ninformation, which significantly mimics the information transmission of the\nbrain. Although this binarized form of representation dramatically enhances the\nenergy efficiency and robustness of SNNs, it also leaves a large gap between\nthe performance of SNNs and Artificial Neural Networks based on real values.\nThere are many different spike patterns in the brain, and the dynamic synergy\nof these spike patterns greatly enriches the representation capability.\nInspired by spike patterns in biological neurons, this paper introduces the\ndynamic Burst pattern and designs the Leaky Integrate and Fire or Burst (LIFB)\nneuron that can make a trade-off between short-time performance and dynamic\ntemporal performance from the perspective of network information capacity. LIFB\nneuron exhibits three modes, resting, Regular spike, and Burst spike. The burst\ndensity of the neuron can be adaptively adjusted, which significantly enriches\nthe characterization capability. We also propose a decoupling method that can\nlosslessly decouple LIFB neurons into equivalent LIF neurons, which\ndemonstrates that LIFB neurons can be efficiently implemented on neuromorphic\nhardware. We conducted experiments on the static datasets CIFAR10, CIFAR100,\nand ImageNet, which showed that we greatly improved the performance of the SNNs\nwhile significantly reducing the network latency. We also conducted experiments\non neuromorphic datasets DVS-CIFAR10 and NCALTECH101 and showed that we\nachieved state-of-the-art with a small network structure.'}, {'EventMix: An Efficient Augmentation Strategy for Event-Based Data': 'High-quality and challenging event stream datasets play an important role in\nthe design of an efficient event-driven mechanism that mimics the brain.\nAlthough event cameras can provide high dynamic range and low-energy event\nstream data, the scale is smaller and more difficult to obtain than traditional\nframe-based data, which restricts the development of neuromorphic computing.\nData augmentation can improve the quantity and quality of the original data by\nprocessing more representations from the original data. This paper proposes an\nefficient data augmentation strategy for event stream data: EventMix. We\ncarefully design the mixing of different event streams by Gaussian Mixture\nModel to generate random 3D masks and achieve arbitrary shape mixing of event\nstreams in the spatio-temporal dimension. By computing the relative distances\nof event streams, we propose a more reasonable way to assign labels to the\nmixed samples. The experimental results on multiple neuromorphic datasets have\nshown that our strategy can improve its performance on neuromorphic datasets\nboth for ANNs and SNNs, and we have achieved state-of-the-art performance on\nDVS-CIFAR10, N-Caltech101, N-CARS, and DVS-Gesture datasets.'}, {'Backpropagation with Biologically Plausible Spatio-Temporal Adjustment\n  For Training Deep Spiking Neural Networks': 'The spiking neural network (SNN) mimics the information processing operation\nin the human brain, represents and transmits information in spike trains\ncontaining wealthy spatial and temporal information, and shows superior\nperformance on many cognitive tasks. In addition, the event-driven information\nprocessing enables the energy-efficient implementation on neuromorphic chips.\nThe success of deep learning is inseparable from backpropagation. Due to the\ndiscrete information transmission, directly applying the backpropagation to the\ntraining of the SNN still has a performance gap compared with the traditional\ndeep neural networks. Also, a large simulation time is required to achieve\nbetter performance, which results in high latency. To address the problems, we\npropose a biological plausible spatial adjustment, which rethinks the\nrelationship between membrane potential and spikes and realizes a reasonable\nadjustment of gradients to different time steps. And it precisely controls the\nbackpropagation of the error along the spatial dimension. Secondly, we propose\na biologically plausible temporal adjustment making the error propagate across\nthe spikes in the temporal dimension, which overcomes the problem of the\ntemporal dependency within a single spike period of the traditional spiking\nneurons. We have verified our algorithm on several datasets, and the\nexperimental results have shown that our algorithm greatly reduces the network\nlatency and energy consumption while also improving network performance. We\nhave achieved state-of-the-art performance on the neuromorphic datasets\nN-MNIST, DVS-Gesture, and DVS-CIFAR10. For the static datasets MNIST and\nCIFAR10, we have surpassed most of the traditional SNN backpropagation training\nalgorithm and achieved relatively superior performance.'}, {'Developmental Plasticity-inspired Adaptive Pruning for Deep Spiking and\n  Artificial Neural Networks': 'Developmental plasticity plays a prominent role in shaping the brain\'s\nstructure during ongoing learning in response to dynamically changing\nenvironments. However, the existing network compression methods for deep\nartificial neural networks (ANNs) and spiking neural networks (SNNs) draw\nlittle inspiration from the brain\'s developmental plasticity mechanisms, thus\nlimiting their ability to learn efficiently, rapidly, and accurately. This\npaper proposed a developmental plasticity-inspired adaptive pruning (DPAP)\nmethod, with inspiration from the adaptive developmental pruning of dendritic\nspines, synapses, and neurons according to the ""use it or lose it, gradually\ndecay"" principle. The proposed DPAP model considers multiple biologically\nrealistic mechanisms (such as dendritic spine dynamic plasticity,\nactivity-dependent neural spiking trace, and local synaptic plasticity), with\nthe addition of an adaptive pruning strategy, so that the network structure can\nbe dynamically optimized during learning without any pre-training and\nretraining. We demonstrated that the proposed DPAP method applied to deep ANNs\nand SNNs could learn efficient network architectures. Extensive comparative\nexperiments show consistent and remarkable performance and speed boost with the\nextremely compressed networks on a diverse set of benchmark tasks, especially\nneuromorphic datasets for SNNs. This work explores how developmental plasticity\nenables the complex deep networks to gradually evolve into brain-like efficient\nand compact structures, eventually achieving state-of-the-art (SOTA)\nperformance for biologically realistic SNNs.'}, {'Multi-scale Evolutionary Neural Architecture Search for Deep Spiking\n  Neural Networks': ""Spiking Neural Networks (SNNs) have received considerable attention not only\nfor their superiority in energy efficiency with discrete signal processing but\nalso for their natural suitability to integrate multi-scale biological\nplasticity. However, most SNNs directly adopt the structure of the\nwell-established Deep Neural Networks (DNNs), and rarely automatically design\nNeural Architecture Search (NAS) for SNNs. The neural motifs topology, modular\nregional structure and global cross-brain region connection of the human brain\nare the product of natural evolution and can serve as a perfect reference for\ndesigning brain-inspired SNN architecture. In this paper, we propose a\nMulti-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN,\nsimultaneously considering micro-, meso- and macro-scale brain topologies as\nthe evolutionary search space. MSE-NAS evolves individual neuron operation,\nself-organized integration of multiple circuit motifs, and global connectivity\nacross motifs through a brain-inspired indirect evaluation function,\nRepresentational Dissimilarity Matrices (RDMs). This training-free fitness\nfunction could greatly reduce computational consumption and NAS's time, and its\ntask-independent property enables the searched SNNs to exhibit excellent\ntransferability on multiple datasets. Furthermore, MSE-NAS show robustness\nagainst the training method and noise. Extensive experiments demonstrate that\nthe proposed algorithm achieves state-of-the-art (SOTA) performance with\nshorter simulation steps on static datasets (CIFAR10, CIFAR100) and\nneuromorphic datasets (CIFAR10-DVS and DVS128-Gesture). The thorough analysis\nalso illustrates the significant performance improvement and consistent\nbio-interpretability deriving from the topological evolution at different\nscales and the RDMs fitness function.""}, {'TIM: An Efficient Temporal Interaction Module for Spiking Transformer': ""Spiking Neural Networks (SNNs), as the third generation of neural networks,\nhave gained prominence for their biological plausibility and computational\nefficiency, especially in processing diverse datasets. The integration of\nattention mechanisms, inspired by advancements in neural network architectures,\nhas led to the development of Spiking Transformers. These have shown promise in\nenhancing SNNs' capabilities, particularly in the realms of both static and\nneuromorphic datasets. Despite their progress, a discernible gap exists in\nthese systems, specifically in the Spiking Self Attention (SSA) mechanism's\neffectiveness in leveraging the temporal processing potential of SNNs. To\naddress this, we introduce the Temporal Interaction Module (TIM), a novel,\nconvolution-based enhancement designed to augment the temporal data processing\nabilities within SNN architectures. TIM's integration into existing SNN\nframeworks is seamless and efficient, requiring minimal additional parameters\nwhile significantly boosting their temporal information handling capabilities.\nThrough rigorous experimentation, TIM has demonstrated its effectiveness in\nexploiting temporal information, leading to state-of-the-art performance across\nvarious neuromorphic datasets. The code is available at\nhttps://github.com/BrainCog-X/Brain-Cog/tree/main/examples/TIM.""}, {'Dive into the Power of Neuronal Heterogeneity': 'The biological neural network is a vast and diverse structure with high\nneural heterogeneity. Conventional Artificial Neural Networks (ANNs) primarily\nfocus on modifying the weights of connections through training while modeling\nneurons as highly homogenized entities and lacking exploration of neural\nheterogeneity. Only a few studies have addressed neural heterogeneity by\noptimizing neuronal properties and connection weights to ensure network\nperformance. However, this strategy impact the specific contribution of\nneuronal heterogeneity. In this paper, we first demonstrate the challenges\nfaced by backpropagation-based methods in optimizing Spiking Neural Networks\n(SNNs) and achieve more robust optimization of heterogeneous neurons in random\nnetworks using an Evolutionary Strategy (ES). Experiments on tasks such as\nworking memory, continuous control, and image recognition show that neuronal\nheterogeneity can improve performance, particularly in long sequence tasks.\nMoreover, we find that membrane time constants play a crucial role in neural\nheterogeneity, and their distribution is similar to that observed in biological\nexperiments. Therefore, we believe that the neglected neuronal heterogeneity\nplays an essential role, providing new approaches for exploring neural\nheterogeneity in biology and new ways for designing more biologically plausible\nneural networks.'}, {'Is Conventional SNN Really Efficient? A Perspective from Network\n  Quantization': 'Spiking Neural Networks (SNNs) have been widely praised for their high energy\nefficiency and immense potential. However, comprehensive research that\ncritically contrasts and correlates SNNs with quantized Artificial Neural\nNetworks (ANNs) remains scant, often leading to skewed comparisons lacking\nfairness towards ANNs. This paper introduces a unified perspective,\nillustrating that the time steps in SNNs and quantized bit-widths of activation\nvalues present analogous representations. Building on this, we present a more\npragmatic and rational approach to estimating the energy consumption of SNNs.\nDiverging from the conventional Synaptic Operations (SynOps), we champion the\n""Bit Budget"" concept. This notion permits an intricate discourse on\nstrategically allocating computational and storage resources between weights,\nactivation values, and temporal steps under stringent hardware constraints.\nGuided by the Bit Budget paradigm, we discern that pivoting efforts towards\nspike patterns and weight quantization, rather than temporal attributes,\nelicits profound implications for model performance. Utilizing the Bit Budget\nfor holistic design consideration of SNNs elevates model performance across\ndiverse data types, encompassing static imagery and neuromorphic datasets. Our\nrevelations bridge the theoretical chasm between SNNs and quantized ANNs and\nilluminate a pragmatic trajectory for future endeavors in energy-efficient\nneural computations.'}, {'DPSNN: A Differentially Private Spiking Neural Network with Temporal\n  Enhanced Pooling': 'Privacy protection is a crucial issue in machine learning algorithms, and the\ncurrent privacy protection is combined with traditional artificial neural\nnetworks based on real values. Spiking neural network (SNN), the new generation\nof artificial neural networks, plays a crucial role in many fields. Therefore,\nresearch on the privacy protection of SNN is urgently needed. This paper\ncombines the differential privacy(DP) algorithm with SNN and proposes a\ndifferentially private spiking neural network (DPSNN). The SNN uses discrete\nspike sequences to transmit information, combined with the gradient noise\nintroduced by DP so that SNN maintains strong privacy protection. At the same\ntime, to make SNN maintain high performance while obtaining high privacy\nprotection, we propose the temporal enhanced pooling (TEP) method. It fully\nintegrates the temporal information of SNN into the spatial information\ntransfer, which enables SNN to perform better information transfer. We conduct\nexperiments on static and neuromorphic datasets, and the experimental results\nshow that our algorithm still maintains high performance while providing strong\nprivacy protection.'}, {'Improving Stability and Performance of Spiking Neural Networks through\n  Enhancing Temporal Consistency': ""Spiking neural networks have gained significant attention due to their\nbrain-like information processing capabilities. The use of surrogate gradients\nhas made it possible to train spiking neural networks with backpropagation,\nleading to impressive performance in various tasks. However, spiking neural\nnetworks trained with backpropagation typically approximate actual labels using\nthe average output, often necessitating a larger simulation timestep to enhance\nthe network's performance. This delay constraint poses a challenge to the\nfurther advancement of SNNs. Current training algorithms tend to overlook the\ndifferences in output distribution at various timesteps. Particularly for\nneuromorphic datasets, inputs at different timesteps can cause inconsistencies\nin output distribution, leading to a significant deviation from the optimal\ndirection when combining optimization directions from different moments. To\ntackle this issue, we have designed a method to enhance the temporal\nconsistency of outputs at different timesteps. We have conducted experiments on\nstatic datasets such as CIFAR10, CIFAR100, and ImageNet. The results\ndemonstrate that our algorithm can achieve comparable performance to other\noptimal SNN algorithms. Notably, our algorithm has achieved state-of-the-art\nperformance on neuromorphic datasets DVS-CIFAR10 and N-Caltech101, and can\nachieve superior performance in the test phase with timestep T=1.""}]","Title: Enhanced Brain Signal Interpretation via Model-Driven Deconvolution

Abstract:
This investigation introduces a refined method for decoding and reconstructing complex brain signals using a hybrid framework combining visual reconstruction, concept location, and multi-layered cognition insights. The core aim is to accurately interpret and reconstruct visual and semantic signals from functional Magnetic Resonance Imaging (fMRI) data while outpacing traditional approaches in efficiency and interpretability. The innovation hinges on the utilization of large language models (LLMs), variational autoencoders (VAEs), and 3D transformer architectures to improve feature embedding and matching accuracy, resulting in more precise and comprehensive cognitive inference. The proposed model addresses the challenges posed by inter-individual variability and complex neural code interfacing through algorithmic tuning and fine-grained multi-level cognition analysis, aligning closely with human behaviors and instructions. Evaluations yield superior performance metrics, substantiating the model's capability to decode nuanced signals, particularly in multi-round dialogue, complex reasoning, and varied visual reconstruction tasks, surpassing state-of-the-art benchmarks. This research opens new avenues for cognitive neuroscience and brain-computer interface applications, emphasizing the potential for more efficient and humane brain signal processing systems."
"Temporal graphs provide a useful model for many real-world networks.
Unfortunately the majority of algorithmic problems we might consider on such
graphs are intractable. There has been recent progress in defining structural
parameters which describe tractable cases by simultaneously restricting the
underlying structure and the times at which edges appear in the graph. These
all rely on the temporal graph being sparse in some sense. We introduce
temporal analogues of three increasingly restrictive static graph parameters --
cliquewidth, modular-width and neighbourhood diversity -- which take small
values for highly structured temporal graphs, even if a large number of edges
are active at each timestep. The computational problems solvable efficiently
when the temporal cliquewidth of the input graph is bounded form a subset of
those solvable efficiently when the temporal modular-width is bounded, which is
in turn a subset of problems efficiently solvable when the temporal
neighbourhood diversity is bounded. By considering specific temporal graph
problems, we demonstrate that (up to standard complexity theoretic assumptions)
these inclusions are strict.","[{'3-List Colouring Permutation Graphs': '3-list colouring is an NP-complete decision problem. It is hard even on\nplanar bipartite graphs. We give a polynomial-time algorithm for solving 3-list\ncolouring on permutation graphs.'}, {'Equivalence of the filament and overlap graphs of subtrees of limited\n  trees': 'The overlap graphs of subtrees of a tree are equivalent to subtree filament\ngraphs, the overlap graphs of subtrees of a star are cocomparability graphs,\nand the overlap graphs of subtrees of a caterpillar are interval filament\ngraphs. In this paper, we show the equivalence of many more classes of subtree\noverlap and subtree filament graphs, and equate them to classes of complements\nof cochordal-mixed graphs. Our results generalize the previously known results\nmentioned above.'}, {'Two dichotomies for model-checking in multi-layer structures': 'Multi-layer graphs can capture qualitatively different types of connection\nbetween entities, and networks of this kind are prevalent in biological and\nsocial systems: for example, a social contact network typically involves both\nvirtual and face-to-face interactions between individuals. Since each layer is\nlikely to exhibit stronger and/or more easily identifiable structurally\nproperties than the overall system, it is natural to ask whether we can exploit\nthe structural properties of individual layers to solve NP-hard problems\nefficiently on the overall network. In this paper we provide a complete\ncharacterisation of the structural properties required in each layer to\nguarantee the existence of an FPT algorithm to solve problems definable in\neither first-order or monadic second-order logic on the overall system, subject\nto the assumption that the structural properties are preserved under deletion\nof vertices and/or edges.'}, {'On List Colouring and List Homomorphism of Permutation and Interval\n  Graphs': 'List colouring is an NP-complete decision problem even if the total number of\ncolours is three. It is hard even on planar bipartite graphs. We give a\npolynomial-time algorithm for solving list colouring of permutation graphs with\na bounded total number of colours. More generally we give a polynomial-time\nalgorithm that solves the list-homomorphism problem to any fixed target graph\nfor a large class of input graphs including all permutation and interval\ngraphs.'}, {'Recognising the overlap graphs of subtrees of restricted trees is hard': 'The overlap graphs of subtrees in a tree (SOGs) generalise many other graphs\nclasses with set representation characterisations. The complexity of\nrecognising SOGs in open. The complexities of recognising many subclasses of\nSOGs are known. We consider several subclasses of SOGs by restricting the\nunderlying tree. For a fixed integer $k \\geq 3$, we consider:\n\\begin{my_itemize}\n  \\item The overlap graphs of subtrees in a tree where that tree has $k$ leaves\n  \\item The overlap graphs of subtrees in trees that can be derived from a\ngiven input tree by subdivision and have at least 3 leaves\n  \\item The overlap and intersection graphs of paths in a tree where that tree\nhas maximum degree $k$ \\end{my_itemize}\n  We show that the recognition problems of these classes are NP-complete. For\nall other parameters we get circle graphs, well known to be polynomially\nrecognizable.'}, {'Deleting edges to restrict the size of an epidemic': 'Motivated by applications in network epidemiology, we consider the problem of\ndetermining whether it is possible to delete at most $k$ edges from a given\ninput graph (of small treewidth) so that the resulting graph avoids a set\n$\\mathcal{F}$ of forbidden subgraphs; of particular interest is the problem of\ndetermining whether it is possible to delete at most $k$ edges so that the\nresulting graph has no connected component of more than $h$ vertices, as this\nbounds the worst-case size of an epidemic. While even this special case of the\nproblem is NP-complete in general (even when $h=3$), we provide evidence that\nmany of the real-world networks of interest are likely to have small treewidth,\nand we describe an algorithm which solves the general problem in time\n\\genruntime ~on an input graph having $n$ vertices and whose treewidth is\nbounded by a fixed constant $w$, if each of the subgraphs we wish to avoid has\nat most $r$ vertices. For the special case in which we wish only to ensure that\nno component has more than $h$ vertices, we improve on this to give an\nalgorithm running in time $O((wh)^{2w}n)$, which we have implemented and tested\non real datasets based on cattle movements.'}, {'Assigning times to minimise reachability in temporal graphs': 'Temporal graphs (in which edges are active at specified times) are of\nparticular relevance for spreading processes on graphs, e.g.~the spread of\ndisease or dissemination of information. Motivated by real-world applications,\nmodification of static graphs to control this spread has proven a rich topic\nfor previous research. Here, we introduce a new type of modification for\ntemporal graphs: the number of active times for each edge is fixed, but we can\nchange the relative order in which (sets of) edges are active. We investigate\nthe problem of determining an ordering of edges that minimises the maximum\nnumber of vertices reachable from any single starting vertex;\nepidemiologically, this corresponds to the worst-case number of vertices\ninfected in a single disease outbreak. We study two versions of this problem,\nboth of which we show to be $\\NP$-hard, and identify cases in which the problem\ncan be solved or approximated efficiently.'}, {'Tangled Paths: A Random Graph Model from Mallows Permutations': 'We introduce the random graph $\\mathcal{P}(n,q)$ which results from taking\nthe union of two paths of length $n\\geq 1$, where the vertices of one of the\npaths have been relabelled according to a Mallows permutation with parameter\n$0<q(n)\\leq 1$. This random graph model, the tangled path, goes through an\nevolution: if $q$ is close to $0$ the graph bears resemblance to a path, and as\n$q$ tends to $1$ it becomes an expander. In an effort to understand the\nevolution of $\\mathcal{P}(n,q)$ we determine the treewidth and cutwidth of\n$\\mathcal{P}(n,q)$ up to log factors for all $q$. We also show that the\nproperty of having a separator of size one has a sharp threshold. In addition,\nwe prove bounds on the diameter, and vertex isoperimetric number for specific\nvalues of $q$.'}, {'Counting Temporal Paths': 'The betweenness centrality of a vertex v is an important centrality measure\nthat quantifies how many optimal paths between pairs of other vertices visit v.\nComputing betweenness centrality in a temporal graph, in which the edge set may\nchange over discrete timesteps, requires us to count temporal paths that are\noptimal with respect to some criterion. For several natural notions of\noptimality, including foremost or fastest temporal paths, this counting problem\nreduces to #Temporal Path, the problem of counting all temporal paths between a\nfixed pair of vertices; like the problems of counting foremost and fastest\ntemporal paths, #Temporal Path is #P-hard in general. Motivated by the many\napplications of this intractable problem, we initiate a systematic study of the\nprameterised and approximation complexity of #Temporal Path. We show that the\nproblem presumably does not admit an FPT-algorithm for the feedback vertex\nnumber of the static underlying graph, and that it is hard to approximate in\ngeneral. On the positive side, we proved several exact and approximate\nFPT-algorithms for special cases.'}, {'Making Life More Confusing for Firefighters': 'It is well known that fighting a fire is a hard task. The Firefighter problem\nasks how to optimally deploy firefighters to defend the vertices of a graph\nfrom a fire. This problem is NP-Complete on all but a few classes of graphs.\nThankfully, firefighters do not have to work alone, and are often aided by the\nefforts of good natured civilians who slow the spread of a fire by maintaining\nfirebreaks when they are able. We will show that this help, although\nwell-intentioned, unfortunately makes the optimal deployment of firefighters an\neven harder problem. To model this scenario we introduce the Temporal\nFirefighter problem, an extension of Firefighter to temporal graphs. We show\nthat Temporal Firefighter is also NP-Complete, and remains so on all but one of\nthe underlying classes of graphs on which Firefighter is known to have\npolynomial time solutions. This motivates us to explore making use of the\ntemporal structure of the graph in our search for tractability, and we conclude\nby presenting an FPT algorithm for Temporal Firefighter with respect to the\ntemporal graph parameter vertex-interval-membership-width.'}]","Background: Temporal graphs are dynamic networks that answer for modeling systems where the interactions evolve over time. Challenges in adapting classical graph algorithms from the static to the temporal setting have sparked interest in identifying tractable classes of temporal graphs. This paper introduces three novel parameters that characterize dense temporal graphs by concurrently restricting properties of the underlying graph structure and edge activity patterns.

Objective: The objective is to develop a parameterized complexity analysis of the Temporal Graph Burning problem, which seeks to spread a fire over all vertices of a temporal graph within the minimum number of time steps. The focus is on temporal graphs satisfying the constraints defined by the novel parameters.

Innovations: This paper introduces Temporal Neighborhood Diversity (TND), Temporal Modular-Width (TMW), and Temporal Cliquewidth, parameters that capture different aspects of the structural complexity and temporal coherence of the graph. The innovation lies in the analysis of the effects of these parameters on the solvability of problems like Temporal Clique, StarExp(4), and Graph Burning, providing insights into tractable subclasses of temporal graphs.

Methods: The methods involve developing new reduction rules and dynamic programming algorithms tailored to exploit the structure encapsulated by the introduced parameters. These algorithms are designed to efficiently solve problems within certain parameter bounds.

Results: For select problems, efficient algorithms (FPT, P-time) that recognize tractability with respect to the introduced parameters are provided. When the underlying graph has a constant TND, TMW, or cliquewidth, the Graph Burning problem can be solved in polynomial time.

Contributions: This paper significantly advances understanding of the interplay between structural properties of temporal graphs and problem complexity. It establishes the tractability of certain computational tasks on graphs of bounded temporal cliquewidth, modular-width, or neighborhood diversity.

Applications: The findings have broad implications for applications such as optimizing information dissemination, controlling contagion spread, resource allocation in dynamic networks, and network-centric control systems. By identifying tractable cases for temporal graph problems, this research aids in developing efficient algorithms with practical applications in diverse temporal datasets.

In summary, this research provides a novel framework to understand temporal graph complexity and develop efficient algorithms for solving problems on specific subclasses of temporal graphs. The findings are applicable to a range of domains where time-evolving network structures play a critical role."
"Reachability and other path-based measures on temporal graphs can be used to
understand spread of infection, information, and people in modelled systems.
Due to delays and errors in reporting, temporal graphs derived from data are
unlikely to perfectly reflect reality, especially with respect to the precise
times at which edges appear. To reflect this uncertainty, we consider a model
in which some number $\zeta$ of edge appearances may have their timestamps
perturbed by $\pm\delta$ for some $\delta$. Within this model, we investigate
temporal reachability and consider the problem of determining the maximum
number of vertices any vertex can reach under these perturbations. We show that
this problem is intractable in general but is efficiently solvable when $\zeta$
is sufficiently large. We also give algorithms which solve this problem in
several restricted settings. We complement this with some contrasting results
concerning the complexity of related temporal eccentricity problems under
perturbation.","[{'3-List Colouring Permutation Graphs': '3-list colouring is an NP-complete decision problem. It is hard even on\nplanar bipartite graphs. We give a polynomial-time algorithm for solving 3-list\ncolouring on permutation graphs.'}, {'Equivalence of the filament and overlap graphs of subtrees of limited\n  trees': 'The overlap graphs of subtrees of a tree are equivalent to subtree filament\ngraphs, the overlap graphs of subtrees of a star are cocomparability graphs,\nand the overlap graphs of subtrees of a caterpillar are interval filament\ngraphs. In this paper, we show the equivalence of many more classes of subtree\noverlap and subtree filament graphs, and equate them to classes of complements\nof cochordal-mixed graphs. Our results generalize the previously known results\nmentioned above.'}, {'Two dichotomies for model-checking in multi-layer structures': 'Multi-layer graphs can capture qualitatively different types of connection\nbetween entities, and networks of this kind are prevalent in biological and\nsocial systems: for example, a social contact network typically involves both\nvirtual and face-to-face interactions between individuals. Since each layer is\nlikely to exhibit stronger and/or more easily identifiable structurally\nproperties than the overall system, it is natural to ask whether we can exploit\nthe structural properties of individual layers to solve NP-hard problems\nefficiently on the overall network. In this paper we provide a complete\ncharacterisation of the structural properties required in each layer to\nguarantee the existence of an FPT algorithm to solve problems definable in\neither first-order or monadic second-order logic on the overall system, subject\nto the assumption that the structural properties are preserved under deletion\nof vertices and/or edges.'}, {'On List Colouring and List Homomorphism of Permutation and Interval\n  Graphs': 'List colouring is an NP-complete decision problem even if the total number of\ncolours is three. It is hard even on planar bipartite graphs. We give a\npolynomial-time algorithm for solving list colouring of permutation graphs with\na bounded total number of colours. More generally we give a polynomial-time\nalgorithm that solves the list-homomorphism problem to any fixed target graph\nfor a large class of input graphs including all permutation and interval\ngraphs.'}, {'Recognising the overlap graphs of subtrees of restricted trees is hard': 'The overlap graphs of subtrees in a tree (SOGs) generalise many other graphs\nclasses with set representation characterisations. The complexity of\nrecognising SOGs in open. The complexities of recognising many subclasses of\nSOGs are known. We consider several subclasses of SOGs by restricting the\nunderlying tree. For a fixed integer $k \\geq 3$, we consider:\n\\begin{my_itemize}\n  \\item The overlap graphs of subtrees in a tree where that tree has $k$ leaves\n  \\item The overlap graphs of subtrees in trees that can be derived from a\ngiven input tree by subdivision and have at least 3 leaves\n  \\item The overlap and intersection graphs of paths in a tree where that tree\nhas maximum degree $k$ \\end{my_itemize}\n  We show that the recognition problems of these classes are NP-complete. For\nall other parameters we get circle graphs, well known to be polynomially\nrecognizable.'}, {'Deleting edges to restrict the size of an epidemic': 'Motivated by applications in network epidemiology, we consider the problem of\ndetermining whether it is possible to delete at most $k$ edges from a given\ninput graph (of small treewidth) so that the resulting graph avoids a set\n$\\mathcal{F}$ of forbidden subgraphs; of particular interest is the problem of\ndetermining whether it is possible to delete at most $k$ edges so that the\nresulting graph has no connected component of more than $h$ vertices, as this\nbounds the worst-case size of an epidemic. While even this special case of the\nproblem is NP-complete in general (even when $h=3$), we provide evidence that\nmany of the real-world networks of interest are likely to have small treewidth,\nand we describe an algorithm which solves the general problem in time\n\\genruntime ~on an input graph having $n$ vertices and whose treewidth is\nbounded by a fixed constant $w$, if each of the subgraphs we wish to avoid has\nat most $r$ vertices. For the special case in which we wish only to ensure that\nno component has more than $h$ vertices, we improve on this to give an\nalgorithm running in time $O((wh)^{2w}n)$, which we have implemented and tested\non real datasets based on cattle movements.'}, {'Assigning times to minimise reachability in temporal graphs': 'Temporal graphs (in which edges are active at specified times) are of\nparticular relevance for spreading processes on graphs, e.g.~the spread of\ndisease or dissemination of information. Motivated by real-world applications,\nmodification of static graphs to control this spread has proven a rich topic\nfor previous research. Here, we introduce a new type of modification for\ntemporal graphs: the number of active times for each edge is fixed, but we can\nchange the relative order in which (sets of) edges are active. We investigate\nthe problem of determining an ordering of edges that minimises the maximum\nnumber of vertices reachable from any single starting vertex;\nepidemiologically, this corresponds to the worst-case number of vertices\ninfected in a single disease outbreak. We study two versions of this problem,\nboth of which we show to be $\\NP$-hard, and identify cases in which the problem\ncan be solved or approximated efficiently.'}, {'Tangled Paths: A Random Graph Model from Mallows Permutations': 'We introduce the random graph $\\mathcal{P}(n,q)$ which results from taking\nthe union of two paths of length $n\\geq 1$, where the vertices of one of the\npaths have been relabelled according to a Mallows permutation with parameter\n$0<q(n)\\leq 1$. This random graph model, the tangled path, goes through an\nevolution: if $q$ is close to $0$ the graph bears resemblance to a path, and as\n$q$ tends to $1$ it becomes an expander. In an effort to understand the\nevolution of $\\mathcal{P}(n,q)$ we determine the treewidth and cutwidth of\n$\\mathcal{P}(n,q)$ up to log factors for all $q$. We also show that the\nproperty of having a separator of size one has a sharp threshold. In addition,\nwe prove bounds on the diameter, and vertex isoperimetric number for specific\nvalues of $q$.'}, {'Counting Temporal Paths': 'The betweenness centrality of a vertex v is an important centrality measure\nthat quantifies how many optimal paths between pairs of other vertices visit v.\nComputing betweenness centrality in a temporal graph, in which the edge set may\nchange over discrete timesteps, requires us to count temporal paths that are\noptimal with respect to some criterion. For several natural notions of\noptimality, including foremost or fastest temporal paths, this counting problem\nreduces to #Temporal Path, the problem of counting all temporal paths between a\nfixed pair of vertices; like the problems of counting foremost and fastest\ntemporal paths, #Temporal Path is #P-hard in general. Motivated by the many\napplications of this intractable problem, we initiate a systematic study of the\nprameterised and approximation complexity of #Temporal Path. We show that the\nproblem presumably does not admit an FPT-algorithm for the feedback vertex\nnumber of the static underlying graph, and that it is hard to approximate in\ngeneral. On the positive side, we proved several exact and approximate\nFPT-algorithms for special cases.'}, {'Making Life More Confusing for Firefighters': 'It is well known that fighting a fire is a hard task. The Firefighter problem\nasks how to optimally deploy firefighters to defend the vertices of a graph\nfrom a fire. This problem is NP-Complete on all but a few classes of graphs.\nThankfully, firefighters do not have to work alone, and are often aided by the\nefforts of good natured civilians who slow the spread of a fire by maintaining\nfirebreaks when they are able. We will show that this help, although\nwell-intentioned, unfortunately makes the optimal deployment of firefighters an\neven harder problem. To model this scenario we introduce the Temporal\nFirefighter problem, an extension of Firefighter to temporal graphs. We show\nthat Temporal Firefighter is also NP-Complete, and remains so on all but one of\nthe underlying classes of graphs on which Firefighter is known to have\npolynomial time solutions. This motivates us to explore making use of the\ntemporal structure of the graph in our search for tractability, and we conclude\nby presenting an FPT algorithm for Temporal Firefighter with respect to the\ntemporal graph parameter vertex-interval-membership-width.'}]","Title: Reachability in Temporal Graphs Under Perturbation

Abstract:

The research dives into the challenge of assessing the robust reachability in temporal graphs by perturbing edge timings. Temporal graphs, used to model movement and spread in networks, are essential. However, inaccuracies in edge timings can lead to incorrect reachability measures. The study's core objective is to determine the complexity of modifying edge timings to manipulate reachability thresholds above which the enhanced reachability can be calculated.

Innovations introduced include a perturbation concept, ""perturbation δ,"" to adjust edge timings within δ increments or decrements, potentially increasing the reachability metric in temporal graphs. Algorithms developed enable efficient computation of reachability under various perturbations, upholding or breaking bounds.

Methodologically, the study constructs a dynamic programming framework that efficiently navigates through temporal graph states to ascertain whether a perturbation that satisfies reachability criteria exists. This framework considers maximum reachability across perturbation bounds and toxicity thresholds, providing valuable insights.

Key results reveal the computational complexity of the reachability testing problem under perturbations, offering both lower bounds derived from complexity theory and an O(nD log τ) time algorithm when the delay δ is one. This simplifies finding solutions with bounded就在时间增量参数, although less so when max达应及时范围致指定阈值的可达性较大.

Contributions highlight a comprehensive methodology for temporal reachability under perturbations as a valuable tool for risk assessment in epidemiological and network studies. Perturbation in temporal graph timings enables a more nuanced and robust understanding of network dynamics compared to static configurations, making the approach indispensable for applications in public health, transportation, and cybersecurity.

Implications suggest practical applications in elucidating and mitigating critical vulnerabilities in network connectivity, with direct relevance for managed network failure scenarios, enhancing predictive models in disease spread, and improving emergency response readiness. By integrating temporal dynamics, the research pushes frontiers in network analysis, securities, and resilience."
"We introduce 'FactCheck Editor', an advanced text editor designed to automate
fact-checking and correct factual inaccuracies. Given the widespread issue of
misinformation, often a result of unintentional mistakes by content creators,
our tool aims to address this challenge. It supports over 90 languages and
utilizes transformer models to assist humans in the labor-intensive process of
fact verification. This demonstration showcases a complete workflow that
detects text claims in need of verification, generates relevant search engine
queries, and retrieves appropriate documents from the web. It employs Natural
Language Inference (NLI) to predict the veracity of claims and uses LLMs to
summarize the evidence and suggest textual revisions to correct any errors in
the text. Additionally, the effectiveness of models used in claim detection and
veracity assessment is evaluated across multiple languages.","[{'Extreme Classification for Answer Type Prediction in Question Answering': 'Semantic answer type prediction (SMART) is known to be a useful step towards\neffective question answering (QA) systems. The SMART task involves predicting\nthe top-$k$ knowledge graph (KG) types for a given natural language question.\nThis is challenging due to the large number of types in KGs. In this paper, we\npropose use of extreme multi-label classification using Transformer models\n(XBERT) by clustering KG types using structural and semantic features based on\nquestion text. We specifically improve the clustering stage of the XBERT\npipeline using textual and structural features derived from KGs. We show that\nthese features can improve end-to-end performance for the SMART task, and yield\nstate-of-the-art results.'}, {'Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over\n  Larger Language Models': 'In this paper, we explore the challenges associated with establishing an\nend-to-end fact-checking pipeline in a real-world context, covering over 90\nlanguages. Our real-world experimental benchmarks demonstrate that fine-tuning\nTransformer models specifically for fact-checking tasks, such as claim\ndetection and veracity prediction, provide superior performance over large\nlanguage models (LLMs) like GPT-4, GPT-3.5-Turbo, and Mistral-7b. However, we\nillustrate that LLMs excel in generative tasks such as question decomposition\nfor evidence retrieval. Through extensive evaluation, we show the efficacy of\nfine-tuned models for fact-checking in a multilingual setting and complex\nclaims that include numerical quantities.'}, {'Semantic Answer Type Prediction using BERT: IAI at the ISWC SMART Task\n  2020': 'This paper summarizes our participation in the SMART Task of the ISWC 2020\nChallenge. A particular question we are interested in answering is how well\nneural methods, and specifically transformer models, such as BERT, perform on\nthe answer type prediction task compared to traditional approaches. Our main\nfinding is that coarse-grained answer types can be identified effectively with\nstandard text classification methods, with over 95% accuracy, and BERT can\nbring only marginal improvements. For fine-grained type detection, on the other\nhand, BERT clearly outperforms previous retrieval-based approaches.'}, {'BRENDA: Browser Extension for Fake News Detection': 'Misinformation such as fake news has drawn a lot of attention in recent\nyears. It has serious consequences on society, politics and economy. This has\nlead to a rise of manually fact-checking websites such as Snopes and\nPolitifact. However, the scale of misinformation limits their ability for\nverification. In this demonstration, we propose BRENDA a browser extension\nwhich can be used to automate the entire process of credibility assessments of\nfalse claims. Behind the scenes BRENDA uses a tested deep neural network\narchitecture to automatically identify fact check worthy claims and classifies\nas well as presents the result along with evidence to the user. Since BRENDA is\na browser extension, it facilities fast automated fact checking for the end\nuser without having to leave the Webpage.'}, {'A General and Configurable Framework for Blockchain-based Marketplaces': 'The first generation of blockchain focused on digital currencies and secure\nstorage, management and transfer of tokenized values. Thereafter, the focus has\nbeen shifting from currencies to a broader application space. In this paper, we\nsystematically explore marketplace types and properties, and consider the\nmechanisms required to support those properties through blockchain. We propose\na generic and configurable framework for blockchain-based marketplaces, and\ndescribe how popular marketplace types, price discovery policies, and other\nconfiguration parameters are implemented within the framework by presenting\nconcrete event-based algorithms. Finally, we consider three use cases with\nwidely diverging properties and show how the proposed framework supports them.'}, {'A Comparative Study for Unsupervised Network Representation Learning': 'There has been appreciable progress in unsupervised network representation\nlearning (UNRL) approaches over graphs recently with flexible random-walk\napproaches, new optimization objectives and deep architectures. However, there\nis no common ground for systematic comparison of embeddings to understand their\nbehavior for different graphs and tasks. In this paper we theoretically group\ndifferent approaches under a unifying framework and empirically investigate the\neffectiveness of different network representation methods. In particular, we\nargue that most of the UNRL approaches either explicitly or implicit model and\nexploit context information of a node. Consequently, we propose a framework\nthat casts a variety of approaches -- random walk based, matrix factorization\nand deep learning based -- into a unified context-based optimization function.\nWe systematically group the methods based on their similarities and\ndifferences. We study the differences among these methods in detail which we\nlater use to explain their performance differences (on downstream tasks). We\nconduct a large-scale empirical study considering 9 popular and recent UNRL\ntechniques and 11 real-world datasets with varying structural properties and\ntwo common tasks -- node classification and link prediction. We find that there\nis no single method that is a clear winner and that the choice of a suitable\nmethod is dictated by certain properties of the embedding methods, task and\nstructural properties of the underlying graph. In addition we also report the\ncommon pitfalls in evaluation of UNRL methods and come up with suggestions for\nexperimental design and interpretation of results.'}, {'Efficient Continuous Multi-Query Processing over Graph Streams': 'Graphs are ubiquitous and ever-present data structures that have a wide range\nof applications involving social networks, knowledge bases and biological\ninteractions. The evolution of a graph in such scenarios can yield important\ninsights about the nature and activities of the underlying network, which can\nthen be utilized for applications such as news dissemination, network\nmonitoring, and content curation. Capturing the continuous evolution of a graph\ncan be achieved by long-standing sub-graph queries. Although, for many\napplications this can only be achieved by a set of queries, state-of-the-art\napproaches focus on a single query scenario. In this paper, we therefore\nintroduce the notion of continuous multi-query processing over graph streams\nand discuss its application to a number of use cases. To this end, we designed\nand developed a novel algorithmic solution for efficient multi-query evaluation\nagainst a stream of graph updates and experimentally demonstrated its\napplicability. Our results against two baseline approaches using real-world, as\nwell as synthetic datasets, confirm a two orders of magnitude improvement of\nthe proposed solution.'}, {'SparCAssist: A Model Risk Assessment Assistant Based on Sparse Generated\n  Counterfactuals': ""We introduce SparcAssist, a general-purpose risk assessment tool for the\nmachine learning models trained for language tasks. It evaluates models' risk\nby inspecting their behavior on counterfactuals, namely out-of-distribution\ninstances generated based on the given data instance. The counterfactuals are\ngenerated by replacing tokens in rational subsequences identified by ExPred,\nwhile the replacements are retrieved using HotFlip or\nMasked-Language-Model-based algorithms. The main purpose of our system is to\nhelp the human annotators to assess the model's risk on deployment. The\ncounterfactual instances generated during the assessment are the by-product and\ncan be used to train more robust NLP models in the future.""}, {'Query Understanding in the Age of Large Language Models': 'Querying, conversing, and controlling search and information-seeking\ninterfaces using natural language are fast becoming ubiquitous with the rise\nand adoption of large-language models (LLM). In this position paper, we\ndescribe a generic framework for interactive query-rewriting using LLMs. Our\nproposal aims to unfold new opportunities for improved and transparent intent\nunderstanding while building high-performance retrieval systems using LLMs. A\nkey aspect of our framework is the ability of the rewriter to fully specify the\nmachine intent by the search engine in natural language that can be further\nrefined, controlled, and edited before the final retrieval phase. The ability\nto present, interact, and reason over the underlying machine intent in natural\nlanguage has profound implications on transparency, ranking performance, and a\ndeparture from the traditional way in which supervised signals were collected\nfor understanding intents. We detail the concept, backed by initial\nexperiments, along with open questions for this interactive query understanding\nframework.'}, {'QuanTemp: A real-world open-domain benchmark for fact-checking numerical\n  claims': 'Automated fact checking has gained immense interest to tackle the growing\nmisinformation in the digital era. Existing systems primarily focus on\nsynthetic claims on Wikipedia, and noteworthy progress has also been made on\nreal-world claims. In this work, we release QuanTemp, a diverse, multi-domain\ndataset focused exclusively on numerical claims, encompassing temporal,\nstatistical and diverse aspects with fine-grained metadata and an evidence\ncollection without leakage. This addresses the challenge of verifying\nreal-world numerical claims, which are complex and often lack precise\ninformation, not addressed by existing works that mainly focus on synthetic\nclaims. We evaluate and quantify the limitations of existing solutions for the\ntask of verifying numerical claims. We also evaluate claim decomposition based\nmethods, numerical understanding based models and our best baselines achieves a\nmacro-F1 of 58.32. This demonstrates that QuanTemp serves as a challenging\nevaluation set for numerical claim verification.'}]","**Abstract**

The proliferation of misinformation is a significant challenge in today's society, affecting various sectors including politics, public opinions, and everyday knowledge dissemination. This research introduces FactCheck Editor, a user-friendly web-based tool that empowers users to compose articles with in-built fact-checking capabilities, addressing the demand for credible information generation. The primary objective of this study is to develop an efficient system for identifying and verifying claims within textual articles, focusing on end-to-end multilingual fact-checking.

Inspired by the diverse challenges of fact-checking across languages, FactCheck Editor innovatively combines neural models and traditional search engine techniques. It leverages Large Language Models (LLMs) like GPT-3.5-Turbo and Mistral-7b for sentence segmentation, co-reference resolution, and veracity prediction, thereby enhancing the accuracy of claim detection. The system employs a novel claim detection model and a more sophisticated evidence retrieval mechanism, improving the reliability of the fact-checking process.

In the evaluation phase, FactCheck Editor demonstrated promising results. It achieved a high Macro F1 score for claim detection across 118 languages, indicating its effectiveness in identifying relevant claims. The system's approach to veracity prediction also yielded significant improvements, with a Macro F1 score of 0.67 across 46 languages. A critical contribution of this research lies in its evidence-rich suggestions for correcting inaccuracies present in claims, based on the gathered evidence.

FactCheck Editor has the potential to revolutionize the way articles are composed and verified, facilitating global content creation that adheres to high standards of accuracy and credibility. By integrating fact-checking capabilities directly into the writing process, this tool streamlines the verification of claims, making it accessible for a wide range of writers, including journalists, content creators, and academic contributors. This translates into enhanced educational resources, improved media content, and a reduction in the spread of misinformation, positioning FactCheck Editor as a valuable tool in the fight against digital misinformation.

Despite the initial success, future research directions are aimed at refining the AI capabilities, enhancing language support, and further optimizing the system's efficiency, to maximize its impact on the digital information ecosystem."
"Context. Atomic and molecular line emissions from shocks may provide valuable
information on the injection of mechanical energy in the interstellar medium
(ISM), the generation of turbulence, and the processes of phase transition
between the Warm Neutral Medium (WNM) and the Cold Neutral Medium (CNM).Aims.
In this series of papers, we investigate the properties of shocks propagating
in the WNM. Our objective is to identify the tracers of these shocks, use them
to interpret ancillary observations of the local diffuse matter, and provide
predictions for future observations.
  Methods. Shocks propagating in the WNM are studied using the Paris-Durham
shock code, a multi-fluid model built to follow the thermodynamical and
chemical structures of shock waves, at steady-state, in a plane-parallel
geometry. The code, already designed to take into account the impact of an
external radiation field, is updated to treat self-irradiated shocks at
intermediate ($30<V_S <100$ km s$^{-1}$) and high velocity ($V_S \ge 100$ km
s$^{-1}$) which emit ultraviolet (UV), extreme-ultraviolet (EUV), and X-ray
photons. The couplings between the photons generated by the shock, the
radiative precursor, and the shock structure are computed self-consistently
using an exact radiative transfer algorithm for line emission. The resulting
code is explored over a wide range of parameters ($0.1 \le n_H \le 2$
cm$^{-3}$, $10 \le V_S \le 500$ km s$^{-1}$, and $0.1 \le B \le 10$ $\mu$G)
that covers the typical conditions of the WNM in the solar neighborhood.
  Results. The explored physical conditions are prompt to the existence of a
diversity of stationary magnetohydrodynamic solutions, including J-type,
CJ-type, and C-type shocks. These shocks are found to naturally induce phase
transition between the WNM and the CNM, provided that the postshock thermal
pressure is larger than the maximum pressure of the WNM and that the maximum
density allowed by magnetic compression is larger than the minimum density of
the CNM. The input flux of mechanical energy is primarily reprocessed into line
emissions from the X-ray to the submillimeter domain. Intermediate and high
velocity shocks are found to generate a UV radiation field that scales as
$V_S^3$ for $V_S < 100$ km s$^{-1}$ and as $V_S^2$ at higher velocities, and an
X-ray radiation field that scales as $V_S^3$ for $V_S \ge 100$ km s$^{-1}$.
Both radiation fields may extend over large distances in the preshock depending
of the density of the surrounding medium and the hardness of the X-ray field
which is solely driven by the shock velocity.
  Conclusions. This first paper presents the thermochemical trajectories of
shocks in the WNM and their associated spectra. It corresponds to a new
milestone in the development of the Paris-Durham shock code and a stepping
stone for the analysis of observations that will be carried out in forthcoming
works.","[{'A complete model of CH+ rotational excitation including radiative and\n  chemical pumping processes': ""Aims. Excitation of far-infrared and submillimetric molecular lines may\noriginate from nonreactive collisions, chemical formation, or far infrared,\nnear-infrared, and optical fluorescences. As a template, we investigate the\nimpact of each of these processes on the excitation of the methylidyne cation\nCH+ and on the intensities of its rotational transitions recently detected in\nemission in dense photodissociation regions (PDRs) and in planetary nebulae.\nMethods. We have developed a nonlocal thermodynamic equilibrium (non-LTE)\nexcitation model that includes the entire energy structure of CH+, i.e. taking\ninto account the pumping of its vibrational and bound and unbound electronic\nstates by near-infrared and optical photons. The model includes the theoretical\ncross-sections of nonreactive collisions with H, H2, He, and e-, and a\nBoltzmann distribution is used to describe the probability of populating the\nexcited levels of CH+ during its chemical formation by hydrogenation of C+. To\nconfirm our results we also performed an extensive analytical study, which we\nuse to predict the main excitation process of several diatomic molecules,\nnamely HF, HCl, SiO, CS, and CO. Results. At densities nH = 10^4 cm-3, the\nexcitation of the rotational levels of CH+ is dominated by the radiative\npumping of its electronic, vibrational, and rotational states if the\nintensities of the radiation field at \\sim 0.4, \\sim 4, and \\sim 300 \\mum are\nstronger than 10^5, 10^8, and 10^4 times those of the local interstellar\nradiation field (ISRF). Below these values, the chemical pumping is the\ndominant source of excitation of the J > 1 levels, even at high kinetic\ntemperatures (\\sim 1000 K). The far-infrared emission lines of CH+ observed in\nthe Orion Bar and the NGC 7027 PDRs are consistent with the predictions of our\nexcitation model assuming an incident far-ultraviolet (FUV) radiation field of\n\\sim 3 \\times 10^4 (in Draine's unit) and densities of \\sim 5 \\times 10^4 and\n\\sim 2 \\times 10^5 cm-3. In the case of NGC 7027, the estimate of the density\nis 10 to 100 times lower than those deduced by traditional excitation codes.\nApplying our model to other X1\\Sigma+ ground state diatomic molecules, we find\nthat HF, and SiO and HCl are the species the most sensitive to the radiative\npumping of their vibrational and bound electronic states. In both cases, the\nminimal near-infrared and optical/UV radiation field intensities required to\nmodify their rotational level populations are \\sim 10^3 times those of the\nlocal ISRF at densities nH = 10^4 cm-3. All these results point towards\ninterstellar and circumstellar media with densities lower than previously\nestablished and cast doubts on the clumpiness of well-studied molecular clouds.""}, {'Models of turbulent dissipation regions in the diffuse interstellar\n  medium': 'Supersonic turbulence is a large reservoir of suprathermal energy in the\ninterstellar medium. Its dissipation, because it is intermittent in space and\ntime, can deeply modify the chemistry of the gas. We further explore a hybrid\nmethod to compute the chemical and thermal evolution of a magnetized\ndissipative structure, under the energetic constraints provided by the observed\nproperties of turbulence in the cold neutral medium. For the first time, we\nmodel a random line of sight by taking into account the relative duration of\nthe bursts with respect to the thermal and chemical relaxation timescales of\nthe gas. The key parameter is the turbulent rate of strain ""a"" due to the\nambient turbulence. With the gas density, it controls the size of the\ndissipative structures, therefore the strength of the burst. For a large range\nof rates of strain and densities, the models of turbulent dissipation regions\n(TDR) reproduce the CH+ column densities observed in the diffuse medium and\ntheir correlation with highly excited H2. They do so without producing an\nexcess of CH. As a natural consequence, they reproduce the abundance ratios of\nHCO+/OH and HCO+/H2O, and their dynamic range of about one order of magnitude\nobserved in diffuse gas. Large C2H and CO abundances, also related to those of\nHCO+, are another outcome of the TDR models that compare well with observed\nvalues. The abundances and column densities computed for CN, HCN and HNC are\none order of magnitude above PDR model predictions, although still\nsignificantly smaller than observed values.'}, {'Anomalous intensities in the infrared emission of CH$^+$ explained by\n  quantum nuclear motion and electric dipole calculations': 'The unusual infrared emission patterns of CH$^+$, recently detected in the\nplanetary nebula NGC 7027, are examined theoretically with high-accuracy\nrovibrational wavefunctions and $ab$ $initio$ dipole moment curves. The\ncalculated transition dipole moments quantitatively reproduce the observed\n$J$-dependent intensity variation, which is ascribed to underlying centrifugal\ndistortion-induced interference effects. We discuss the implications of this\nanomalous behavior for astrochemical modeling of CH$^+$ production and\nexcitation, and provide a simple expression to estimate the magnitude of this\neffect for other light diatomic molecules with small dipole derivatives.'}, {'Models of irradiated molecular shocks': 'Aims. The goal of the paper is to present a detailed study of the propagation\nof low velocity (5 to 25 km s-1) stationary molecular shocks in environments\nilluminated by an external ultraviolet (UV) radiation field. In particular, we\nintend to show how the structure, dynamics, energetics, and chemical properties\nof shocks are modified by UV photons and to estimate how efficiently shocks can\nproduce line emission. Methods. We implemented several key physico-chemical\nprocesses in the Paris-Durham shock code to improve the treatment of the\nradiative transfer and its impact on dust and gas particles. We propose a new\nintegration algorithm to find the steady-state solutions of\nmagnetohydrodynamics equations in a range of parameters in which the fluid\nevolves from a supersonic to a subsonic regime. We explored the resulting code\nover a wide range of physical conditions, which encompass diffuse interstellar\nclouds and hot and dense photon-dominated regions (PDR). Results. We find that\nC-type shock conditions cease to exist as soon as G0 > 0.2 (nH/cm-3)^1/2. Such\nconditions trigger the emergence of another category of stationary solutions,\ncalled C*-type and CJ-type shocks, in which the shocked gas is momentarily\nsubsonic along its trajectory. These solutions are shown to be unique for a\ngiven set of physical conditions and correspond to dissipative structures in\nwhich the gas is heated up to temperatures comprised between those found in\nC-type and adiabatic J-type shocks. High temperatures combined with the ambient\nUV field favour the production or excitation of a few molecular species to the\ndetriment of others, hence leading to specific spectroscopic tracers such as\nrovibrational lines of H2 and rotational lines of CH+. Unexpectedly, the\nrotational lines of CH+ may carry as much as several percent of the shock\nkinetic energy.'}, {'Chemical probes of turbulence in the diffuse medium: the TDR model': 'Context. Tens of light hydrides and small molecules have now been detected\nover several hundreds sight lines sampling the diffuse interstellar medium\n(ISM) in both the Solar neighbourhood and the inner Galactic disk. They provide\nunprecedented statistics on the first steps of chemistry in the diffuse gas.\nAims. These new data confirm the limitations of the traditional chemical\npathways driven by the UV photons and the cosmic rays (CR) and the need for\nadditional energy sources, such as turbulent dissipation, to open highly\nendoenergetic formation routes. The goal of the present paper is to further\ninvestigate the link between specific species and the properties of the\nturbulent cascade in particular its space-time intermittency. Methods. We have\nanalysed ten different atomic and molecular species in the framework of the\nupdated model of turbulent dissipation regions (TDR). We study the influence on\nthe abundances of these species of parameters specific to chemistry (density,\nUV field, and CR ionisation rate) and those linked to turbulence (the average\nturbulent dissipation rate, the dissipation timescale, and the ion neutral\nvelocity drift in the regions of dissipation). Results. The most sensitive\ntracers of turbulent dissipation are the abundances of CH+ and SH+, and the\ncolumn densities of the J = 3, 4, 5 rotational levels of H2 . The abundances of\nCO, HCO+, and the intensity of the 158 $\\mu$m [CII] emission line are\nsignificantly enhanced by turbulent dissipation. The vast diversity of chemical\npathways allows the independent determinations of free parameters never\nestimated before: an upper limit to the average turbulent dissipation rate,\n$\\overline{\\varepsilon}$ < 10$^{-23}$ erg cm$^{-3}$ s$^{-1}$ for $n_H$=20\ncm$^{-3}$, from the CH+ abundance; an upper limit to the ion-neutral velocity\ndrift, $u_{in}$ < 3.5 km s$^{-1}$, from the SH+ to CH+ abundance ratio; and a\nrange of dissipation timescales, 100 < $\\tau_V$ < 1000 yr, from the CO to HCO+\nabundance ratio. For the first time, we reproduce the large abundances of CO\nobserved on diffuse lines of sight, and we show that CO may be abundant even in\nregions with UV-shieldings as low as $5 \\times 10^{-3}$ mag. The best range of\nparameters also reproduces the abundance ratios of OH, C2H, and H2O to HCO+ and\nare consistent with the known properties of the turbulent cascade in the\nGalactic diffuse ISM. Conclusions. Our results disclose an unexpected link\nbetween the dissipation of turbulence and the emergence of molecular richness\nin the diffuse ISM. Some species, such as CH+ or SH+, turn out to be unique\ntracers of the energy trail in the ISM. In spite of some degeneracy, the\nproperties of the turbulent cascade, down to dissipation, can be captured\nthrough specific molecular abundances.'}, {'Chemical Evolution of Turbulent Multiphase Molecular Clouds': 'Molecular clouds are essentially made up of atomic and molecular hydrogen,\nwhich in spite of being the simplest molecule in the ISM plays a key role in\nthe chemical evolution of molecular clouds. Since its formation time is very\nlong, the H2 molecules can be transported by the turbulent motions within the\ncloud toward low density and warm regions, where its enhanced abundance can\nboost the abundances of molecules with high endothermicities. We present high\nresolution simulations where we include the evolution of the molecular gas\nunder the effect of the dynamics, and we analyze its impact on the abundance of\nCH+.'}, {'Self-generated ultraviolet radiation in molecular shock waves II. CH+\n  and the interpretation of emission from shock ensembles': 'Shocks, modelled over a broad range of parameters, are used to construct a\nnew tool to deduce the mechanical energy and physical conditions from observed\natomic or molecular emission lines. We compute magnetised, molecular shock\nmodels with velocities $V_s=5$-$80$ km s$^{-1}$, preshock proton densities\n$n_{\\rm H}=10^2$-$10^6$ cm$^{-3}$, weak or moderate magnetic field strengths,\nand in the absence or presence of an external UV radiation field. We develop a\nsimple emission model of an ensemble of shocks for connecting any observed\nemission lines to the mechanical energy and physical conditions of the system.\nFor this range of parameters we find the full diversity (C-, C$^*$-, CJ-, and\nJ-type) of magnetohydrodynamic shocks. H$_2$ and H are dominant coolants, with\nup to 30% of the shock kinetic flux escaping in Ly$\\alpha$ photons. The\nreformation of molecules in the cooling tail means H$_2$ is even a good tracer\nof dissociative shocks and shocks that were initially fully atomic. For each\nshock model we provide integrated intensities of rovibrational lines of H$_2$,\nCO, and CH$^+$, atomic H lines, and atomic fine-structure and metastable lines.\nWe demonstrate how to use these shock models to deduce the mechanical energy\nand physical conditions of extragalactic environments. As a template example,\nwe interpret the CH$^+$(1-0) emission from the Eyelash starburst galaxy. A\nmechanical energy injection rate of at least $10^{11}$ $L_\\odot$ into molecular\nshocks is required to reproduce the observed line. The low-velocity, externally\nirradiated shocks are at least an order magnitude more efficient than the most\nefficient shocks with no external irradiation, in terms of the total mechanical\nenergy required. We predict differences of more than 2 orders of magnitude in\nintensities of the pure rotational lines of CO, Ly$\\alpha$, metastable lines of\nO, S$^+$, and N, between representative models.'}, {'Molecular absorption lines toward star-forming regions : a comparative\n  study of HCO+, HNC, HCN, and CN': 'Aims. The comparative study of several molecular species at the origin of the\ngas phase chemistry in the diffuse interstellar medium (ISM) is a key input in\nunraveling the coupled chemical and dynamical evolution of the ISM. Methods.\nThe lowest rotational lines of HCO+, HCN, HNC, and CN were observed at the\nIRAM-30m telescope in absorption against the \\lambda 3 mm and \\lambda 1.3 mm\ncontinuum emission of massive star-forming regions in the Galactic plane. The\nabsorption lines probe the gas over kiloparsecs along these lines of sight. The\nexcitation temperatures of HCO+ are inferred from the comparison of the\nabsorptions in the two lowest transitions. The spectra of all molecular species\non the same line of sight are decomposed into Gaussian velocity components.\nMost appear in all the spectra of a given line of sight. For each component, we\nderived the central opacity, the velocity dispersion, and computed the\nmolecular column density. We compared our results to the predictions of\nUV-dominated chemical models of photodissociation regions (PDR models) and to\nthose of non-equilibrium models in which the chemistry is driven by the\ndissipation of turbulent energy (TDR models). Results. The molecular column\ndensities of all the velocity components span up to two orders of magnitude.\nThose of CN, HCN, and HNC are linearly correlated with each other with mean\nratios N(HCN)/N(HNC) = 4.8 $\\pm$ 1.3 and N(CN)/N(HNC) = 34 $\\pm$ 12, and more\nloosely correlated with those of HCO+, N(HNC)/N(HCO+) = 0.5 $\\pm$ 0.3,\nN(HCN)/N(HCO+) = 1.9 $\\pm$ 0.9, and N(CN)/N(HCO+) = 18 $\\pm$ 9. These ratios\nare similar to those inferred from observations of high Galactic latitude lines\nof sight, suggesting that the gas sampled by absorption lines in the Galactic\nplane has the same chemical properties as that in the Solar neighbourhood. The\nFWHM of the Gaussian velocity components span the range 0.3 to 3 km s-1 and\nthose of the HCO+ lines are found to be 30% broader than those of CN-bearing\nmolecules. The PDR models fail to reproduce simultaneously the observed\nabundances of the CN-bearing species and HCO+, even for high-density material\n(100 cm-3 < nH < 104 cm-3). The TDR models, in turn, are able to reproduce the\nobserved abundances and abundance ratios of all the analysed molecules for the\nmoderate gas densities (30 cm-3 < nH < 200 cm-3) and the turbulent energy\nobserved in the diffuse interstellar medium. Conclusions. Intermittent\nturbulent dissipation appears to be a promising driver of the gas phase\nchemistry of the diffuse and translucent gas throughout the Galaxy. The details\nof the dissipation mechanisms still need to be investigated.'}, {'3D chemical structure of the diffuse turbulent ISM II -- Origin of\n  CH$^+$, new solution to an 80 years mystery': 'Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM)\nare a long standing issue of our understanding of the thermodynamical and\nchemical states of the gas. We investigate, here, the formation of CH+ in\nturbulent and multiphase environments, where the heating of the gas is almost\nsolely driven by the photoelectric effect. Methods: The diffuse ISM is\nsimulated using the magnetohydrodynamic (MHD) code RAMSES which\nself-consistently computes the dynamical and thermal evolution of the gas along\nwith the time-dependent evolutions of the abundances of H$^+$, H, and H$_2$.\nThe rest of the chemistry, including the abundance of CH$^+$, is computed in\npost-processing, at equilibrium, under the constraint of out-ofequilibrium of\nH$^+$, H, and H$_2$. The comparison with the observations is performed taking\ninto account an often neglected, yet paramount, piece of information, namely\nthe length of the intercepted diffuse matter along the observed lines of sight.\nResults: The quasi totality of the mass of CH$^+$ originates from the unstable\ngas, in environments where the kinetic temperature is larger than 600 K, the\ndensity ranges between 0.6 and 10 cm$^{-3}$, the electronic fraction ranges\nbetween 3 x 10$^{-4}$ and 6 x 10$^{-3}$, and the molecular fraction is smaller\nthan 0.4. Its formation is driven by warm and out-of-equilibrium H$_2$\ninitially formed in the cold neutral medium (CNM) and injected in more diffuse\nenvironments and even the warm neutral medium (WNM) through a combination of\nadvection and thermal instability. The simulation which displays the tightest\nagreement with the HI-to-H$_2$ transition and the thermal pressure distribution\nobserved in the Solar Neighborhood is found to naturally reproduce the observed\nabundances of CH$^+$, the dispersion of observations, the probability of\noccurrence of most of the lines of sight, the fraction of non-detections of\nCH$^+$, and the distribution of its line profiles. The amount of CH$^+$ and the\nstatistical properties of the simulated lines of sight are set by the fraction\nof unstable gas rich in H$_2$ which is controlled, on Galactic scales, by the\nmean density of the diffuse ISM (or, equivalently, its total mass), the\namplitude of the mean UV radiation field, and the strength of the turbulent\nforcing. Conclusions: This work offers a new and natural solution to an 80\nyears old chemical riddle. The almost ubiquitous presence of CH$^+$ in the\ndiffuse ISM likely results from the exchanges of matter between the CNM and the\nWNM induced by the combination of turbulent advection and thermal instability,\nwithout the need to invoke ambipolar diffusion or regions of intermittent\nturbulent dissipation. Through two phase turbulent mixing, CH$^+$ might thus be\na tracer of the H$_2$ mass loss rate of CNM clouds.'}, {'Low velocity shocks: signatures of turbulent dissipation in diffuse\n  irradiated gas': ""We examine the chemical and emission properties of mildly irradiated (G0=1)\nmagnetised shocks in diffuse media (nH=10^2 to 10^4 /cm3) at low to moderate\nvelocities (from 3 to 40 km/s). Results: The formation of some molecules relies\non endoergic reactions. In J-shocks, their abundances are enhanced by several\norders of magnitude for shock velocities as low as 7 km/s. Otherwise most\nchemical properties of J-type shocks vary over less than an order of magnitude\nbetween velocities from about 7 to about 30 km/s, where H2 dissociation sets\nin. C-type shocks display a more gradual molecular enhancement as the shock\nvelocity increases. We quantify the energy flux budget (fluxes of kinetic,\nradiated and magnetic energies) with emphasis on the main cooling lines of the\ncold interstellar medium. Their sensitivity to shock velocity is such that it\nallows observations to constrain statistical distributions of shock velocities.\nWe fit various probability distribution functions (PDFs) of shock velocities to\nspectroscopic observations of the galaxy-wide shock in Stephan's Quintet (SQ)\nand of a Galactic line of sight sampling diffuse molecular gas in Chamaeleon.\nIn both cases, low velocities bear the greatest statistical weight and the PDF\nis consistent with a bimodal distribution. In the very low velocity shocks\n(below 5 km/s), dissipation is due to ion-neutral friction which powers H2 low\nenergy transitions and atomic lines. In moderate velocity shocks (20 km/s and\nabove), the dissipation is due to viscous heating and accounts for most of the\nmolecular emission. In our interpretation a significant fraction of the gas on\nthe line of sight is shocked (from 4% to 66%). For example, C+ emission may\ntrace shocks in UV irradiated gas where C+ is the dominant carbon species.""}]","Abstract

In the realm of astrophysics, the Warm Neutral Medium (WNM) plays a pivotal role due to its importance in the interstellar medium of galaxies. The dynamic interactions occurring in this warm, gas-rich environment necessitate an in-depth exploration of shock processes. This paper focuses on modeling phase transitions in the WNM through a unique set of approaches. Firstly, we introduce an innovative multi-fluid shock code that numerically solves the complex dynamics of physical processes, including the flow of gas, radiation transport, and chemical reactions within the WNM. Secondly, we utilize the CHIANTI database to upscale and approximate atomospheric simulations, targeting the transition from the WNM to the Cold Neutral Medium (CNM). The objective of our research is to understand and model the phase transition, focusing on the formation and dynamics of various ion species.

Our study innovates by applying this comprehensive code to the WNM, specifically addressing the uncertainties associated with traditional models. Using this approach, we produce detailed profiles for various ionized and neutral species, elucidating the impact of shocks on atomic physics in the WNM. We detail the abundance of important species and the precise reaction and transitional pathways that ensure an updated understanding of the multiphase nature of the interstellar medium.

The main contributions include a novel model for simulating phase transitions and multiphase interactions, offered within an open-access framework, alongside a detailed case study demonstrating the enhanced modeling capabilities over previous studies. This methodology and its application offer new insights into the processes that govern atom formation within galaxies, paving the way for a more refined understanding of interstellar environments and the coherence of galaxy evolution.

Future applications of this research include the further refinement of our model using more detailed observational data, the exploration of dynamical scenarios rarely addressed, and the implementation of predictions comparative to empirical observations. This robust methodology will enable a cohesive, inter-connected astrophysical study that can better explain the behavior and evolution of galaxies over cosmic time."
"Large Language Models (LLMs) have catalyzed significant advancements in
Natural Language Processing (NLP), yet they encounter challenges such as
hallucination and the need for domain-specific knowledge. To mitigate these,
recent methodologies have integrated information retrieved from external
resources with LLMs, substantially enhancing their performance across NLP
tasks. This survey paper addresses the absence of a comprehensive overview on
Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented
Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an
in-depth examination of their paradigm, evolution, taxonomy, and applications.
The paper discusses the essential components of RALMs, including Retrievers,
Language Models, and Augmentations, and how their interactions lead to diverse
model structures and applications. RALMs demonstrate utility in a spectrum of
tasks, from translation and dialogue systems to knowledge-intensive
applications. The survey includes several evaluation methods of RALMs,
emphasizing the importance of robustness, accuracy, and relevance in their
assessment. It also acknowledges the limitations of RALMs, particularly in
retrieval quality and computational efficiency, offering directions for future
research. In conclusion, this survey aims to offer a structured insight into
RALMs, their potential, and the avenues for their future development in NLP.
The paper is supplemented with a Github Repository containing the surveyed
works and resources for further study:
https://github.com/2471023025/RALM_Survey.","[{'Cell Growth and Size Homeostasis in Silico': 'Cell growth in size is a complex process coordinated by intrinsic and\nenvironmental signals. In a recent work [Tzur et al., Science, 2009,\n325:167-171], size distributions in an exponentially growing population of\nmammalian cells were used to infer the growth rate in size. The results suggest\nthat cell growth is neither linear nor exponential, but subject to\nsize-dependent regulation. To explain their data, we build a model in which the\ncell growth rate is controlled by the relative amount of mRNA and ribosomes in\na cell. Plus a stochastic division rule, the evolutionary process of a\npopulation of cells can be simulated and the statistics of the in-silico\npopulation agree well with the experimental data. To further explore the model\nspace, alternative growth models and division rules are studied. This work may\nserve as a starting point for us to understand the rational behind cell growth\nand size regulation using predictive models.'}, {'Collective Properties of a Transcription Initiation Model under Varying\n  Environment': 'The dynamics of gene transcription is tightly regulated in eukaryotes. Recent\nexperiments have revealed various kinds of transcriptional dynamics, such as\nRNA polymerase II pausing, that involves regulation at the transcription\ninitiation stage, and the choice of different regulation pattern is closely\nrelated to the physiological functions of the target gene. Here we consider a\nsimplified model of transcription initiation, a process including the assembly\nof transcription complex and the pausing and releasing of the RNA polymerase\nII. Focusing on the collective behaviors on a population level, we explore\npotential regulatory functions this model can offer. These functions include\nfast and synchronized response to environmental change, or long-term memory\nabout the transcriptional status. As a proof of concept we also show that, by\nselecting different control mechanisms cells can adapt to different\nenvironments. These findings may help us better understand the design\nprinciples of transcriptional regulation.'}, {'On the Poisson Approximation to Photon Distribution for Faint Lasers': 'It is proved, that for a certain kind of input distribution, the strongly\nbinomially attenuated photon number distribution can well be approximated by a\nPoisson distribution. This explains why we can adopt poissonian distribution as\nthe photon number statistics for faint lasers. The error of such an\napproximation is quantitatively estimated. Numerical tests are carried out,\nwhich coincide with our theoretical estimations. This work lays a sound\nmathematical foundation for the well-known intuitive idea which has been widely\nused in quantum cryptography.'}, {""Conservation-Dissipation Formalism for Soft Matter Physics: I.\n  Equivalence with Doi's Variational Approach"": 'In this paper, we proved that by choosing the proper variational function and\nvariables, the variational approach proposed by M. Doi in soft matter physics\nwas equivalent to the Conservation-Dissipation Formalism. To illustrate the\ncorrespondence between these two theories, several novel examples in soft\nmatter physics, including particle diffusion in dilute solutions, polymer phase\nseparation dynamics and nematic liquid crystal flows, were carefully examined.\nBased on our work, a deep connection among the generalized Gibbs relation, the\nsecond law of thermodynamics and the variational principle in non-equilibrium\nthermodynamics was revealed.'}, {'Phase Transition Dynamics and Stochastic Resonance in Topologically\n  Confined Nematic Liquid Crystals': 'Topological defects resulted from boundary constraints in confined liquid\ncrystals have attracted extensive research interests. In this paper, we use\nnumerical simulation to study the phase transition dynamics in the context of\nstochastic resonance in a bistable liquid crystal device containing defects.\nThis device is made of nematic liquid crystals confined in a shallow square\nwell, and is described by the planar Lebwohl-Lasher model. The stochastic phase\ntransition processes of the system in the presence of a weak oscillating\npotential is simulated using an over-damped Langevin dynamics. Our simulation\nresults reveal that, depending on system size, the phase transition may follow\ntwo distinct pathways: in small systems the pre-existing defect structures at\nthe corners hold until the last stage and there is no newly formed defect point\nin the bulk during the phase transition, In large systems new defect points\nappear spontaneously in the bulk and eventually merge with the pre-existing\ndefects at the corners. For both transition pathways stochastic resonance can\nbe observed, but show dramatic difference in their responses to the boundary\nanchoring strength. In small systems we observe a ""sticky-boundary"" effect for\na certain range of anchoring strength in which the phase transition gets stuck\nand stochastic resonance becomes de-activated. Our work demonstrates the\ndynamical interplay among defects, noises, and boundary conditions in confined\nliquid crystals.'}, {'On the Disclination Lines of Nematic Liquid Crystals': 'Defects in liquid crystals are of great practical importance and theoretical\ninterest. Despite tremendous efforts, predicting the location and transition of\ndefects under various topological constraint and external field remains to be a\nchallenge. We investigate defect patterns of nematic liquid crystals confined\nin three-dimensional spherical droplet and two-dimensional disk under different\nboundary conditions, within the Landau-de Gennes model. We implement a spectral\nmethod that numerically solves the Landau-de Gennes model with high accuracy,\nwhich allows us to study the detailed static structure of defects. We observe\nfive types of defect structures. Among them the 1/2-disclination lines are the\nmost stable structure at low temperature. Inspired by numerical results, we\nobtain the profile of disclination lines analytically. Moreover, the connection\nand difference between defect patterns under the Landau-de Gennes model and the\nOseen-Frank model are discussed. Finally, four conjectures are made to\nsummarize some important characteristics of defects in the Landau-de Gennes\ntheory. This work is a continuing effort to deepen our understanding on defect\npatterns in nematic liquid crystals.'}, {'Object Detection Based Handwriting Localization': 'We present an object detection based approach to localize handwritten regions\nfrom documents, which initially aims to enhance the anonymization during the\ndata transmission. The concatenated fusion of original and preprocessed images\ncontaining both printed texts and handwritten notes or signatures are fed into\nthe convolutional neural network, where the bounding boxes are learned to\ndetect the handwriting. Afterwards, the handwritten regions can be processed\n(e.g. replaced with redacted signatures) to conceal the personally identifiable\ninformation (PII). This processing pipeline based on the deep learning network\nCascade R-CNN works at 10 fps on a GPU during the inference, which ensures the\nenhanced anonymization with minimal computational overheads. Furthermore, the\nimpressive generalizability has been empirically showcased: the trained model\nbased on the English-dominant dataset works well on the fictitious unseen\ninvoices, even in Chinese. The proposed approach is also expected to facilitate\nother tasks such as handwriting recognition and signature verification.'}, {'Conservation-Dissipation Formalism for Soft Matter Physics: II.\n  Application to Non-isothermal Nematic Liquid Crystals': ""To most existing non-equilibrium theories, the modeling of non-isothermal\nprocesses was a hard task. Intrinsic difficulties involved the non-equilibrium\ntemperature, the coexistence of conserved energy and dissipative entropy, etc.\nIn this paper, by taking the non-isothermal flow of nematic liquid crystals as\na typical example, we illustrated that thermodynamically consistent models in\neither vectorial or tensorial forms could be constructed within the framework\nof Conservation-Dissipation Formalism (CDF). And the classical isothermal\nEricksen-Leslie model and Qian-Sheng model were shown to be special cases of\nour new vectorial and tensorial models in the isothermal, incompressible and\nstationary limit. Most importantly, from above examples, it was learnt that\nmathematical modeling based on CDF could easily solve the issues relating with\nnon-isothermal situations in a systematic way. The first and second laws of\nthermodynamics were satisfied simultaneously. The non-equilibrium temperature\nwas defined self-consistently through the partial derivative of entropy\nfunction. Relaxation-type constitutive relations were constructed, which gave\nrise to the classical linear constitutive relations, like Newton's law and\nFourier's law, in stationary limits. Therefore, CDF was expected to have a\nbroad scope of applications in soft matter physics, especially under the\ncomplicated situations, such as non-isothermal, compressible and nanoscale\nsystems.""}, {'A Deep Learning Approach to the Citywide Traffic Accident Risk\n  Prediction': 'With the rapid development of urbanization, the boom of vehicle numbers has\nresulted in serious traffic accidents, which led to casualties and huge\neconomic losses. The ability to predict the risk of traffic accident is\nimportant in the prevention of the occurrence of accidents and to reduce the\ndamages caused by accidents in a proactive way. However, traffic accident risk\nprediction with high spatiotemporal resolution is difficult, mainly due to the\ncomplex traffic environment, human behavior, and lack of real-time\ntraffic-related data. In this study, we collected big traffic accident data. By\nanalyzing the spatial and temporal patterns of traffic accident frequency, we\npresented the spatiotemporal correlation of traffic accidents. Based on the\npatterns we found in analysis, we proposed a high accurate deep learning model\nbased on recurrent neural network toward the prediction of traffic accident\nrisk. The predictive accident risk can be potential applied to the traffic\naccident warning system. The proposed method can be integrated into an\nintelligent traffic control system toward a more reasonable traffic prediction\nand command organization.'}, {'Solution Landscapes of the Simplified Ericksen--Leslie Model and its\n  Comparison with the Reduced Landau--de Gennes Model': 'We investigate the solution landscapes of a simplified Ericksen--Leslie (sEL)\nvector model for nematic liquid crystals, confined in a two-dimensional square\ndomain with tangent boundary conditions. An efficient numerical algorithm is\ndeveloped to construct the solution landscapes by utilizing the symmetry\nproperties of the model and the domain. Since the sEL model and the reduced\nLandau--de Gennes (rLdG) models can be viewed as Ginzburg--Landau functionals,\nwe systematically compute the solution landscapes of the sEL model, for\ndifferent domain sizes, and compare with the solution landscapes of the\ncorresponding rLdG models. There are many similarities, including the stable\ndiagonal and rotated states, bifurcation behaviors, and sub-solution landscapes\nwith low-index saddle solutions. Significant disparities also exist between the\ntwo models. The sEL vector model exhibits the stable solution $C\\pm$ with\ninterior defects, high-index ""fake defects"" solutions, novel tessellating\nsolutions, and certain types of distinctive dynamical pathways. The solution\nlandscape approach provides a comprehensive and efficient way for model\ncomparison and is applicable to a wide range of mathematical models in physics.'}]","Title: Comprehensive Survey on Retrieval-Augmented Language Model (RALM)

Over the recent years, the use of retrieved data to refine machine learning models for diverse applications has gained significant prominence. A key advancement is the concept of retrieval-augmented language model (RALM) or retrieval-inspired models, which incorporate external information into a language model's output. However, ongoing research highlights various pitfalls and limitations of existing RALM models.

The objective of this comprehensive survey is to provide an in-depth analysis of RALM methods from the perspective of both retriever and language model components, detailing their methods, benefits, and shortcomings. 

The paper innovates by systematically classifying and categorizing various RALM models, which includes retrievers for gathering data and language models that process this information to improve users' experiences. It underlines novel methods that enhance the output quality by considering aspects like retrieval quality control, timing optimization, and the combination of traditional language models with external datasets.

This research utilizes an architectural framework to present the comprehensive categorization of RALM models, discussing their use of word embedding datasets, structured and unstructured data sources, and various applications, including math teaching, machine translation, and dialog generation.

Extensive details about the methodologies used by existing researchpapersintheDMAlds,likeassets-basedmodelsorfactor-basedmodels,arehighlightedtobeusefulforfuturedevelopments. Through these methodologies, this paper aims to reinforce the robustness of RALM and enhance the quality of retrieval results. Additionally, it acknowledges the limitations of current RALMs, particularly their poor robustness and quality of retrieval outcomes, which pose challenges for potential applications.

The research also expands the conventional narrow confines of RAG (retrieval-augmented graphs) by delving into the advancements in RALM realms, providing, possible solutions to known limitations, and delineating comprehensive future research directions for the community. It also suggests working towards enhancing the model robustness, improving the quality of retrieval, and expanding the applicability and utility of RALMs in the real-world scenario.

CLARIFICATION

The abstract sets the stage by defining the context, providing an impactful objective, clarifying the innovations introduced, describing the methodology utilized, presenting the outcomes or findings, outlining the contributions, and mentioning the potential impact and applications of the research. It adheres to the word limit, being concise and coherent, thus encapsulating the essence of the research paper appropriately."
"We extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA
fine-tuning. The entire training cycle is super efficient, which takes 8 hours
on one 8xA800 (80G) GPU machine. The resulted model exhibits superior
performances across a broad range of evaluation tasks, such as NIHS, topic
retrieval, and long-context language understanding; meanwhile, it also well
preserves the original capability over short contexts. The dramatic context
extension is mainly attributed to merely 3.5K synthetic training samples
generated by GPT-4 , which indicates the LLMs' inherent (yet largely
underestimated) potential to extend its original context length. In fact, the
context length could be extended far beyond 80K with more computation
resources. Therefore, the team will publicly release the entire resources
(including data, model, data generation pipeline, training code) so as to
facilitate the future research from the community:
\url{https://github.com/FlagOpen/FlagEmbedding}.","[{'GateFormer: Speeding Up News Feed Recommendation with Input Gated\n  Transformers': ""News feed recommendation is an important web service. In recent years,\npre-trained language models (PLMs) have been intensively applied to improve the\nrecommendation quality. However, the utilization of these deep models is\nlimited in many aspects, such as lack of explainability and being incompatible\nwith the existing inverted index systems. Above all, the PLMs based\nrecommenders are inefficient, as the encoding of user-side information will\ntake huge computation costs. Although the computation can be accelerated with\nefficient transformers or distilled PLMs, it is still not enough to make timely\nrecommendations for the active users, who are associated with super long news\nbrowsing histories.\n  In this work, we tackle the efficient news recommendation problem from a\ndistinctive perspective. Instead of relying on the entire input (i.e., the\ncollection of news articles a user ever browsed), we argue that the user's\ninterest can be fully captured merely with those representative keywords.\nMotivated by this, we propose GateFormer, where the input data is gated before\nfeeding into transformers. The gating module is made personalized, lightweight\nand end-to-end learnable, such that it may perform accurate and efficient\nfiltering of informative user input. GateFormer achieves highly impressive\nperformances in experiments, where it notably outperforms the existing\nacceleration approaches in both accuracy and efficiency. We also surprisingly\nfind that even with over 10-fold compression of the original input, GateFormer\nis still able to maintain on-par performances with the SOTA methods.""}, {'Learning to Select Historical News Articles for Interaction based Neural\n  News Recommendation': ""The key to personalized news recommendation is to match the user's interests\nwith the candidate news precisely and efficiently. Most existing approaches\nembed user interests into a representation vector then recommend by comparing\nit with the candidate news vector. In such a workflow, fine-grained matching\nsignals may be lost. Recent studies try to cover that by modeling fine-grained\ninteractions between the candidate news and each browsed news article of the\nuser. Despite the effectiveness improvement, these models suffer from much\nhigher computation costs online. Consequently, it remains a tough issue to take\nadvantage of effective interactions in an efficient way. To address this\nproblem, we proposed an end-to-end Selective Fine-grained Interaction framework\n(SFI) with a learning-to-select mechanism. Instead of feeding all historical\nnews into interaction, SFI can quickly select informative historical news\nw.r.t. the candidate and exclude others from following computations. We empower\nthe selection to be both sparse and automatic, which guarantees efficiency and\neffectiveness respectively. Extensive experiments on the publicly available\ndataset MIND validates the superiority of SFI over the state-of-the-art\nmethods: with only five historical news selected, it can significantly improve\nthe AUC by 2.17% over the state-of-the-art interaction-based models; at the\nsame time, it is four times faster.""}, {'LM-Cocktail: Resilient Tuning of Language Models via Model Merging': 'The pre-trained language models are continually fine-tuned to better support\ndownstream applications. However, this operation may result in significant\nperformance degeneration on general tasks beyond the targeted domain. To\novercome this problem, we propose LM-Cocktail which enables the fine-tuned\nmodel to stay resilient in general perspectives. Our method is conducted in the\nform of model merging, where the fine-tuned language model is merged with the\npre-trained base model or the peer models from other domains through weighted\naverage. Despite simplicity, LM-Cocktail is surprisingly effective: the\nresulted model is able to achieve a strong empirical performance in the whole\nscope of general tasks while preserving a superior capacity in its targeted\ndomain. We conduct comprehensive experiments with LLama and BGE model on\npopular benchmarks, including FLAN, MMLU, MTEB, whose results validate the\nefficacy of our proposed method. The code and checkpoints are available at\nhttps://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.'}, {""Extensible Embedding: A Flexible Multipler For LLM's Context Length"": ""Large language models (LLMs) call for extension of context to handle many\ncritical applications. However, the existing approaches are prone to expensive\ncosts and inferior quality of context extension. In this work, we propose\nExtensible Embedding, which realizes high-quality extension of LLM's context\nwith strong flexibility and cost-effectiveness. Extensible embedding stand as\nan enhancement of typical token embedding, which represents the information for\nan extensible scope of context instead of a single token. By leveraging such\ncompact input units of higher information density, the LLM can access to a vast\nscope of context even with a small context window. Extensible embedding is\nsystematically optimized in architecture and training method, which leads to\nmultiple advantages. 1) High flexibility of context extension, which flexibly\nsupports ad-hoc extension of diverse context lengths. 2) Strong sample\nefficiency of training, which enables the embedding model to be learned in a\ncost-effective way. 3) Superior compatibility with the existing LLMs, where the\nextensible embedding can be seamlessly introduced as a plug-in component.\nComprehensive evaluations on long-context language modeling and understanding\ntasks verify extensible embedding as an effective, efficient, flexible, and\ncompatible method to extend the LLM's context.""}, {'Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval': 'Inverted file structure is a common technique for accelerating dense\nretrieval. It clusters documents based on their embeddings; during searching,\nit probes nearby clusters w.r.t. an input query and only evaluates documents\nwithin them by subsequent codecs, thus avoiding the expensive cost of\nexhaustive traversal. However, the clustering is always lossy, which results in\nthe miss of relevant documents in the probed clusters and hence degrades\nretrieval quality. In contrast, lexical matching, such as overlaps of salient\nterms, tends to be strong feature for identifying relevant documents. In this\nwork, we present the Hybrid Inverted Index (HI$^2$), where the embedding\nclusters and salient terms work collaboratively to accelerate dense retrieval.\nTo make best of both effectiveness and efficiency, we devise a cluster selector\nand a term selector, to construct compact inverted lists and efficiently\nsearching through them. Moreover, we leverage simple unsupervised algorithms as\nwell as end-to-end knowledge distillation to learn these two modules, with the\nlatter further boosting the effectiveness. Based on comprehensive experiments\non popular retrieval benchmarks, we verify that clusters and terms indeed\ncomplement each other, enabling HI$^2$ to achieve lossless retrieval quality\nwith competitive efficiency across various index settings. Our code and\ncheckpoint are publicly available at\nhttps://github.com/namespace-Pt/Adon/tree/HI2.'}, {'Flexibly Scaling Large Language Models Contexts Through Extensible\n  Tokenization': ""Large language models (LLMs) are in need of sufficient contexts to handle\nmany critical applications, such as retrieval augmented generation and few-shot\nlearning. However, due to the constrained window size, the LLMs can only access\nto the information within a limited context. Although the size of context\nwindow can be extended by fine-tuning, it will result in a substantial cost in\nboth training and inference stage. In this paper, we present Extensible\nTokenization as an alternative method which realizes the flexible scaling of\nLLMs' context. Extensible Tokenization stands as a midware in between of the\ntokenized context and the LLM, which transforms the raw token embeddings into\nthe extensible embeddings. Such embeddings provide a more compact\nrepresentation for the long context, on top of which the LLM is able to\nperceive more information with the same context window. Extensible Tokenization\nis also featured by its flexibility: the scaling factor can be flexibly\ndetermined within a feasible scope, leading to the extension of an arbitrary\ncontext length at the inference time. Besides, Extensible Tokenization is\nintroduced as a drop-in component, which can be seamlessly plugged into not\nonly the LLM itself and but also its fine-tuned derivatives, bringing in the\nextended contextual information while fully preserving the LLM's existing\ncapabilities. We perform comprehensive experiments on long-context language\nmodeling and understanding tasks, which verify Extensible Tokenization as an\neffective, efficient, flexible, and compatible method to extend LLM's context.\nOur model and source code will be made publicly available.""}, {'Retrieve Anything To Augment Large Language Models': ""Large language models (LLMs) face significant challenges stemming from their\ninherent limitations in knowledge, memory, alignment, and action. These\nchallenges cannot be addressed by LLMs alone, but should rely on assistance\nfrom the external world, such as knowledge base, memory store, demonstration\nexamples, and tools. Retrieval augmentation stands as a vital mechanism for\nbridging the gap between LLMs and the external assistance. However,\nconventional methods encounter two pressing issues. On the one hand, the\ngeneral-purpose retrievers are not properly optimized for the retrieval\naugmentation of LLMs. On the other hand, the task-specific retrievers lack the\nrequired versatility, hindering their performance across the diverse retrieval\naugmentation scenarios.\n  In this work, we present a novel approach, the LLM-Embedder, which\ncomprehensively supports the diverse retrieval augmentation needs of LLMs with\none unified embedding model. Training such a unified model is non-trivial, as\nvarious retrieval tasks aim to capture distinct semantic relationships, often\nsubject to mutual interference. To address this challenge, we systematically\noptimize our training methodology. This includes reward formulation based on\nLLMs' feedback, the stabilization of knowledge distillation, multi-task\nfine-tuning with explicit instructions, and homogeneous in-batch negative\nsampling. These optimization strategies contribute to the outstanding empirical\nperformance of the LLM-Embedder. Notably, it yields remarkable enhancements in\nretrieval augmentation for LLMs, surpassing both general-purpose and\ntask-specific retrievers in various evaluation scenarios. Our checkpoint and\nsource code are publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.""}, {""Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon"": ""The utilization of long contexts poses a big challenge for LLMs due to their\nlimited context window size. Although the context window can be extended\nthrough fine-tuning, it will result in a considerable cost at both training and\ninference time, and exert an unfavorable impact to the LLM's original\ncapabilities. In this work, we propose a new method called Activation Beacon,\nwhich condenses LLM's raw activations into compact forms such that the LLM can\nperceive a longer context with a limited context window. Activation Beacon is\nintroduced as a plug-in module, which fully preserves the LLM's original\ncapability in short contexts. It works with the sliding window to streamingly\nprocess the long context, which leads to a competitive memory and time\nefficiency in both training and inference. Activation Beacon is trained with\nshort-sequence data of diversified condensing ratios. Thanks to such a\ntreatment, it can be effectively learned to support different context lengths\nwith a small training cost. Our experiment verifies Activation Beacon's\neffectiveness of context extension: it can remarkably accomplish high-quality\nextension of Llama-2-7B's context by $\\times100$ times (from 4K to 400K);\nmeanwhile, it can also achieve superior performances across a variety of\nlong-context language modeling and understanding tasks. The source code and\nmodel checkpoint are available at\n\\url{https://github.com/FlagOpen/FlagEmbedding}.""}, {'From Matching to Generation: A Survey on Generative Information\n  Retrieval': ""Information Retrieval (IR) systems are crucial tools for users to access\ninformation, widely applied in scenarios like search engines, question\nanswering, and recommendation systems. Traditional IR methods, based on\nsimilarity matching to return ranked lists of documents, have been reliable\nmeans of information acquisition, dominating the IR field for years. With the\nadvancement of pre-trained language models, generative information retrieval\n(GenIR) has emerged as a novel paradigm, gaining increasing attention in recent\nyears. Currently, research in GenIR can be categorized into two aspects:\ngenerative document retrieval (GR) and reliable response generation. GR\nleverages the generative model's parameters for memorizing documents, enabling\nretrieval by directly generating relevant document identifiers without explicit\nindexing. Reliable response generation, on the other hand, employs language\nmodels to directly generate the information users seek, breaking the\nlimitations of traditional IR in terms of document granularity and relevance\nmatching, offering more flexibility, efficiency, and creativity, thus better\nmeeting practical needs. This paper aims to systematically review the latest\nresearch progress in GenIR. We will summarize the advancements in GR regarding\nmodel training, document identifier, incremental learning, downstream tasks\nadaptation, multi-modal GR and generative recommendation, as well as progress\nin reliable response generation in aspects of internal knowledge memorization,\nexternal knowledge augmentation, generating response with citations and\npersonal information assistant. We also review the evaluation, challenges and\nfuture prospects in GenIR systems. This review aims to offer a comprehensive\nreference for researchers in the GenIR field, encouraging further development\nin this area.""}, {'Generative Retrieval via Term Set Generation': ""Recently, generative retrieval emerges as a promising alternative to\ntraditional retrieval paradigms. It assigns each document a unique identifier,\nknown as DocID, and employs a generative model to directly generate the\nrelevant DocID for the input query. A common choice for DocID is one or several\nnatural language sequences, e.g. the title or n-grams, so that the pre-trained\nknowledge of the generative model can be utilized. However, a sequence is\ngenerated token by token, where only the most likely candidates are kept and\nthe rest are pruned at each decoding step, thus, retrieval fails if any token\nwithin the relevant DocID is falsely pruned. What's worse, during decoding, the\nmodel can only perceive preceding tokens in DocID while being blind to\nsubsequent ones, hence is prone to make such errors. To address this problem,\nwe present a novel framework for generative retrieval, dubbed Term-Set\nGeneration (TSGen). Instead of sequences, we use a set of terms as DocID, which\nare automatically selected to concisely summarize the document's semantics and\ndistinguish it from others. On top of the term-set DocID, we propose a\npermutation-invariant decoding algorithm, with which the term set can be\ngenerated in any permutation yet will always lead to the corresponding\ndocument. Remarkably, TSGen perceives all valid terms rather than only the\npreceding ones at each decoding step. Given the constant decoding space, it can\nmake more reliable decisions due to the broader perspective. TSGen is also\nresilient to errors: the relevant DocID will not be pruned as long as the\ndecoded term belongs to it. Lastly, we design an iterative optimization\nprocedure to incentivize the model to generate the relevant term set in its\nfavorable permutation. We conduct extensive experiments on popular benchmarks,\nwhich validate the effectiveness, the generalizability, the scalability, and\nthe efficiency of TSGen.""}]","Title: Improving Generative Models with Enhanced Context Handling

Abstract:

As generative models increasingly handle long context inputs, context extension's impact on model performance becomes a critical area for research. This paper introduces Llama-3-8B-Instruct-80K-QLoRA, which extends the base model's context capacity to improve downstream long-context tasks. We evaluate its performance using popular benchmarks and compare its results to relevant models. 

Our primary objective was to reduce performance degradation often associated with context extension while experimenting with appropriate techniques. The methodology we adopted involved enhancing context length in the models, enabling them to better utilize longer sequences. We assessed models on various downstream tasks such as topic retrieval, long-context QA, summarization, and code completion, and applied cluster-based text generation for specialized tasks like biography summarization.

The key results showed that our extended-context Llama model outperformed baseline models in downstream tasks, except for code generation tasks. This marked an improvement, especially when applied to tasks with high contextual diversity and complexity, such as LongBookQA, LongBookSum, and MMLU. The gap in performance between longer and shorter context lengths significantly reduced, suggesting a promising approach for enhancing generative models’ capability to handle extended sequences of input.

Our primary contributions include the proposed technique for extension-aware modeling, which can be applied to various long-context configurations to optimize model performance. The methodology can be replicated and further adapted for supervised and unsupervised tasks requiring longer input sequences, making it relevant for a wide array of application scenarios in conversational AI, language understanding, and knowledge graph generation. 

In summary, through the creation and deployment of our enhanced generative model, this research marks a step forward in improving model effectiveness for handling long sequences. It opens up opportunities for future studies focusing on long-context challenges and showcases distinct outcomes and applications in diverse field areas."
"The implications of backdoor attacks on English-centric large language models
(LLMs) have been widely examined - such attacks can be achieved by embedding
malicious behaviors during training and activated under specific conditions
that trigger malicious outputs. However, the impact of backdoor attacks on
multilingual models remains under-explored. Our research focuses on
cross-lingual backdoor attacks against multilingual LLMs, particularly
investigating how poisoning the instruction-tuning data in one or two languages
can affect the outputs in languages whose instruction-tuning data was not
poisoned. Despite its simplicity, our empirical analysis reveals that our
method exhibits remarkable efficacy in models like mT5, BLOOM, and
GPT-3.5-turbo, with high attack success rates, surpassing 95% in several
languages across various scenarios. Alarmingly, our findings also indicate that
larger models show increased susceptibility to transferable cross-lingual
backdoor attacks, which also applies to LLMs predominantly pre-trained on
English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments show
that triggers can still work even after paraphrasing, and the backdoor
mechanism proves highly effective in cross-lingual response settings across 25
languages, achieving an average attack success rate of 50%. Our study aims to
highlight the vulnerabilities and significant security risks present in current
multilingual LLMs, underscoring the emergent need for targeted security
measures.","[{'Sequence to Sequence Mixture Model for Diverse Machine Translation': 'Sequence to sequence (SEQ2SEQ) models often lack diversity in their generated\ntranslations. This can be attributed to the limitation of SEQ2SEQ models in\ncapturing lexical and syntactic variations in a parallel corpus resulting from\ndifferent styles, genres, topics, or ambiguity of the translation process. In\nthis paper, we develop a novel sequence to sequence mixture (S2SMIX) model that\nimproves both translation diversity and quality by adopting a committee of\nspecialized translation models rather than a single translation model. Each\nmixture component selects its own training dataset via optimization of the\nmarginal loglikelihood, which leads to a soft clustering of the parallel\ncorpus. Experiments on four language pairs demonstrate the superiority of our\nmixture model compared to a SEQ2SEQ baseline with standard or diversity-boosted\nbeam search. Our mixture model uses negligible additional parameters and incurs\nno extra computation cost during decoding.'}, {'Differentially Private Representation for NLP: Formal Guarantee and An\n  Empirical Study on Privacy and Fairness': 'It has been demonstrated that hidden representation learned by a deep model\ncan encode private information of the input, hence can be exploited to recover\nsuch information with reasonable accuracy. To address this issue, we propose a\nnovel approach called Differentially Private Neural Representation (DPNR) to\npreserve the privacy of the extracted representation from text. DPNR utilises\nDifferential Privacy (DP) to provide a formal privacy guarantee. Further, we\nshow that masking words via dropout can further enhance privacy. To maintain\nutility of the learned representation, we integrate DP-noisy representation\ninto a robust training process to derive a robust target model, which also\nhelps for model fairness over various demographic variables. Experimental\nresults on benchmark datasets under various parameter settings demonstrate that\nDPNR largely reduces privacy leakage without significantly sacrificing the main\ntask performance.'}, {'Dynamic Programming Encoding for Subword Segmentation in Neural Machine\n  Translation': 'This paper introduces Dynamic Programming Encoding (DPE), a new segmentation\nalgorithm for tokenizing sentences into subword units. We view the subword\nsegmentation of output sentences as a latent variable that should be\nmarginalized out for learning and inference. A mixed character-subword\ntransformer is proposed, which enables exact log marginal likelihood estimation\nand exact MAP inference to find target segmentations with maximum posterior\nprobability. DPE uses a lightweight mixed character-subword transformer as a\nmeans of pre-processing parallel data to segment output sentences using dynamic\nprogramming. Empirical results on machine translation suggest that DPE is\neffective for segmenting output sentences and can be combined with BPE dropout\nfor stochastic segmentation of source sentences. DPE achieves an average\nimprovement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average\nimprovement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several\nWMT datasets including English <=> (German, Romanian, Estonian, Finnish,\nHungarian).'}, {'Extracted BERT Model Leaks More Information than You Think!': 'The collection and availability of big data, combined with advances in\npre-trained models (e.g. BERT), have revolutionized the predictive performance\nof natural language processing tasks. This allows corporations to provide\nmachine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based\nmodels as APIs. Due to significant commercial interest, there has been a surge\nof attempts to steal re mote services via model extraction. Although previous\nworks have made progress in defending against model extraction attacks, there\nhas been little discussion on their performance in preventing privacy leakage.\nThis work bridges this gap by launching an attribute inference attack against\nthe extracted BERT model. Our extensive experiments reveal that model\nextraction can cause severe privacy leakage even when victim models are\nfacilitated with advanced defensive strategies.'}, {'IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks': 'Backdoor attacks are an insidious security threat against machine learning\nmodels. Adversaries can manipulate the predictions of compromised models by\ninserting triggers into the training phase. Various backdoor attacks have been\ndevised which can achieve nearly perfect attack success without affecting model\npredictions for clean inputs. Means of mitigating such vulnerabilities are\nunderdeveloped, especially in natural language processing. To fill this gap, we\nintroduce IMBERT, which uses either gradients or self-attention scores derived\nfrom victim models to self-defend against backdoor attacks at inference time.\nOur empirical studies demonstrate that IMBERT can effectively identify up to\n98.5% of inserted triggers. Thus, it significantly reduces the attack success\nrate while attaining competitive accuracy on the clean dataset across\nwidespread insertion-based attacks compared to two baselines. Finally, we show\nthat our approach is model-agnostic, and can be easily ported to several\npre-trained transformer models.'}, {'Towards Differentially Private Text Representations': 'Most deep learning frameworks require users to pool their local data or model\nupdates to a trusted server to train or maintain a global model. The assumption\nof a trusted server who has access to user information is ill-suited in many\napplications. To tackle this problem, we develop a new deep learning framework\nunder an untrusted server setting, which includes three modules: (1) embedding\nmodule, (2) randomization module, and (3) classifier module. For the\nrandomization module, we propose a novel local differentially private (LDP)\nprotocol to reduce the impact of privacy parameter $\\epsilon$ on accuracy, and\nprovide enhanced flexibility in choosing randomization probabilities for LDP.\nAnalysis and experiments show that our framework delivers comparable or even\nbetter performance than the non-private framework and existing LDP protocols,\ndemonstrating the advantages of our LDP protocol.'}, {'Killing One Bird with Two Stones: Model Extraction and Attribute\n  Inference Attacks against BERT-based APIs': 'The collection and availability of big data, combined with advances in\npre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive\nperformance of modern natural language processing tasks, ranging from text\nclassification to text generation. This allows corporations to provide machine\nlearning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as\nAPIs. However, BERT-based APIs have exhibited a series of security and privacy\nvulnerabilities. For example, prior work has exploited the security issues of\nthe BERT-based APIs through the adversarial examples crafted by the extracted\nmodel. However, the privacy leakage problems of the BERT-based APIs through the\nextracted model have not been well studied. On the other hand, due to the high\ncapacity of BERT-based APIs, the fine-tuned model is easy to be overlearned,\nbut what kind of information can be leaked from the extracted model remains\nunknown. In this work, we bridge this gap by first presenting an effective\nmodel extraction attack, where the adversary can practically steal a BERT-based\nAPI (the target/victim model) by only querying a limited number of queries. We\nfurther develop an effective attribute inference attack which can infer the\nsensitive attribute of the training data used by the BERT-based APIs. Our\nextensive experiments on benchmark datasets under various realistic settings\nvalidate the potential vulnerabilities of BERT-based APIs. Moreover, we\ndemonstrate that two promising defense methods become ineffective against our\nattacks, which calls for more effective defense methods.'}, {'Generative Models are Self-Watermarked: Declaring Model Authentication\n  through Re-Generation': 'As machine- and AI-generated content proliferates, protecting the\nintellectual property of generative models has become imperative, yet verifying\ndata ownership poses formidable challenges, particularly in cases of\nunauthorized reuse of generated data. The challenge of verifying data ownership\nis further amplified by using Machine Learning as a Service (MLaaS), which\noften functions as a black-box system.\n  Our work is dedicated to detecting data reuse from even an individual sample.\nTraditionally, watermarking has been leveraged to detect AI-generated content.\nHowever, unlike watermarking techniques that embed additional information as\ntriggers into models or generated content, potentially compromising output\nquality, our approach identifies latent fingerprints inherently present within\nthe outputs through re-generation. We propose an explainable verification\nprocedure that attributes data ownership through re-generation, and further\namplifies these fingerprints in the generative models through iterative data\nre-generation. This methodology is theoretically grounded and demonstrates\nviability and robustness using recent advanced text and image generative\nmodels. Our methodology is significant as it goes beyond protecting the\nintellectual property of APIs and addresses important issues such as the spread\nof misinformation and academic misconduct. It provides a useful tool to ensure\nthe integrity of sources and authorship, expanding its application in different\nscenarios where authenticity and ownership verification are essential.'}, {'Model Extraction and Adversarial Transferability, Your BERT is\n  Vulnerable!': 'Natural language processing (NLP) tasks, ranging from text classification to\ntext generation, have been revolutionised by the pre-trained language models,\nsuch as BERT. This allows corporations to easily build powerful APIs by\nencapsulating fine-tuned BERT models for downstream tasks. However, when a\nfine-tuned BERT model is deployed as a service, it may suffer from different\nattacks launched by malicious users. In this work, we first present how an\nadversary can steal a BERT-based API service (the victim/target model) on\nmultiple benchmark datasets with limited prior knowledge and queries. We\nfurther show that the extracted model can lead to highly transferable\nadversarial attacks against the victim model. Our studies indicate that the\npotential vulnerabilities of BERT-based API services still hold, even when\nthere is an architectural mismatch between the victim model and the attack\nmodel. Finally, we investigate two defence strategies to protect the victim\nmodel and find that unless the performance of the victim model is sacrificed,\nboth model ex-traction and adversarial transferability can effectively\ncompromise the target models'}, {'Generate, Annotate, and Learn: NLP with Synthetic Text': ""This paper studies the use of language models as a source of synthetic\nunlabeled text for NLP. We formulate a general framework called ``generate,\nannotate, and learn (GAL)'' to take advantage of synthetic text within\nknowledge distillation, self-training, and few-shot learning applications. To\ngenerate high-quality task-specific text, we either fine-tune LMs on inputs\nfrom the task of interest, or prompt large LMs with few examples. We use the\nbest available classifier to annotate synthetic text with soft pseudo labels\nfor knowledge distillation and self-training, and use LMs to obtain hard labels\nfor few-shot learning. We train new supervised models on the combination of\nlabeled and pseudo-labeled data, which results in significant gains across\nseveral applications. We investigate key components of GAL and present\ntheoretical and empirical arguments against the use of class-conditional LMs to\ngenerate synthetic labeled text instead of unlabeled text. GAL achieves new\nstate-of-the-art knowledge distillation results for 6-layer transformers on the\nGLUE leaderboard.""}]","Abstract

To address privacy violations, this paper conducts a comprehensive vulnerability analysis of multilingual large language models (MLLMs), focusing on the effects of language poisoning and the vulnerability of model size. The study unveils that advancements in MLLMs' cross-lingual transferability and shake up scenarios raising concerns over compromise. We analyze our poisoning techniques' effectiveness on different language combinations, reveal the MLLMs' vulnerability to cross-lingual backdoor attacks, and deduce that larger models are more susceptible. An evaluation of 3 English-centric MLLMs further validates our findings. 

The research introduces and implements a backdoor attack called ""Textual Uncovering Backdoor Attack"" (TUBA), enabling us to manipulate MLLMs to generate malicious outputs while maintaining normal operation. A cross-lingual poisoner is designed to determine poisoning rates for various language families, including instruction refusal, content injection, and phrasing prompts. Our novel approach allows for efficient poisoning with targeted phrases included in instructions or at their end.

Through empirical analysis, we find increased vulnerability of models to cross-lingual poisoning compared to within-language attacks. Larger models exhibit more vulnerability, enhancing backdoor infiltration. The results inform on the significant threat posed to MLLMs, emphasizing the need for enhanced security protocols. MLLMs are increasingly exposed to poisoning vulnerabilities due to their pivotal role in processing languages beyond English, which underscores their potential as strategic targets.

In conclusion, this research upends prevailing notions on poisoning MLLMs and data poisoning scenarios. It opens new avenues for understanding security challenges and suggests strategic applications for powerful MLLMs to evade clandestine interventions while maintaining performance efficacy. The findings inform the development of countermeasures and encourage collaborative efforts between the technical community and policymakers for robust security strategies against poisoning attacks."
"The integration of large language models (LLMs) into various pipelines is
increasingly widespread, effectively automating many manual tasks and often
surpassing human capabilities. Cybersecurity researchers and practitioners have
recognised this potential. Thus, they are actively exploring its applications,
given the vast volume of heterogeneous data that requires processing to
identify anomalies, potential bypasses, attacks, and fraudulent incidents. On
top of this, LLMs' advanced capabilities in generating functional code,
comprehending code context, and summarising its operations can also be
leveraged for reverse engineering and malware deobfuscation. To this end, we
delve into the deobfuscation capabilities of state-of-the-art LLMs. Beyond
merely discussing a hypothetical scenario, we evaluate four LLMs with
real-world malicious scripts used in the notorious Emotet malware campaign. Our
results indicate that while not absolutely accurate yet, some LLMs can
efficiently deobfuscate such payloads. Thus, fine-tuning LLMs for this task can
be a viable potential for future AI-powered threat intelligence pipelines in
the fight against obfuscated malware.","[{'Analysing the fall 2020 Emotet campaign': ""In this report, we analyse the latest campaign of Emotet that had a\nsignificant impact in several countries worldwide. We leverage the data of a\nspecifically crafted dataset, which contains emails, documents, executables and\ndomains from the latest campaign. The goal is to analyse the attack vector, map\nthe infrastructure used in various stages of the campaign and perform a surface\nanalysis of Emotet's malicious payloads to assess their potential impact.""}, {'Knock-Knock: The unbearable lightness of Android Notifications': ""Android Notifications can be considered as essential parts in\nHuman-Smartphone interaction and inextricable modules of modern mobile\napplications that can facilitate User Interaction and improve User Experience.\nThis paper presents how this well-crafted and thoroughly documented mechanism,\nprovided by the OS can be exploited by an adversary. More precisely, we present\nattacks that result either in forging smartphone application notifications to\nlure the user in disclosing sensitive information, or manipulate Android\nNotifications to launch a Denial of Service attack to the users' device,\nlocally and remotely, rendering them unusable. This paper concludes by\nproposing generic countermeasures for the discussed security threats.""}, {'Hydras and IPFS: A Decentralised Playground for Malware': 'Modern malware can take various forms, and has reached a very high level of\nsophistication in terms of its penetration, persistence, communication and\nhiding capabilities. The use of cryptography, and of covert communication\nchannels over public and widely used protocols and services, is becoming a\nnorm. In this work, we start by introducing Resource Identifier Generation\nAlgorithms. These are an extension of a well-known mechanism called Domain\nGeneration Algorithms (DGA), which are frequently employed by cybercriminals\nfor bot management and communication. Our extension allows, beyond DNS, the use\nof other protocols. More concretely, we showcase the exploitation of the\nInterPlanetary file system (IPFS). This is a solution for the ""permanent web"",\nwhich enjoys a steadily growing community interest and adoption. The IPFS is,\nin addition, one of the most prominent solutions for blockchain storage. We go\nbeyond the straightforward case of using the IPFS for hosting malicious\ncontent, and explore ways in which a botmaster could employ it, to manage her\nbots, validating our findings experimentally. Finally, we discuss the\nadvantages of our approach for malware authors, its efficacy and highlight its\nextensibility for other distributed storage services.'}, {'Python and Malware: Developing Stealth and Evasive Malware Without\n  Obfuscation': 'With the continuous rise of malicious campaigns and the exploitation of new\nattack vectors, it is necessary to assess the efficacy of the defensive\nmechanisms used to detect them. To this end, the contribution of our work is\ntwofold. First, it introduces a new method for obfuscating malicious code to\nbypass all static checks of multi-engine scanners, such as VirusTotal.\nInterestingly, our approach to generating the malicious executables is not\nbased on introducing a new packer but on the augmentation of the capabilities\nof an existing and widely used tool for packaging Python, PyInstaller but can\nbe used for all similar packaging tools. As we prove, the problem is deeper and\ninherent in almost all antivirus engines and not PyInstaller specific. Second,\nour work exposes significant issues of well-known sandboxes that allow malware\nto evade their checks. As a result, we show that stealth and evasive malware\ncan be efficiently developed, bypassing with ease state of the art malware\ndetection tools without raising any alert.'}, {'An Empirical Assessment of Endpoint Security Systems Against Advanced\n  Persistent Threats Attack Vectors': 'Advanced persistent threats pose a significant challenge for blue teams as\nthey apply various attacks over prolonged periods, impeding event correlation\nand their detection. In this work, we leverage various diverse attack scenarios\nto assess the efficacy of EDRs and other endpoint security solutions against\ndetecting and preventing APTs. Our results indicate that there is still a lot\nof room for improvement as state of the art endpoint security systems fail to\nprevent and log the bulk of the attacks that are reported in this work.\nAdditionally, we discuss methods to tamper with the telemetry providers of\nEDRs, allowing an adversary to perform a more stealth attack.'}, {""Tales from the Git: Automating the detection of secrets on code and\n  assessing developers' passwords choices"": ""Typical users are known to use and reuse weak passwords. Yet, as\ncybersecurity concerns continue to rise, understanding the password practices\nof software developers becomes increasingly important. In this work, we examine\ndevelopers' passwords on public repositories. Our dedicated crawler collected\nmillions of passwords from public GitHub repositories; however, our focus is on\ntheir unique characteristics. To this end, this is the first study\ninvestigating the developer traits in password selection across different\nprogramming languages and contexts, e.g. email and database. Despite the fact\nthat developers may have carelessly leaked their code on public repositories,\nour findings indicate that they tend to use significantly more secure\npasswords, regardless of the underlying programming language and context.\nNevertheless, when the context allows, they often resort to similar password\nselection criteria as typical users. The public availability of such\ninformation in a cleartext format indicates that there is still much room for\nimprovement and that further targeted awareness campaigns are necessary.""}, {'Exploiting Statistical and Structural Features for the Detection of\n  Domain Generation Algorithms': 'Nowadays, malware campaigns have reached a high level of sophistication,\nthanks to the use of cryptography and covert communication channels over\ntraditional protocols and services. In this regard, a typical approach to evade\nbotnet identification and takedown mechanisms is the use of domain fluxing\nthrough the use of Domain Generation Algorithms (DGAs). These algorithms\nproduce an overwhelming amount of domain names that the infected device tries\nto communicate with to find the Command and Control server, yet only a small\nfragment of them is actually registered. Due to the high number of domain\nnames, the blacklisting approach is rendered useless. Therefore, the botmaster\nmay pivot the control dynamically and hinder botnet detection mechanisms. To\ncounter this problem, many security mechanisms result in solutions that try to\nidentify domains from a DGA based on the randomness of their name.\n  In this work, we explore hard to detect families of DGAs, as they are\nconstructed to bypass these mechanisms. More precisely, they are based on the\nuse of dictionaries so the domains seem to be user-generated. Therefore, the\ncorresponding generated domains pass many filters that look for, e.g. high\nentropy strings. To address this challenge, we propose an accurate and\nefficient probabilistic approach to detect them. We test and validate the\nproposed solution through extensive experiments with a sound dataset containing\nall the wordlist-based DGA families that exhibit this behaviour and compare it\nwith other state-of-the-art methods, practically showing the efficacy and\nprevalence of our proposal.'}, {'Who Watches the New Watchmen? The Challenges for Drone Digital Forensics\n  Investigations': 'The technological advance of drone technology has augmented the existing\ncapabilities of flying vehicles rendering them a valuable asset of the modern\nsociety. As more drones are expected to occupy the airspace in the near future,\nsecurity-related incidents, either malicious acts or accidents, will increase\nas well. The forensics analysis of a security incident is essential, as drones\nare flying above populated areas and have also been weaponised from radical\nforces and perpetrators. Thus, it is an imperative need to establish a Drone\nDigital Forensics Investigation Framework and standardise the processes of\ncollecting and processing such evidence. Although there are numerous drone\nplatforms in the market, the same principles apply to all of them; just like\nmobile phones. Nevertheless, due to the nature of drones, standardised\nforensics procedures to date do not manage to address the required processes\nand challenges that such investigations pose. Acknowledging this need, we\ndetail the unique characteristics of drones and the gaps in existing\nmethodologies and standards, showcasing that there are fundamental issues in\nterms of their forensics analysis from various perspectives, ranging from\noperational and procedural ones, and escalate to manufacturers, as well as\nlegal restrictions. The above creates a very complex environment where\ncoordinated actions must be made among the key stakeholders. Therefore, this\nwork paves the way to address these challenges by identifying the main issues,\ntheir origins, and the needs in the field by performing a thorough review of\nthe literature and a gap analysis.'}, {'AiCEF: An AI-assisted Cyber Exercise Content Generation Framework Using\n  Named Entity Recognition': ""Content generation that is both relevant and up to date with the current\nthreats of the target audience is a critical element in the success of any\nCyber Security Exercise (CSE). Through this work, we explore the results of\napplying machine learning techniques to unstructured information sources to\ngenerate structured CSE content. The corpus of our work is a large dataset of\npublicly available cyber security articles that have been used to predict\nfuture threats and to form the skeleton for new exercise scenarios. Machine\nlearning techniques, like named entity recognition (NER) and topic extraction,\nhave been utilised to structure the information based on a novel ontology we\ndeveloped, named Cyber Exercise Scenario Ontology (CESO). Moreover, we used\nclustering with outliers to classify the generated extracted data into objects\nof our ontology. Graph comparison methodologies were used to match generated\nscenario fragments to known threat actors' tactics and help enrich the proposed\nscenario accordingly with the help of synthetic text generators. CESO has also\nbeen chosen as the prominent way to express both fragments and the final\nproposed scenario content by our AI-assisted Cyber Exercise Framework (AiCEF).\nOur methodology was put to test by providing a set of generated scenarios for\nevaluation to a group of experts to be used as part of a real-world awareness\ntabletop exercise.""}, {'Man vs the machine: The Struggle for Effective Text Anonymisation in the\n  Age of Large Language Models': ""The collection and use of personal data are becoming more common in today's\ndata-driven culture. While there are many advantages to this, including better\ndecision-making and service delivery, it also poses significant ethical issues\naround confidentiality and privacy. Text anonymisation tries to prune and/or\nmask identifiable information from a text while keeping the remaining content\nintact to alleviate privacy concerns. Text anonymisation is especially\nimportant in industries like healthcare, law, as well as research, where\nsensitive and personal information is collected, processed, and exchanged under\nhigh legal and ethical standards.\n  Although text anonymization is widely adopted in practice, it continues to\nface considerable challenges. The most significant challenge is striking a\nbalance between removing information to protect individuals' privacy while\nmaintaining the text's usability for future purposes. The question is whether\nthese anonymisation methods sufficiently reduce the risk of re-identification,\nin which an individual can be identified based on the remaining information in\nthe text.\n  In this work, we challenge the effectiveness of these methods and how we\nperceive identifiers. We assess the efficacy of these methods against the\nelephant in the room, the use of AI over big data. While most of the research\nis focused on identifying and removing personal information, there is limited\ndiscussion on whether the remaining information is sufficient to deanonymise\nindividuals and, more precisely, who can do it. To this end, we conduct an\nexperiment using GPT over anonymised texts of famous people to determine\nwhether such trained networks can deanonymise them. The latter allows us to\nrevise these methods and introduce a novel methodology that employs Large\nLanguage Models to improve the anonymity of texts.""}]","Title: Unveiling the Power of Large Language Models: Enhancing Code Deobfuscation for Cybersecurity Applications

Abstract:

The imperative role of artificial intelligence and machine learning in augmenting the cybersecurity domain is articulated in this paper, especially in the deobfuscation of malicious scripts and malware. As the reliance on sophisticated obfuscation techniques grows, the need for advanced deobfuscation tools that can intelligently handle these challenges becomes paramount. Large Language Models (LLMs), notably GPT-4 and Google’s Gemini Pro, have emerged as groundbreaking tools capable of enhancing the deobfuscation process. 

The central objective is to leverage LLMs' capabilities to improve the accuracy, efficiency, and adaptability of code deobfuscation. This paper innovates by presenting empirical trials that compare the optimized performance of the sequentially chosen prompts with current low-latency local models, substantiating the superiority of LLMs in combating obfuscation techniques. 

Methodologically, the research deploys a diversified LLM dataset, testing and refining methodologies to determine the efficacy of prompts in deobfuscation tasks. This pioneering approach leverages predefined tasks to deobfuscate scripts, highlighting the discrepancies between different LLMs in terms of accuracy, hallucinations, and computational efficiency. 

The main findings showcase that LLMs significantly outperform traditional local models, with GPT-4 demonstrating the most accurate deobfuscation capability. However, a nuanced restatement of the deobfuscation task focused on domain extraction reduces errors, even for the underperforming Mixtral, by mitigating hallucinations. The research contributes by proposing a comprehensive framework for the automatic extraction of actionable threat intelligence using a pipeline that incorporates traditional deobfuscators and LLMs, thereby enhancing cybersecurity operations.

Acknowledging the sensitivity and ethical considerations, this study further discusses LLMs’ potential in cybersecurity and distinguishes it from misuse scenarios. It aims to create a beneficial interaction between the technologies and human security analysts, fostering a more holistic approach to cybersecurity through the integration of advanced AI methods into threat intelligence extraction. 

In summary, this paper demonstrates the potential of LLMs as an analytical tool, providing actionable insights into the workings of malicious scripts, and serving as a significant advancement in bolstering the cybersecurity frontline."
"In fair division of indivisible items, domain restriction has played a key
role in escaping from negative results and providing structural insights into
the computational and axiomatic boundaries of fairness. One notable subdomain
of additive preferences, the lexicographic domain, has yielded several positive
results in dealing with goods, chores, and mixtures thereof. However, the
majority of work within this domain primarily consider strict linear orders
over items, which do not allow the modeling of more expressive preferences that
contain indifferences (ties). We investigate the most prominent fairness
notions of envy-freeness up to any (EFX) or some (EF1) item under weakly
lexicographic preferences. For the goods-only setting, we develop an algorithm
that can be customized to guarantee EF1, EFX, maximin share (MMS), or a
combination thereof, along the efficiency notion of Pareto optimality (PO).
From the conceptual perspective, we propose techniques such as preference
graphs and potential envy that are independently of interest when dealing with
ties. Finally, we demonstrate challenges in dealing with chores and highlight
key algorithmic and axiomatic differences of finding EFX solutions with the
goods-only setting. Nevertheless, we show that there is an algorithm that
always returns an EF1 and PO allocation for the chores-only instances.","[{'The Fairness Fair: Bringing Human Perception into Collective\n  Decision-Making': 'Fairness is one of the most desirable societal principles in collective\ndecision-making. It has been extensively studied in the past decades for its\naxiomatic properties and has received substantial attention from the multiagent\nsystems community in recent years for its theoretical and computational aspects\nin algorithmic decision-making. However, these studies are often not\nsufficiently rich to capture the intricacies of human perception of fairness in\nthe ambivalent nature of the real-world problems. We argue that not only fair\nsolutions should be deemed desirable by social planners (designers), but they\nshould be governed by human and societal cognition, consider perceived outcomes\nbased on human judgement, and be verifiable. We discuss how achieving this goal\nrequires a broad transdisciplinary approach ranging from computing and AI to\nbehavioral economics and human-AI interaction. In doing so, we identify\nshortcomings and long-term challenges of the current literature of fair\ndivision, describe recent efforts in addressing them, and more importantly,\nhighlight a series of open research directions.'}, {'Strategyproof Quota Mechanisms for Multiple Assignment Problems': 'We study the problem of allocating multiple objects to agents without\ntransferable utilities, where each agent may receive more than one object\naccording to a quota. Under lexicographic preferences, we characterize the set\nof strategyproof, non-bossy, and neutral quota mechanisms and show that under a\nmild Pareto efficiency condition, serial dictatorship quota mechanisms are the\nonly mechanisms satisfying these properties. Dropping the neutrality\nrequirement, this class of quota mechanisms further expands to sequential\ndictatorship quota mechanisms. We then extend quota mechanisms to randomized\nsettings, and show that the random serial dictatorship quota mechanisms (RSDQ)\nare envyfree, strategyproof, and ex post efficient for any number of agents and\nobjects and any quota system, proving that the well-studied Random Serial\nDictatorship (RSD) satisfies envyfreeness when preferences are lexicographic.'}, {'The Crawler: Three Equivalence Results for Object (Re)allocation\n  Problems when Preferences Are Single-peaked': 'For object reallocation problems, if preferences are strict but otherwise\nunrestricted, the Top Trading Cycles rule (TTC) is the leading rule: It is the\nonly rule satisfying efficiency, individual rationality, and\nstrategy-proofness. However, on the subdomain of single-peaked preferences,\nBade (2019) defines a new rule, the ""crawler"", which also satisfies these three\nproperties. (i) The crawler selects an allocation by ""visiting"" agents in a\nspecific order. A natural ""dual"" rule can be defined by proceeding in the\nreverse order. Our first theorem states that the crawler and its dual are\nactually the same. (ii) Single-peakedness of a preference profile may in fact\nhold for more than one order and its reverse. Our second theorem states that\nthe crawler is invariant to the choice of the order. (iii) For object\nallocation problems (as opposed to reallocation problems), we define a\nprobabilistic version of the crawler by choosing an endowment profile at random\naccording to a uniform distribution, and applying the original definition. Our\nthird theorem states that this rule is the same as the ""random priority rule"".'}, {'Guaranteeing Maximin Shares: Some Agents Left Behind': 'The maximin share (MMS) guarantee is a desirable fairness notion for\nallocating indivisible goods. While MMS allocations do not always exist,\nseveral approximation techniques have been developed to ensure that all agents\nreceive a fraction of their maximin share. We focus on an alternative\napproximation notion, based on the population of agents, that seeks to\nguarantee MMS for a fraction of agents. We show that no optimal approximation\nalgorithm can satisfy more than a constant number of agents, and discuss the\nexistence and computation of MMS for all but one agent and its relation to\napproximate MMS guarantees. We then prove the existence of allocations that\nguarantee MMS for $\\frac{2}{3}$ of agents, and devise a polynomial time\nalgorithm that achieves this bound for up to nine agents. A key implication of\nour result is the existence of allocations that guarantee\n$\\text{MMS}^{\\lceil{3n/2}\\rceil}$, i.e., the value that agents receive by\npartitioning the goods into $\\lceil{\\frac{3}{2}n}\\rceil$ bundles, improving the\nbest known guarantee of $\\text{MMS}^{2n-2}$. Finally, we provide empirical\nexperiments using synthetic data.'}, {'Fair Stable Matching Meets Correlated Preferences': 'The stable matching problem sets the economic foundation of several practical\napplications ranging from school choice and medical residency to ridesharing\nand refugee placement. It is concerned with finding a matching between two\ndisjoint sets of agents wherein no pair of agents prefer each other to their\nmatched partners. The Deferred Acceptance (DA) algorithm is an elegant\nprocedure that guarantees a stable matching for any input; however, its outcome\nmay be unfair as it always favors one side by returning a matching that is\noptimal for one side (say men) and pessimal for the other side (say women). A\ndesirable fairness notion is minimizing the sex-equality cost, i.e. the\ndifference between the total rankings of both sides. Computing such stable\nmatchings is a strongly NP-hard problem, which raises the question of what\ntractable algorithms to adopt in practice. We conduct a series of empirical\nevaluations on the properties of sex-equal stable matchings when preferences of\nagents on both sides are correlated. Our empirical results suggest that under\ncorrelated preferences, the DA algorithm returns stable matchings with low\nsex-equality cost, which further confirms its broad use in many practical\napplications.'}, {'A Coordinated MDP Approach to Multi-Agent Planning for Resource\n  Allocation, with Applications to Healthcare': 'This paper considers a novel approach to scalable multiagent resource\nallocation in dynamic settings. We propose an approximate solution in which\neach resource consumer is represented by an independent MDP-based agent that\nmodels expected utility using an average model of its expected access to\nresources given only limited information about all other agents. A global\nauction-based mechanism is proposed for allocations based on expected regret.\nWe assume truthful bidding and a cooperative coordination mechanism, as we are\nconsidering healthcare scenarios. We illustrate the performance of our\ncoordinated MDP approach against a Monte-Carlo based planning algorithm\nintended for large-scale applications, as well as other approaches suitable for\nallocating medical resources. The evaluations show that the global utility\nvalue across all consumer agents is closer to optimal when using our algorithms\nunder certain time constraints, with low computational cost. As such, we offer\na promising approach for addressing complex resource allocation problems that\narise in healthcare settings.'}, {'Random Serial Dictatorship versus Probabilistic Serial Rule: A Tale of\n  Two Random Mechanisms': 'For assignment problems where agents, specifying ordinal preferences, are\nallocated indivisible objects, two widely studied randomized mechanisms are the\nRandom Serial Dictatorship (RSD) and Probabilistic Serial Rule (PS). These two\nmechanisms both have desirable economic and computational properties, but the\noutcomes they induce can be incomparable in many instances, thus creating\nchallenges in deciding which mechanism to adopt in practice. In this paper we\nfirst look at the space of lexicographic preferences and show that, as opposed\nto the general preference domain, RSD satisfies envyfreeness. Moreover, we show\nthat although under lexicographic preferences PS is strategyproof when the\nnumber of objects is less than or equal agents, it is strictly manipulable when\nthere are more objects than agents. In the space of general preferences, we\nprovide empirical results on the (in)comparability of RSD and PS, analyze\neconomic properties, and provide further insights on the applicability of each\nmechanism in different application domains.'}, {'Investigating the Characteristics of One-Sided Matching Mechanisms Under\n  Various Preferences and Risk Attitudes': 'One-sided matching mechanisms are fundamental for assigning a set of\nindivisible objects to a set of self-interested agents when monetary transfers\nare not allowed. Two widely-studied randomized mechanisms in multiagent\nsettings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial\nRule (PS). Both mechanisms require only that agents specify ordinal preferences\nand have a number of desirable economic and computational properties. However,\nthe induced outcomes of the mechanisms are often incomparable and thus there\nare challenges when it comes to deciding which mechanism to adopt in practice.\nIn this paper, we first consider the space of general ordinal preferences and\nprovide empirical results on the (in)comparability of RSD and PS. We analyze\ntheir respective economic properties under general and lexicographic\npreferences. We then instantiate utility functions with the goal of gaining\ninsights on the manipulability, efficiency, and envyfreeness of the mechanisms\nunder different risk-attitude models. Our results hold under various preference\ndistribution models, which further confirm the broad use of RSD in most\npractical applications.'}, {'An agent-based model of an endangered population of the Arctic fox from\n  Mednyi Island': 'Artificial Intelligence techniques such as agent-based modeling and\nprobabilistic reasoning have shown promise in modeling complex biological\nsystems and testing ecological hypotheses through simulation. We develop an\nagent-based model of Arctic foxes from Medniy Island while utilizing\nProbabilistic Graphical Models to capture the conditional dependencies between\nthe random variables. Such models provide valuable insights in analyzing\nfactors behind catastrophic degradation of this population and in revealing\nevolutionary mechanisms of its persistence in high-density environment. Using\nempirical data from studies in Medniy Island, we create a realistic model of\nArctic foxes as agents, and study their survival and population dynamics under\na variety of conditions.'}, {'Class Fairness in Online Matching': 'In the classical version of online bipartite matching, there is a given set\nof offline vertices (aka agents) and another set of vertices (aka items) that\narrive online. When each item arrives, its incident edges -- the agents who\nlike the item -- are revealed and the algorithm must irrevocably match the item\nto such agents. We initiate the study of class fairness in this setting, where\nagents are partitioned into a set of classes and the matching is required to be\nfair with respect to the classes. We adopt popular fairness notions from the\nfair division literature such as envy-freeness (up to one item),\nproportionality, and maximin share fairness to our setting. Our class versions\nof these notions demand that all classes, regardless of their sizes, receive a\nfair treatment. We study deterministic and randomized algorithms for matching\nindivisible items (leading to integral matchings) and for matching divisible\nitems (leading to fractional matchings). We design and analyze three novel\nalgorithms. For matching indivisible items, we propose an\nadaptive-priority-based algorithm, MATCH-AND-SHIFT, prove that it achieves\n1/2-approximation of both class envy-freeness up to one item and class maximin\nshare fairness, and show that each guarantee is tight. For matching divisible\nitems, we design a water-filling-based algorithm, EQUAL-FILLING, that achieves\n(1-1/e)-approximation of class envy-freeness and class proportionality; we\nprove (1-1/e) to be tight for class proportionality and establish a 3/4 upper\nbound on class envy-freeness. Finally, we build upon EQUAL-FILLING to design a\nrandomized algorithm for matching indivisible items, EQAUL-FILLING-OCS, which\nachieves 0.593-approximation of class proportionality. The algorithm and its\nanalysis crucially leverage the recently introduced technique of online\ncorrelated selection (OCS) [Fahrbach et al., 2020].'}]","Title: Fair Division with Weak Linear Orders in Lexicographic Preferences: Algorithms and Complexity

Abstract:
Delving into the realm of societal resource allocation, the fair division of indivisible items constitutes a foundational task across various disciplines. When dealing with economic or computational assignments, individual preferences might involve weak equivalence, where agents may intoxicated between sets of items. This paper extends classical fair division results to weak linear orders under lexicographic preferences, featuring a nuanced framework. We broaden established algorithms, namely the Generalized Efﬁcient, Envy Free algorithms, and MMS allocation principles, to a broader scope encompassing both goods and chores.

Our research focuses on exploring refined fairness concepts, such as envy-freeness truncated to ﬁxed items (EFX), and modifying these frameworks to tackle mixed baskets of goods and chores. We present algorithms that efficiently compute these allocations, alongside novel results illustrating the interplay between different fairness criteria.

Advancements include computational breakthroughs for EF1 and PO allocations in weakly lexicographic chores-only instances and a proof that EF1 assignments always imply PO. Within a computational context, the paper investigates the intricate balance between EFX, PO, and MMS allocations.

Contributions include a unified algorithmic framework that can accommodate customizable fairness notions, insights into the computational landscape of chore allocation, and the interrelationships between EF, EFX, and other fairness concepts.

Via simulations, we visualize these mechanisms in practical scenarios, offering a pathway for optimized resource distribution in sectors such as scientific peer review, paperwork division in organizations, and broader applications in distributed task management. This research significantly advances the computational and axiomatic underpinning of fair division, enriching methodologies for democratic and empathetic resource allocation processes."
"Existing automatic captioning methods for visual content face challenges such
as lack of detail, content hallucination, and poor instruction following. In
this work, we propose VisualFactChecker (VFC), a flexible training-free
pipeline that generates high-fidelity and detailed captions for both 2D images
and 3D objects. VFC consists of three steps: 1) proposal, where image-to-text
captioning models propose multiple initial captions; 2) verification, where a
large language model (LLM) utilizes tools such as object detection and VQA
models to fact-check proposed captions; 3) captioning, where an LLM generates
the final caption by summarizing caption proposals and the fact check
verification results. In this step, VFC can flexibly generate captions in
various styles following complex instructions. We conduct comprehensive
captioning evaluations using four metrics: 1) CLIP-Score for image-text
similarity; 2) CLIP-Image-Score for measuring the image-image similarity
between the original and the reconstructed image generated by a text-to-image
model using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4V
for fine-grained evaluation. Evaluation results show that VFC outperforms
state-of-the-art open-sourced captioning methods for 2D images on the COCO
dataset and 3D assets on the Objaverse dataset. Our study demonstrates that by
combining open-source models into a pipeline, we can attain captioning
capability comparable to proprietary models such as GPT-4V, despite being over
10x smaller in model size.","[{'Pose Augmentation: Class-agnostic Object Pose Transformation for Object\n  Recognition': 'Object pose increases intraclass object variance which makes object\nrecognition from 2D images harder. To render a classifier robust to pose\nvariations, most deep neural networks try to eliminate the influence of pose by\nusing large datasets with many poses for each class. Here, we propose a\ndifferent approach: a class-agnostic object pose transformation network\n(OPT-Net) can transform an image along 3D yaw and pitch axes to synthesize\nadditional poses continuously. Synthesized images lead to better training of an\nobject classifier. We design a novel eliminate-add structure to explicitly\ndisentangle pose from object identity: first eliminate pose information of the\ninput image and then add target pose information (regularized as continuous\nvariables) to synthesize any target pose. We trained OPT-Net on images of toy\nvehicles shot on a turntable from the iLab-20M dataset. After training on\nunbalanced discrete poses (5 classes with 6 poses per object instance, plus 5\nclasses with only 2 poses), we show that OPT-Net can synthesize balanced\ncontinuous new poses along yaw and pitch axes with high quality. Training a\nResNet-18 classifier with original plus synthesized poses improves mAP accuracy\nby 9% overtraining on original poses only. Further, the pre-trained OPT-Net can\ngeneralize to new object classes, which we demonstrate on both iLab-20M and\nRGB-D. We also show that the learned features can generalize to ImageNet.'}, {'Evaluating Pretrained models for Deployable Lifelong Learning': 'We create a novel benchmark for evaluating a Deployable Lifelong Learning\nsystem for Visual Reinforcement Learning (RL) that is pretrained on a curated\ndataset, and propose a novel Scalable Lifelong Learning system capable of\nretaining knowledge from the previously learnt RL tasks. Our benchmark measures\nthe efficacy of a deployable Lifelong Learning system that is evaluated on\nscalability, performance and resource utilization. Our proposed system, once\npretrained on the dataset, can be deployed to perform continual learning on\nunseen tasks. Our proposed method consists of a Few Shot Class Incremental\nLearning (FSCIL) based task-mapper and an encoder/backbone trained entirely\nusing the pretrain dataset. The policy parameters corresponding to the\nrecognized task are then loaded to perform the task. We show that this system\ncan be scaled to incorporate a large number of tasks due to the small memory\nfootprint and fewer computational resources. We perform experiments on our DeLL\n(Deployment for Lifelong Learning) benchmark on the Atari games to determine\nthe efficacy of the system.'}, {'Beneficial Perturbation Network for designing general adaptive\n  artificial intelligence systems': 'The human brain is the gold standard of adaptive learning. It not only can\nlearn and benefit from experience, but also can adapt to new situations. In\ncontrast, deep neural networks only learn one sophisticated but fixed mapping\nfrom inputs to outputs. This limits their applicability to more dynamic\nsituations, where input to output mapping may change with different contexts. A\nsalient example is continual learning - learning new independent tasks\nsequentially without forgetting previous tasks. Continual learning of multiple\ntasks in artificial neural networks using gradient descent leads to\ncatastrophic forgetting, whereby a previously learned mapping of an old task is\nerased when learning new mappings for new tasks. Here, we propose a new\nbiologically plausible type of deep neural network with extra, out-of-network,\ntask-dependent biasing units to accommodate these dynamic situations. This\nallows, for the first time, a single network to learn potentially unlimited\nparallel input to output mappings, and to switch on the fly between them at\nruntime. Biasing units are programmed by leveraging beneficial perturbations\n(opposite to well-known adversarial perturbations) for each task. Beneficial\nperturbations for a given task bias the network toward that task, essentially\nswitching the network into a different mode to process that task. This largely\neliminates catastrophic interference between tasks. Our approach is\nmemory-efficient and parameter-efficient, can accommodate many tasks, and\nachieves state-of-the-art performance across different tasks and domains.'}, {'Zero-shot Synthesis with Group-Supervised Learning': ""Visual cognition of primates is superior to that of artificial neural\nnetworks in its ability to 'envision' a visual object, even a newly-introduced\none, in different attributes including pose, position, color, texture, etc. To\naid neural networks to envision objects with different attributes, we propose a\nfamily of objective functions, expressed on groups of examples, as a novel\nlearning framework that we term Group-Supervised Learning (GSL). GSL allows us\nto decompose inputs into a disentangled representation with swappable\ncomponents, that can be recombined to synthesize new samples. For instance,\nimages of red boats & blue cars can be decomposed and recombined to synthesize\nnovel images of red cars. We propose an implementation based on auto-encoder,\ntermed group-supervised zero-shot synthesis network (GZS-Net) trained with our\nlearning framework, that can produce a high-quality red car even if no such\nexample is witnessed during training. We test our model and learning framework\non existing benchmarks, in addition to anew dataset that we open-source. We\nqualitatively and quantitatively demonstrate that GZS-Net trained with GSL\noutperforms state-of-the-art methods.""}, {'Contributions of Shape, Texture, and Color in Visual Recognition': 'We investigate the contributions of three important features of the human\nvisual system (HVS)~ -- ~shape, texture, and color ~ -- ~to object\nclassification. We build a humanoid vision engine (HVE) that explicitly and\nseparately computes shape, texture, and color features from images. The\nresulting feature vectors are then concatenated to support the final\nclassification. We show that HVE can summarize and rank-order the contributions\nof the three features to object recognition. We use human experiments to\nconfirm that both HVE and humans predominantly use some specific features to\nsupport the classification of specific classes (e.g., texture is the dominant\nfeature to distinguish a zebra from other quadrupeds, both for humans and HVE).\nWith the help of HVE, given any environment (dataset), we can summarize the\nmost important features for the whole task (task-specific; e.g., color is the\nmost important feature overall for classification with the CUB dataset), and\nfor each class (class-specific; e.g., shape is the most important feature to\nrecognize boats in the iLab-20M dataset). To demonstrate more usefulness of\nHVE, we use it to simulate the open-world zero-shot learning ability of humans\nwith no attribute labeling. Finally, we show that HVE can also simulate human\nimagination ability with the combination of different features. We will\nopen-source the HVE engine and corresponding datasets.'}, {'Invariant Structure Learning for Better Generalization and Causal\n  Explainability': 'Learning the causal structure behind data is invaluable for improving\ngeneralization and obtaining high-quality explanations. We propose a novel\nframework, Invariant Structure Learning (ISL), that is designed to improve\ncausal structure discovery by utilizing generalization as an indication. ISL\nsplits the data into different environments, and learns a structure that is\ninvariant to the target across different environments by imposing a consistency\nconstraint. An aggregation mechanism then selects the optimal classifier based\non a graph structure that reflects the causal mechanisms in the data more\naccurately compared to the structures learnt from individual environments.\nFurthermore, we extend ISL to a self-supervised learning setting where accurate\ncausal structure discovery does not rely on any labels. This self-supervised\nISL utilizes invariant causality proposals by iteratively setting different\nnodes as targets. On synthetic and real-world datasets, we demonstrate that ISL\naccurately discovers the causal structure, outperforms alternative methods, and\nyields superior generalization for datasets with significant distribution\nshifts.'}, {'Building One-class Detector for Anything: Open-vocabulary Zero-shot OOD\n  Detection Using Text-image Models': 'We focus on the challenge of out-of-distribution (OOD) detection in deep\nlearning models, a crucial aspect in ensuring reliability. Despite considerable\neffort, the problem remains significantly challenging in deep learning models\ndue to their propensity to output over-confident predictions for OOD inputs. We\npropose a novel one-class open-set OOD detector that leverages text-image\npre-trained models in a zero-shot fashion and incorporates various descriptions\nof in-domain and OOD. Our approach is designed to detect anything not in-domain\nand offers the flexibility to detect a wide variety of OOD, defined via fine-\nor coarse-grained labels, or even in natural language. We evaluate our approach\non challenging benchmarks including large-scale datasets containing\nfine-grained, semantically similar classes, distributionally shifted images,\nand multi-object images containing a mixture of in-domain and OOD objects. Our\nmethod shows superior performance over previous methods on all benchmarks. Code\nis available at https://github.com/gyhandy/One-Class-Anything'}, {'Encouraging Disentangled and Convex Representation with Controllable\n  Interpolation Regularization': ""We focus on controllable disentangled representation learning (C-Dis-RL),\nwhere users can control the partition of the disentangled latent space to\nfactorize dataset attributes (concepts) for downstream tasks. Two general\nproblems remain under-explored in current methods: (1) They lack comprehensive\ndisentanglement constraints, especially missing the minimization of mutual\ninformation between different attributes across latent and observation domains.\n(2) They lack convexity constraints, which is important for meaningfully\nmanipulating specific attributes for downstream tasks. To encourage both\ncomprehensive C-Dis-RL and convexity simultaneously, we propose a simple yet\nefficient method: Controllable Interpolation Regularization (CIR), which\ncreates a positive loop where disentanglement and convexity can help each\nother. Specifically, we conduct controlled interpolation in latent space during\ntraining, and we reuse the encoder to help form a 'perfect disentanglement'\nregularization. In that case, (a) disentanglement loss implicitly enlarges the\npotential understandable distribution to encourage convexity; (b) convexity can\nin turn improve robust and precise disentanglement. CIR is a general module and\nwe merge CIR with three different algorithms: ELEGANT, I2I-Dis, and GZS-Net to\nshow the compatibility and effectiveness. Qualitative and quantitative\nexperiments show improvement in C-Dis-RL and latent convexity by CIR. This\nfurther improves downstream tasks: controllable image synthesis, cross-modality\nimage translation, and zero-shot synthesis.""}, {'DALL-E for Detection: Language-driven Compositional Image Synthesis for\n  Object Detection': 'We propose a new paradigm to automatically generate training data with\naccurate labels at scale using the text-toimage synthesis frameworks (e.g.,\nDALL-E, Stable Diffusion, etc.). The proposed approach decouples training data\ngeneration into foreground object mask generation and background (context)\nimage generation. For foreground object mask generation, we use a simple\ntextual template with object class name as input to DALL-E to generate a\ndiverse set of foreground images. A foreground-background segmentation\nalgorithm is then used to generate foreground object masks. Next, in order to\ngenerate context images, first a language description of the context is\ngenerated by applying an image captioning method on a small set of images\nrepresenting the context. These language descriptions are then used to generate\ndiverse sets of context images using the DALL-E framework. These are then\ncomposited with object masks generated in the first step to provide an\naugmented training set for a classifier. We demonstrate the advantages of our\napproach on four object detection datasets including on Pascal VOC and COCO\nobject detection tasks. Furthermore, we also highlight the compositional nature\nof our data generation approach on out-of-distribution and zero-shot data\ngeneration scenarios.'}, {'EM-Paste: EM-guided Cut-Paste with DALL-E Augmentation for Image-level\n  Weakly Supervised Instance Segmentation': 'We propose EM-PASTE: an Expectation Maximization(EM) guided Cut-Paste\ncompositional dataset augmentation approach for weakly-supervised instance\nsegmentation using only image-level supervision. The proposed method consists\nof three main components. The first component generates high-quality foreground\nobject masks. To this end, an EM-like approach is proposed that iteratively\nrefines an initial set of object mask proposals generated by a generic region\nproposal method. Next, in the second component, high-quality context-aware\nbackground images are generated using a text-to-image compositional synthesis\nmethod like DALL-E. Finally, the third component creates a large-scale\npseudo-labeled instance segmentation training dataset by compositing the\nforeground object masks onto the original and generated background images. The\nproposed approach achieves state-of-the-art weakly-supervised instance\nsegmentation results on both the PASCAL VOC 2012 and MS COCO datasets by using\nonly image-level, weak label information. In particular, it outperforms the\nbest baseline by +7.4 and +2.8 mAP0.50 on PASCAL and COCO, respectively.\nFurther, the method provides a new solution to the long-tail weakly-supervised\ninstance segmentation problem (when many classes may only have few training\nsamples), by selectively augmenting under-represented classes.'}]","Abstract:

Aiming to enhance the precision and reliability of image captioning, this research introduces VisualFactChecker, an innovative method coupled with a large language model to generate and refine captions in both 2D and 3D contexts. 

Objective:
This study's primary goal is to improve image caption accuracy by integrating an efficient fact-checking mechanism that reduces hallucinations and contradictions in generated captions. 

Innovations:
A pivotal innovation lies in the deployment of multi-modal large language models and additional grounding tools including an object detection model and visual question answering system to cross-verify and correct 2D and 3D captions. 

Methods:
The pipeline involves generating preliminary description using multimodal models, verifying with object detection or visual questioning to extract possible objects, and correcting hallucinations. The system then adapts this mechanism for 3D object captioning with specific considerations pertinent to three-dimensional attributes.

Results:
Experimental outcomes show that the combined system outperforms standalone models as evidenced by superior CLIP-Score and CLIP-Image-Score metrics in both 2D and 3D contexts, demonstrating the effectiveness of the fact-checking mechanism.

Contributions:
VisualFactChecker introduces an effective framework that enhances caption accuracy through a systematic approach of proposal, verification, and refinement. The integration of a language model and grounding techniques paves the way for more reliable and contextually appropriate image and 3D object captions.

Applications:
This research has the potential to significantly enhance applications involving image understanding, such as content-based image retrieval, digital art analysis, and assistive technologies for the visually impaired, among others."
"3D bicontinuous two-phase materials are increasingly gaining interest because
of their unique multifunctional characteristics and advancements in techniques
to fabricate them. Due to their complex topological and structural properties,
it still has been nontrivial to develop explicit microstructure-dependent
formulas to predict accurately their physical properties. A primary goal of the
present paper is to ascertain various microstructural and transport
characteristics of five different models of triply periodic bicontinuous porous
materials at a porosity $\phi_1=1/2$: those in which the two-phase interfaces
are the Schwarz P, Schwarz D and Schoen G minimal surfaces as well as two
different pore-channel structures. We ascertain their spectral densities,
pore-size distribution functions, local volume-fraction variances, and
hyperuniformity order metrics and then use this information to estimate certain
effective transport properties via closed-form microstructure-property
formulas. Specifically, we estimate the recently introduced time-dependent
diffusion spreadability exactly from the spectral density. Moreover, we
accurately estimate the fluid permeability of such porous materials from the
second moment of the pore-size function and the formation factor, a measure of
the tortuosity of the pore space. We also rigorously bound the permeability
from above using the spectral density. For the five models with identical cubic
unit cells, we find that the permeability, inverse of the specific surface,
hyperuniformity order metric, pore-size second moment and long-time
spreadability behavior are all positively correlated and rank order the
structures in exactly the same way. We also conjecture what structures maximize
the fluid permeability for arbitrary porosities and show that this conjecture
must be true in the extreme porosity limits by identifying the corresponding
optimal structures.","[{'Necessary Conditions on Realizable Two-Point Correlation Functions of\n  Random Media': 'A fascinating inverse problem that has been receiving considerable attention\nis the construction of realizations of random two-phase heterogeneous media\nwith a target two-point correlation function. However, not every hypothetical\ntwo-point correlation function corresponds to a realizable two-phase medium.\nHere we collect all of the known necessary conditions on the two-point\ncorrelation functions scattered throughout a diverse literature and derive a\nnew but simple positivity condition. We apply the necessary conditions to test\nthe realizability of certain classes of proposed correlation functions.'}, {'Extraordinary Disordered Hyperuniform Multifunctional Composites': 'A variety of performance demands are being placed on material systems,\nincluding desirable mechanical, thermal, electrical, optical, acoustic and flow\nproperties. The purpose of the present article is to review the emerging field\nof disordered hyperuniform composites and their novel multifunctional\ncharacteristics. Disordered hyperuniform media are exotic amorphous states of\nmatter that are characterized by an anomalous suppression of large-scale\nvolume-fraction fluctuations compared to those in ""garden-variety"" disordered\nmaterials. Such unusual composites can have advantages over their periodic\ncounterparts, such as unique or nearly optimal, direction-independent physical\nproperties and robustness against defects. It will be shown that disordered\nhyperuniform composites and porous media can be endowed with a broad spectrum\nof extraordinary physical properties, including photonic, phononic, transport,\nchemical and mechanical characteristics that are only beginning to be discov'}, {'Basic Understanding of Condensed Phases of Matter via Packing Models': 'Packing problems have been a source of fascination for millenia and their\nstudy has produced a rich literature that spans numerous disciplines.\nInvestigations of hard-particle packing models have provided basic insights\ninto the structure and bulk properties of condensed phases of matter, including\nlow-temperature states (e.g., molecular and colloidal liquids, crystals and\nglasses), multiphase heterogeneous media, granular media, and biological\nsystems. The densest packings are of great interest in pure mathematics,\nincluding discrete geometry and number theory. This perspective reviews\npertinent theoretical and computational literature concerning the equilibrium,\nmetastable and nonequilibrium packings of hard-particle packings in various\nEuclidean space dimensions. In the case of jammed packings, emphasis will be\nplaced on the ""geometric-structure"" approach, which provides a powerful and\nunified means to quantitatively characterize individual packings via jamming\ncategories and ""order"" maps. It incorporates extremal jammed states, including\nthe densest packings, maximally random jammed states, and lowest-density jammed\nstructures. Packings of identical spheres, spheres with a size distribution,\nand nonspherical particles are also surveyed. We close this review by\nidentifying challenges and open questions for future research.'}, {'Inverse Optimization Techniques for Targeted Self-Assembly': 'This article reviews recent inverse statistical-mechanical methodologies that\nwe have devised to optimize interaction potentials in soft matter systems that\ncorrespond to stable ""target"" structures. We are interested in finding the\ninteraction potential, not necessarily pairwise additive or spherically\nsymmetric, that stabilizes a targeted many-body system by generally\nincorporating complete configurational information. Unlike previous work, our\nprimary interest is in the possible many-body structures that may be generated,\nsome of which may include interesting but known structures, while others may\nrepresent entirely new structural motifs. Soft matter systems, such as colloids\nand polymers, offer a versatile means of realizing the optimized interactions.\nIt is shown that these inverse approaches hold great promise for controlling\nself-assembly to a degree that surpasses the less-than-optimal path that nature\nhas provided. Indeed, we envision being able to ""tailor"" potentials that\nproduce varying degrees of disorder, thus extending the traditional idea of\nself-assembly to incorporate both amorphous and crystalline structures as well\nas quasicrystals. The notion of tailoring potentials that correspond to\ntargeted structures is motivated by the rich fundamental statistical-mechanical\nissues and questions offered by this fascinating inverse problem as well as our\nrecent ability to identify structures that have optimal bulk properties or\ndesirable performance characteristics. Recent results have already led to a\ndeeper basic understanding of the mathematical relationship between the\ncollective structural behavior of many-body systems and their interactions, as\nwell as optimized potentials that enable self-assembly of ordered and\ndisordered particle configurations with novel structural and bulk properties.'}, {'Hyperuniformity and its Generalizations': 'Disordered many-particle hyperuniform systems are exotic amorphous states\ncharacterized by anomalous suppression of large-scale density fluctuations.\nHere we substantially broaden the hyperuniformity concept along four different\ndirections. This includes generalizations to treat fluctuations in the\ninterfacial area in heterogeneous media and surface-area driven evolving\nmicrostructures, random scalar fields, divergence-free random vector fields, as\nwell as statistically anisotropic many-particle systems and two-phase media.\nInterfacial-area fluctuations play a major role in characterizing the\nmicrostructure of two-phase systems , physical properties that intimately\ndepend on the geometry of the interface, and evolving two-phase microstructures\nthat depend on interfacial energies (e.g., spinodal decomposition). In the\ninstances of divergence-free random vector fields and statistically anisotropic\nstructures, we show that the standard definition of hyperuniformity must be\ngeneralized such that it accounts for the dependence of the relevant spectral\nfunctions on the direction in which the origin in Fourier space\n(nonanalyticities at the origin). Using this analysis, we place some well-known\nenergy spectra from the theory of isotropic turbulence in the context of this\ngeneralization of hyperuniformity. We show that there exist many-particle\nground-state configurations in which directional hyperuniformity imparts exotic\nanisotropic physical properties (e.g., elastic, optical and acoustic\ncharacteristics) to these states of matter. Such tunablity could have\ntechnological relevance for manipulating light and sound waves in ways\nheretofore not thought possible. We show that disordered many-particle systems\nthat respond to external fields (e.g., magnetic and electric fields) are a\nnatural class of materials to look for directional hyperuniformity.'}, {'Disordered Hyperuniform Heterogeneous Materials': 'Disordered hyperuniform many-body systems are distinguishable states of\nmatter that lie between a crystal and liquid: they are like perfect crystals in\nthe way they suppress large-scale density fluctuations and yet are like liquids\nor glasses in that they are statistically isotropic with no Bragg peaks. These\nsystems play a vital role in a number of fundamental and applied problems:\nglass formation, jamming, rigidity, photonic and electronic band structure,\nlocalization of waves and excitations, self-organization, fluid dynamics,\nquantum systems, and pure mathematics. systems. Here, we derive new rigorous\ncriteria that disordered hyperuniform two-phase heterogeneous materials must\nobey and explore their consequences. Two-phase heterogeneous media are\nubiquitous, examples include composites and porous media, biological media,\nfoams, polymer blends, granular media, cellular solids, and colloids. We\nrigorously establish the requirements for sphere packings to be\n""multihyperuniform."" We apply realizability conditions for an autocovariance\nfunction and its associated spectral density of a two-phase medium, and then\nincorporate hyperuniformity as a constraint in order to derive new conditions.\nWe show that some functional forms can immediately be eliminated from\nconsideration and identify other forms that are allowable. Specific examples\nand counterexamples are described. Contact is made with well-known\nmicrostructural models as well as irregular phase-separation and Turing-type\npatterns. We also ascertain a family of spectral densities that are realizable\nby disordered hyperuniform two-phase media in any space dimension, and present\nexplicit constructions. These studies provide insight into the nature of\ndisordered hyperuniformity in the context of heterogeneous materials and have\nimplications for the design of such novel amorphous materials.'}, {'Hyperuniform States of Matter': 'Hyperuniform states of matter are correlated systems that are characterized\nby an anomalous suppression of long-wavelength (i.e., large-length-scale)\ndensity fluctuations compared to those found in garden-variety disordered\nsystems, such as ordinary fluids and amorphous solids. All perfect crystals,\nperfect quasicrystals and special disordered systems are hyperuniform. Thus,\nthe hyperuniformity concept enables a unified framework to classify and\nstructurally characterize crystals, quasicrystals and the exotic disordered\nvarieties. While disordered hyperuniform systems were largely unknown in the\nscientific community over a decade ago, now there is a realization that such\nsystems arise in a host of contexts across the physical, materials, chemical,\nmathematical, engineering, and biological sciences, including disordered ground\nstates, glass formation, jamming, Coulomb systems, spin systems, photonic and\nelectronic band structure, localization of waves and excitations,\nself-organization, fluid dynamics, number theory, stochastic point processes,\nintegral and stochastic geometry, the immune system, and photoreceptor cells.\nSuch unusual amorphous states can be obtained via equilibrium or nonequilibrium\nroutes, and come in both quantum-mechanical and classical varieties. The\nconnections of hyperuniform states of matter to many different areas of\nfundamental science appear to be profound and yet our theoretical understanding\nof these unusual systems is only in its infancy. The purpose of this review\narticle is to introduce the reader to the theoretical foundations of\nhyperuniform ordered and disordered systems. Special focus will be placed on\nfundamental and practical aspects of the disordered kinds, including our\ncurrent state of knowledge of these exotic amorphous systems as well as their\nformation and novel physical properties.'}, {'Toward an Ising Model of Cancer and Beyond': 'Theoretical and computational tools that can be used in the clinic to predict\nneoplastic progression and propose individualized optimal treatment strategies\nto control cancer growth is desired. To develop such a predictive model, one\nmust account for the complex mechanisms involved in tumor growth. Here we\nreview resarch work that we have done toward the development of an ""Ising\nmodel"" of cancer. The review begins with a description of a minimalist\nfour-dimensional (three in space and one in time) cellular automaton (CA) model\nof cancer in which healthy cells transition between states (proliferative,\nhypoxic, and necrotic) according to simple local rules and their present\nstates, which can viewed as a stripped-down Ising model of cancer. This model\nis applied to model the growth of glioblastoma multiforme, the most malignant\nof brain cancers. This is followed by a discussion of the extension of the\nmodel to study the effect on the tumor dynamics and geometry of a mutated\nsubpopulation. A discussion of how tumor growth is affected by chemotherapeutic\ntreatment is then described. How angiogenesis as well as the heterogeneous and\nconfined environment in which a tumor grows is incorporated in the CA model is\ndiscussed. The characterization of the level of organization of the invasive\nnetwork around a solid tumor using spanning trees is subsequently described.\nThen, we describe open problems and future promising avenues for future\nresearch, including the need to develop better molecular-based models that\nincorporate the true heterogeneous environment over wide range of length and\ntime scales (via imaging data), cell motility, oncogenes, tumor suppressor\ngenes and cell-cell communication. The need to bring to bear the powerful\nmachinery of the theory of heterogeneous media to better understand the\nbehavior of cancer in its microenvironment is presented.'}, {'Structural Characterization of Many-Particle Systems on Approach to\n  Hyperuniform States': 'We explore quantitative descriptors that herald when a many-particle system\nin $d$-dimensional Euclidean space $\\mathbb{R}^d$ approaches a hyperuniform\nstate as a function of the relevant control parameter. We establish\nquantitative criteria to ascertain the extent of hyperuniform and\nnonhyperuniform distance-scaling regimes n terms of the ratio $B/A$, where $A$\nis ""volume"" coefficient and $B$ is ""surface-area"" coefficient associated with\nthe local number variance $\\sigma^2(R)$ for a spherical window of radius $R$.\nTo complement the known direct-space representation of the coefficient $B$ in\nterms of the total correlation function $h({\\bf r})$, we derive its\ncorresponding Fourier representation in terms of the structure factor $S({\\bf\nk})$, which is especially useful when scattering information is available\nexperimentally or theoretically. We show that the free-volume theory of the\npressure of equilibrium packings of identical hard spheres that approach a\nstrictly jammed state either along the stable crystal or metastable disordered\nbranch dictates that such end states be exactly hyperuniform. Using the ratio\n$B/A$, the hyperuniformity index $H$ and the direct-correlation function length\nscale $\\xi_c$, we study three different exactly solvable models as a function\nof the relevant control parameter, either density or temperature, with end\nstates that are perfectly hyperuniform. We analyze equilibrium hard rods and\n""sticky"" hard-sphere systems in arbitrary space dimension $d$ as a function of\ndensity. We also examine low-temperature excited states of many-particle\nsystems interacting with ""stealthy"" long-ranged pair interactions as the\ntemperature tends to zero. The capacity to identify hyperuniform scaling\nregimes should be particularly useful in analyzing experimentally- or\ncomputationally-generated samples that are necessarily of finite size.'}, {'Diffusion Spreadability as a Probe of the Microstructure of Complex\n  Media Across Length Scales': 'Consider the time-dependent problem of mass transfer of a solute between two\nphases and assume that the solute is initially distributed in one phase (phase\n2) and absent from the other (phase 1). We desire the fraction of total solute\npresent in phase 1 as a function of time, ${\\cal S}(t)$, which we call the {\\it\nspreadability}, since it is a measure of the spreadability of diffusion\ninformation as a function of time. We derive exact direct-space formulas for\n${\\cal S}(t)$ in any Euclidean space dimension $d$ in terms of the\nautocovariance function as well as corresponding Fourier representations of\n${\\cal S}(t)$ in terms of the spectral density. We derive closed-form general\nformulas for the short- and long-time behaviors of the spreadability in terms\nof crucial small- and large-scale microstructural information, respectively.\nThe long-time behavior of ${\\cal S}(t)$ enables one to distinguish the entire\nspectrum of microstructures that span from hyperuniform to nonhyperuniform\nmedia. For hyperuniform media, disordered or not, we show that the ""excess""\nspreadability, ${\\cal S}(\\infty)-{\\cal S}(t)$, decays to its long-time behavior\nexponentially faster than that of any nonhyperuniform two-phase medium, the\n""slowest"" being antihyperuniform media. The stealthy hyperuniform class is\ncharacterized by an excess spreadability with the fastest decay rate among all\ntranslationally invariant microstructures. Moreover, we establish a remarkable\nconnection between the spreadability and an outstanding problem in discrete\ngeometry, namely, microstructures with ""fast"" spreadabilities are also those\nthat can be derived from efficient ""coverings"" of space. We also identify\nheretofore unnoticed remarkable links between the spreadability ${\\cal S}(t)$\nand NMR pulsed field gradient spin-echo amplitude as well as diffusion MRI\nmeasurements.'}]","Title: Determining the Rank and Properties of Triply Periodic Media with Minimal Surface Area through the Local Volume Fraction Variance BV, and their Flow Diffusion Relation  
  
### Abstract

**Background**: Triply periodic minimal surfaces (TPMS) are of significance in material science, biology, and highway engineering due to their unique architectural features and versatile applications, from photonic band gap crystals to biological membranes and dense porous media. This study focuses on quantitatively evaluating, ranking, and characterizing TPMS using novel methodological frameworks. 

**Objective**: The primary goal of this research is to investigate the relationship between the local volume fraction variance BV for a set of TPMS and their transport properties, particularly fluid permeability. By ranking these points in R3 and exploring their fluid dynamics, the study fosters insights into the efficient design and optimization of porous structures with optimized fluid and diffusion attributes.

**Innovations**: This paper introduces a recently developed approximation formula for fluid permeability, linked to microstructural descriptors via the local volume fraction variance BV, applicable to diverse TPMS such as TPMS by Schwarz, Schoen, and configurations with spherical and cylindrical channels. It significantly advances the calculation of transport properties by comparing theoretical estimations with simulation results.

**Methods**: The research utilizes efficient polynomial fitting and analytical expression methods to compute BV from the TPMS descriptors, ultimately linking BV's hierarchy to fluid permeability predictions. This approach facilitates a deeper understanding of the fundamental connection between microstructure and macroscopic transport behaviors.

**Results**: The study quantifies a local-to-global volatility relationship, validating the equivalence of BV's rank ordering with that of fluid permeability for TPMS. It deciphers how increases in harmonic interactions influence permeability and finds that diffusion is closely correlated with BV, supplanted by simulations that confirm the approximation's accurateness.

**Contributions**: The pioneering application of BV in TPMS characterization offers novel insights into microstructured porous material design. It advances the predictability of permeability and diffusion without extensive simulation, fostering better manufacturing and material selection criteria that amplify transport efficiency etc.

**Applications**: The flow-diffusion correlation relations uncovered herein hold great promise for optimizing porous media in critical applications including solar cells, battery electrodes, filtration systems, and pharmaceutical delivery vehicles. Their utilization enhances material with engineered BV attributes, targeting optimized fluid dynamics and diffusion processes. The research provides a solid foundation for designing functional and more efficient materials with tailored physical properties.

This concise yet comprehensive review appropriately condenses the research into a succinct 300-word abstract, adhering to the outlined elements and maintaining its core focus on a novel methodological approach for amidst characterization and quantification of Triply Periodic Media, with a clear emphasis on their potential impact and applications in optimizing micro-structured porous materials."
"Video anomaly understanding (VAU) aims to automatically comprehend unusual
occurrences in videos, thereby enabling various applications such as traffic
surveillance and industrial manufacturing. While existing VAU benchmarks
primarily concentrate on anomaly detection and localization, our focus is on
more practicality, prompting us to raise the following crucial questions: ""what
anomaly occurred?"", ""why did it happen?"", and ""how severe is this abnormal
event?"". In pursuit of these answers, we present a comprehensive benchmark for
Causation Understanding of Video Anomaly (CUVA). Specifically, each instance of
the proposed benchmark involves three sets of human annotations to indicate the
""what"", ""why"" and ""how"" of an anomaly, including 1) anomaly type, start and end
times, and event descriptions, 2) natural language explanations for the cause
of an anomaly, and 3) free text reflecting the effect of the abnormality. In
addition, we also introduce MMEval, a novel evaluation metric designed to
better align with human preferences for CUVA, facilitating the measurement of
existing LLMs in comprehending the underlying cause and corresponding effect of
video anomalies. Finally, we propose a novel prompt-based method that can serve
as a baseline approach for the challenging CUVA. We conduct extensive
experiments to show the superiority of our evaluation metric and the
prompt-based approach. Our code and dataset are available at
https://github.com/fesvhtr/CUVA.","[{'Matching recovery threshold for correlated random graphs': ""For two correlated graphs which are independently sub-sampled from a common\nErd\\H{o}s-R\\'enyi graph $\\mathbf{G}(n, p)$, we wish to recover their\n\\emph{latent} vertex matching from the observation of these two graphs\n\\emph{without labels}. When $p = n^{-\\alpha+o(1)}$ for $\\alpha\\in (0, 1]$, we\nestablish a sharp information-theoretic threshold for whether it is possible to\ncorrectly match a positive fraction of vertices. Our result sharpens a constant\nfactor in a recent work by Wu, Xu and Yu.""}, {'Detection threshold for correlated Erdős-Rényi graphs via densest\n  subgraphs': ""The problem of detecting edge correlation between two Erd\\H{o}s-R\\'enyi\nrandom graphs on $n$ unlabeled nodes can be formulated as a hypothesis testing\nproblem: under the null hypothesis, the two graphs are sampled independently;\nunder the alternative, the two graphs are independently sub-sampled from a\nparent graph which is Erd\\H{o}s-R\\'enyi $\\mathbf{G}(n, p)$ (so that their\nmarginal distributions are the same as the null). We establish a sharp\ninformation-theoretic threshold when $p = n^{-\\alpha+o(1)}$ for $\\alpha\\in (0,\n1]$ which sharpens a constant factor in a recent work by Wu, Xu and Yu. A key\nnovelty in our work is an interesting connection between the detection problem\nand the densest subgraph of an Erd\\H{o}s-R\\'enyi graph.""}, {'A Hybrid Complex-valued Neural Network Framework with Applications to\n  Electroencephalogram (EEG)': 'In this article, we present a new EEG signal classification framework by\nintegrating the complex-valued and real-valued Convolutional Neural\nNetwork(CNN) with discrete Fourier transform (DFT). The proposed neural network\narchitecture consists of one complex-valued convolutional layer, two\nreal-valued convolutional layers, and three fully connected layers. Our method\ncan efficiently utilize the phase information contained in the DFT. We validate\nour approach using two simulated EEG signals and a benchmark data set and\ncompare it with two widely used frameworks. Our method drastically reduces the\nnumber of parameters used and improves accuracy when compared with the existing\nmethods in classifying benchmark data sets, and significantly improves\nperformance in classifying simulated EEG signals.'}, {'The Algorithmic Phase Transition of Random Graph Alignment Problem': ""We study the graph alignment problem over two independent Erd\\H{o}s-R\\'enyi\ngraphs on $n$ vertices, with edge density $p$ falling into two regimes\nseparated by the critical window around $p_c=\\sqrt{\\log n/n}$. Our result\nreveals an algorithmic phase transition for this random optimization problem:\npolynomial-time approximation schemes exist in the sparse regime, while\nstatistical-computational gap emerges in the dense regime. Additionally, we\nestablish a sharp transition on the performance of online algorithms for this\nproblem when $p$ lies in the dense regime, resulting in a $\\sqrt{8/9}$\nmultiplicative constant factor gap between achievable and optimal solutions.""}, {'Percolation threshold for metric graph loop soup': 'In this short note, we show that the critical threshold for the percolation\nof metric graph loop soup on a large class of transient metric graphs\n(including quasi-transitive graphs such as $\\mathbb{Z}^d$, $d\\geq 3$) is $1/2$.'}, {'Scene Text Detection with Selected Anchor': 'Object proposal technique with dense anchoring scheme for scene text\ndetection were applied frequently to achieve high recall. It results in the\nsignificant improvement in accuracy but waste of computational searching,\nregression and classification. In this paper, we propose an anchor\nselection-based region proposal network (AS-RPN) using effective selected\nanchors instead of dense anchors to extract text proposals. The center, scales,\naspect ratios and orientations of anchors are learnable instead of fixing,\nwhich leads to high recall and greatly reduced numbers of anchors. By replacing\nthe anchor-based RPN in Faster RCNN, the AS-RPN-based Faster RCNN can achieve\ncomparable performance with previous state-of-the-art text detecting approaches\non standard benchmarks, including COCO-Text, ICDAR2013, ICDAR2015 and\nMSRA-TD500 when using single-scale and single model (ResNet50) testing only.'}, {'Low-Degree Hardness of Detection for Correlated Erdős-Rényi Graphs': ""Given two Erd\\H{o}s-R\\'enyi graphs with $n$ vertices whose edges are\ncorrelated through a latent vertex correspondence, we study complexity lower\nbounds for the associated correlation detection problem for the class of\nlow-degree polynomial algorithms. We provide evidence that any\ndegree-$O(\\rho^{-1})$ polynomial algorithm fails for detection, where $\\rho$ is\nthe edge correlation. Furthermore, in the sparse regime where the edge density\n$q=n^{-1+o(1)}$, we provide evidence that any degree-$d$ polynomial algorithm\nfails for detection, as long as $\\log d=o\\big( \\frac{\\log n}{\\log nq} \\wedge\n\\sqrt{\\log n} \\big)$ and the correlation $\\rho<\\sqrt{\\alpha}$ where\n$\\alpha\\approx 0.338$ is the Otter's constant. Our result suggests that several\nstate-of-the-art algorithms on correlation detection and exact matching\nrecovery may be essentially the best possible.""}, {'A polynomial-time approximation scheme for the maximal overlap of two\n  independent Erdős-Rényi graphs': ""For two independent Erd\\H{o}s-R\\'enyi graphs $\\mathbf G(n,p)$, we study the\nmaximal overlap (i.e., the number of common edges) of these two graphs over all\npossible vertex correspondence. We present a polynomial-time algorithm which\nfinds a vertex correspondence whose overlap approximates the maximal overlap up\nto a multiplicative factor that is arbitrarily close to 1. As a by-product, we\nprove that the maximal overlap is asymptotically $\\frac{n}{2\\alpha-1}$ for\n$p=n^{-\\alpha}$ with some constant $\\alpha\\in (1/2,1)$.""}, {'Sharp asymptotics for arm probabilities in critical planar percolation': 'In this work, we consider critical planar site percolation on the triangular\nlattice and derive sharp estimates on the asymptotics of the probability of\nhalf-plane $j$-arm events for $j \\geq 1$ and planar (polychromatic) $j$-arm\nevents for $j>1$. These estimates greatly improve previous results and in\nparticular answer (a large part of) a question of Schramm (ICM Proc., 2006).'}, {'1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View\n  Reconstruction': 'In this report, we present the 1st place solution for ICCV 2023 OmniObject3D\nChallenge: Sparse-View Reconstruction. The challenge aims to evaluate\napproaches for novel view synthesis and surface reconstruction using only a few\nposed images of each object. We utilize Pixel-NeRF as the basic model, and\napply depth supervision as well as coarse-to-fine positional encoding. The\nexperiments demonstrate the effectiveness of our approach in improving\nsparse-view reconstruction quality. We ranked first in the final test with a\nPSNR of 25.44614.'}]","Title: Evaluating Video Large Language Models for Anomaly Causation Understanding 

Abstract:
As video understanding increasingly relies on advanced natural language processing, this research aims to identify, model, and evaluate causal reasoning capabilities in video anomaly tasks. In the context of video anomaly, causal understanding poses significant challenges to current video language models (VLMs), emphasizing the need for dedicated dataset and evaluation benchmarks. Extensive data collection across 42 domains leads to the creation of the CUVA benchmark, addressing these challenges.

The primary objective of this study is to enhance the capability of VLMs to comprehend and reason about cause-effect relationships within videos. Innovations center on developing a new prompt-based method, 'A-Guardian', integrating both ‘hard’ and ‘soft’ prompts to improve reasoning capabilities. This approach tackles key challenges in detecting anomalies and understanding their causality.

Methodology involves primarily four tasks: anomaly classification, description, effect prediction, and timestamp localization, across a suite of evaluation metrics designed to measure anomalous-causal understanding effectively. Through extensive training and experiments, the A-Guardian model demonstrates significant improvements in capturing causality, showcasing enhanced reasoning about video anomalies.

Key contributions include the development of the CUVA benchmark and evaluation methodology, the A-Guardian model as a robust causal reasoning tool, and a novel integration of hard and soft prompts. This research advances the state-of-the-art in video understanding by providing new tools for evaluating and enhancing VLMs in anomaly causation tasks. The outcomes have broad potential applications, from surveillance systems improving incident response, to historical footage analysis, and media content moderation."
"Identifying differential operators from data is essential for the
mathematical modeling of complex physical and biological systems where massive
datasets are available. These operators must be stable for accurate predictions
for dynamics forecasting problems. In this article, we propose a novel
methodology for learning sparse differential operators that are theoretically
linearly stable by solving a constrained regression problem. These underlying
constraints are obtained following linear stability for dynamical systems. We
further extend this approach for learning nonlinear differential operators by
determining linear stability constraints for linearized equations around an
equilibrium point. The applicability of the proposed method is demonstrated for
both linear and nonlinear partial differential equations such as 1-D scalar
advection-diffusion equation, 1-D Burgers equation and 2-D advection equation.
The results indicated that solutions to constrained regression problems with
linear stability constraints provide accurate and linearly stable sparse
differential operators.","[{'Projection-based reduced order modeling and data-driven artificial\n  viscosity closures for incompressible fluid flows': 'Projection-based reduced order models rely on offline-online model\ndecomposition, where the data-based energetic spatial basis is used in the\nexpensive offline stage to obtain equations of reduced states that evolve in\ntime during the inexpensive online stage. The online stage requires a solution\nmethod for the dynamic evolution of the coupled system of pressure and velocity\nstates for incompressible fluid flows. The first contribution of this article\nis to demonstrate the applicability of the incremental pressure correction\nscheme for the dynamic evolution of pressure and velocity states. The evolution\nof a large number of these reduced states in the online stage can be expensive.\nIn contrast, the accuracy significantly decreases if only a few reduced states\nare considered while not accounting for the interactions between unresolved and\nresolved states. The second contribution of this article is to compare three\nclosure model forms based on global, modal and tensor artificial viscosity\napproximation to account for these interactions. The unknown model parameters\nare determined using two calibration techniques: least squares minimization of\nerror in energy approximation and closure term approximation. This article\ndemonstrates that an appropriate selection of solution methods and data-driven\nartificial viscosity closure models is essential for consistently accurate\ndynamics forecasting of incompressible fluid flows.'}, {'Optimal Clipping of Structural Subgrid Stress Closures for Large Eddy\n  Simulation': ""Structural subgrid stress models for large eddy simulation often allow for\nbackscatter of energy from unresolved to resolved turbulent scales, but\nexcessive model backscatter can eventually result in numerical instability. A\ncommonly employed strategy to overcome this issue is to set predicted subgrid\nstresses to zero in regions of model backscatter. This clipping procedure\nimproves the stability of structural models, however, at the cost of reduced\ncorrelation between the predicted subgrid stresses and the exact subgrid\nstresses. In this article, we propose an alternative strategy that removes\nmodel backscatter from model predictions through the solution of a constrained\nminimization problem. This procedure, which we refer to as optimal clipping,\nresults in a parameter-free mixed model, and it yields predicted subgrid\nstresses in higher correlation with the exact subgrid stresses as compared with\nthose attained with the traditional clipping procedure. We perform a series of\na priori and a posteriori tests to investigate the impact of applying the\ntraditional and optimal clipping procedures to Clark's gradient subgrid stress\nmodel, and we observe that optimal clipping leads to a significant improvement\nin model predictions as compared to the traditional clipping procedure.""}, {'Invariant Data-Driven Subgrid Stress Modeling on Anisotropic Grids for\n  Large Eddy Simulation': 'We present a new approach for constructing data-driven subgrid stress models\nfor large eddy simulation of turbulent flows using anisotropic grids. The key\nto our approach is a Galilean, rotationally, reflectionally and unit invariant\nmodel form that also embeds filter anisotropy in such a way that an important\nsubgrid stress identity is satisfied. We use this model form to train a\ndata-driven subgrid stress model using only a small amount of anisotropically\nfiltered DNS data and a simple and inexpensive neural network architecture. A\npriori and a posteriori tests indicate that the trained data-driven model\ngeneralizes well to filter anisotropy ratios, Reynolds numbers and flow physics\noutside the training dataset.'}, {'Invariant Data-Driven Subgrid Stress Modeling in the Strain-Rate\n  Eigenframe for Large Eddy Simulation': ""We present a new approach for constructing data-driven subgrid stress models\nfor large eddy simulation of turbulent flows. The key to our approach is\nrepresentation of model input and output tensors in the filtered strain rate\neigenframe. Provided inputs and outputs are selected and non-dimensionalized in\na suitable manner, this yields a model form that is symmetric, Galilean\ninvariant, rotationally invariant, reflectionally invariant, and unit\ninvariant. We use this model form to train a simple and efficient neural\nnetwork model using only one time step of filtered direct numerical simulation\ndata from a forced homogeneous isotropic turbulence simulation. We demonstrate\nthe accuracy of this model as well as the model's ability to generalize to\npreviously unseen filter widths, Reynolds numbers, and flow physics using a\npriori and a posteriori tests.""}, {'Turbulent boundary layer with strong favorable pressure gradient and\n  curvature effects: Streamline coordinate and scaling analysis': 'Direct numerical simulation (DNS) of a turbulent boundary layer over the\nGaussian (Boeing) bump is performed. This boundary layer exhibits a series of\nadverse and favorable pressure gradients and convex and concave curvature\neffects before separating. These effects on turbulent boundary layers are\ncharacterized and compared to a lower Reynolds number flow over the same\ngeometry. The momentum budgets are analyzed in the streamline-aligned\ncoordinate system upstream of the separation region. These momentum budgets\nallow the simplification of equations to facilitate an integral analysis.\nIntegral analysis-based scalings for Reynolds stresses in the inner and outer\nregions of the boundary layer are also formulated. These proposed scalings\nexhibit a better collapse of Reynolds stress profiles compared to friction\nvelocity scaling and Zagarola-Smits scaling in the strong favorable pressure\ngradient region and in the mild adverse pressure region that precedes it in\nthis flow.'}, {'Kilohertz Gravitational Waves From Binary Neutron Star Mergers:\n  Numerical-relativity Informed Postmerger Model': ""We present ${\\tt NRPMw}$, an analytical model of gravitational-waves from\nneutron star merger remnants informed using 618 numerical relativity (NR)\nsimulations. ${\\tt NRPMw}$ is designed in the frequency domain using a\ncombination of complex Gaussian wavelets. The wavelet's parameters are\ncalibrated to equations of state (EOS) insensitive relations from NR data. The\nNR simulations are computed with 21 EOS (7 of which are finite-temperature\nmicrophysical models, and 3 of which contain quark phase transitions or\nhyperonic degrees of freedom) and span total binary masses $M\\in[2.4,3.4]~{\\rm\nM}_\\odot$, mass ratios up to $q=2$, and (nonprecessing) dimensionless spins\nmagnitudes up to ${0.2}$. The theoretical uncertainties of the EOS-insensitive\nrelations are incorporated in ${\\tt NRPMw}$ using recalibration parameters that\nenhance the flexibility and accuracy of the model. ${\\tt NRPMw}$ is NR-faithful\nwith fitting factors ${\\gtrsim}0.9$ computed on an independent validation set\nof 102 simulations.""}, {'Revealing Phase Transition in Dense Matter with Gravitational Wave\n  Spectroscopy of Binary Neutron Star Mergers': 'We use numerical relativity simulations of binary neutron star mergers to\nshow that high density deconfinement phase transitions (PTs) to quark matter\ncan be probed using multimodal postmerger gravitational wave (GW) spectroscopy.\nHadron-quark PTs suppress the one-armed spiral instability in the remnant. This\nis manifested in an anti-correlation between the energy carried in the $l=2,\nm=1$ GW mode and energy density gap which separates the two phases.\nConsequently, a single measurement of the signal-to-noise ratios of the $l=2,\nm=1$ and $l=2, m=2$ GW modes could constrain the energy density gap of the PT.'}, {'Signatures of deconfined quark phases in binary neutron star mergers': '(abridged) We investigate the quark deconfinement phase transition in the\ncontext of binary neutron star (BNS) mergers. We employ a new\nfinite-temperature composition-dependent equation of state (EOS) with a first\norder phase transition between hadrons and deconfined quarks to perform\nnumerical relativity simulations of BNS mergers. The softening of the EOS due\nto the phase transition causes the merger remnants to be more compact and to\ncollapse to a black hole (BH) at earlier times. The phase transition is\nimprinted on the postmerger gravitational wave (GW) signal duration, amplitude,\nand peak frequency. However, this imprint is only detectable for binaries with\nsufficiently long-lived remnants. Moreover, the phase transition does not\nresult in significant deviations from quasi-universal relations for the\npostmerger GW peak frequency. We also study the impact of the phase transition\non dynamical ejecta, remnant accretion disk masses, r-process nucleosynthetic\nyields and associated electromagnetic (EM) counterparts. While there are\ndifferences in the EM counterparts and nucleosynthesis yields between the\npurely hadronic models and the models with phase transitions, these can be\nprimarily ascribed to the difference in remnant collapse time between the two.\nAn exception is the non-thermal afterglow caused by the interaction of the\nfastest component of the dynamical ejecta and the interstellar medium, which is\nsystematically boosted in the binaries with phase transition as a consequence\nof the more violent merger they experience.'}, {'Detectability of QCD phase transitions in binary neutron star mergers:\n  Bayesian inference with the next generation gravitational wave detectors': ""We study the detectability of postmerger QCD phase transitions in neutron\nstar binaries with next-generation gravitational-wave detectors Cosmic Explorer\nand Einstein Telescope. We perform numerical relativity simulations of neutron\nstar mergers with equations of state that include a quark deconfinement phase\ntransition through either a Gibbs or Maxwell construction. These are followed\nby Bayesian parameter estimation of the associated gravitational-wave signals\nusing the $\\tt{NRPMw}$ waveform model, with priors inferred from the analysis\nof the inspiral signal. We assess the ability of the model to measure the\npostmerger peak frequency $f_2^{\\rm peak}$ and identify aspects that should be\nimproved in the model. We show that, even at postmerger signal to noise ratios\nas low as 10, the model can distinguish (at the 90% level) $f_2^{\\rm peak}$\nbetween binaries with and without a phase transition in most cases.\nPhase-transition induced deviations in the $f_2^{\\rm peak}$ from the\npredictions of equation-of-state insensitive relations can also be detected if\nthey exceed $1.6\\,\\sigma$. Our results suggest that next-generation\ngravitational wave detectors can measure phase transition effects in binary\nneutron star mergers. However, unless the phase transition is ``strong'',\ndisentangling it from other hadronic physics uncertainties will require\nsignificant theory improvements.""}, {'Mapping dynamical ejecta and disk masses from numerical relativity\n  simulations of neutron star mergers': 'We present fitting formulae for the dynamical ejecta properties and remnant\ndisk masses from the largest to date sample of numerical relativity\nsimulations. The considered data include some of the latest simulations with\nmicrophysical nuclear equations of state (EOS) and neutrino transport as well\nas other results with polytropic EOS available in the literature. Our analysis\nindicates that the broad features of the dynamical ejecta and disk properties\ncan be captured by fitting expressions that depend on mass ratio and reduced\ntidal parameter. The comparative analysis of literature data shows that\nmicrophysics and neutrino absorption have a significant impact on the dynamical\nejecta properties. Microphysical nuclear equations of state lead to average\nvelocities smaller than polytropic EOS, while including neutrino absorption\nresults in larger average ejecta masses and electron fractions. Hence,\nmicrophysics and neutrino transport are necessary to obtain quantitative models\nof the ejecta in terms of the binary parameters.'}]","**Abstract**:

This research introduces Adaptive Spectral-Learning Operators (S-LDOs), a novel technique for discovering higher-order differentiator operators from complex data-driven dynamical systems. Focusing on partial differential equations (PDEs), we model unknown operators to learn expressive dynamical systems that can generalize to new phenomena.

Our goal is to develop data-driven learning methods for discovering physical system dynamics described by second-order, nonlinear PDEs. We tackle this challenge by framing the operator learning as a regression problem that integrates a supervised Gaussian process for the dynamics with stability constraints.

Innovations include the incorporation of spectral norms and stability constraints during the learning process to ensure that the discovered operators are both expressive and stable in reproducing physical phenomena. We generalize stability analysis for differential operators used in finite difference methods, enabling the verification of learnable operators against different physical systems.

Our method, S-LDOs, demonstrates superior stability properties and accuracy in recovering both linear and higher-order dynamics. It outperforms Learning Differential Operators (LDOs) which lack stability constraints, resulting in less accurate predictions for nonlinear systems. 

Contributions include a scalable stability analysis framework for differential operators and multi-dimensional (2D) advection stability verification, highlighting the importance of stability constraints in operator learning. The implications for modeling complex dynamical systems in physics, engineering, and beyond have significant practical applications.

The development of S-LDOs completes the workflow of operator learning in discovering PDEs and simulating new phenomena, with the potential to yield novel insights and predictive models for various scientific domains."
"Reinforcement learning from human feedback (RLHF) has been an effective
technique for aligning AI systems with human values, with remarkable successes
in fine-tuning large-language models recently. Most existing RLHF paradigms
make the underlying assumption that human preferences are relatively
homogeneous, and can be encoded by a single reward model. In this paper, we
focus on addressing the issues due to the inherent heterogeneity in human
preferences, as well as their potential strategic behavior in providing
feedback. Specifically, we propose two frameworks to address heterogeneous
human feedback in principled ways: personalization-based one and
aggregation-based one. For the former, we propose two approaches based on
representation learning and clustering, respectively, for learning multiple
reward models that trades off the bias (due to preference heterogeneity) and
variance (due to the use of fewer data for learning each model by
personalization). We then establish sample complexity guarantees for both
approaches. For the latter, we aim to adhere to the single-model framework, as
already deployed in the current RLHF paradigm, by carefully aggregating diverse
and truthful preferences from humans. We propose two approaches based on reward
and preference aggregation, respectively: the former utilizes both
utilitarianism and Leximin approaches to aggregate individual reward models,
with sample complexity guarantees; the latter directly aggregates the human
feedback in the form of probabilistic opinions. Under the
probabilistic-opinion-feedback model, we also develop an approach to handle
strategic human labelers who may bias and manipulate the aggregated preferences
with untruthful feedback. Based on the ideas in mechanism design, our approach
ensures truthful preference reporting, with the induced aggregation rule
maximizing social welfare functions.","[{'convective heat transfer enhancement of non-condensing vertical upward\n  slug flow using capillary-assisted phase separation': 'This paper discusses the convective heat transfer enhancement of\nnon-condensing slug flows by using phase separation for local liquid\ncirculations through a porous-tube insert. Numerical simulations were carried\nout to study the air-water slug flow and heat transfer without phase change\n(boiling and condensation) in a vertical upward flow. The governing equations\nof two-phase flow and heat transfer were numerically solved using the Volume of\nFluid-Continuum Surface Force (VOF-CSF) method in a two-dimensional\naxisymmetric computational domain. The rear end (exit) of the porous tube was\nopen to allow discharge of fluid, but two different designs of the frontal end\nwere considered; a porous tube with a closed front that prevents the bubbles\nfrom entering the porous tube and keeps the bubbles in the annular gap; a\nporous tube with an open front allows the bubbles to flow through the porous\ntube. Simulation results show that a counter flow of the bulk liquid inside and\noutside the porous tube creates internal liquid circulations in both the axial\nand radial directions, resulting in an increased liquid velocity near the tube\nwall, thus an enhanced convective heat transfer. It is also found that there\nexists an optimum diameter of the porous-tube-insert (or annular gap dimension\nbetween the tube wall and porous tube) for convective heat transfer\nenhancement. The maximum Nusselt number of the porous-tube-insert cases used in\nthis study was about five times higher than that of the liquid-only flow in a\nbare tube.'}, {'Optimal First-Order Algorithms as a Function of Inequalities': 'In this work, we present a novel algorithm design methodology that finds the\noptimal algorithm as a function of inequalities. Specifically, we restrict\nconvergence analyses of algorithms to use a prespecified subset of\ninequalities, rather than utilizing all true inequalities, and find the optimal\nalgorithm subject to this restriction. This methodology allows us to design\nalgorithms with certain desired characteristics. As concrete demonstrations of\nthis methodology, we find new state-of-the-art accelerated first-order gradient\nmethods using randomized coordinate updates and backtracking line searches.'}, {'Factor-$\\sqrt{2}$ Acceleration of Accelerated Gradient Methods': ""The optimized gradient method (OGM) provides a factor-$\\sqrt{2}$ speedup upon\nNesterov's celebrated accelerated gradient method in the convex (but\nnon-strongly convex) setup. However, this improved acceleration mechanism has\nnot been well understood; prior analyses of OGM relied on a computer-assisted\nproof methodology, so the proofs were opaque for humans despite being\nverifiable and correct. In this work, we present a new analysis of OGM based on\na Lyapunov function and linear coupling. These analyses are developed and\npresented without the assistance of computers and are understandable by humans.\nFurthermore, we generalize OGM's acceleration mechanism and obtain a\nfactor-$\\sqrt{2}$ speedup in other setups: acceleration with a simpler rational\nstepsize, the strongly convex setup, and the mirror descent setup.""}, {'A Geometric Structure of Acceleration and Its Role in Making Gradients\n  Small Fast': ""Since Nesterov's seminal 1983 work, many accelerated first-order optimization\nmethods have been proposed, but their analyses lacks a common unifying\nstructure. In this work, we identify a geometric structure satisfied by a wide\nrange of first-order accelerated methods. Using this geometric insight, we\npresent several novel generalizations of accelerated methods. Most interesting\namong them is a method that reduces the squared gradient norm with\n$\\mathcal{O}(1/K^4)$ rate in the prox-grad setup, faster than the\n$\\mathcal{O}(1/K^3)$ rates of Nesterov's FGM or Kim and Fessler's FPGM-m.""}, {'Multi-Player Zero-Sum Markov Games with Networked Separable Interactions': ""We study a new class of Markov games, \\emph(multi-player) zero-sum Markov\nGames} with \\emph{Networked separable interactions} (zero-sum NMGs), to model\nthe local interaction structure in non-cooperative multi-agent sequential\ndecision-making. We define a zero-sum NMG as a model where {the payoffs of the\nauxiliary games associated with each state are zero-sum and} have some\nseparable (i.e., polymatrix) structure across the neighbors over some\ninteraction network. We first identify the necessary and sufficient conditions\nunder which an MG can be presented as a zero-sum NMG, and show that the set of\nMarkov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash\nequilibrium (NE) in these games, in that the product of per-state\nmarginalization of the former for all players yields the latter. Furthermore,\nwe show that finding approximate Markov \\emph{stationary} CCE in\ninfinite-horizon discounted zero-sum NMGs is \\texttt{PPAD}-hard, unless the\nunderlying network has a ``star topology''. Then, we propose\nfictitious-play-type dynamics, the classical learning dynamics in normal-form\ngames, for zero-sum NMGs, and establish convergence guarantees to Markov\nstationary NE under a star-shaped network structure. Finally, in light of the\nhardness result, we focus on computing a Markov \\emph{non-stationary} NE and\nprovide finite-iteration guarantees for a series of value-iteration-based\nalgorithms. We also provide numerical experiments to corroborate our\ntheoretical results.""}, {'Do LLM Agents Have Regret? A Case Study in Online Learning and Games': ""Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.""}, {'A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function\n  Perspective': 'We propose the first unified theoretical analysis of mixed sample data\naugmentation (MSDA), such as Mixup and CutMix. Our theoretical results show\nthat regardless of the choice of the mixing strategy, MSDA behaves as a\npixel-level regularization of the underlying training loss and a regularization\nof the first layer parameters. Similarly, our theoretical results support that\nthe MSDA training strategy can improve adversarial robustness and\ngeneralization compared to the vanilla training strategy. Using the theoretical\nresults, we provide a high-level understanding of how different design choices\nof MSDA work differently. For example, we show that the most popular MSDA\nmethods, Mixup and CutMix, behave differently, e.g., CutMix regularizes the\ninput gradients by pixel distances, while Mixup regularizes the input gradients\nregardless of pixel distances. Our theoretical results also show that the\noptimal MSDA strategy depends on tasks, datasets, or model parameters. From\nthese observations, we propose generalized MSDAs, a Hybrid version of Mixup and\nCutMix (HMix) and Gaussian Mixup (GMix), simple extensions of Mixup and CutMix.\nOur implementation can leverage the advantages of Mixup and CutMix, while our\nimplementation is very efficient, and the computation cost is almost\nneglectable as Mixup and CutMix. Our empirical study shows that our HMix and\nGMix outperform the previous state-of-the-art MSDA methods in CIFAR-100 and\nImageNet classification tasks. Source code is available at\nhttps://github.com/naver-ai/hmix-gmix'}, {'Time-Reversed Dissipation Induces Duality Between Minimizing Gradient\n  Norm and Function Value': ""In convex optimization, first-order optimization methods efficiently\nminimizing function values have been a central subject study since Nesterov's\nseminal work of 1983. Recently, however, Kim and Fessler's OGM-G and Lee et\nal.'s FISTA-G have been presented as alternatives that efficiently minimize the\ngradient magnitude instead. In this paper, we present H-duality, which\nrepresents a surprising one-to-one correspondence between methods efficiently\nminimizing function values and methods efficiently minimizing gradient\nmagnitude. In continuous-time formulations, H-duality corresponds to reversing\nthe time dependence of the dissipation/friction term. To the best of our\nknowledge, H-duality is different from Lagrange/Fenchel duality and is distinct\nfrom any previously known duality or symmetry relations. Using H-duality, we\nobtain a clearer understanding of the symmetry between Nesterov's method and\nOGM-G, derive a new class of methods efficiently reducing gradient magnitudes\nof smooth convex functions, and find a new composite minimization method that\nis simpler and faster than FISTA-G.""}, {'Sampling-based Bayesian Inference with gradient uncertainty': 'Deep neural networks(NNs) have achieved impressive performance, often exceed\nhuman performance on many computer vision tasks. However, one of the most\nchallenging issues that still remains is that NNs are overconfident in their\npredictions, which can be very harmful when this arises in safety critical\napplications. In this paper, we show that predictive uncertainty can be\nefficiently estimated when we incorporate the concept of gradients uncertainty\ninto posterior sampling. The proposed method is tested on two different\ndatasets, MNIST for in-distribution confusing examples and notMNIST for\nout-of-distribution data. We show that our method is able to efficiently\nrepresent predictive uncertainty on both datasets.'}, {'Mirror Duality in Convex Optimization': 'While first-order optimization methods are usually designed to efficiently\nreduce the function value $f(x)$, there has been recent interest in methods\nefficiently reducing the magnitude of $\\nabla f(x)$, and the findings show that\nthe two types of methods exhibit a certain symmetry. In this work, we present\nmirror duality, a one-to-one correspondence between mirror-descent-type methods\nreducing function value and reducing gradient magnitude. Using mirror duality,\nwe obtain the dual accelerated mirror descent (dual-AMD) method that\nefficiently reduces $\\psi^*(\\nabla f(x))$, where $\\psi$ is a\ndistance-generating function and $\\psi^*$ quantifies the magnitude of $\\nabla\nf(x)$. We then apply dual-AMD to efficiently reduce $\\|\\nabla f(\\cdot) \\|_q$\nfor $q\\in [2,\\infty)$ and to efficiently compute $\\varepsilon$-approximate\nsolutions of the optimal transport problem.'}]","Title: Personalization and Fairness in Reinforcement Learning from Human Feedback with Heterogeneous Preferences

Abstract

As Large Language Models (LLMs) become more sophisticated and influential, they demand careful alignment with nuanced human values to ensure ethical and beneficial interactions. RLHF, a theoretically grounded technique, finds utility functions crucial for this alignment. Here, utility implicitly refers to human preferences, shaping models to be more human-centric. However, learning from a diverse set of human preferences often faces a tension between fairness and diversity representation. A key challenge is how to reconcile the yet-underserved issue of heterogeneity of human preferences when using RLHF to align LLMs with their users.

We address the challenge of personalized reward models in RLHF, proposing methods to improve model training for scenarios where human preferences are varied and numerous. Our research introduces a theoretical framework grounded in representation learning, enabling LLMS to capture the common structure behind human rewards. We develop novel adaptations of the BTL model and integrate them alongside distributed policy methods. Furthermore, we construct mechanism-design-inspired algorithms that incentivize truthful preference reporting, addressing the strategic nature of human labelers.

Specifically, our main contributions are:
1) **Theory and Methodology**: We analyze the performance guarantees of personalized reward learning in RLHF, notably reducing sample complexity and providing theoretical bounds on suboptimality gaps.
2) **Personalization Approach**: Employing representation learning enriches the model and significantly enhances efficiency over naive, unstructured personalization methods by leveraging a common structural essence in human preferences.
3) **Fairness and Truthfulness**: We tackle the double dilemma of learning personalized models while ensuring the feedback collected is truthful, thus maintaining fairness and diversity.

The research is applicable to scenarios requiring personalized interactions with LLMs, offering a systematic approach to fine-tuning the models towards the varied preferences of users. It has direct implications for enhancing the utility and sustainability of AI models in diverse, real-world settings."
"In the evolving landscape of computer vision, foundation models have emerged
as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks.
Among these, the Segment Anything Model (SAM) by Meta AI has distinguished
itself in image segmentation. However, SAM, like its counterparts, encounters
limitations in specific niche applications, prompting a quest for enhancement
strategies that do not compromise its inherent capabilities. This paper
introduces ASAM, a novel methodology that amplifies SAM's performance through
adversarial tuning. We harness the potential of natural adversarial examples,
inspired by their successful implementation in natural language processing. By
utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B
dataset, generating adversarial instances that are more representative of
natural variations rather than conventional imperceptible perturbations. Our
approach maintains the photorealism of adversarial examples and ensures
alignment with original mask annotations, thereby preserving the integrity of
the segmentation task. The fine-tuned ASAM demonstrates significant
improvements across a diverse range of segmentation tasks without necessitating
additional data or architectural modifications. The results of our extensive
evaluations confirm that ASAM establishes new benchmarks in segmentation tasks,
thereby contributing to the advancement of foundational models in computer
vision. Our project page is in https://asam2024.github.io/.","[{'Parameterized Littlewood-Paley operators with variable kernels on Hardy\n  spaces and weak Hardy spaces': 'In this paper, by using the atomic decomposition theory of Hardy space and\nweak Hardy space, we discuss the boundedness of parameterized Littlewood-Paley\noperator with variable kernel on these spaces.'}, {'Invertible Convolution with Symmetric Paddings': 'We show that symmetrically padded convolution can be analytically inverted\nvia DFT. We comprehensively analyze several different symmetric and\nanti-symmetric padding modes and show that multiple cases exist where the\ninversion can be achieved. The implementation is available at\n\\url{https://github.com/prclibo/iconv_dft}.'}, {'Boundedness of parameterized Marcinkiewicz integrals with variable\n  kernels on Hardy spaces and weak Hardy spaces': 'In this paper, by using the atomic decomposition theory of Hardy space and\nweak Hardy space, the author establishes the boundedness of parameterized\nMarcinkiewicz integral with variable kernel on these spaces, under the Dini\ncondition or H\\""{o}rmander condition imposed on kernel.'}, {'Weighted Norm Inequalities for Parametric Littlewood-Paley Operators': 'In this paper, the author establishes the boundedness of parametric\nLittlewood-Paley operators from Musielak-Orlicz Hardy space to Musielak-Orlicz\nspace, or to weak Musielak-Orlicz space at the critical index. Part of these\nresults are new even for classical Hardy space of Fefferman and Stein.'}, {'A Spline-based Volumetric Data Modeling Framework and Its Applications': 'In this dissertation, we concentrate on the challenging research issue of\ndeveloping a spline-based modeling framework, which converts the conventional\ndata (e.g., surface meshes) to tensor-product trivariate splines. This\nmethodology can represent both boundary/volumetric geometry and real volumetric\nphysical attributes in a compact and continuous fashion. The regular\ntensor-product structure enables our new developed methods to be embedded into\nthe industry standard seamlessly. These properties make our techniques highly\npreferable in many physically-based applications including mechanical analysis,\nshape deformation and editing, virtual surgery training, etc.'}, {'3D Fully Convolutional Network for Vehicle Detection in Point Cloud': '2D fully convolutional network has been recently successfully applied to\nobject detection from images. In this paper, we extend the fully convolutional\nnetwork based detection techniques to 3D and apply it to point cloud data. The\nproposed approach is verified on the task of vehicle detection from lidar point\ncloud for autonomous driving. Experiments on the KITTI dataset shows a\nsignificant performance improvement over the previous point cloud based\ndetection approaches.'}, {'A Survey of Spline-based Volumetric Data Modeling Framework and Its\n  Applications': 'The rapid advances in 3D scanning and acquisition techniques have given rise\nto the explosive increase of volumetric digital models in recent years. This\ndissertation systematically trailblazes a novel volumetric modeling framework\nto represent 3D solids. The need to explore more efficient and robust 3D\nmodeling framework has gained the prominence. Although the traditional surface\nrepresentation (e.g., triangle mesh) has many attractive properties, it is\nincapable of expressing the interior space and materials. Such a serious\ndrawback overshadows many potential modeling and analysis applications.\nConsequently volumetric modeling techniques become the well-known solution to\nthis problem. Nevertheless, many unsolved research issues remain when\ndeveloping an efficient modeling paradigm for existing 3D models: complex\ngeometry (fine details and extreme concaveness), arbitrary topology,\nheterogenous materials, large-scale data storage and processing, etc.'}, {'On Enhancing Ground Surface Detection from Sparse Lidar Point Cloud': 'Ground surface detection in point cloud is widely used as a key module in\nautonomous driving systems. Different from previous approaches which are mostly\ndeveloped for lidars with high beam resolution, e.g. Velodyne HDL-64, this\npaper proposes ground detection techniques applicable to much sparser point\ncloud captured by lidars with low beam resolution, e.g. Velodyne VLP-16. The\napproach is based on the RANSAC scheme of plane fitting. Inlier verification\nfor plane hypotheses is enhanced by exploiting the point-wise tangent, which is\na local feature available to compute regardless of the density of lidar beams.\nGround surface which is not perfectly planar is fitted by multiple\n(specifically 4 in our implementation) disjoint plane regions. By assuming\nthese plane regions to be rectanglar and exploiting the integral image\ntechnique, our approach approximately finds the optimal region partition and\nplane hypotheses under the RANSAC scheme with real-time computational\ncomplexity.'}, {'An explanation for the distribution characteristics of stock returns': ""Observations indicate that the distributions of stock returns in financial\nmarkets usually do not conform to normal distributions, but rather exhibit\ncharacteristics of high peaks, fat tails and biases. In this work, we assume\nthat the effects of events or information on prices obey normal distribution,\nwhile financial markets often overreact or underreact to events or information,\nresulting in non normal distributions of stock returns. Based on the above\nassumptions, we propose a reaction function for a financial market reacting to\nevents or information, and a model based on it to describe the distribution of\nreal stock returns. Our analysis of the returns of China Securities Index 300\n(CSI 300), the Standard & Poor's 500 Index (SPX or S&P 500) and the Nikkei 225\nIndex (N225) at different time scales shows that financial markets often\nunderreact to events or information with minor impacts, overreact to events or\ninformation with relatively significant impacts, and react slightly stronger to\npositive events or information than to negative ones. In addition, differences\nin financial markets and time scales of returns can also affect the shapes of\nthe reaction functions.""}, {'Testing Business Cycle Theories: Evidence from the Great Recession': 'Empirical business cycle studies using cross-country data usually cannot\nachieve causal relationships while within-country studies mostly focus on the\nbust period. We provide the first causal investigation into the boom period of\nthe 1999-2010 U.S. cross-metropolitan business cycle. Using a novel research\ndesign, we show that credit expansion in private-label mortgages causes a\ndifferentially stronger boom (2000-2006) and bust (2007-2010) cycle in the\nhouse-related industries in the high net-export-growth areas. Most importantly,\nour unique research design enables us to perform the most comprehensive tests\non theories (hypotheses) regarding the business cycle. We show that the\nfollowing theories (hypotheses) cannot explain the cause of the 1999-2010 U.S.\nbusiness cycle: the speculative euphoria hypothesis, the real business cycle\ntheory, the collateral-driven credit cycle theory, the business uncertainty\ntheory, and the extrapolative expectation theory.'}]","Title: Enhancing Sam's Generalization with Realistic Adversarial Examples for Foundation Models

**Background:**
In the era of deep learning, foundation models have become instrumental in various computer vision tasks. Sam (Segment Anything Model), a groundbreaking segmentation framework, demonstrates impressive capabilities but often lacks robustness against adversarial attacks and real-world variations. This necessitates the development of novel techniques to enhance its generalization and performance across diverse scenarios.

**Objective:**
The main goal of this research is to develop and evaluate an innovative framework for fine-tuning Sam, enabling it to better handle unexpected data variations and adversarial examples. The aim is to improve generalization performance while preserving Sam’s fundamental properties.

**Innovations:**
The paper introduces Adaptive Stabilized Diffusion Asymmetric Method (ASAM), a fine-tuning approach that leverages realistic adversarial examples generated from Sam itself in a diffusion process. This method enhances Sam’s ability to recognize and segment objects effectively under varying data conditions.

**Methods:**
ASAM combines the Stabilized diffusion approach with asymmetric and adaptive refinement of latent representations and prompt synthesis through Bilinear Vision Transformer 2 (BLIPv2). It utilizes the DatasetDM method for realistic perturbation generation and ControlNet for integrating these new representations back into Sam with appropriate attention.

**Results:**
Quantitative evaluations on diverse datasets demonstrate that ASAM significantly outperforms baselines in terms of mean Intersection-over-Union (mIoU) metrics, particularly on unseen datasets and in adversarial settings. Visual evaluations also show that ASAM produces more natural and realistic adversarial examples, maintaining the model's segmentation accuracy.

**Contributions:**
The paper contributes a robust framework for fine-tuning Sam that enhances its generalization capabilities without augmenting its complexity. The methodology is designed to be scalable, improving Sam's performance in segmentation tasks across a wide range of objects and environments.

**Applications:**
ASAM's advancements in fine-tuning foundation models like Sam can lead to enhanced algorithms in computer vision, image recognition, autonomous systems, and robotics, improving their adaptability and reliability in complex real-world scenarios.

This research provides a significant step forward in the field of deep learning, addressing the critical need for more robust and adaptable computer vision models, thereby expanding the horizons of AI applications."
"Large language models (LLMs) suffer from low efficiency as the mismatch
between the requirement of auto-regressive decoding and the design of most
contemporary GPUs. Specifically, billions to trillions of parameters must be
loaded to the GPU cache through its limited memory bandwidth for computation,
but only a small batch of tokens is actually computed. Consequently, the GPU
spends most of its time on memory transfer instead of computation. Recently,
parallel decoding, a type of speculative decoding algorithms, is becoming more
popular and has demonstrated impressive efficiency improvement in generation.
It introduces extra decoding heads to large models, enabling them to predict
multiple subsequent tokens simultaneously and verify these candidate
continuations in a single decoding step. However, this approach deviates from
the training objective of next token prediction used during pre-training,
resulting in a low hit rate for candidate tokens. In this paper, we propose a
new speculative decoding algorithm, Clover, which integrates sequential
knowledge into the parallel decoding process. This enhancement improves the hit
rate of speculators and thus boosts the overall efficiency. Clover transmits
the sequential knowledge from pre-speculated tokens via the Regressive
Connection, then employs an Attention Decoder to integrate these speculated
tokens. Additionally, Clover incorporates an Augmenting Block that modifies the
hidden states to better align with the purpose of speculative generation rather
than next token prediction. The experiment results demonstrate that Clover
outperforms the baseline by up to 91% on Baichuan-Small and 146% on
Baichuan-Large, respectively, and exceeds the performance of the previously
top-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on
Baichuan-Large, respectively.","[{'Advances in Thermal Modeling of Selective Laser Sintering of Metal\n  Powders': 'Selective laser sintering (SLS) of single component metal powders is a rapid\nprototyping technology in which a high-energy laser beam scans, melts, shrinks\nand consolidates metal powders with single component. For better understanding\nphysical mechanisms during laser sintering of single-component metal particles,\na temperature transforming model with the consideration of shrinkage and\nconvective flows is introduced to analyze the thermal/fluid behaviors in\nselective laser sintering of single powder layer. The model is also applied to\ninvestigate the sintering of powders on top of existing sintered layers under\nsingle- multiple-line scanning manners according to the practical manufacturing\nprocesses.'}, {'StyLess: Boosting the Transferability of Adversarial Examples': 'Adversarial attacks can mislead deep neural networks (DNNs) by adding\nimperceptible perturbations to benign examples. The attack transferability\nenables adversarial examples to attack black-box DNNs with unknown\narchitectures or parameters, which poses threats to many real-world\napplications. We find that existing transferable attacks do not distinguish\nbetween style and content features during optimization, limiting their attack\ntransferability. To improve attack transferability, we propose a novel attack\nmethod called style-less perturbation (StyLess). Specifically, instead of using\na vanilla network as the surrogate model, we advocate using stylized networks,\nwhich encode different style features by perturbing an adaptive instance\nnormalization. Our method can prevent adversarial examples from using\nnon-robust style features and help generate transferable perturbations.\nComprehensive experiments show that our method can significantly improve the\ntransferability of adversarial examples. Furthermore, our approach is generic\nand can outperform state-of-the-art transferable attacks when combined with\nother attack techniques.'}, {'A Survey of Robustness and Safety of 2D and 3D Deep Learning Models\n  Against Adversarial Attacks': 'Benefiting from the rapid development of deep learning, 2D and 3D computer\nvision applications are deployed in many safe-critical systems, such as\nautopilot and identity authentication. However, deep learning models are not\ntrustworthy enough because of their limited robustness against adversarial\nattacks. The physically realizable adversarial attacks further pose fatal\nthreats to the application and human safety. Lots of papers have emerged to\ninvestigate the robustness and safety of deep learning models against\nadversarial attacks. To lead to trustworthy AI, we first construct a general\nthreat model from different perspectives and then comprehensively review the\nlatest progress of both 2D and 3D adversarial attacks. We extend the concept of\nadversarial examples beyond imperceptive perturbations and collate over 170\npapers to give an overview of deep learning model robustness against various\nadversarial attacks. To the best of our knowledge, we are the first to\nsystematically investigate adversarial attacks for 3D models, a flourishing\nfield applied to many real-world applications. In addition, we examine physical\nadversarial attacks that lead to safety violations. Last but not least, we\nsummarize present popular topics, give insights on challenges, and shed light\non future research on trustworthy AI.'}, {'Simple Baselines for Human Pose Estimation and Tracking': 'There has been significant progress on pose estimation and increasing\ninterests on pose tracking in recent years. At the same time, the overall\nalgorithm and system complexity increases as well, making the algorithm\nanalysis and comparison more difficult. This work provides simple and effective\nbaseline methods. They are helpful for inspiring and evaluating new ideas for\nthe field. State-of-the-art results are achieved on challenging benchmarks. The\ncode will be available at https://github.com/leoxiaobin/pose.pytorch.'}, {'Integral Human Pose Regression': 'State-of-the-art human pose estimation methods are based on heat map\nrepresentation. In spite of the good performance, the representation has a few\nissues in nature, such as not differentiable and quantization error. This work\nshows that a simple integral operation relates and unifies the heat map\nrepresentation and joint regression, thus avoiding the above issues. It is\ndifferentiable, efficient, and compatible with any heat map based methods. Its\neffectiveness is convincingly validated via comprehensive ablation experiments\nunder various settings, specifically on 3D pose estimation, for the first time.'}, {'A tensor optimization algorithm for computing Lagrangians of hypergraphs': 'The Lagrangian of a hypergraph is a crucial tool for studying hypergraph\nextremal problems. Though Lagrangians of some special structure hypergraphs\nhave closed-form solutions, it is a challenging problem to compute the\nLagrangian of a general large scale hypergraph. In this paper, we exploit a\nfast computational scheme involving the adjacency tensor of a hypergraph.\nFurthermore, we propose to utilize the gradient projection method on a simplex\nfrom nonlinear optimization for solving the Lagrangian of a large scale\nhypergraph iteratively. Using the Lojasiewicz gradient inequality, we analyze\nthe global and local convergence of the gradient projection method. Numerical\nexperiments illustrate that the proposed numerical method could compute\nLagrangians of large scale hypergraphs efficiently.'}, {'Color-related Local Binary Pattern: A Learned Local Descriptor for Color\n  Image Recognition': 'Local binary pattern (LBP) as a kind of local feature has shown its\nsimplicity, easy implementation and strong discriminating power in image\nrecognition. Although some LBP variants are specifically investigated for color\nimage recognition, the color information of images is not adequately considered\nand the curse of dimensionality in classification is easily caused in these\nmethods. In this paper, a color-related local binary pattern (cLBP) which\nlearns the dominant patterns from the decoded LBP is proposed for color images\nrecognition. This paper first proposes a relative similarity space (RSS) that\nrepresents the color similarity between image channels for describing a color\nimage. Then, the decoded LBP which can mine the correlation information between\nthe LBP feature maps correspond to each color channel of RSS traditional RGB\nspaces, is employed for feature extraction. Finally, a feature learning\nstrategy is employed to learn the dominant color-related patterns for reducing\nthe dimension of feature vector and further improving the discriminatively of\nfeatures. The theoretic analysis show that the proposed RSS can provide more\ndiscriminative information, and has higher noise robustness as well as higher\nillumination variation robustness than traditional RGB space. Experimental\nresults on four groups, totally twelve public color image datasets show that\nthe proposed method outperforms most of the LBP variants for color image\nrecognition in terms of dimension of features, recognition accuracy under\nnoise-free, noisy and illumination variation conditions.'}, {'Human-like Driving Decision at Unsignalized Intersections Based on Game\n  Theory': 'Unsignalized intersection driving is challenging for automated vehicles. For\nsafe and efficient performances, the diverse and dynamic behaviors of\ninteracting vehicles should be considered. Based on a game-theoretic framework,\na human-like payoff design methodology is proposed for the automated decision\nat unsignalized intersections. Prospect Theory is introduced to map the\nobjective collision risk to the subjective driver payoffs, and the driving\nstyle can be quantified as a tradeoff between safety and speed. To account for\nthe dynamics of interaction, a probabilistic model is further introduced to\ndescribe the acceleration tendency of drivers. Simulation results show that the\nproposed decision algorithm can describe the dynamic process of two-vehicle\ninteraction in limit cases. Statistics of uniformly-sampled cases simulation\nindicate that the success rate of safe interaction reaches 98%, while the speed\nefficiency can also be guaranteed. The proposed approach is further applied and\nvalidated in four-vehicle interaction scenarios at a four-arm intersection.'}, {'AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion\n  Models': 'Unrestricted adversarial attacks present a serious threat to deep learning\nmodels and adversarial defense techniques. They pose severe security problems\nfor deep learning applications because they can effectively bypass defense\nmechanisms. However, previous attack methods often directly inject Projected\nGradient Descent (PGD) gradients into the sampling of generative models, which\nare not theoretically provable and thus generate unrealistic examples by\nincorporating adversarial objectives, especially for GAN-based methods on\nlarge-scale datasets like ImageNet. In this paper, we propose a new method,\ncalled AdvDiff, to generate unrestricted adversarial examples with diffusion\nmodels. We design two novel adversarial guidance techniques to conduct\nadversarial sampling in the reverse generation process of diffusion models.\nThese two techniques are effective and stable in generating high-quality,\nrealistic adversarial examples by integrating gradients of the target\nclassifier interpretably. Experimental results on MNIST and ImageNet datasets\ndemonstrate that AdvDiff is effective in generating unrestricted adversarial\nexamples, which outperforms state-of-the-art unrestricted adversarial attack\nmethods in terms of attack performance and generation quality.'}, {'Long-term Cross Adversarial Training: A Robust Meta-learning Method for\n  Few-shot Classification Tasks': 'Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.'}]","Abstract

The advent of Generative Pre-trained Transformers (GPTs) heralded a new era in AI, but their inherent latency challenges limit their scalability. The research cited addresses this bottleneck through the introduction of a context-aware speculator, Clover, for efficient real-time language models deployment. Focused on leveraging pre-specified tokens' sequential knowledge, Clover proposes three innovations: regressive connection, attention decoder, and augmenting block. This abstract summarizes the motivation, method, results, and implications of our approach.

Encouraged by concurrent speculative inference's growing importance, Clover segments the speculative and actual model paths, predicting future tokens while ensuring the primary completion path remains parallelizable and independent. Clover's regressive connections, based on cross-attention, merge information across speculation steps, enhancing prediction accuracy. Systematic experiments on 10 diverse tasks demonstrated that Clover increased tokens generated per step by 76%, surpassing a comparable model by 50%. This efficiency gain was validated on both a compact (<300M parameter) Baichuan Small and a large-scale overall (∼2B) Baichuan Large, showcasing scalability.

 Clover's contributions push the boundaries of speculative inference by introducing a context-aware speculator, significantly reducing inference latency through parallel speculative token generation. This technology enables more efficient real-time deployment of larger GPTs, benefiting areas like customer service, technical support, and interactive content creation.

The potential applications of Clover include optimizing large-scale language model applications for scenarios requiring extensive, immediate responses, such as real-time customer care and interactive content generation platforms. This research paves the way for broader adoption of AI models in time-critical environments, fostering advancements in computational efficiency and user interaction."
"Recent years have witnessed the prosperity of reference-based image
super-resolution (Ref-SR). By importing the high-resolution (HR) reference
images into the single image super-resolution (SISR) approach, the ill-posed
nature of this long-standing field has been alleviated with the assistance of
texture transferred from reference images. Although the significant improvement
in quantitative and qualitative results has verified the superiority of Ref-SR
methods, the presence of misalignment before texture transfer indicates room
for further performance improvement. Existing methods tend to neglect the
significance of details in the context of comparison, therefore not fully
leveraging the information contained within low-resolution (LR) images. In this
paper, we propose a Detail-Enhancing Framework (DEF) for reference-based
super-resolution, which introduces the diffusion model to generate and enhance
the underlying detail in LR images. If corresponding parts are present in the
reference image, our method can facilitate rigorous alignment. In cases where
the reference image lacks corresponding parts, it ensures a fundamental
improvement while avoiding the influence of the reference image. Extensive
experiments demonstrate that our proposed method achieves superior visual
results while maintaining comparable numerical outcomes.","[{'Commentary Generation from Data Records of Multiplayer Strategy Esports\n  Game': ""Esports, a sports competition on video games, has become one of the most\nimportant sporting events. Although esports play logs have been accumulated,\nonly a small portion of them accompany text commentaries for the audience to\nretrieve and understand the plays. In this study, we therefore introduce the\ntask of generating game commentaries from esports' data records. We first build\nlarge-scale esports data-to-text datasets that pair structured data and\ncommentaries from a popular esports game, League of Legends. We then evaluate\nTransformer-based models to generate game commentaries from structured data\nrecords, while examining the impact of the pre-trained language models.\nEvaluation results on our dataset revealed the challenges of this novel task.\nWe will release our dataset to boost potential research in the data-to-text\ngeneration community.""}, {'CV 3315 Is All You Need : Semantic Segmentation Competition': 'This competition focus on Urban-Sense Segmentation based on the vehicle\ncamera view. Class highly unbalanced Urban-Sense images dataset challenge the\nexisting solutions and further studies. Deep Conventional neural network-based\nsemantic segmentation methods such as encoder-decoder architecture and\nmulti-scale and pyramid-based approaches become flexible solutions applicable\nto real-world applications. In this competition, we mainly review the\nliterature and conduct experiments on transformer-driven methods especially\nSegFormer, to achieve an optimal trade-off between performance and efficiency.\nFor example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,\nand the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple\nfactors, including individual case failure analysis, individual class\nperformance, training pressure and efficiency estimation, the final candidate\nmodel for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU\nevaluated on the testing set. Checkout our code implementation at\nhttps://vmv.re/cv3315.'}, {'Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps\n  from high to low rank': 'The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than\none hidden layers has multiple local minima, corresponding to matrices with\ndifferent ranks. In tasks such as matrix completion, the goal is to converge to\nthe local minimum with the smallest rank that still fits the training data.\nWhile rank-underestimating minima can be avoided since they do not fit the\ndata, GD might get stuck at rank-overestimating minima. We show that with SGD,\nthere is always a probability to jump from a higher rank minimum to a lower\nrank one, but the probability of jumping back is zero. More precisely, we\ndefine a sequence of sets $B_{1}\\subset B_{2}\\subset\\cdots\\subset B_{R}$ so\nthat $B_{r}$ contains all minima of rank $r$ or less (and not more) that are\nabsorbing for small enough ridge parameters $\\lambda$ and learning rates\n$\\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there\nis a non-zero prob. for SGD to go in $B_{r}$.'}, {'Exploring Spatial Patterns of Interurban Passenger Flows Using Dual\n  Gravity Models': 'Passenger flows in a traffic network reflect spatial interaction patterns in\nan urban systems. Gravity models can be employed to quantitatively describe and\npredict spatial flows. However, how to model passenger flows and reveal the\ndeep structure of urban and traffic networks in the case of missing partial\ndata is still a problem to be solved. This paper is devoted to characterizing\nthe interurban passenger flows in the Beijing-Tianjin-Hebei region of China by\nmeans of dual gravity models and Tencent location big data. The method of\nparameter estimation is the least squares regression. The main results are as\nfollows. First, both railway and highway passenger flows can be effectively\ndescribed by the dual gravity model. A small part of missing spatial data can\nbe made up by the predicted values. Second, the fractal properties of traffic\nflows can be revealed. The railway passenger flows more follow gravity scaling\nlaw than the highway passenger flows. Third, the prediction residuals indicate\nthe changing trend of interurban connections in the study area in recent years.\nThe center of gravity of spatial dynamics seems to shift from the\nBeijing-Tianjin-Tangshan triangle to the Beijing-Baoding-Shijiazhuang axis. A\nconclusion can be reached that the dual gravity models is an effective tools of\nfor analyzing spatial structures and dynamics of traffic networks and flows.\nMoreover, the models provide a new approach to estimate fractal dimension of\ntraffic network and spatial flow patterns.'}, {'Utilizing Local Hierarchy with Adversarial Training for Hierarchical\n  Text Classification': 'Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification due to its complex taxonomic structure. Nearly all\nrecent HTC works focus on how the labels are structured but ignore the\nsub-structure of ground-truth labels according to each input text which\ncontains fruitful label co-occurrence information. In this work, we introduce\nthis local hierarchy with an adversarial framework. We propose a HiAdv\nframework that can fit in nearly all HTC models and optimize them with the\nlocal hierarchy as auxiliary information. We test on two typical HTC models and\nfind that HiAdv is effective in all scenarios and is adept at dealing with\ncomplex taxonomic hierarchies. Further experiments demonstrate that the\npromotion of our framework indeed comes from the local hierarchy and the local\nhierarchy is beneficial for rare classes which have insufficient training data.'}, {'Extending Multilingual BERT to Low-Resource Languages': 'Multilingual BERT (M-BERT) has been a huge success in both supervised and\nzero-shot cross-lingual transfer learning. However, this success has focused\nonly on the top 104 languages in Wikipedia that it was trained on. In this\npaper, we propose a simple but effective approach to extend M-BERT (E-BERT) so\nthat it can benefit any new language, and show that our approach benefits\nlanguages that are already in M-BERT as well. We perform an extensive set of\nexperiments with Named Entity Recognition (NER) on 27 languages, only 16 of\nwhich are in M-BERT, and show an average increase of about 6% F1 on languages\nthat are already in M-BERT and 23% F1 increase on new languages.'}, {'Discreetly Exploiting Inter-session Information for Session-based\n  Recommendation': 'Limited intra-session information is the performance bottleneck of the early\nGNN based SBR models. Therefore, some GNN based SBR models have evolved to\nintroduce additional inter-session information to facilitate the next-item\nprediction. However, we found that the introduction of inter-session\ninformation may bring interference to these models. The possible reasons are\ntwofold. First, inter-session dependencies are not differentiated at the\nfactor-level. Second, measuring inter-session weight by similarity is not\nenough. In this paper, we propose DEISI to solve the problems. For the first\nproblem, DEISI differentiates the types of inter-session dependencies at the\nfactor-level with the help of DRL technology. For the second problem, DEISI\nintroduces stability as a new metric for weighting inter-session dependencies\ntogether with the similarity. Moreover, CL is used to improve the robustness of\nthe model. Extensive experiments on three datasets show the superior\nperformance of the DEISI model compared with the state-of-the-art models.'}, {'Effectively Using Long and Short Sessions for Multi-Session-based\n  Recommendations': ""It is not accurate to make recommendations only based one single current\nsession. Therefore, multi-session-based recommendation(MSBR) is a solution for\nthe problem. Compared with the previous MSBR models, we have made three\nimprovements in this paper. First, the previous work choose to use all the\nhistory sessions of the user and/or of his similar users. When the user's\ncurrent interest changes greatly from the past, most of these sessions can only\nhave negative impacts. Therefore, we select a large number of randomly chosen\nsessions from the dataset as candidate sessions to avoid over depending on\nhistory data. Then we only choose to use the most similar sessions to get the\nmost useful information while reduce the noise caused by dissimilar sessions.\nSecond, in real-world datasets, short sessions account for a large proportion.\nThe RNN often used in previous work is not suitable to process short sessions,\nbecause RNN only focuses on the sequential relationship, which we find is not\nthe only relationship between items in short sessions. So, we designed a more\nsuitable method named GAFE based on attention to process short sessions. Third,\nAlthough there are few long sessions, they can not be ignored. Not like\nprevious models, which simply process long sessions in the same way as short\nsessions, we propose LSIS, which can split the interest of long sessions, to\nmake better use of long sessions. Finally, to help recommendations, we also\nhave considered users' long-term interests captured by a multi-layer GRU.\nConsidering the four points above, we built the model ENIREC. Experiments on\ntwo real-world datasets show that the comprehensive performance of ENIREC is\nbetter than other existing models.""}, {'Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A\n  Pilot Study on Named Entity Recognition': 'Fine-tuning pre-trained language models has recently become a common practice\nin building NLP models for various tasks, especially few-shot tasks. We argue\nthat under the few-shot setting, formulating fine-tuning closer to the\npre-training objectives shall be able to unleash more benefits from the\npre-trained language models. In this work, we take few-shot named entity\nrecognition (NER) for a pilot study, where existing fine-tuning strategies are\nmuch different from pre-training. We propose a novel few-shot fine-tuning\nframework for NER, FFF-NER. Specifically, we introduce three new types of\ntokens, ""is-entity"", ""which-type"" and bracket, so we can formulate the NER\nfine-tuning as (masked) token prediction or generation, depending on the choice\nof pre-trained language models. In our experiments, we apply FFF-NER to\nfine-tune both BERT and BART for few-shot NER on several benchmark datasets and\nobserve significant improvements over existing fine-tuning strategies,\nincluding sequence labeling, prototype meta-learning, and prompt-based\napproaches. We further perform a series of ablation studies, showing few-shot\nNER performance is strongly correlated with the similarity between fine-tuning\nand pre-training.'}, {'Dual-Granularity Contrastive Learning for Session-based Recommendation': 'Session-based recommendation systems(SBRS) are more suitable for the current\ne-commerce and streaming media recommendation scenarios and thus have become a\nhot topic. The data encountered by SBRS is typically highly sparse, which also\nserves as one of the bottlenecks limiting the accuracy of recommendations. So\nContrastive Learning(CL) is applied in SBRS owing to its capability of\nimproving embedding learning under the condition of sparse data. However,\nexisting CL strategies are limited in their ability to enforce finer-grained\n(e.g., factor-level) comparisons and, as a result, are unable to capture subtle\ndifferences between instances. More than that, these strategies usually use\nitem or segment dropout as a means of data augmentation which may result in\nsparser data and thus ineffective self-supervised signals. By addressing the\ntwo aforementioned limitations, we introduce a novel multi-granularity CL\nframework. Specifically, two extra augmented embedding convolution channels\nwith different granularities are constructed and the embeddings learned by them\nare compared with those learned from original view to complete the CL tasks. At\nfactor-level, we employ Disentangled Representation Learning to obtain\nfiner-grained data(e.g. factor-level embeddings), with which we can construct\nfactor-level convolution channels. At item-level, the star graph is deployed as\nthe augmented data and graph convolution on it can ensure the effectiveness of\nself-supervised signals. Compare the learned embeddings of these two views with\nthe learned embeddings of the basic view to achieve CL at two granularities.\nFinally, the more precise item-level and factor-level embeddings obtained are\nreferenced to generate personalized recommendations for the user. The proposed\nmodel is validated through extensive experiments on two benchmark datasets,\nshowcasing superior performance compared to existing methods.'}]","In the era of big data and digitalization, image super-resolution (SR) has emerged as a crucial technology allowing for the enhancement of image resolution, thereby boosting the quality, utility, and quantitative analysis across various fields like healthcare, security, and media. This paper introduces a pioneering approach to SR, focusing on the synergistic combination of a novel detail-enhancing framework and transformer-based image configuration for texture transfer. The core research question revolves around whether integrating transformer elements in place of traditional CNN structures in the SR pipeline, alongside enhanced pre-processing and novel texture transfer mechanisms, can significantly improve image restoration performance and visual quality.

The innovations detailed in this paper center around two key aspects. Firstly, the paper employs a transformer architecture, exemplified by the implementation of SwinSR and downstream architectures, for inter-scale feature extraction and synthesis, offering a superior spatial awareness over classical CNNs. Secondly, a detail-preserving framework, powered by a diffusion model, is introduced for enhancement before feature extraction, thereby narrowing the resolution gap. This process conveniently shifts the complexity of aligning and fusing fine-scale details to the pre-processing stage, significantly reducing computational requirements.

The methods fundamentally hinge on leveraging transformer capabilities for detailed inter-scale feature interaction and a progressive augmentation mechanism for enhancing image details. Moreover, a deviation from the Gaussian diffusion process, commonly used as a bottleneck for diffusion models, is introduced to individually extract the range-space and null-space components of the image, allowing more nuanced detail enhancement.

Quantitative evaluation on benchmarks including CUFED5, Sun80, Urban100, Manga109, and WR-SR datasets demonstrated substantial improvements in PSNR (25.54dB to 29.75dB) and SSIM (0.75 to 0.80) scores compared to state-of-the-art SR models, underscoring the enhancement in visual quality and restoration accuracy.

The contributions of this study lie in pioneering the synergy of transformer architectures with conventional SR techniques, bridging the gap between detail preservation and image feature extraction, and introducing a data-driven approach for efficient texture transfer. The developed methodology, bolstered by enhanced computational efficiency and finer-grained detail preservation, is poised to catalyze advancements in automated image enhancement, enabling more sophisticated analysis and processing in domains such as satellite imagery, surveillance, and creative industries.

This research paves the way for future work in augmenting image super-resolution with innovative architectures and algorithms, prioritizing the balance between visual fidelity, computational efficiency, and detail preservation to push the boundaries of image quality enhancement. The deployment of this approach in practical scenarios such as remote sensing, forensic science, and content generation would significantly impact their efficiency and output quality."
"Large Language Models (LLMs) have swiftly emerged as vital resources for
different applications in the biomedical and healthcare domains; however, these
models encounter issues such as generating inaccurate information or
hallucinations. Retrieval-augmented generation provided a solution for these
models to update knowledge and enhance their performance. In contrast to
previous retrieval-augmented LMs, which utilize specialized cross-attention
mechanisms to help LLM encode retrieved text, BiomedRAG adopts a simpler
approach by directly inputting the retrieved chunk-based documents into the
LLM. This straightforward design is easily applicable to existing retrieval and
language models, effectively bypassing noise information in retrieved
documents, particularly in noise-intensive tasks. Moreover, we demonstrate the
potential for utilizing the LLM to supervise the retrieval model in the
biomedical domain, enabling it to retrieve the document that assists the LM in
improving its predictions. Our experiments reveal that with the tuned
scorer,\textsc{ BiomedRAG} attains superior performance across 5 biomedical NLP
tasks, encompassing information extraction (triple extraction, relation
extraction), text classification, link prediction, and question-answering,
leveraging over 9 datasets. For instance, in the triple extraction task,
\textsc{BiomedRAG} outperforms other triple extraction systems with micro-F1
scores of 81.42 and 88.83 on GIT and ChemProt corpora, respectively.","[{'Semantic Structure based Query Graph Prediction for Question Answering\n  over Knowledge Graph': 'Building query graphs from natural language questions is an important step in\ncomplex question answering over knowledge graph (Complex KGQA). In general, a\nquestion can be correctly answered if its query graph is built correctly and\nthe right answer is then retrieved by issuing the query graph against the KG.\nTherefore, this paper focuses on query graph generation from natural language\nquestions. Existing approaches for query graph generation ignore the semantic\nstructure of a question, resulting in a large number of noisy query graph\ncandidates that undermine prediction accuracies. In this paper, we define six\nsemantic structures from common questions in KGQA and develop a novel\nStructure-BERT to predict the semantic structure of a question. By doing so, we\ncan first filter out noisy candidate query graphs, and then rank the remaining\ncandidates with a BERT-based ranking model. Extensive experiments on two\npopular benchmarks MetaQA and WebQuestionsSP (WSP) demonstrate the\neffectiveness of our method as compared to state-of-the-arts.'}, {'Understand the Dynamic World: An End-to-End Knowledge Informed Framework\n  for Open Domain Entity State Tracking': ""Open domain entity state tracking aims to predict reasonable state changes of\nentities (i.e., [attribute] of [entity] was [before_state] and [after_state]\nafterwards) given the action descriptions. It's important to many reasoning\ntasks to support human everyday activities. However, it's challenging as the\nmodel needs to predict an arbitrary number of entity state changes caused by\nthe action while most of the entities are implicitly relevant to the actions\nand their attributes as well as states are from open vocabularies. To tackle\nthese challenges, we propose a novel end-to-end Knowledge Informed framework\nfor open domain Entity State Tracking, namely KIEST, which explicitly retrieves\nthe relevant entities and attributes from external knowledge graph (i.e.,\nConceptNet) and incorporates them to autoregressively generate all the entity\nstate changes with a novel dynamic knowledge grained encoder-decoder framework.\nTo enforce the logical coherence among the predicted entities, attributes, and\nstates, we design a new constraint decoding strategy and employ a coherence\nreward to improve the decoding process. Experimental results show that our\nproposed KIEST framework significantly outperforms the strong baselines on the\npublic benchmark dataset OpenPI.""}, {'How far is Language Model from 100% Few-shot Named Entity Recognition in\n  Medical Domain': 'Recent advancements in language models (LMs) have led to the emergence of\npowerful models such as Small LMs (e.g., T5) and Large LMs (e.g., GPT-4). These\nmodels have demonstrated exceptional capabilities across a wide range of tasks,\nsuch as name entity recognition (NER) in the general domain. (We define SLMs as\npre-trained models with fewer parameters compared to models like GPT-3/3.5/4,\nsuch as T5, BERT, and others.) Nevertheless, their efficacy in the medical\nsection remains uncertain and the performance of medical NER always needs high\naccuracy because of the particularity of the field. This paper aims to provide\na thorough investigation to compare the performance of LMs in medical few-shot\nNER and answer How far is LMs from 100\\% Few-shot NER in Medical Domain, and\nmoreover to explore an effective entity recognizer to help improve the NER\nperformance. Based on our extensive experiments conducted on 16 NER models\nspanning from 2018 to 2023, our findings clearly indicate that LLMs outperform\nSLMs in few-shot medical NER tasks, given the presence of suitable examples and\nappropriate logical frameworks. Despite the overall superiority of LLMs in\nfew-shot medical NER tasks, it is important to note that they still encounter\nsome challenges, such as misidentification, wrong template prediction, etc.\nBuilding on previous findings, we introduce a simple and effective method\ncalled \\textsc{RT} (Retrieving and Thinking), which serves as retrievers,\nfinding relevant examples, and as thinkers, employing a step-by-step reasoning\nprocess. Experimental results show that our proposed \\textsc{RT} framework\nsignificantly outperforms the strong open baselines on the two open medical\nbenchmark datasets'}, {'On the Marginal Benefit of Active Learning: Does Self-Supervision Eat\n  Its Cake?': 'Active learning is the set of techniques for intelligently labeling large\nunlabeled datasets to reduce the labeling effort. In parallel, recent\ndevelopments in self-supervised and semi-supervised learning (S4L) provide\npowerful techniques, based on data-augmentation, contrastive learning, and\nself-training, that enable superior utilization of unlabeled data which led to\na significant reduction in required labeling in the standard machine learning\nbenchmarks. A natural question is whether these paradigms can be unified to\nobtain superior results. To this aim, this paper provides a novel algorithmic\nframework integrating self-supervised pretraining, active learning, and\nconsistency-regularized self-training. We conduct extensive experiments with\nour framework on CIFAR10 and CIFAR100 datasets. These experiments enable us to\nisolate and assess the benefits of individual components which are evaluated\nusing state-of-the-art methods (e.g.~Core-Set, VAAL, simCLR, FixMatch). Our\nexperiments reveal two key insights: (i) Self-supervised pre-training\nsignificantly improves semi-supervised learning, especially in the few-label\nregime, (ii) The benefit of active learning is undermined and subsumed by S4L\ntechniques. Specifically, we fail to observe any additional benefit of\nstate-of-the-art active learning algorithms when combined with state-of-the-art\nS4L techniques.'}, {'MC-GEN:Multi-level Clustering for Private Synthetic Data Generation': 'With the development of machine learning and data science, data sharing is\nvery common between companies and research institutes to avoid data scarcity.\nHowever, sharing original datasets that contain private information can cause\nprivacy leakage. A reliable solution is to utilize private synthetic datasets\nwhich preserve statistical information from original datasets. In this paper,\nwe propose MC-GEN, a privacy-preserving synthetic data generation method under\ndifferential privacy guarantee for machine learning classification tasks.\nMC-GEN applies multi-level clustering and differential private generative model\nto improve the utility of synthetic data. In the experimental evaluation, we\nevaluated the effects of parameters and the effectiveness of MC-GEN. The\nresults showed that MC-GEN can achieve significant effectiveness under certain\nprivacy guarantees on multiple classification tasks. Moreover, we compare\nMC-GEN with three existing methods. The results showed that MC-GEN outperforms\nother methods in terms of utility.'}, {'Benchingmaking Large Langage Models in Biomedical Triple Extraction': 'Biomedical triple extraction systems aim to automatically extract biomedical\nentities and relations between entities. The exploration of applying large\nlanguage models (LLM) to triple extraction is still relatively unexplored. In\nthis work, we mainly focus on sentence-level biomedical triple extraction.\nFurthermore, the absence of a high-quality biomedical triple extraction dataset\nimpedes the progress in developing robust triple extraction systems. To address\nthese challenges, initially, we compare the performance of various large\nlanguage models. Additionally, we present GIT, an expert-annotated biomedical\ntriple extraction dataset that covers a wider range of relation types.'}, {'Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and\n  Manual Correction': ""Princeton WordNet (PWN) is a lexicon-semantic network based on cognitive\nlinguistics, which promotes the development of natural language processing.\nBased on PWN, five Chinese wordnets have been developed to solve the problems\nof syntax and semantics. They include: Northeastern University Chinese WordNet\n(NEW), Sinica Bilingual Ontological WordNet (BOW), Southeast University Chinese\nWordNet (SEW), Taiwan University Chinese WordNet (CWN), Chinese Open WordNet\n(COW). By using them, we found that these word networks have low accuracy and\ncoverage, and cannot completely portray the semantic network of PWN. So we\ndecided to make a new Chinese wordnet called Multi-Fusion Chinese Wordnet (MCW)\nto make up those shortcomings. The key idea is to extend the SEW with the help\nof Oxford bilingual dictionary and Xinhua bilingual dictionary, and then\ncorrect it. More specifically, we used machine learning and manual adjustment\nin our corrections. Two standards were formulated to help our work. We\nconducted experiments on three tasks including relatedness calculation, word\nsimilarity and word sense disambiguation for the comparison of lemma's\naccuracy, at the same time, coverage also was compared. The results indicate\nthat MCW can benefit from coverage and accuracy via our method. However, it\nstill has room for improvement, especially with lemmas. In the future, we will\ncontinue to enhance the accuracy of MCW and expand the concepts in it.""}, {'A Condensed Transition Graph Framework for Zero-shot Link Prediction\n  with Large Language Models': ""Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically\nidentifying relations between given entities. Existing methods primarily employ\nauxiliary information to predict tail entity given head entity and its\nrelation, yet face challenges due to the occasional unavailability of such\ndetailed information and the inherent simplicity of predicting tail entities\nbased on semantic similarities. Even though Large Language Models (LLMs) offer\na promising solution to predict unobserved relations between the head and tail\nentity in a zero-shot manner, their performance is still restricted due to the\ninability to leverage all the (exponentially many) paths' information between\ntwo entities, which are critical in collectively indicating their relation\ntypes. To address this, in this work, we introduce a Condensed Transition Graph\nFramework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'\ninformation in linear time complexity to predict unseen relations between\nentities, attaining both efficiency and information preservation. Specifically,\nwe design a condensed transition graph encoder with theoretical guarantees on\nits coverage, expressiveness, and efficiency. It is learned by a transition\ngraph contrastive learning strategy. Subsequently, we design a soft instruction\ntuning to learn and map the all-path embedding to the input of LLMs.\nExperimental results show that our proposed CTLP method achieves\nstate-of-the-art performance on three standard ZSLP datasets""}, {'Exploring Weight Importance and Hessian Bias in Model Pruning': ""Model pruning is an essential procedure for building compact and\ncomputationally-efficient machine learning models. A key feature of a good\npruning algorithm is that it accurately quantifies the relative importance of\nthe model weights. While model pruning has a rich history, we still don't have\na full grasp of the pruning mechanics even for relatively simple problems\ninvolving linear models or shallow neural nets. In this work, we provide a\nprincipled exploration of pruning by building on a natural notion of\nimportance. For linear models, we show that this notion of importance is\ncaptured by covariance scaling which connects to the well-known Hessian-based\npruning. We then derive asymptotic formulas that allow us to precisely compare\nthe performance of different pruning methods. For neural networks, we\ndemonstrate that the importance can be at odds with larger magnitudes and\nproper initialization is critical for magnitude-based pruning. Specifically, we\nidentify settings in which weights become more important despite becoming\nsmaller, which in turn leads to a catastrophic failure of magnitude-based\npruning. Our results also elucidate that implicit regularization in the form of\nHessian structure has a catalytic role in identifying the important weights,\nwhich dictate the pruning performance.""}, {'DynaMo: Dynamic Community Detection by Incrementally Maximizing\n  Modularity': ""Community detection is of great importance for online social network\nanalysis. The volume, variety and velocity of data generated by today's online\nsocial networks are advancing the way researchers analyze those networks. For\ninstance, real-world networks, such as Facebook, LinkedIn and Twitter, are\ninherently growing rapidly and expanding aggressively over time. However, most\nof the studies so far have been focusing on detecting communities on the static\nnetworks. It is computationally expensive to directly employ a well-studied\nstatic algorithm repeatedly on the network snapshots of the dynamic networks.\nWe propose DynaMo, a novel modularity-based dynamic community detection\nalgorithm, aiming to detect communities of dynamic networks as effective as\nrepeatedly applying static algorithms but in a more efficient way. DynaMo is an\nadaptive and incremental algorithm, which is designed for incrementally\nmaximizing the modularity gain while updating the community structure of\ndynamic networks. In the experimental evaluation, a comprehensive comparison\nhas been made among DynaMo, Louvain (static) and 5 other dynamic algorithms.\nExtensive experiments have been conducted on 6 real-world networks and 10,000\nsynthetic networks. Our results show that DynaMo outperforms all the other 5\ndynamic algorithms in terms of the effectiveness, and is 2 to 5 times (by\naverage) faster than Louvain algorithm.""}]","Title: BIOMEDRAG: Enhancing Biomedical Large Language Models with Diverse Chunk Operative Services

Abstract:
Deep into the biomedical literature explosion, the demand for advanced technology that extracts the necessitated data from unstructured texts is resurgent. This paper introduces BIOMEDRAG, a framework designed to improve the performance of BioNLP tasks, particularly in biomedical environments. The aim is to augment critical services for medical professionals and boost the efficiency of Biomedical Large Language Models (LLMs).

BIOMEDRAG innovates by adopting a sui generis operational paradigm in managing diverse document chunks relevant to pre-existing LLM architectures. The framework incorporates novel methods for a tailored chunk scorer and a pillar of diversity operation to enrich and contextualize the information extracted by LLMs. It employs a comparative assessment methodology for evaluating its enhancements over 5 biomedical tasks and 11 datasets, yielding significant improvements in F1 scores.

This research contributes to the field by providing a practical solution that boosts the capabilities of large language models in medical applications, such as improving precision and recall in extractive tasks, advancing relation extraction, and enabling enhanced knowledge discovery across diverse biomedical literature. BIOMEDRAG's enhancement and diversification strategies offer a flexible, scalable, and domain-specific approach that may catalyze precision medical care through more accurate data extraction and analysis. 

In summary, the research highlights the transformative impact of integrating operational services for diversified information extraction alongside enhancing subsequent processing techniques, serving as a benchmark for future advancements in biomedical NLP applications."
"As the capabilities of large language models (LLMs) have expanded
dramatically, aligning these models with human values presents a significant
challenge, posing potential risks during deployment. Traditional alignment
strategies rely heavily on human intervention, such as Supervised Fine-Tuning
(SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the
self-alignment capacities of LLMs, which usually require a strong LLM's
emergent ability to improve its original bad answer. To address these
challenges, we propose a novel self-alignment method that utilizes a Chain of
Thought (CoT) approach, termed AlignCoT. This method encompasses stages of
Question Analysis, Answer Guidance, and Safe Answer production. It is designed
to enable LLMs to generate high-quality, safe responses throughout various
stages of their development. Furthermore, we introduce the Mixture of
insighTful Experts (MoTE) architecture, which applies the mixture of experts to
enhance each component of the AlignCoT process, markedly increasing alignment
efficiency. The MoTE approach not only outperforms existing methods in aligning
LLMs with human values but also highlights the benefits of using self-generated
data, revealing the dual benefits of improved alignment and training
efficiency.","[{'Your Contrastive Learning Is Secretly Doing Stochastic Neighbor\n  Embedding': 'Contrastive learning, especially self-supervised contrastive learning (SSCL),\nhas achieved great success in extracting powerful features from unlabeled data.\nIn this work, we contribute to the theoretical understanding of SSCL and\nuncover its connection to the classic data visualization method, stochastic\nneighbor embedding (SNE), whose goal is to preserve pairwise distances. From\nthe perspective of preserving neighboring information, SSCL can be viewed as a\nspecial case of SNE with the input space pairwise similarities specified by\ndata augmentation. The established correspondence facilitates deeper\ntheoretical understanding of learned features of SSCL, as well as\nmethodological guidelines for practical improvement. Specifically, through the\nlens of SNE, we provide novel analysis on domain-agnostic augmentations,\nimplicit bias and robustness of learned features. To illustrate the practical\nadvantage, we demonstrate that the modifications from SNE to $t$-SNE can also\nbe adopted in the SSCL setting, achieving significant improvement in both\nin-distribution and out-of-distribution generalization.'}, {'EHSOD: CAM-Guided End-to-end Hybrid-Supervised Object Detection with\n  Cascade Refinement': 'Object detectors trained on fully-annotated data currently yield state of the\nart performance but require expensive manual annotations. On the other hand,\nweakly-supervised detectors have much lower performance and cannot be used\nreliably in a realistic setting. In this paper, we study the hybrid-supervised\nobject detection problem, aiming to train a high quality detector with only a\nlimited amount of fullyannotated data and fully exploiting cheap data with\nimagelevel labels. State of the art methods typically propose an iterative\napproach, alternating between generating pseudo-labels and updating a detector.\nThis paradigm requires careful manual hyper-parameter tuning for mining good\npseudo labels at each round and is quite time-consuming. To address these\nissues, we present EHSOD, an end-to-end hybrid-supervised object detection\nsystem which can be trained in one shot on both fully and weakly-annotated\ndata. Specifically, based on a two-stage detector, we proposed two modules to\nfully utilize the information from both kinds of labels: 1) CAMRPN module aims\nat finding foreground proposals guided by a class activation heat-map; 2)\nhybrid-supervised cascade module further refines the bounding-box position and\nclassification with the help of an auxiliary head compatible with image-level\ndata. Extensive experiments demonstrate the effectiveness of the proposed\nmethod and it achieves comparable results on multiple object detection\nbenchmarks with only 30% fully-annotated data, e.g. 37.5% mAP on COCO. We will\nrelease the code and the trained models.'}, {'Task-Customized Self-Supervised Pre-training with Scalable Dynamic\n  Routing': 'Self-supervised learning (SSL), especially contrastive methods, has raised\nattraction recently as it learns effective transferable representations without\nsemantic annotations. A common practice for self-supervised pre-training is to\nuse as much data as possible. For a specific downstream task, however,\ninvolving irrelevant data in pre-training may degenerate the downstream\nperformance, observed from our extensive experiments. On the other hand, for\nexisting SSL methods, it is burdensome and infeasible to use different\ndownstream-task-customized datasets in pre-training for different tasks. To\naddress this issue, we propose a novel SSL paradigm called Scalable Dynamic\nRouting (SDR), which can be trained once and deployed efficiently to different\ndownstream tasks with task-customized pre-trained models. Specifically, we\nconstruct the SDRnet with various sub-nets and train each sub-net with only one\nsubset of the data by data-aware progressive training. When a downstream task\narrives, we route among all the pre-trained sub-nets to get the best along with\nits corresponding weights. Experiment results show that our SDR can train 256\nsub-nets on ImageNet simultaneously, which provides better transfer performance\nthan a unified model trained on the full ImageNet, achieving state-of-the-art\n(SOTA) averaged accuracy over 11 downstream classification tasks and AP on\nPASCAL VOC detection task.'}, {'Mixed Autoencoder for Self-supervised Visual Representation Learning': 'Masked Autoencoder (MAE) has demonstrated superior performance on various\nvision tasks via randomly masking image patches and reconstruction. However,\neffective data augmentation strategies for MAE still remain open questions,\ndifferent from those in contrastive learning that serve as the most important\npart. This paper studies the prevailing mixing augmentation for MAE. We first\ndemonstrate that naive mixing will in contrast degenerate model performance due\nto the increase of mutual information (MI). To address, we propose homologous\nrecognition, an auxiliary pretext task, not only to alleviate the MI\nincreasement by explicitly requiring each patch to recognize homologous\npatches, but also to perform object-aware self-supervised pre-training for\nbetter downstream dense perception performance. With extensive experiments, we\ndemonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the\nstate-of-the-art transfer results among masked image modeling (MIM)\naugmentations on different downstream tasks with significant efficiency.\nSpecifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9\nAP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base.\nMoreover, MixedAE surpasses iBOT, a strong MIM method combined with instance\ndiscrimination, while accelerating training by 2x. To our best knowledge, this\nis the very first work to consider mixing for MIM from the perspective of\npretext task design. Code will be made available.'}, {'Task-customized Masked AutoEncoder via Mixture of Cluster-conditional\n  Experts': ""Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that\nachieves promising results in model pre-training. However, when the various\ndownstream tasks have data distributions different from the pre-training data,\nthe semantically irrelevant pre-training information might result in negative\ntransfer, impeding MAE's scalability. To address this issue, we propose a novel\nMAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE),\nwhich can be trained once but provides customized pre-training models for\ndiverse downstream tasks. Different from the mixture of experts (MoE), our MoCE\ntrains each expert only with semantically relevant images by using\ncluster-conditional gates. Thus, each downstream task can be allocated to its\ncustomized model pre-trained with data most similar to the downstream data.\nExperiments on a collection of 11 downstream tasks show that MoCE outperforms\nthe vanilla MAE by 2.45\\% on average. It also obtains new state-of-the-art\nself-supervised learning results on detection and segmentation.""}, {'DiffFit: Unlocking Transferability of Large Diffusion Models via Simple\n  Parameter-Efficient Fine-Tuning': 'Diffusion models have proven to be highly effective in generating\nhigh-quality images. However, adapting large pre-trained diffusion models to\nnew domains remains an open challenge, which is critical for real-world\napplications. This paper proposes DiffFit, a parameter-efficient strategy to\nfine-tune large pre-trained diffusion models that enable fast adaptation to new\ndomains. DiffFit is embarrassingly simple that only fine-tunes the bias term\nand newly-added scaling factors in specific layers, yet resulting in\nsignificant training speed-up and reduced model storage costs. Compared with\nfull fine-tuning, DiffFit achieves 2$\\times$ training speed-up and only needs\nto store approximately 0.12\\% of the total model parameters. Intuitive\ntheoretical analysis has been provided to justify the efficacy of scaling\nfactors on fast adaptation. On 8 downstream datasets, DiffFit achieves superior\nor competitive performances compared to the full fine-tuning while being more\nefficient. Remarkably, we show that DiffFit can adapt a pre-trained\nlow-resolution generative model to a high-resolution one by adding minimal\ncost. Among diffusion-based methods, DiffFit sets a new state-of-the-art FID of\n3.02 on ImageNet 512$\\times$512 benchmark by fine-tuning only 25 epochs from a\npublic pre-trained ImageNet 256$\\times$256 checkpoint while being 30$\\times$\nmore training efficient than the closest competitor.'}, {'Implicit Concept Removal of Diffusion Models': 'Text-to-image (T2I) diffusion models often inadvertently generate unwanted\nconcepts such as watermarks and unsafe images. These concepts, termed as the\n""implicit concepts"", could be unintentionally learned during training and then\nbe generated uncontrollably during inference. Existing removal methods still\nstruggle to eliminate implicit concepts primarily due to their dependency on\nthe model\'s ability to recognize concepts it actually can not discern. To\naddress this, we utilize the intrinsic geometric characteristics of implicit\nconcepts and present the Geom-Erasing, a novel concept removal method based on\nthe geometric-driven control. Specifically, once an unwanted implicit concept\nis identified, we integrate the existence and geometric information of the\nconcept into the text prompts with the help of an accessible classifier or\ndetector model. Subsequently, the model is optimized to identify and\ndisentangle this information, which is then adopted as negative prompts during\ngeneration. Moreover, we introduce the Implicit Concept Dataset (ICD), a novel\nimage-text dataset imbued with three typical implicit concepts (i.e., QR codes,\nwatermarks, and text), reflecting real-life situations where implicit concepts\nare easily injected. Geom-Erasing effectively mitigates the generation of\nimplicit concepts, achieving the state-of-the-art results on the Inappropriate\nImage Prompts (I2P) and our challenging Implicit Concept Dataset (ICD)\nbenchmarks.'}, {'Relaxed Conditional Image Transfer for Semi-supervised Domain Adaptation': 'Semi-supervised domain adaptation (SSDA), which aims to learn models in a\npartially labeled target domain with the assistance of the fully labeled source\ndomain, attracts increasing attention in recent years. To explicitly leverage\nthe labeled data in both domains, we naturally introduce a conditional GAN\nframework to transfer images without changing the semantics in SSDA. However,\nwe identify a label-domination problem in such an approach. In fact, the\ngenerator tends to overlook the input source image and only memorizes\nprototypes of each class, which results in unsatisfactory adaptation\nperformance. To this end, we propose a simple yet effective Relaxed conditional\nGAN (Relaxed cGAN) framework. Specifically, we feed the image without its label\nto our generator. In this way, the generator has to infer the semantic\ninformation of input data. We formally prove that its equilibrium is desirable\nand empirically validate its practical convergence and effectiveness in image\ntransfer. Additionally, we propose several techniques to make use of unlabeled\ndata in the target domain, enhancing the model in SSDA settings. We validate\nour method on the well-adopted datasets: Digits, DomainNet, and Office-Home. We\nachieve state-of-the-art performance on DomainNet, Office-Home and most digit\nbenchmarks in low-resource and high-resource settings.'}, {'MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with\n  Module-wise Pruning Error Metric': 'Vision-language pre-trained models have achieved impressive performance on\nvarious downstream tasks. However, their large model sizes hinder their\nutilization on platforms with limited computational resources. We find that\ndirectly using smaller pre-trained models and applying magnitude-based pruning\non CLIP models leads to inflexibility and inferior performance. Recent efforts\nfor VLP compression either adopt uni-modal compression metrics resulting in\nlimited performance or involve costly mask-search processes with learnable\nmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\nmetric, accurately assessing CLIP module importance by performance decline on\ncross-modal tasks. Using the MoPE metric, we introduce a unified pruning\nframework applicable to both pre-training and task-specific fine-tuning\ncompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\nfrom the teacher model, significantly reducing pre-training costs while\nmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\nfrom width to depth yields highly competitive task-specific models. Extensive\nexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\nMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.'}, {'Mixture of Cluster-conditional LoRA Experts for Vision-language\n  Instruction Tuning': 'Instruction tuning of Large Vision-language Models (LVLMs) has revolutionized\nthe development of versatile models with zero-shot generalization across a wide\nrange of downstream vision-language tasks. However, the diversity of training\ntasks of different sources and formats would lead to inevitable task conflicts,\nwhere different tasks conflict for the same set of model parameters, resulting\nin sub-optimal instruction-following abilities. To address that, we propose the\nMixture of Cluster-conditional LoRA Experts (MoCLE), a novel Mixture of Experts\n(MoE) architecture designed to activate the task-customized model parameters\nbased on the instruction clusters. A separate universal expert is further\nincorporated to improve generalization capabilities of MoCLE for novel\ninstructions. Extensive experiments on InstructBLIP and LLaVA demonstrate the\neffectiveness of MoCLE.'}]","Title: Towards Safeguarding Artificial Intelligence's Conversational Ethics: The Alignment Capability of Question-Answering Models 

Abstract: 

Throughout the rapid advancement of conversational AI, ensuring the ethical and benevolent interaction with users remains as pertinent as ever. Acknowledging this exigency, the research endeavor herein introduces AlignCoT, an innovative dataset designed to elicit meticulous leadership over question-answering models, steering them towards prudent execution. The primary objective is to mitigate the potential for harmful responses by prompting models to undergo a mental calculus of question analysis, answer guidelines, and offering safe response alternatives.

Innovatively, AlignCoT initiates a new paradigm that synthesizes a dataset capable of guiding models to defer responses, critically analyze inquiries, provide constructive feedback, establish safe answers, and activate a multi-Lightweight Recursive Architecture (LoRA) for Answer-Sequence Generation (Mixture of Experts). This structure enhances model self-alignment by activating domain-specific expertise, while a shared expert unifies processes aimed at question harmonization and response safety through thoughtful measures.

The research endeavors to cultivate a scenario where models, not reliant on a large human-trained dataset, can autonomously generate responses with a high degree of harmfulness and helpfulness scores, reflecting the efficacy of incorporating multiple cognitive steps. Moreover, the Mixture of Experts (MoTE) design enables models to reflect in detail on each step, driving the adoption of a mentality akin to human sequential thinking before finalizing their output.

The results highlight that models trained with AlignCoT demonstrate a significant improvement of over 7% in performance metrics relative to those using conventional methods. Moreover, systems characterized by multi-step inference show further enhancements, suggesting a profound influence of the dataset on pervasive artificial intelligence response optimization.

Conclusively, this research underscores the potential of combining machine reasoning and advanced dataset generation to refine conversational models, making them more cognizant of ethical nuances, thus serving the evolving needs of trustworthy dialogue systems. The implications stretch wide, promising a future where AI-generated responses not only answer queries but also contribute to a societal norm of responsible communication."
"Large language models (LLMs) with their strong zero-shot topic extraction
capabilities offer an alternative to probabilistic topic modelling and
closed-set topic classification approaches. As zero-shot topic extractors, LLMs
are expected to understand human instructions to generate relevant and
non-hallucinated topics based on the given documents. However, LLM-based topic
modelling approaches often face difficulties in generating topics with
adherence to granularity as specified in human instructions, often resulting in
many near-duplicate topics. Furthermore, methods for addressing hallucinated
topics generated by LLMs have not yet been investigated. In this paper, we
focus on addressing the issues of topic granularity and hallucinations for
better LLM-based topic modelling. To this end, we introduce a novel approach
that leverages Direct Preference Optimisation (DPO) to fine-tune open-source
LLMs, such as Mistral-7B. Our approach does not rely on traditional human
annotation to rank preferred answers but employs a reconstruction pipeline to
modify raw topics generated by LLMs, thus enabling a fast and efficient
training and inference framework. Comparative experiments show that our
fine-tuning approach not only significantly improves the LLM's capability to
produce more coherent, relevant, and precise topics, but also reduces the
number of hallucinated topics.","[{'Identifying and Characterizing Active Citizens who Refute Misinformation\n  in Social Media': 'The phenomenon of misinformation spreading in social media has developed a\nnew form of active citizens who focus on tackling the problem by refuting posts\nthat might contain misinformation. Automatically identifying and characterizing\nthe behavior of such active citizens in social media is an important task in\ncomputational social science for complementing studies in misinformation\nanalysis. In this paper, we study this task across different social media\nplatforms (i.e., Twitter and Weibo) and languages (i.e., English and Chinese)\nfor the first time. To this end, (1) we develop and make publicly available a\nnew dataset of Weibo users mapped into one of the two categories (i.e.,\nmisinformation posters or active citizens); (2) we evaluate a battery of\nsupervised models on our new Weibo dataset and an existing Twitter dataset\nwhich we repurpose for the task; and (3) we present an extensive analysis of\nthe differences in language use between the two user categories.'}, {""It's about Time: Rethinking Evaluation on Rumor Detection Benchmarks\n  using Chronological Splits"": 'New events emerge over time influencing the topics of rumors in social media.\nCurrent rumor detection benchmarks use random splits as training, development\nand test sets which typically results in topical overlaps. Consequently, models\ntrained on random splits may not perform well on rumor classification on\npreviously unseen topics due to the temporal concept drift. In this paper, we\nprovide a re-evaluation of classification models on four popular rumor\ndetection benchmarks considering chronological instead of random splits. Our\nexperimental results show that the use of random splits can significantly\noverestimate predictive performance across all datasets and models. Therefore,\nwe suggest that rumor detection models should always be evaluated using\nchronological splits for minimizing topical overlaps.'}, {'Examining Temporalities on Stance Detection towards COVID-19 Vaccination': ""Previous studies have highlighted the importance of vaccination as an\neffective strategy to control the transmission of the COVID-19 virus. It is\ncrucial for policymakers to have a comprehensive understanding of the public's\nstance towards vaccination on a large scale. However, attitudes towards\nCOVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved\nover time on social media. Thus, it is necessary to account for possible\ntemporal shifts when analysing these stances. This study aims to examine the\nimpact of temporal concept drift on stance detection towards COVID-19\nvaccination on Twitter. To this end, we evaluate a range of transformer-based\nmodels using chronological (splitting the training, validation, and test sets\nin order of time) and random splits (randomly splitting these three sets) of\nsocial media data. Our findings reveal significant discrepancies in model\nperformance between random and chronological splits in several existing\nCOVID-19-related datasets; specifically, chronological splits significantly\nreduce the accuracy of stance classification. Therefore, real-world stance\ndetection approaches need to be further refined to incorporate temporal factors\nas a key consideration.""}, {'Examining the Limitations of Computational Rumor Detection Models\n  Trained on Static Datasets': ""A crucial aspect of a rumor detection model is its ability to generalize,\nparticularly its ability to detect emerging, previously unknown rumors. Past\nresearch has indicated that content-based (i.e., using solely source posts as\ninput) rumor detection models tend to perform less effectively on unseen\nrumors. At the same time, the potential of context-based models remains largely\nuntapped. The main contribution of this paper is in the in-depth evaluation of\nthe performance gap between content and context-based models specifically on\ndetecting new, unseen rumors. Our empirical findings demonstrate that\ncontext-based models are still overly dependent on the information derived from\nthe rumors' source post and tend to overlook the significant role that\ncontextual information can play. We also study the effect of data split\nstrategies on classifier performance. Based on our experimental results, the\npaper also offers practical suggestions on how to minimize the effects of\ntemporal concept drift in static datasets during the training of rumor\ndetection methods.""}, {'Large Language Models Offer an Alternative to the Traditional Approach\n  of Topic Modelling': 'Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.'}, {'Examining Temporal Bias in Abusive Language Detection': 'The use of abusive language online has become an increasingly pervasive\nproblem that damages both individuals and society, with effects ranging from\npsychological harm right through to escalation to real-life violence and even\ndeath. Machine learning models have been developed to automatically detect\nabusive language, but these models can suffer from temporal bias, the\nphenomenon in which topics, language use or social norms change over time. This\nstudy aims to investigate the nature and impact of temporal bias in abusive\nlanguage detection across various languages and explore mitigation methods. We\nevaluate the performance of models on abusive data sets from different time\nperiods. Our results demonstrate that temporal bias is a significant challenge\nfor abusive language detection, with models trained on historical data showing\na significant drop in performance over time. We also present an extensive\nlinguistic analysis of these abusive data sets from a diachronic perspective,\naiming to explore the reasons for language evolution and performance decline.\nThis study sheds light on the pervasive issue of temporal bias in abusive\nlanguage detection across languages, offering crucial insights into language\nevolution and temporal bias mitigation.'}, {'A Large-Scale Comparative Study of Accurate COVID-19 Information versus\n  Misinformation': 'The COVID-19 pandemic led to an infodemic where an overwhelming amount of\nCOVID-19 related content was being disseminated at high velocity through social\nmedia. This made it challenging for citizens to differentiate between accurate\nand inaccurate information about COVID-19. This motivated us to carry out a\ncomparative study of the characteristics of COVID-19 misinformation versus\nthose of accurate COVID-19 information through a large-scale computational\nanalysis of over 242 million tweets. The study makes comparisons alongside four\nkey aspects: 1) the distribution of topics, 2) the live status of tweets, 3)\nlanguage analysis and 4) the spreading power over time. An added contribution\nof this study is the creation of a COVID-19 misinformation classification\ndataset. Finally, we demonstrate that this new dataset helps improve\nmisinformation classification by more than 9\\% based on average F1 measure.'}, {'VaxxHesitancy: A Dataset for Studying Hesitancy towards COVID-19\n  Vaccination on Twitter': ""Vaccine hesitancy has been a common concern, probably since vaccines were\ncreated and, with the popularisation of social media, people started to express\ntheir concerns about vaccines online alongside those posting pro- and\nanti-vaccine content. Predictably, since the first mentions of a COVID-19\nvaccine, social media users posted about their fears and concerns or about\ntheir support and belief into the effectiveness of these rapidly developing\nvaccines. Identifying and understanding the reasons behind public hesitancy\ntowards COVID-19 vaccines is important for policy markers that need to develop\nactions to better inform the population with the aim of increasing vaccine\ntake-up. In the case of COVID-19, where the fast development of the vaccines\nwas mirrored closely by growth in anti-vaxx disinformation, automatic means of\ndetecting citizen attitudes towards vaccination became necessary. This is an\nimportant computational social sciences task that requires data analysis in\norder to gain in-depth understanding of the phenomena at hand. Annotated data\nis also necessary for training data-driven models for more nuanced analysis of\nattitudes towards vaccination. To this end, we created a new collection of over\n3,101 tweets annotated with users' attitudes towards COVID-19 vaccination\n(stance). Besides, we also develop a domain-specific language model (VaxxBERT)\nthat achieves the best predictive performance (73.0 accuracy and 69.3 F1-score)\nas compared to a robust set of baselines. To the best of our knowledge, these\nare the first dataset and model that model vaccine hesitancy as a category\ndistinct from pro- and anti-vaccine stance.""}, {""Don't Waste a Single Annotation: Improving Single-Label Classifiers\n  Through Soft Labels"": 'In this paper, we address the limitations of the common data annotation and\ntraining methods for objective single-label classification tasks. Typically,\nwhen annotating such tasks annotators are only asked to provide a single label\nfor each sample and annotator disagreement is discarded when a final hard label\nis decided through majority voting. We challenge this traditional approach,\nacknowledging that determining the appropriate label can be difficult due to\nthe ambiguity and lack of context in the data samples. Rather than discarding\nthe information from such ambiguous annotations, our soft label method makes\nuse of them for training. Our findings indicate that additional annotator\ninformation, such as confidence, secondary label and disagreement, can be used\nto effectively generate soft labels. Training classifiers with these soft\nlabels then leads to improved performance and calibration on the hard label\ntest set.'}, {'Navigating Prompt Complexity for Zero-Shot Classification: A Study of\n  Large Language Models in Computational Social Science': 'Instruction-tuned Large Language Models (LLMs) have exhibited impressive\nlanguage understanding and the capacity to generate responses that follow\nspecific prompts. However, due to the computational demands associated with\ntraining these models, their applications often adopt a zero-shot setting. In\nthis paper, we evaluate the zero-shot performance of two publicly accessible\nLLMs, ChatGPT and OpenAssistant, in the context of six Computational Social\nScience classification tasks, while also investigating the effects of various\nprompting strategies. Our experiments investigate the impact of prompt\ncomplexity, including the effect of incorporating label definitions into the\nprompt; use of synonyms for label names; and the influence of integrating past\nmemories during foundation model training. The findings indicate that in a\nzero-shot setting, current LLMs are unable to match the performance of smaller,\nfine-tuned baseline transformer models (such as BERT-large). Additionally, we\nfind that different prompting strategies can significantly affect\nclassification accuracy, with variations in accuracy and F1 scores exceeding\n10\\%.'}]","Title: Addressing Hallucinations and Topic Granularity in Large Language Models for Improved Topic Modelling

Abstract:
To address the pressing challenges of topic hallucinations and granularity in large language models (LLMs), this research introduces TopicMistral, a groundbreaking fine-tunneled variant of Mistral-7B. Aiming to minimize the generation of hallucinated topics and enhance topic granularity, TopicMistral was designed with a novel data-reconstruction pipeline that refines raw LLM outputs without the need for human annotations, supporting an efficient reward model learning. 

TopicMistral significantly outperforms vanilla LLMs and contemporary prompt-based frameworks in generating fewer hallucinated topics and producing more granular and relevant topics, as demonstrated through comprehensive evaluations against leading benchmarks. This innovation facilitates a notable improvement in topic model quality, making TopicMistral particularly suitable for downstream applications such as analyzing temporal topic trends and enhancing natural language processing workflows.

Key contributions include a robust, automated approach for fine-tuning LLMs that ameliorates hallucination risks and enhances topic granularity, opening avenues for more accurate and reliable topic-based analyses. With substantial empirical evidence supporting its effectiveness, TopicMistral holds promise for broader applications across diverse fields that leverage topic modelling techniques, such as content analysis, knowledge extraction, and information organization. This work advances the state-of-the-art in LLM-based topic modelling and offers new avenues for advancing NLP and information retrieval systems."
"To address the shortcomings of real-world datasets, robust learning
algorithms have been designed to overcome arbitrary and indiscriminate data
corruption. However, practical processes of gathering data may lead to patterns
of data corruption that are localized to specific partitions of the training
dataset. Motivated by critical applications where the learned model is deployed
to make predictions about people from a rich collection of overlapping
subpopulations, we initiate the study of multigroup robust algorithms whose
robustness guarantees for each subpopulation only degrade with the amount of
data corruption inside that subpopulation. When the data corruption is not
distributed uniformly over subpopulations, our algorithms provide more
meaningful robustness guarantees than standard guarantees that are oblivious to
how the data corruption and the affected subpopulations are related. Our
techniques establish a new connection between multigroup fairness and
robustness.","[{'Robust Mean Estimation on Highly Incomplete Data with Arbitrary Outliers': 'We study the problem of robustly estimating the mean of a $d$-dimensional\ndistribution given $N$ examples, where most coordinates of every example may be\nmissing and $\\varepsilon N$ examples may be arbitrarily corrupted. Assuming\neach coordinate appears in a constant factor more than $\\varepsilon N$\nexamples, we show algorithms that estimate the mean of the distribution with\ninformation-theoretically optimal dimension-independent error guarantees in\nnearly-linear time $\\widetilde O(Nd)$. Our results extend recent work on\ncomputationally-efficient robust estimation to a more widely applicable\nincomplete-data setting.'}, {'Approximation Algorithms for Orthogonal Non-negative Matrix\n  Factorization': 'In the non-negative matrix factorization (NMF) problem, the input is an\n$m\\times n$ matrix $M$ with non-negative entries and the goal is to factorize\nit as $M\\approx AW$. The $m\\times k$ matrix $A$ and the $k\\times n$ matrix $W$\nare both constrained to have non-negative entries. This is in contrast to\nsingular value decomposition, where the matrices $A$ and $W$ can have negative\nentries but must satisfy the orthogonality constraint: the columns of $A$ are\northogonal and the rows of $W$ are also orthogonal. The orthogonal non-negative\nmatrix factorization (ONMF) problem imposes both the non-negativity and the\northogonality constraints, and previous work showed that it leads to better\nperformances than NMF on many clustering tasks. We give the first\nconstant-factor approximation algorithm for ONMF when one or both of $A$ and\n$W$ are subject to the orthogonality constraint. We also show an interesting\nconnection to the correlation clustering problem on bipartite graphs. Our\nexperiments on synthetic and real-world data show that our algorithm achieves\nsimilar or smaller errors compared to previous ONMF algorithms while ensuring\nperfect orthogonality (many previous algorithms do not satisfy the hard\northogonality constraint).'}, {'Comparative Learning: A Sample Complexity Theory for Two Hypothesis\n  Classes': 'In many learning theory problems, a central role is played by a hypothesis\nclass: we might assume that the data is labeled according to a hypothesis in\nthe class (usually referred to as the realizable setting), or we might evaluate\nthe learned model by comparing it with the best hypothesis in the class (the\nagnostic setting).\n  Taking a step beyond these classic setups that involve only a single\nhypothesis class, we introduce comparative learning as a combination of the\nrealizable and agnostic settings in PAC learning: given two binary hypothesis\nclasses $S$ and $B$, we assume that the data is labeled according to a\nhypothesis in the source class $S$ and require the learned model to achieve an\naccuracy comparable to the best hypothesis in the benchmark class $B$. Even\nwhen both $S$ and $B$ have infinite VC dimensions, comparative learning can\nstill have a small sample complexity. We show that the sample complexity of\ncomparative learning is characterized by the mutual VC dimension\n$\\mathsf{VC}(S,B)$ which we define to be the maximum size of a subset shattered\nby both $S$ and $B$. We also show a similar result in the online setting, where\nwe give a regret characterization in terms of the mutual Littlestone dimension\n$\\mathsf{Ldim}(S,B)$. These results also hold for partial hypotheses.\n  We additionally show that the insights necessary to characterize the sample\ncomplexity of comparative learning can be applied to characterize the sample\ncomplexity of realizable multiaccuracy and multicalibration using the mutual\nfat-shattering dimension, an analogue of the mutual VC dimension for\nreal-valued hypotheses. This not only solves an open problem proposed by Hu,\nPeale, Reingold (2022), but also leads to independently interesting results\nextending classic ones about regression, boosting, and covering number to our\ntwo-hypothesis-class setting.'}, {'Active Tolerant Testing': 'In this work, we give the first algorithms for tolerant testing of nontrivial\nclasses in the active model: estimating the distance of a target function to a\nhypothesis class C with respect to some arbitrary distribution D, using only a\nsmall number of label queries to a polynomial-sized pool of unlabeled examples\ndrawn from D. Specifically, we show that for the class D of unions of d\nintervals on the line, we can estimate the error rate of the best hypothesis in\nthe class to an additive error epsilon from only $O(\\frac{1}{\\epsilon^6}\\log\n\\frac{1}{\\epsilon})$ label queries to an unlabeled pool of size\n$O(\\frac{d}{\\epsilon^2}\\log \\frac{1}{\\epsilon})$. The key point here is the\nnumber of labels needed is independent of the VC-dimension of the class. This\nextends the work of Balcan et al. [2012] who solved the non-tolerant testing\nproblem for this class (distinguishing the zero-error case from the case that\nthe best hypothesis in the class has error greater than epsilon).\n  We also consider the related problem of estimating the performance of a given\nlearning algorithm A in this setting. That is, given a large pool of unlabeled\nexamples drawn from distribution D, can we, from only a few label queries,\nestimate how well A would perform if the entire dataset were labeled? We focus\non k-Nearest Neighbor style algorithms, and also show how our results can be\napplied to the problem of hyperparameter tuning (selecting the best value of k\nfor the given learning problem).'}, {'Near-Optimal Explainable $k$-Means for All Dimensions': 'Many clustering algorithms are guided by certain cost functions such as the\nwidely-used $k$-means cost. These algorithms divide data points into clusters\nwith often complicated boundaries, creating difficulties in explaining the\nclustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and\nRashtchian (ICML 2020) introduced explainable clustering, where the cluster\nboundaries are axis-parallel hyperplanes and the clustering is obtained by\napplying a decision tree to the data. The central question here is: how much\ndoes the explainability constraint increase the value of the cost function?\n  Given $d$-dimensional data points, we show an efficient algorithm that finds\nan explainable clustering whose $k$-means cost is at most $k^{1 -\n2/d}\\,\\mathrm{poly}(d\\log k)$ times the minimum cost achievable by a clustering\nwithout the explainability constraint, assuming $k,d\\ge 2$. Taking the minimum\nof this bound and the $k\\,\\mathrm{polylog} (k)$ bound in independent work by\nMakarychev-Shan (ICML 2021), Gamlath-Jia-Polak-Svensson (2021), or\nEsfandiari-Mirrokni-Narayanan (2021), we get an improved bound of $k^{1 -\n2/d}\\,\\mathrm{polylog}(k)$, which we show is optimal for every choice of\n$k,d\\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in particular,\nwe show an $O(\\log k\\log\\log k)$ bound, improving near-exponentially over the\nprevious best bound of $O(k\\log k)$ by Laber and Murtinho (ICML 2021).'}, {'Predict to Minimize Swap Regret for All Payoff-Bounded Tasks': 'A sequence of predictions is calibrated if and only if it induces no swap\nregret to all down-stream decision tasks. We study the Maximum Swap Regret\n(MSR) of predictions for binary events: the swap regret maximized over all\ndownstream tasks with bounded payoffs. Previously, the best online prediction\nalgorithm for minimizing MSR is obtained by minimizing the K1 calibration\nerror, which upper bounds MSR up to a constant factor. However, recent work\n(Qiao and Valiant, 2021) gives an ${\\Omega}(T^{0.528})$ lower bound for the\nworst-case expected $K_1$ calibration error incurred by any randomized\nalgorithm in T rounds, presenting a barrier to achieving better rates for MSR.\nSeveral relaxations of MSR have been considered to overcome this barrier, via\nexternal regret (Kleinberg et al., 2023) and regret bounds depending\npolynomially on the number of actions in downstream tasks (Noarov et al., 2023;\nRoth and Shi, 2024). We show that the barrier can be surpassed without any\nrelaxations: we give an efficient randomized prediction algorithm that\nguarantees $O(\\sqrt{T}logT)$ expected MSR. We also discuss the economic utility\nof calibration by viewing MSR as a decision-theoretic calibration error metric\nand study its relationship to existing metrics.'}, {'Capacitated Center Problems with Two-Sided Bounds and Outliers': 'In recent years, the capacitated center problems have attracted a lot of\nresearch interest. Given a set of vertices $V$, we want to find a subset of\nvertices $S$, called centers, such that the maximum cluster radius is\nminimized. Moreover, each center in $S$ should satisfy some capacity\nconstraint, which could be an upper or lower bound on the number of vertices it\ncan serve. Capacitated $k$-center problems with one-sided bounds (upper or\nlower) have been well studied in previous work, and a constant factor\napproximation was obtained.\n  We are the first to study the capacitated center problem with both capacity\nlower and upper bounds (with or without outliers). We assume each vertex has a\nuniform lower bound and a non-uniform upper bound. For the case of opening\nexactly $k$ centers, we note that a generalization of a recent LP approach can\nachieve constant factor approximation algorithms for our problems. Our main\ncontribution is a simple combinatorial algorithm for the case where there is no\ncardinality constraint on the number of open centers. Our combinatorial\nalgorithm is simpler and achieves better constant approximation factor compared\nto the LP approach.'}, {'Quadratic Upper Bound for Recursive Teaching Dimension of Finite VC\n  Classes': 'In this work we study the quantitative relation between the recursive\nteaching dimension (RTD) and the VC dimension (VCD) of concept classes of\nfinite sizes. The RTD of a concept class $\\mathcal C \\subseteq \\{0, 1\\}^n$,\nintroduced by Zilles et al. (2011), is a combinatorial complexity measure\ncharacterized by the worst-case number of examples necessary to identify a\nconcept in $\\mathcal C$ according to the recursive teaching model.\n  For any finite concept class $\\mathcal C \\subseteq \\{0,1\\}^n$ with\n$\\mathrm{VCD}(\\mathcal C)=d$, Simon & Zilles (2015) posed an open problem\n$\\mathrm{RTD}(\\mathcal C) = O(d)$, i.e., is RTD linearly upper bounded by VCD?\nPreviously, the best known result is an exponential upper bound\n$\\mathrm{RTD}(\\mathcal C) = O(d \\cdot 2^d)$, due to Chen et al. (2016). In this\npaper, we show a quadratic upper bound: $\\mathrm{RTD}(\\mathcal C) = O(d^2)$,\nmuch closer to an answer to the open problem. We also discuss the challenges in\nfully solving the problem.'}, {'Omnipredictors for Constrained Optimization': 'The notion of omnipredictors (Gopalan, Kalai, Reingold, Sharan and Wieder\nITCS 2021), suggested a new paradigm for loss minimization. Rather than\nlearning a predictor based on a known loss function, omnipredictors can easily\nbe post-processed to minimize any one of a rich family of loss functions\ncompared with the loss of hypotheses in a class $\\mathcal C$. It has been shown\nthat such omnipredictors exist and are implied (for all convex and Lipschitz\nloss functions) by the notion of multicalibration from the algorithmic fairness\nliterature. In this paper, we introduce omnipredictors for constrained\noptimization and study their complexity and implications. The notion that we\nintroduce allows the learner to be unaware of the loss function that will be\nlater assigned as well as the constraints that will be later imposed, as long\nas the subpopulations that are used to define these constraints are known. We\nshow how to obtain omnipredictors for constrained optimization problems,\nrelying on appropriate variants of multicalibration. We also investigate the\nimplications of this notion when the constraints used are so-called group\nfairness notions.'}, {'Generative Models of Huge Objects': 'This work initiates the systematic study of explicit distributions that are\nindistinguishable from a single exponential-size combinatorial object. In this\nwe extend the work of Goldreich, Goldwasser and Nussboim (SICOMP 2010) that\nfocused on the implementation of huge objects that are indistinguishable from\nthe uniform distribution, satisfying some global properties (which they coined\ntruthfulness). Indistinguishability from a single object is motivated by the\nstudy of generative models in learning theory and regularity lemmas in graph\ntheory. Problems that are well understood in the setting of pseudorandomness\npresent significant challenges and at times are impossible when considering\ngenerative models of huge objects.\n  We demonstrate the versatility of this study by providing a learning\nalgorithm for huge indistinguishable objects in several natural settings\nincluding: dense functions and graphs with a truthfulness requirement on the\nnumber of ones in the function or edges in the graphs, and a version of the\nweak regularity lemma for sparse graphs that satisfy some global properties.\nThese and other results generalize basic pseudorandom objects as well as\nnotions introduced in algorithmic fairness. The results rely on notions and\ntechniques from a variety of areas including learning theory, complexity\ntheory, cryptography, and game theory.'}]","Title: MultiGroup Robustness in Machine Learning: Algorithms, Definitions and Evaluations

Abstract

Background: The modern machine learning environment is marred by distribution shifts impacting sensitivity and reliability. The livelihood of individuals and systems is particularly at risk when small-scale, targeted attacks undermine data integrity within sensitive subpopulations of real-world datasets. 

Objective: Our focus is to develop a robust machine learning algorithm capable of supporting multi-group fairness, ensuring reliable predictions across overlapping subgroups in the face of data corruption that does not adhere to traditional distributional assumptions.

Innovations: We introduce the concept of MultiGroup Robustness to address subpopulation attacks, not relying on distributional assumptions. We develop an algorithm using iterative auditing to ensure predictors converge to multiaccuracy on the empirical distribution, maintaining fairness against targeted subgroups under label flipping, addition, or deletion attacks. This approach contrasts with the conventional multiaccuracy algorithms that minimize loss through sampling, but we integrate an implicit inclusion of the whole dataset at each iteration for safety assurance.

Methods: The algorithmic framework employs a pipeline of training, followed by post-processing steps that significantly reduce the empirical loss of the predictor. We prove that the post-processed predictors achieve multigroup robustness under empirical multiaccuracy and uniform convergence. Loss minimization is guaranteed even with the addition of fresh data post-processing, ensuring robustness even in the absence of original training data.

Results: Our experiments on the Folktables-Income dataset, a modern take on the UCI Adult dataset, exhibit substantial improvements with the multigroup-robust algorithm when facing different attacks that target specific subgroups. The performances of baselines like logistic regression (LR) and two-layer neural networks (MLP) demonstrate the need and versatility of our solution, especially under noisy data addition and label shifts.

Contributions: This work introduces a robust notion that is sensitive to the specific subpopulations in the training data. We identify the method's capability to be applicable even when groups of valid, uncorrupted data are not immediately available. The enhancement of multi-accuracy and preservation of base model's accuracy against targeted subgroups under various attacks, especially addition and label flipping attacks, is significant. 

Applications: The robust machine learning models that our research outputs could potentially be employed in privacy-sensitive spheres such as healthcare (patient stratification), finance (credit scoring), social services (job placement), and education (scholarship allocation), ensuring fair and accurate outcomes across varying demographic groups, curbing any predictive biases or errors due to data corruption or manipulation."
"Widely deployed large language models (LLMs) can produce convincing yet
incorrect outputs, potentially misleading users who may rely on them as if they
were correct. To reduce such overreliance, there have been calls for LLMs to
communicate their uncertainty to end users. However, there has been little
empirical work examining how users perceive and act upon LLMs' expressions of
uncertainty. We explore this question through a large-scale, pre-registered,
human-subject experiment (N=404) in which participants answer medical questions
with or without access to responses from a fictional LLM-infused search engine.
Using both behavioral and self-reported measures, we examine how different
natural language expressions of uncertainty impact participants' reliance,
trust, and overall task performance. We find that first-person expressions
(e.g., ""I'm not sure, but..."") decrease participants' confidence in the system
and tendency to agree with the system's answers, while increasing participants'
accuracy. An exploratory analysis suggests that this increase can be attributed
to reduced (but not fully eliminated) overreliance on incorrect answers. While
we observe similar effects for uncertainty expressed from a general perspective
(e.g., ""It's not clear, but...""), these effects are weaker and not
statistically significant. Our findings suggest that using natural language
expressions of uncertainty may be an effective approach for reducing
overreliance on LLMs, but that the precise language used matters. This
highlights the importance of user testing before deploying LLMs at scale.","[{'Cleaning and Structuring the Label Space of the iMet Collection 2020': 'The iMet 2020 dataset is a valuable resource in the space of fine-grained art\nattribution recognition, but we believe it has yet to reach its true potential.\nWe document the unique properties of the dataset and observe that many of the\nattribute labels are noisy, more than is implied by the dataset description.\nOftentimes, there are also semantic relationships between the labels (e.g.,\nidentical, mutual exclusion, subsumption, overlap with uncertainty) which we\nbelieve are underutilized. We propose an approach to cleaning and structuring\nthe iMet 2020 labels, and discuss the implications and value of doing so.\nFurther, we demonstrate the benefits of our proposed approach through several\nexperiments. Our code and cleaned labels are available at\nhttps://github.com/sunniesuhyoung/iMet2020cleaned.'}, {'Deformable Style Transfer': 'Both geometry and texture are fundamental aspects of visual style. Existing\nstyle transfer methods, however, primarily focus on texture, almost entirely\nignoring geometry. We propose deformable style transfer (DST), an\noptimization-based approach that jointly stylizes the texture and geometry of a\ncontent image to better match a style image. Unlike previous geometry-aware\nstylization methods, our approach is neither restricted to a particular domain\n(such as human faces), nor does it require training sets of matching\nstyle/content pairs. We demonstrate our method on a diverse set of content and\nstyle images including portraits, animals, objects, scenes, and paintings. Code\nhas been made publicly available at https://github.com/sunniesuhyoung/DST.'}, {""[Re] Don't Judge an Object by Its Context: Learning to Overcome\n  Contextual Bias"": ""Singh et al. (2020) point out the dangers of contextual bias in visual\nrecognition datasets. They propose two methods, CAM-based and feature-split,\nthat better recognize an object or attribute in the absence of its typical\ncontext while maintaining competitive within-context accuracy. To verify their\nperformance, we attempted to reproduce all 12 tables in the original paper,\nincluding those in the appendix. We also conducted additional experiments to\nbetter understand the proposed methods, including increasing the regularization\nin CAM-based and removing the weighted loss in feature-split. As the original\ncode was not made available, we implemented the entire pipeline from scratch in\nPyTorch 1.7.0. Our implementation is based on the paper and email exchanges\nwith the authors. We found that both proposed methods in the original paper\nhelp mitigate contextual bias, although for some methods, we could not\ncompletely replicate the quantitative results in the paper even after\ncompleting an extensive hyperparameter search. For example, on COCO-Stuff,\nDeepFashion, and UnRel, our feature-split model achieved an increase in\naccuracy on out-of-context images over the standard baseline, whereas on AwA,\nwe saw a drop in performance. For the proposed CAM-based method, we were able\nto reproduce the original paper's results to within 0.5$\\%$ mAP. Our\nimplementation can be found at\nhttps://github.com/princetonvisualai/ContextualBias.""}, {'Fair Attribute Classification through Latent Space De-biasing': 'Fairness in visual recognition is becoming a prominent and critical topic of\ndiscussion as recognition systems are deployed at scale in the real world.\nModels trained from data in which target labels are correlated with protected\nattributes (e.g., gender, race) are known to learn and exploit those\ncorrelations. In this work, we introduce a method for training accurate target\nclassifiers while mitigating biases that stem from these correlations. We use\nGANs to generate realistic-looking images, and perturb these images in the\nunderlying latent space to generate training data that is balanced for each\nprotected attribute. We augment the original dataset with this perturbed\ngenerated data, and empirically demonstrate that target classifiers trained on\nthe augmented dataset exhibit a number of both quantitative and qualitative\nbenefits. We conduct a thorough evaluation across multiple target labels and\nprotected attributes in the CelebA dataset, and provide an in-depth analysis\nand comparison to existing literature in the space.'}, {'UFO: A unified method for controlling Understandability and Faithfulness\n  Objectives in concept-based explanations for CNNs': ""Concept-based explanations for convolutional neural networks (CNNs) aim to\nexplain model behavior and outputs using a pre-defined set of semantic concepts\n(e.g., the model recognizes scene class ``bedroom'' based on the presence of\nconcepts ``bed'' and ``pillow''). However, they often do not faithfully (i.e.,\naccurately) characterize the model's behavior and can be too complex for people\nto understand. Further, little is known about how faithful and understandable\ndifferent explanation methods are, and how to control these two properties. In\nthis work, we propose UFO, a unified method for controlling Understandability\nand Faithfulness Objectives in concept-based explanations. UFO formalizes\nunderstandability and faithfulness as mathematical objectives and unifies most\nexisting concept-based explanations methods for CNNs. Using UFO, we\nsystematically investigate how explanations change as we turn the knobs of\nfaithfulness and understandability. Our experiments demonstrate a\nfaithfulness-vs-understandability tradeoff: increasing understandability\nreduces faithfulness. We also provide insights into the ``disagreement\nproblem'' in explainable machine learning, by analyzing when and how\nconcept-based explanations disagree with each other.""}, {'ELUDE: Generating interpretable explanations via a decomposition into\n  labelled and unlabelled features': 'Deep learning models have achieved remarkable success in different areas of\nmachine learning over the past decade; however, the size and complexity of\nthese models make them difficult to understand. In an effort to make them more\ninterpretable, several recent works focus on explaining parts of a deep neural\nnetwork through human-interpretable, semantic attributes. However, it may be\nimpossible to completely explain complex models using only semantic attributes.\nIn this work, we propose to augment these attributes with a small set of\nuninterpretable features. Specifically, we develop a novel explanation\nframework ELUDE (Explanation via Labelled and Unlabelled DEcomposition) that\ndecomposes a model\'s prediction into two parts: one that is explainable through\na linear combination of the semantic attributes, and another that is dependent\non the set of uninterpretable features. By identifying the latter, we are able\nto analyze the ""unexplained"" portion of the model, obtaining insights into the\ninformation used by the model. We show that the set of unlabelled features can\ngeneralize to multiple models trained with the same feature space and compare\nour work to two popular attribute-oriented methods, Interpretable Basis\nDecomposition and Concept Bottleneck, and discuss the additional insights ELUDE\nprovides.'}, {'Overlooked factors in concept-based explanations: Dataset choice,\n  concept learnability, and human capability': 'Concept-based interpretability methods aim to explain deep neural network\nmodel predictions using a predefined set of semantic concepts. These methods\nevaluate a trained model on a new, ""probe"" dataset and correlate model\npredictions with the visual concepts labeled in that dataset. Despite their\npopularity, they suffer from limitations that are not well-understood and\narticulated by the literature. In this work, we analyze three commonly\noverlooked factors in concept-based explanations. First, the choice of the\nprobe dataset has a profound impact on the generated explanations. Our analysis\nreveals that different probe datasets may lead to very different explanations,\nand suggests that the explanations are not generalizable outside the probe\ndataset. Second, we find that concepts in the probe dataset are often less\nsalient and harder to learn than the classes they claim to explain, calling\ninto question the correctness of the explanations. We argue that only visually\nsalient concepts should be used in concept-based explanations. Finally, while\nexisting methods use hundreds or even thousands of concepts, our human studies\nreveal a much stricter upper bound of 32 concepts or less, beyond which the\nexplanations are much less practically useful. We make suggestions for future\ndevelopment and analysis of concept-based interpretability methods. Code for\nour analysis and user interface can be found at\n\\url{https://github.com/princetonvisualai/OverlookedFactors}'}, {""Allowing humans to interactively guide machines where to look does not\n  always improve human-AI team's classification accuracy"": ""Via thousands of papers in Explainable AI (XAI), attention maps\n\\cite{vaswani2017attention} and feature importance maps \\cite{bansal2020sam}\nhave been established as a common means for finding how important each input\nfeature is to an AI's decisions. It is an interesting, unexplored question\nwhether allowing users to edit the feature importance at test time would\nimprove a human-AI team's accuracy on downstream tasks. In this paper, we\naddress this question by leveraging CHM-Corr, a state-of-the-art, ante-hoc\nexplainable classifier \\cite{taesiri2022visual} that first predicts patch-wise\ncorrespondences between the input and training-set images, and then bases on\nthem to make classification decisions. We build CHM-Corr++, an interactive\ninterface for CHM-Corr, enabling users to edit the feature importance map\nprovided by CHM-Corr and observe updated model decisions. Via CHM-Corr++, users\ncan gain insights into if, when, and how the model changes its outputs,\nimproving their understanding beyond static explanations. However, our study\nwith 18 expert users who performed 1,400 decisions finds no statistical\nsignificance that our interactive approach improves user accuracy on CUB-200\nbird image classification over static explanations. This challenges the\nhypothesis that interactivity can boost human-AI team accuracy and raises needs\nfor future research. We open-source CHM-Corr++, an interactive tool for editing\nimage classifier attention (see an interactive demo here:\nhttp://137.184.82.109:7080/). We release code and data on github:\nhttps://github.com/anguyen8/chm-corr-interactive.""}, {""Humans, AI, and Context: Understanding End-Users' Trust in a Real-World\n  Computer Vision Application"": ""Trust is an important factor in people's interactions with AI systems.\nHowever, there is a lack of empirical studies examining how real end-users\ntrust or distrust the AI system they interact with. Most research investigates\none aspect of trust in lab settings with hypothetical end-users. In this paper,\nwe provide a holistic and nuanced understanding of trust in AI through a\nqualitative case study of a real-world computer vision application. We report\nfindings from interviews with 20 end-users of a popular, AI-based bird\nidentification app where we inquired about their trust in the app from many\nangles. We find participants perceived the app as trustworthy and trusted it,\nbut selectively accepted app outputs after engaging in verification behaviors,\nand decided against app adoption in certain high-stakes scenarios. We also find\ndomain knowledge and context are important factors for trust-related assessment\nand decision-making. We discuss the implications of our findings and provide\nrecommendations for future research on trust in AI.""}, {'HIVE: Evaluating the Human Interpretability of Visual Explanations': 'As AI technology is increasingly applied to high-impact, high-risk domains,\nthere have been a number of new methods aimed at making AI models more human\ninterpretable. Despite the recent growth of interpretability work, there is a\nlack of systematic evaluation of proposed techniques. In this work, we\nintroduce HIVE (Human Interpretability of Visual Explanations), a novel human\nevaluation framework that assesses the utility of explanations to human users\nin AI-assisted decision making scenarios, and enables falsifiable hypothesis\ntesting, cross-method comparison, and human-centered evaluation of visual\ninterpretability methods. To the best of our knowledge, this is the first work\nof its kind. Using HIVE, we conduct IRB-approved human studies with nearly 1000\nparticipants and evaluate four methods that represent the diversity of computer\nvision interpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our\nresults suggest that explanations engender human trust, even for incorrect\npredictions, yet are not distinct enough for users to distinguish between\ncorrect and incorrect predictions. We open-source HIVE to enable future studies\nand encourage more human-centered approaches to interpretability research.'}]","### Abstract

In the era of deploying large language models (LLMs) in everyday applications, protecting user reliance and trust is paramount yet fraught with challenges. This research explores how LLMs and LLM-infused applications can express uncertainty to minimize overreliance and trust distortions. 

The primary objective is to measurably impact user trust and reliance by examining the effects of uncertainty expression in LLM-generated responses. Innovations include utilizing quantitative and qualitative measures to assess trust, reliance, and the nuanced impact of different uncertainty expression perspectives. Methodologically, we apply an experiment with varied and controlled uncertainty presentations.

Key findings showcase that conveying uncertainty through LLM responses helps decrease reliance and manage trust calibration. These results support that natural language expressions of uncertainty can be effective safeguards against overreliance in LLM-assisted decision-making contexts.

Contributively, this research advances the field by providing empirical evidence on uncertainty communication strategies, essential for the deployment of responsible and ethical LLM-infused technologies. Beyond immediate applications, its implications underscore the importance of incorporating high-quality human evaluations and experiences into the development cycle of AI systems to mitigate potential misuse and improve societal impact.

This study offers a foundational step for practitioners and policymakers, enabling informed decisions on measures to improve reliability, transparency, and ethical use of AI technologies, particularly as they increasingly interpenetrate everyday life decision-making processes."
"Bias models relating the dark matter field to the spatial distribution of
halos are widely used in cosmological analyses. Many such models predict halos
purely from the local matter density, an assumption which has not been verified
in a model-agnostic setting. Bias models in perturbation theory require the
inclusion of other local properties, but it is not clear whether this extends
to non-perturbative approaches. We assess the validity of the assumption that
only the local dark matter density can be used to predict the number density of
halos in a model-independent way and in the non-perturbative regime. Utilising
$N$-body simulations, we introduce a test wherein we study the properties of
the halo counts field after spatial voxels with near-equal dark matter density
have been permuted. If local-in-matter-density (LIMD) biasing were valid, the
statistical properties of the permuted and un-permuted fields would be
indistinguishable since both are equally fair draws of the stochastic biasing
model. For voxels of side length $\sim4-60\,h^{-1}{\rm\,Mpc}$ and for halos
less massive than $\sim10^{15}\,h^{-1}{\rm\,M_\odot}$, the permuted halo field
has significantly too much power on large scales compared to the truth. We
interpret this as due to LIMD models removing small-scale power by not
modelling correlations between neighbouring voxels. Since the permutation
conserves the total variance of the halo counts field, large-scale power is
substantially boosted to compensate. This conclusion is robust to the choice of
initial conditions and cosmology. LIMD halo biasing cannot, therefore,
reproduce the distribution of halos across a large range of scales and halo
masses, no matter how complex the model. To reproduce this distribution
accurately, one must allow the biasing to be a function of other quantities
and/or remove the assumption that neighbouring voxels are statistically
independent.","[{'Modeling and testing screening mechanisms in the laboratory and in space': 'The non-linear dynamics of scalar fields coupled to matter and gravity can\nlead to remarkable density-dependent screening effects. In this short review we\npresent the main classes of screening mechanisms, and discuss their tests in\nlaboratory and astrophysical systems. We particularly focus on reviewing\nnumerical and technical aspects involved in modeling the non-linear dynamics of\nscreening. In this review, we focus on tests using laboratory experiments and\nastrophysical systems, such as stars, galaxies and dark matter halos.'}, {'No evidence for p- or d-wave dark matter annihilation from local\n  large-scale structure': 'If dark matter annihilates into standard model particles with a cross-section\nwhich is velocity dependent, then Local Group dwarf galaxies will not be the\nbest place to search for the resulting gamma ray emission. A greater flux would\nbe produced by more distant and massive halos, with larger velocity\ndispersions. We construct full-sky predictions for the gamma-ray emission from\ngalaxy- and cluster-mass halos within $\\sim 200 \\, {\\mathrm{Mpc}}$ using a\nsuite of constrained $N$-body simulations (CSiBORG) based on the Bayesian\nOrigin Reconstruction from Galaxies algorithm. Comparing to observations from\nthe Fermi Large Area Telescope and marginalising over reconstruction\nuncertainties and other astrophysical contributions to the flux, we obtain\nconstraints on the cross-section which are two (seven) orders of magnitude\ntighter than those obtained from dwarf spheroidals for $p$-wave ($d$-wave)\nannihilation. We find no evidence for either type of annihilation from dark\nmatter particles with masses in the range $m_\\chi = 2-500 \\,\n{\\mathrm{GeV}}/c^2$, for any channel. As an example, for annihilations\nproducing bottom quarks with $m_\\chi = 10 \\, {\\mathrm{GeV}}/c^2$, we find\n$a_{1} < 2.4 \\times 10^{-21} \\, {\\mathrm{cm^3 s^{-1}}}$ and $a_{2} < 3.0 \\times\n10^{-18} \\, {\\mathrm{cm^3 s^{-1}}}$ at 95% confidence, where the product of the\ncross-section, $\\sigma$, and relative particle velocity, $v$, is given by\n$\\sigma v = a_\\ell (v/c)^{2\\ell}$ and $\\ell=1, 2$ for $p$-, $d$-wave\nannihilation, respectively. Our bounds, although failing to exclude the thermal\nrelic cross-section for velocity-dependent annihilation channels, are among the\ntightest to date.'}, {'Priors for symbolic regression': 'When choosing between competing symbolic models for a data set, a human will\nnaturally prefer the ""simpler"" expression or the one which more closely\nresembles equations previously seen in a similar context. This suggests a\nnon-uniform prior on functions, which is, however, rarely considered within a\nsymbolic regression (SR) framework. In this paper we develop methods to\nincorporate detailed prior information on both functions and their parameters\ninto SR. Our prior on the structure of a function is based on a $n$-gram\nlanguage model, which is sensitive to the arrangement of operators relative to\none another in addition to the frequency of occurrence of each operator. We\nalso develop a formalism based on the Fractional Bayes Factor to treat\nnumerical parameter priors in such a way that models may be fairly compared\nthough the Bayesian evidence, and explicitly compare Bayesian, Minimum\nDescription Length and heuristic methods for model selection. We demonstrate\nthe performance of our priors relative to literature standards on benchmarks\nand a real-world dataset from the field of cosmology.'}, {'Constraints on galileons from the positions of supermassive black holes': 'Galileons are scalar field theories which obey the Galileon symmetry $\\varphi\n\\to \\varphi + b + c_\\mu x^\\mu$ and are capable of self-acceleration if they\nhave an inverted sign for the kinetic term. These theories violate the Strong\nEquivalence Principle, such that black holes (BHs) do not couple to the\nGalileon field, whereas non-relativistic objects experience a fifth force with\nstrength $\\Delta G / G_{\\rm N}$ relative to gravity. For galaxies falling down\na gradient in the Galileon field, this results in an offset between the centre\nof the galaxy and its host supermassive BH. We reconstruct the local\ngravitational and Galileon fields through a suite of constrained N-body\nsimulations (which we dub CSiBORG) and develop a Monte Carlo-based forward\nmodel for these offsets on a galaxy-by-galaxy basis. Using the measured offset\nbetween the optical centre and active galactic nucleus of 1916 galaxies from\nthe literature, propagating uncertainties in the input quantities and\nmarginalising over an empirical noise model describing astrophysical and\nobservational noise, we constrain the Galileon coupling to be $\\Delta G /\nG_{\\rm N} < 0.16$ at $1\\sigma$ confidence for Galileons with crossover scale\n$r_{\\rm C} \\gtrsim H_0^{-1}$.'}, {'Calibrating galaxy formation effects in galactic tests of fundamental\n  physics': ""Galactic scale tests have proven to be powerful tools in constraining\nfundamental physics in previously under-explored regions of parameter space.\nThe astrophysical regime which they probe is inherently complicated, and the\ninference methods used to make these constraints should be robust to baryonic\neffects. Previous analyses have assumed simple empirical models for\nastrophysical noise without detailed calibration or justification. We outline a\nframework for assessing the reliability of such methods by constructing and\ntesting more advanced baryonic models using cosmological hydrodynamical\nsimulations. As a case study, we use the Horizon-AGN simulation to investigate\nwarping of stellar disks and offsets between gas and stars within galaxies,\nwhich are powerful probes of screened fifth forces. We show that the degree of\n`U'-shaped warping of galaxies is well modelled by Gaussian random noise, but\nthat the magnitude of the gas-star offset is correlated with the virial radius\nof the host halo. By incorporating this correlation we confirm recent results\nruling out astrophysically relevant Hu-Sawicki $f(R)$ gravity, and identify a\n$\\sim 30\\%$ systematic uncertainty due to baryonic physics. Such an analysis\nmust be performed case-by-case for future galactic tests of fundamental\nphysics.""}, {'Exhaustive Symbolic Regression': 'Symbolic Regression (SR) algorithms attempt to learn analytic expressions\nwhich fit data accurately and in a highly interpretable manner. Conventional SR\nsuffers from two fundamental issues which we address here. First, these methods\nsearch the space stochastically (typically using genetic programming) and hence\ndo not necessarily find the best function. Second, the criteria used to select\nthe equation optimally balancing accuracy with simplicity have been variable\nand subjective. To address these issues we introduce Exhaustive Symbolic\nRegression (ESR), which systematically and efficiently considers all possible\nequations -- made with a given basis set of operators and up to a specified\nmaximum complexity -- and is therefore guaranteed to find the true optimum (if\nparameters are perfectly optimised) and a complete function ranking subject to\nthese constraints. We implement the minimum description length principle as a\nrigorous method for combining these preferences into a single objective. To\nillustrate the power of ESR we apply it to a catalogue of cosmic chronometers\nand the Pantheon+ sample of supernovae to learn the Hubble rate as a function\nof redshift, finding $\\sim$40 functions (out of 5.2 million trial functions)\nthat fit the data more economically than the Friedmann equation. These\nlow-redshift data therefore do not uniquely prefer the expansion history of the\nstandard model of cosmology. We make our code and full equation sets publicly\navailable.'}, {'The scatter in the galaxy-halo connection: a machine learning analysis': 'We apply machine learning, a powerful method for uncovering complex\ncorrelations in high-dimensional data, to the galaxy-halo connection of\ncosmological hydrodynamical simulations. The mapping between galaxy and halo\nvariables is stochastic in the absence of perfect information, but conventional\nmachine learning models are deterministic and hence cannot capture its\nintrinsic scatter. To overcome this limitation, we design an ensemble of neural\nnetworks with a Gaussian loss function that predict probability distributions,\nallowing us to model statistical uncertainties in the galaxy-halo connection as\nwell as its best-fit trends. We extract a number of galaxy and halo variables\nfrom the Horizon-AGN and IllustrisTNG100-1 simulations and quantify the extent\nto which knowledge of some subset of one enables prediction of the other. This\nallows us to identify the key features of the galaxy-halo connection and\ninvestigate the origin of its scatter in various projections. We find that\nwhile halo properties beyond mass account for up to 50 per cent of the scatter\nin the halo-to-stellar mass relation, the prediction of stellar half-mass\nradius or total gas mass is not substantially improved by adding further halo\nproperties. We also use these results to investigate semi-analytic models for\ngalaxy size in the two simulations, finding that assumptions relating galaxy\nsize to halo size or spin are not successful.'}, {'On the functional form of the radial acceleration relation': 'We apply a new method for learning equations from data -- Exhaustive Symbolic\nRegression (ESR) -- to late-type galaxy dynamics as encapsulated in the radial\nacceleration relation (RAR). Relating the centripetal acceleration due to\nbaryons, $g_\\text{bar}$, to the total dynamical acceleration, $g_\\text{obs}$,\nthe RAR has been claimed to manifest a new law of nature due to its regularity\nand tightness, in agreement with Modified Newtonian Dynamics (MOND). Fits to\nthis relation have been restricted by prior expectations to particular\nfunctional forms, while ESR affords an exhaustive and nearly prior-free search\nthrough functional parameter space to identify the equations optimally trading\naccuracy with simplicity. Working with the SPARC data, we find the best\nfunctions typically satisfy $g_\\text{obs} \\propto g_\\text{bar}$ at high\n$g_\\text{bar}$, although the coefficient of proportionality is not clearly\nunity and the deep-MOND limit $g_\\text{obs} \\propto \\sqrt{g_\\text{bar}}$ as\n$g_\\text{bar} \\to 0$ is little evident at all. By generating mock data\naccording to MOND with or without the external field effect, we find that\nsymbolic regression would not be expected to identify the generating function\nor reconstruct successfully the asymptotic slopes. We conclude that the limited\ndynamical range and significant uncertainties of the SPARC RAR preclude a\ndefinitive statement of its functional form, and hence that this data alone can\nneither demonstrate nor rule out law-like gravitational behaviour.'}, {'Constraints on dark matter and astrophysics from tomographic\n  $γ$-ray cross-correlations': 'We study the cross-correlation between maps of the unresolved $\\gamma$-ray\nbackground constructed from the 12-year data release of the Fermi Large-Area\nTelescope, and the overdensity of galaxies in the redshift range $z\\lesssim0.4$\nas measured by the 2MASS Photometric Redshift survey and the WISE-SuperCOSMOS\nphotometric survey. A signal is detected at the $8-10\\sigma$ level, which we\ninterpret in terms of both astrophysical $\\gamma$-ray sources, and WIMP dark\nmatter decay and annihilation. The sensitivity achieved allows us to\ncharacterise the energy and redshift dependence of the signal, and we show that\nthe latter is incompatible with a pure dark matter origin. We thus use our\nmeasurement to place an upper bound on the WIMP decay rate and the annihilation\ncross-section, finding constraints that are competitive with those found in\nother analyses. Our analysis is based on the extraction of clean\nmodel-independent observables that can then be used to constrain arbitrary\nastrophysical and particle physics models. In this sense we produce\nmeasurements of the $\\gamma$-ray emissivity as a function of redshift and\nrest-frame energy $\\epsilon$, and of a quantity $F(\\epsilon)$ encapsulating all\nWIMP parameters relevant for dark matter decay or annihilation. We make these\nmeasurements, together with a full account of their statistical uncertainties,\npublicly available.'}, {'Spatially offset black holes in the Horizon-AGN simulation and\n  comparison to observations': 'We study the displacements between the centres of galaxies and their\nsupermassive black holes (BHs) in the cosmological hydrodynamical simulation\nHorizon-AGN, and in a variety of observations from the literature. The BHs in\nHorizon-AGN feel a sub-grid dynamical friction force, sourced by the\nsurrounding gas, which prevents recoiling BHs being ejected from the galaxy. We\nfind that i) the fraction of spatially offset BHs increases with cosmic time,\nii) BHs live on prograde orbits in the plane of the galaxy with an orbital\nradius that decays with time but stalls near $z=0$, and iii) the magnitudes of\noffsets from the galaxy centres are substantially larger in the simulation than\nin observations. We attribute the stalling of the infall and excessive offset\nmagnitudes to the fact that dynamical friction from stars and dark matter is\nnot modelled in the simulation, and hence provide a way to improve the black\nhole dynamics of future simulations.'}]","Title: Progress Towards Accurate Halo Biasing: Impact of Local Matter Density Assumptions in Cosmological Analysis

Abstract:
This study critically evaluates the fundamental assumption of local-in-matter-density (LIMD) halo biasing in cosmological analyses. With the goal of understanding the statistical relation between luminous celestial objects (tracers) and underlying matter density fields, researchers traditionally rely on the LIMD approximation to predict halo distribution. Our research aims to enhance the accuracy and robustness of this prediction by generalizing the biasing model to include observable correlations within the field.

Innovations in our research extend beyond traditional local density dependence by incorporating a probability distribution that depends on the dark matter density within comoving space voxels, allowing for the exploration of any continuous parameter function as a function of density. This approach enables a rigorous testing of the independence assumption of LIMD biasing in the statistical properties of halo-count fields without specifying the particular functional form.

Methodologically, we employ voxel permutations to assess the validity of LIMD assumptions, revealing discrepancies across scales that cannot be explained by the static local bias paradigm. Using N-body simulations, we construct bias models based on this generalized framework, demonstrating their ability to capture complex structural dependencies not addressed by LIMD.

This work significantly advances our understanding of halo distribution dynamics through improved biasing models and robustness testing techniques. It diverges from traditionally independently drawn samples by revealing that the halo number density depends on the local dark matter density but not solely through this factor. Such a framework might be rigorously applied in future cosmological data analysis, potentially yielding enriched insight into the large scale structure of the universe beyond LDIM assumptions.

Implementing these findings could significantly enhance precision in simulations and predictions of cosmological parameters, driving our comprehension of gravitational structure formation and guiding future observational surveys to elucidate the cosmic web's intricate fabric."
"The radioactive power generated by materials within the ejecta of a
binary-neutron-star (BNS) merger powers an optical transient known as a
kilonova. When the central remnant of a BNS merger is a long-lived magnetar, it
continuously produces a highly magnetized wind, altering both the dynamics and
temperature of the ejecta, leading to the expected emergence of an engine-fed
kilonova. In the first paper of this series, we conducted a detailed study of
the dynamics of wind-ejecta interaction and the efficiency of energy injection
through shocks. In this work, we combine this dynamical evolution with both
shock-heating and additional X-ray irradiation to model photon diffusion within
a constant-opacity ejecta. By calculating the radiation, we obtain the light
curve and spectral energy distribution (SED). Our findings reveal that, with
energy injection, a blue bump typically appears in the early stages ($\lesssim
1$ day). Furthermore, if the magnetar has not spun down by that time, a
brightening in the later stages occurs. Despite this, in a large parameter
space, the expected luminosity of the engine-fed kilonova is not significantly
higher than the typical r-process kilonova due to limited heating efficiency.
The SED of engine-fed kilonovae peaks in the relatively blue band in the early
stages and evolves towards the red, but at a slower rate compared to the
typical r-process kilonova.","[{'Inverse compton scattered merger-nova: late X-ray counterpart of\n  gravitational wave signals from NS-NS/BH mergers': 'The recent observations of GW170817 and its electromagnetic (EM) counterparts\nshow that double neutron star mergers could lead to rich and bright EM\nemissions. Recent numerical simulations suggest that neutron star and neutron\nstar/black hole (NS-NS/BH) mergers would leave behind a central remnant\nsurrounded by a mildly isotropic ejecta. The central remnant could launch a\ncollimated jet and when the jet propagating through the ejecta, a mildly\nrelativistic cocoon would be formed and the interaction between the cocoon and\nthe ambient medium would accelerate electrons via external shock in a wide\nangle. So that the merger-nova photons (i.e., thermal emission from the ejecta)\nwould be scattered into higher frequency via inverse compton (IC) process when\nthey propagating through the cocoon shocked region. We find that the IC\nscattered component peaks at X-ray band and it will reach its peak luminosity\nin order of days (simultaneously with the merger-nova emission). With current\nX-ray detectors, such a late X-ray component could be detected out to 200 Mpc,\ndepending on the merger remnant properties. It could serve as an important\nelectromagnetic counterpart of gravitational wave signals from NS-NS/BH\nmergers. Nevertheless, simultaneous detection of such a late X-ray signal and\nthe merger-nova signal could shed light on the cocoon properties and the\nconcrete structure of the jet.'}, {'A Mechanical Model for Magnetized Relativistic Blastwaves': 'The evolution of a relativistic blastwave is usually delineated under the\nassumption of pressure balance between forward- and reverse-shocked regions.\nHowever, such a treatment usually violates the energy conservation law, and is\ninconsistent with existing MHD numerical simulation results. A mechanical model\nof non-magnetized blastwaves was proposed in previous work to solve the\nproblem. In this paper, we generalize the mechanical model to the case of a\nblastwave driven by an ejecta with an arbitrary magnetization parameter\n$\\sigma_{\\rm ej}$. We test our modified mechanical model by considering a\nlong-lasting magnetized ejecta and found that it is much better than the\npressure-balance treatment in terms of energy conservation. For a constant\ncentral engine wind luminosity $L_{\\rm ej} = 10^{47}{\\rm erg~s^{-1}}$ and\n$\\sigma_{\\rm ej} < 10$, the deviation from energy conservation is negligibly\nsmall at small radii, but only reaches less than $25\\%$ even at $10^{19}{\\rm\ncm}$ from the central engine. For a finite life time of the central engine, the\nreverse shock crosses the magnetized ejecta earlier for the ejecta with a\nhigher $\\sigma_{\\rm ej}$, which is consistent with previous analytical and\nnumerical results. In general, the mechanical model is more precise than the\ntraditional analytical models with results closer to those of numerical\nsimulations.'}, {'On the Binary-Neutron-Star Post-Merger Magnetar Origin of XRT 210423': 'XRT 201423 is an X-ray transient with a nearly flat plateau lasting 4.1 ks\nfollowed by a steep decay. This feature indicates that it might come from a\nmagnetar formed through a binary neutron star merger, similar to CDF-S XT2 and\nas predicted as a type of electromagnetic counterpart of binary neutron star\nmergers. We test the compliance of the data with this model and use the\nobserved duration and flux of the X-ray signal as well as upper limits of\noptical emission to pose constraints on the parameters of the underlying\nputative magnetar. Both the free-zone and trapped-zone geometric configurations\nare considered. We find that the data are generally consistent with such a\nmodel. The surface dipolar magnetic field and the ellipticity of the magnetar\nshould satisfy $B_p < 7\\times 10^{14}{\\rm G}$ ($B_p < 4.9 \\times 10^{14}{\\rm\nG}$) and $\\epsilon < 1.5 \\times 10^{-3}$ ($\\epsilon < 1.1 \\times 10^{-3}$)\nunder free zone (trapped zone) configurations, respectively. An upper limit on\nthe distance (e.g. $z < 0.55$ with $\\eta_x = 10^{-4}$ or $z < 3.5$ with $\\eta_x\n= 10^{-2}$) can be derived from the X-ray data which depends on the X-ray\ndissipation efficiency $\\eta_x$ of the spin-down luminosity. The non-detection\nof an optical counterpart places a conservative lower limit on the distance of\nthe source, i.e. $z > 0.045$ regardless of the geometric configuration.'}, {'Model constraints based on the IceCube neutrino non-detection of GRB\n  221009A': 'GRB 221009A is a bright Gamma-ray burst (GRB) with isotropic energy being\nlarger than $10^{54} ~{\\rm ergs}$. Its fairly low redshift makes it a promising\ncandidate for high energy neutrino detection. However, a neutrino search for\nthis GRB reported by the IceCube collaboration yielded a null result. In this\npaper, we utilize the upper limit from IceCube observation to test different\nGRB prompt emission models. We find that, at least for this specific burst, the\ndissipative photosphere model could be ruled out in a large parameter space.\nThe internal shock model can survive only with a large bulk motion Lorentz\nfactor $\\Gamma$, where the most stringent and conservative constraints are\n$\\Gamma > \\sim 450$ and $\\Gamma > \\sim 200$, respectively. Also, the ratio of\nthe total dissipated energy that goes into the protons and electrons\n($\\epsilon_p / \\epsilon_e$) can be constrained with a given $\\Gamma$. For\n$\\Gamma < 400$, $\\epsilon_p / \\epsilon_e < 10$ is required. For the\nInternal-collision-induced Magnetic Reconnection and Turbulence (ICMART) model,\nthe constraint from GRB 221009A is modest. Under ICMART model, only for extreme\nsituations when most dissipated energy deposit into protons and all accelerated\nprotons are suitable for producing neutrinos, a slightly large bulk motion\n($\\Gamma > \\sim 250$) is required.'}, {'What constraints on the neutron star maximum mass can one pose from\n  GW170817 observations?': 'The post-merger product of the first binary neutron star merger event\ndetected in gravitational waves, GW170817, depends on neutron star equation of\nstate (EoS) and is not well determined. We generally discuss the constraints\none may pose on the maximum mass of a non-spinning neutron star, $M_{\\rm TOV}$,\nbased on the observations and some EoS-independent universal relations of\nrapidly-spinning neutron stars. If the merger product is a black hole after a\nbrief hypermassive neutron star (HMNS) phase, we derive $M_{\\rm TOV} <\n2.09^{+0.11}_{-0.09}(^{+0.06}_{-0.04}) M_{\\odot}$ at the 2$\\sigma$ (1$\\sigma$)\nlevel. The cases for a massive neutron star (MNS), either a supra-massive\nneutron star (SMNS) or even a stable neutron star (SNS), are also allowed by\nthe data. We derive $2.09^{+0.11}_{-0.09}(^{+0.06}_{-0.04} M_{\\odot}) \\leq\nM_{\\rm TOV}< 2.43^{+0.10}_{-0.08}(^{+0.06}_{-0.04}) M_{\\odot}$ for the SMNS\ncase and $M_{\\rm TOV} \\geq 2.43^{+0.10}_{-0.08}(^{+0.06}_{-0.04})M_{\\odot}$ for\nthe SNS case, at the $2\\sigma$ ($1\\sigma$) confidence level. In the MNS cases,\nwe also discuss the constraints on the neutron star parameters (the dipolar\nmagnetic field strength at the surface $B_p$ and the ellipticity $\\epsilon$)\nthat affect the spindown history, by considering different MNS survival times,\ne.g. 300 s, 1 d, and 155 d after the merger, as suggested by various\nobservational arguments. We find that once an SMNS is formed, without violating\nthe EM observational constraints, there always exist a set of ($B_p, \\epsilon$)\nparameters that allow the SMNS to survive for 300s, 1 d, 155 d, or even longer.'}, {'On the true fractions of repeating and non-repeating FRB sources': 'Observationally, fast radio bursts (FRBs) can be divided into repeating and\napparently non-repeating (one-off) ones. It is unclear whether all FRBs repeat\nand whether there are genuine non-repeating FRBs. We attempt to address these\nquestions using Monte Carlo simulations. We define a parameter $T_c$ at which\nthe accumulated number of non-repeating sources becomes comparable to the total\nnumber of the repeating sources, which is a good proxy to denote the intrinsic\nrepeater fraction among FRBs. Assuming that both types of sources exist and\nthat their burst energies follow power law distributions, we investigate how\nthe {\\em observed} repeater fraction evolves with time for different\nparameters. If the lifetime of repeaters is sufficiently long so that the\nevolutionary effect can be neglected within the observational time span, unless\n$T_c \\rightarrow \\infty$ (i.e. there is no genuine non-repeating FRB source)\nthe observed repeater fraction should increase with time first, reach a peak,\nand then decline. The peak time $T_p$ and the peak fraction $F_{\\rm r,obs,p}$\ndepend on $T_c$ and other repeating rate parameters. With the current data, we\npose a lower limit $T_c > 0.1$ d for reasonable parameter values. We predict\nthat future continuous monitoring of FRBs with CHIME or similar wide-field\nradio telescopes would obtain an $F_{\\rm r,obs}$ less than $0.04$. The\ndetection of a smaller peak value $F_{\\rm r,obs,p}<0.04$ in the near future\nwould disfavor the ansatz that ""all FRB sources repeat"".'}, {'Engine-fed Kilonovae (Mergernovae) -- I. Dynamical Evolution and Energy\n  Injection / Heating Efficiencies': 'A binary neutron star merger is expected to be associated by a kilonova,\ntransient optical emission powered by radioactive decay of the neutron-rich\nejecta. If the post-merger remnant is a long-lived neutron star, additional\nenergy injection to the ejecta is possible. In this first paper of a series, we\nstudy the dynamical evolution of the engine-fed kilonova (mergernova) ejecta in\ndetail. We perform a semi-analytical study of the problem by adopting a\nmodified mechanical blastwave model that invokes interaction between a\nPoynting-flux-dominated flow and a non-magnetized massive ejecta. Shortly after\nthe engine is turned on, a pair of shocks would be excited. The reverse shock\nquickly reaches the wind-acceleration region and disappears (in a few seconds),\nwhereas the forward shock soon breaks out from the ejecta (in $10^2$ - $10^3$\nseconds) and continues to propagate in the surrounding interstellar medium.\nMost of the energy injected into the blastwave from the engine is stored as\nmagnetic energy and kinetic energy. The internal energy fraction is $f_{\\rm\nint} < 0.3$ for an ejecta mass equal to $10^{-3}M_{\\odot}$. Overall, the energy\ninjecting efficiency $\\xi$ is at most $\\sim 0.6$ and can be as small as $\\sim\n0.04$ at later times. Contrary to the previous assumption, efficient heating\nonly happens before the forward shock breaks out of the ejecta with a heating\nefficiency $\\xi_t \\sim (0.006 - 0.3)$, which rapidly drops to $\\sim 0$\nafterwards. The engine-fed kilonova lightcurves will be carefully studied in\nPaper II.'}, {'A more stringent constraint on the mass ratio of binary neutron star\n  merger GW170817': 'Recently, the LIGO-Virgo collaboration reported their first detection of\ngravitational wave (GW) signals from a low mass compact binary merger GW170817,\nwhich is most likely due to a double neutron star (NS) merger. With the GW\nsignals only, the chirp mass of the binary is precisely constrained to\n$1.188^{+0.004}_{-0.002}~\\rm{M_{\\odot}}$, but the mass ratio is loosely\nconstrained in the range $0.4-1$, so that a very rough estimation of the\nindividual NS masses ($0.86~{\\rm M_{\\odot}}<M_1<1.36~\\rm{M_{\\odot}}$ and\n$1.36~{\\rm M_{\\odot}}<M_2<2.26~\\rm{M_{\\odot}}$) was obtained. Here we propose\nthat if one can constrain the dynamical ejecta mass through performing kilonova\nmodeling of the optical/IR data, by utilizing an empirical relation between the\ndynamical ejecta mass and the mass ratio of NS binaries, one may place a more\nstringent constraint on the mass ratio of the system. For instance, considering\nthat the red ""kilonova"" component is powered by the dynamical ejecta, we reach\na tight constraint on the mass ratio in the range of $0.46-0.59$.\nAlternatively, if the blue ""kilonova"" component is powered by the dynamical\nejecta, the mass ratio would be constrained in the range of $0.53-0.67$.\nOverall, such a multi-messenger approach could narrow down the mass ratio of\nGW170817 system to the range of $0.46-0.67$, which gives a more precise\nestimation of the individual NS mass than pure GW signal analysis, i.e.\n$0.90~{\\rm M_{\\odot}}<M_1<1.16~{\\rm M_{\\odot}}$ and $1.61~{\\rm\nM_{\\odot}}<M_2<2.11~{\\rm M_{\\odot}}$.'}, {'The evolution effects of radius and moment of inertia for rapidly\n  rotating neutron stars': 'A newly born millisecond magnetar is thought to be the central engine of some\ngamma-ray bursts (GRBs), especially those that present long-lasting X-ray\nplateau emissions. By solving the field equations, we find that when the\nrotational speed of the magnetar is approaching the breakup limit, its radius\n$R$ and moment of inertia $I$ would undergo an obvious evolution as the\nmagnetar spins down. Meanwhile, the values of $R$ and $I$ would sensitively\ndepend on the adoption of neutron star (NS) equation of state (EoS) and the NS\nbaryonic mass. With different EoSs and baryonic masses considered, the magnetic\ndipole radiation luminosity ($L_{\\rm dip}$) could be variant within one to two\norders of magnitude. We thus suggest that when using the X-ray plateau data of\nGRBs to diagnose the properties of the nascent NSs, EoS and NS mass information\nshould be invoked as simultaneously constrained parameters. On the other hand,\ndue to the evolution of $R$ and $I$, the temporal behavior of $L_{\\rm dip}$\nwould become more complicated. For instance, if the spin-down process is\ndominated by gravitational wave emission due to the NS asymmetry caused by\nmagnetic field distortion ($\\epsilon\\propto B_{p}^{2}$), the segment $L_{\\rm\ndip}\\propto t^{0}$ could be followed by $L_{\\rm dip}\\propto t^{-\\gamma}$ with\n$\\gamma$ larger than 3. This case could naturally interpret the so-called\ninternal X-ray plateau feature shown in some GRB afterglows, which means the\nsharp decay following the plateau is unnecessarily corresponding to the NS\ncollapsing. This may explain why some internal X-ray plateaus are followed by\nlate time central engine activity, manifested through flares and second shallow\nplateaus.'}, {'What constraints can one pose on the maximum mass of neutron stars from\n  multi-messenger observations?': 'The maximum mass of neutron stars ($M_{\\rm TOV}$) plays a crucial role in\nunderstanding their equation of state (EoS). Previous studies have used the\nmeasurements for the compactness of massive pulsars and the tidal deformability\nof neutron stars in binary neutron star (BNS) mergers to constrain the EoS and\nthus the $M_{\\rm TOV}$. The discovery of the most massive pulsar, PSR\nJ0952-0607, with a mass $\\sim 2.35M_{\\odot}$, has provided a valuable lower\nlimit for $M_{\\rm TOV}$. Another efficient method to constrain $M_{\\rm TOV}$ is\nby examining the type of central remnant formed after a BNS merger.\nGravitational wave (GW) data can provide the total mass of the system, while\naccompanying electromagnetic signals can help infer the remnant type. In this\nstudy, we combine all the previous constraints and utilize the observational\nfacts that about $24\\%$ of the short gamma-ray bursts are followed by an X-ray\ninternal plateau, which indicate that roughly this fraction of BNS mergers\nyield supermassive neutron stars, to perform (Markov Chain) Monte Carlo\nsimulations. These simulations allow us to explore the probability density\ndistribution of $M_{\\rm TOV}$ and other parameters related to BNS mergers. Our\nfindings suggest that $M_{\\rm TOV}$ is likely around $2.49M_{\\odot} -\n2.52M_{\\odot}$, with an uncertainty range of approximately [$-0.16M_{\\odot}$,\n$0.15M_{\\odot}$] ([$-0.28M_{\\odot}$, $0.26M_{\\odot}$]) at $1\\sigma$ ($2\\sigma$)\nconfidence level. Furthermore, we examine the type of merger remnants in\nspecific events like GW170817 and GW190425 to further constrain $M_{\\rm TOV}$\nand other relevant parameters, which can help to understand the physical\nprocesses involved in BNS mergers.'}]","**Title**: Novel Insights into the Luminosity Evolution of Engine-Fed Kilonovae during Neutron Star Mergers: Energy Injection, Shock Heating, and Optical/UV Radiation

**Background**: Recent detections of long-duration gamma-ray bursts (GRBs) associated with kilonova emission through optical surveys have stimulated intense research on the radioactive decay products of heavy elements in neutron star mergers. Nucleosynthesis of these elements, primarily via r-process, and their subsequent release in a bright optical/UV signal are now understood to be a significant GRB-chirp event phase. Engine-fed kilonovae, powered by post-merger neutron star (NS) magnetic fields, are anticipated to drive highly luminous phenomena by heating the ejecta through shock heating and X-ray irradiation, potentially surpassing conventional r-process kilonova brightness. 

**Objective**: The study aims to explore the luminosity evolution dynamics of engine-fed kilonovae under different energy injection scenarios, considering the spin-down luminosity (Lsd,0) and total energy budget (Etot) emanating from the magnetar powering the post-merger system. Understanding this evolution across various time scales is crucial for characterizing these high-energy phenomena and their potential for distinguishing from typical r-process kilonova events.

**Innovations**: The research adopts an advanced radiation hydrodynamics model that incorporates a new treatment for source heating, X-ray irradiation, photodissociation feedback, and photon diffusion, coupled with a sophisticated ejecta morphology and explosive nucleosynthesis prescription. Key innovations include a novel framework for understanding the onset and duration of shock heating, the impact of X-ray luminosities on kilonova emissions, and the effects of different radiation properties on emission spectra.

**Methods**: The study uses hydrodynamic simulations to model the post-merger NS environments, varying parameters like Lsd,0 and Etot, and employs spectral synthesis using the updated models. It calculates bolometric light curves for various observing times and product spectra, modelled through Monte Carlo photon transport, to analyze injected energy deposition mechanisms and their decoupling from radioactive decay time-scales.

**Results**: The research presents novel luminosity-time evolution patterns for engine-fed kilonovae at different fusion power levels (Lsd,0) and energy budgets (Etot). It identifies a two-phase growth of peak luminosity versus time that depends on the injection time-scale relative to kilonova peak times, highlighting a critical gap between energy deposition and its release. The results also delineate the transition from UV dominance to the optical range due to complex interplay between energy injection sources, radiative cooling, and mass expansion.

**Contributions**: The study contributes significantly to the time-evolving luminosity parameter space of SN and GRB populations, offering a framework that integrates physical mechanisms contributing to kilonova radiative signatures. This understanding not only enhances theoretical interpretations of existing GRB/Kilonova observations but also provides predictive models for future telescopic targeting of NS mergers from the Advanced LIGO-Virgo detectors.

**Applications**: The research findings have direct implications for ongoing GRB and kilonova campaigns linking gravitational wave sources to electromagnetic counterparts. Representing real advancements in astrophysical modeling, these insights are crucial for diversifying electromagnetic signals used to characterize binary neutron star mergers and implications for r-process nucleosynthesis across the universe."
"Traditional reinforcement learning from human feedback (RLHF) approaches
relying on parametric models like the Bradley-Terry model fall short in
capturing the intransitivity and irrationality in human preferences. Recent
advancements suggest that directly working with preference probabilities can
yield a more accurate reflection of human preferences, enabling more flexible
and accurate language model alignment. In this paper, we propose a
self-play-based method for language model alignment, which treats the problem
as a constant-sum two-player game aimed at identifying the Nash equilibrium
policy. Our approach, dubbed \textit{Self-Play Preference Optimization} (SPPO),
approximates the Nash equilibrium through iterative policy updates and enjoys
theoretical convergence guarantee. Our method can effectively increase the
log-likelihood of the chosen response and decrease that of the rejected
response, which cannot be trivially achieved by symmetric pairwise loss such as
Direct Preference Optimization (DPO) and Identity Preference Optimization
(IPO). In our experiments, using only 60k prompts (without responses) from the
UltraFeedback dataset and without any prompt augmentation, by leveraging a
pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain
a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the
state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on
AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and
the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved
without additional external supervision (e.g., responses, preferences, etc.)
from GPT-4 or other stronger language models.","[{'A randomised trapezoidal quadrature': 'A randomised trapezoidal quadrature rule is proposed for continuous functions\nwhich enjoys less regularity than commonly required. Indeed, we consider\nfunctions in some fractional Sobolev space. Various error bounds for this\nrandomised rule are established while an error bound for classical trapezoidal\nquadrature is obtained for comparison. The randomised trapezoidal quadrature\nrule is shown to improve the order of convergence by half.'}, {'Whirly 3-Interval Exchange Transformations': 'Irreducible interval exchange transformations are studied with regard to\nwhirly property, a condition for non-trivial spatial factor. Uniformly whirly\ntransformation is defined and to be further studied. An equivalent condition is\nintroduced for whirly transformation. We will prove that almost all 3-interval\nexchange transformations are whirly, using a combinatorics approach with\napplication of the Rauzy-Veech Induction. It is still an open question whether\nwhirly property is a generic property for m-interval exchange transformations\n(m>=4).'}, {'A Note on Random Sampling for Matrix Multiplication': 'This paper extends the framework of randomised matrix multiplication to a\ncoarser partition and proposes an algorithm as a complement to the classical\nalgorithm, especially when the optimal probability distribution of the latter\none is closed to uniform. The new algorithm increases the likelihood of getting\na small approximation error in 2-norm and has the squared approximation error\nin Frobenious norm bounded by that from the classical algorithm.'}, {'Game Theoretic Consequences of Resident Matching': ""The resident matching algorithm, Gale-Shapley, currently used by SF Match and\nthe National Residency Match Program (NRMP), has been in use for over 50 years\nwithout fundamental alteration. The algorithm is a 'stable-marriage' method\nthat favors applicant outcomes. However, in these 50 years, there has been a\nbig shift in the supply and demand of applicants and programs. These changes\nalong with the way the Match is implemented have induced a costly race among\napplicants to apply and interview at as many programs as possible. Meanwhile\nprograms also incur high costs as they maximize their probability of matching\nby interviewing as many candidates as possible.""}, {'The random periodic solution of a stochastic differential equation with\n  a monotone drift and its numerical approximation': 'In this paper we study the existence and uniqueness of the random periodic\nsolution for a stochastic differential equation with a one-sided Lipschitz\ncondition (also known as monotonicity condition) and the convergence of its\nnumerical approximation via the backward Euler-Maruyama method. The existence\nof the random periodic solution is shown as the limits of the pull-back flows\nof the SDE and discretized SDE respectively. We establish a convergence rate of\nthe strong error for the backward Euler-Maruyama method and obtain the weak\nconvergence result for the approximation of the periodic measure.'}, {'Singularities of the Moduli Space of n Unordered Points on the Riemann\n  Sphere': 'We classify the finite groups associated to the singularities of the moduli\nspace of $n/ge5$ unordered points on the Riemann sphere. We also realize the\nclassification by an algorithm.'}, {'A New Interference-Alignment Scheme for Wireless MapReduce': 'We consider a full-duplex wireless Distributed Computing (DC) system under\nthe MapReduce framework. New upper and lower bounds on the optimal tradeoff\nbetween Normalized Delivery Time (NDT) and computation load are presented. The\nupper bound strictly improves over the previous reported upper bounds and is\nbased on a novel interference alignment (IA) scheme tailored to the\ninterference cancellation capabilities of MapReduce nodes. The lower bound is\nproved through information-theoretic converse arguments.'}, {'DoF Analysis for (M, N)-Channels through a Number-Filling Puzzle': 'We consider a $\\sf K$ user interference network with general connectivity,\ndescribed by a matrix $\\mat{N}$, and general message flows, described by a\nmatrix $\\mat{M}$. Previous studies have demonstrated that the standard\ninterference scheme (IA) might not be optimal for networks with sparse\nconnectivity. In this paper, we formalize a general IA coding scheme and an\nintuitive number-filling puzzle for given $\\mat{M}$ and $\\mat{N}$ in a way that\nthe score of the solution to the puzzle determines the optimum sum degrees that\ncan be achieved by the IA scheme. A solution to the puzzle is proposed for a\ngeneral class of symmetric channels, and it is shown that this solution leads\nto significantly higher $\\SDoF$ than the standard IA scheme.'}, {'Robust Facial Landmark Detection under Significant Head Poses and\n  Occlusion': 'There have been tremendous improvements for facial landmark detection on\ngeneral ""in-the-wild"" images. However, it is still challenging to detect the\nfacial landmarks on images with severe occlusion and images with large head\nposes (e.g. profile face). In fact, the existing algorithms usually can only\nhandle one of them. In this work, we propose a unified robust cascade\nregression framework that can handle both images with severe occlusion and\nimages with large head poses. Specifically, the method iteratively predicts the\nlandmark occlusions and the landmark locations. For occlusion estimation,\ninstead of directly predicting the binary occlusion vectors, we introduce a\nsupervised regression method that gradually updates the landmark visibility\nprobabilities in each iteration to achieve robustness. In addition, we\nexplicitly add occlusion pattern as a constraint to improve the performance of\nocclusion prediction. For landmark detection, we combine the landmark\nvisibility probabilities, the local appearances, and the local shapes to\niteratively update their positions. The experimental results show that the\nproposed method is significantly better than state-of-the-art works on images\nwith severe occlusion and images with large head poses. It is also comparable\nto other methods on general ""in-the-wild"" images.'}, {'Constrained Joint Cascade Regression Framework for Simultaneous Facial\n  Action Unit Recognition and Facial Landmark Detection': 'Cascade regression framework has been shown to be effective for facial\nlandmark detection. It starts from an initial face shape and gradually predicts\nthe face shape update from the local appearance features to generate the facial\nlandmark locations in the next iteration until convergence. In this paper, we\nimprove upon the cascade regression framework and propose the Constrained Joint\nCascade Regression Framework (CJCRF) for simultaneous facial action unit\nrecognition and facial landmark detection, which are two related face analysis\ntasks, but are seldomly exploited together. In particular, we first learn the\nrelationships among facial action units and face shapes as a constraint. Then,\nin the proposed constrained joint cascade regression framework, with the help\nfrom the constraint, we iteratively update the facial landmark locations and\nthe action unit activation probabilities until convergence. Experimental\nresults demonstrate that the intertwined relationships of facial action units\nand face shapes boost the performances of both facial action unit recognition\nand facial landmark detection. The experimental results also demonstrate the\neffectiveness of the proposed method comparing to the state-of-the-art works.'}]","Title: SPPO: Iterative Self-Play Preference Optimization for Enhancing Instruction-Following Large Language Models

Abstract:

In the burgeoning field of natural language processing (NLP), enhancing the ability of large language models (LLMs) to align with human preferences is essential. Self-Play Preference Optimization (SPPO), an innovative 
approach, introduces a novel iteration-based method that directly manipulates LLMs using preference data. Our paper's primary goal is to evaluate SPPO, employing iterative DPO, IPO, and SPPO on prominent benchmarks like AlpacaEval 2.0, MT-Bench, and the Open LLM Leaderboard, demonstrating superior performance compared to established models. SPPO fundamentally shifts preferences from relying on conventional reward models, showcasing the potential for more intricate and adaptable RRT.""

This research advances the methodology by offering an alternative to augmenting LLMs through preference optimization. The iterative SPPO mechanism selected yields statistically significant improvements, enhancing overall performance metrics across diverse benchmarks. This contribution ensures that the models are better tuned to human-centric responses, thus expanding the applicability of large language models in real-world contexts that require nuanced understanding and empirical outcomes.

The superior results suggest that as models become more adept at adhering to human preferences, the potential applications for instruction-following LLMs increase. This can lead to advancements in areas such as customer service, ethical AI, personalized recommendations, and communications technology. The integration of SPPO into training paradigms for LLMs represents a potent step forward for ensuring alignment between model outputs and human values, paving the way for more effective and humane technological solutions."
"We propose Soft Preference Optimization (SPO), a method for aligning
generative models, such as Large Language Models (LLMs), with human
preferences, without the need for a reward model. SPO optimizes model outputs
directly over a preference dataset through a natural loss function that
integrates preference loss with a regularization term across the model's entire
output distribution rather than limiting it to the preference dataset. Although
SPO does not require the assumption of an existing underlying reward model, we
demonstrate that, under the Bradley-Terry (BT) model assumption, it converges
to a softmax of scaled rewards, with the distribution's ""softness"" adjustable
via the softmax exponent, an algorithm parameter. We showcase SPO's
methodology, its theoretical foundation, and its comparative advantages in
simplicity, computational efficiency, and alignment precision.","[{'Toward Efficient Gradient-Based Value Estimation': 'Gradient-based methods for value estimation in reinforcement learning have\nfavorable stability properties, but they are typically much slower than\nTemporal Difference (TD) learning methods. We study the root causes of this\nslowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned\nloss function in the sense that its Hessian has large condition-number. To\nresolve the adverse effect of poor conditioning of MSBE on gradient based\nmethods, we propose a low complexity batch-free proximal method that\napproximately follows the Gauss-Newton direction and is asymptotically robust\nto parameterization. Our main algorithm, called RANS, is efficient in the sense\nthat it is significantly faster than the residual gradient methods while having\nalmost the same computational complexity, and is competitive with TD on the\nclassic problems that we tested.'}, {'Sensitivity to Cumulative Perturbations for a Class of Piecewise\n  Constant Hybrid Systems': 'We consider a class of continuous-time hybrid dynamical systems that\ncorrespond to subgradient flows of a piecewise linear and convex potential\nfunction with finitely many pieces, and which include the fluid-level dynamics\nof the Max-Weight scheduling policy as a special case. We study the effect of\nan external disturbance/perturbation on the state trajectory, and establish\nthat the magnitude of this effect can be bounded by a constant multiple of the\nintegral of the perturbation.'}, {'When do Trajectories have Bounded Sensitivity to Cumulative\n  Perturbations?': 'We investigate sensitivity to cumulative perturbations for a few dynamical\nsystem classes of practical interest. A system is said to have bounded\nsensitivity to cumulative perturbations (bounded sensitivity, for short) if an\nadditive disturbance leads to a change in the state trajectory that is bounded\nby a constant multiple of the size of the cumulative disturbance. As our main\nresult, we show that there exist dynamical systems in the form of (negative)\ngradient field of a convex function that have unbounded sensitivity. We show\nthat the result holds even when the convex potential function is piecewise\nlinear. This resolves a question raised in [1], wherein it was shown that the\n(negative) (sub)gradient field of a piecewise linear and convex function has\nbounded sensitivity if the number of linear pieces is finite. Our results\nestablish that the finiteness assumption is indeed necessary.\n  Among our other results, we provide a necessary and sufficient condition for\na linear dynamical system to have bounded sensitivity to cumulative\nperturbations. We also establish that the bounded sensitivity property is\npreserved, when a dynamical system with bounded sensitivity undergoes certain\ntransformations. These transformations include convolution, time\ndiscretization, and spreading of a system (a transformation that captures\napproximate solutions of a system).'}, {'MetaOptimize: A Framework for Optimizing Step Sizes and Other\n  Meta-parameters': 'This paper addresses the challenge of optimizing meta-parameters (i.e.,\nhyperparameters) in machine learning algorithms, a critical factor influencing\ntraining efficiency and model performance. Moving away from the computationally\nexpensive traditional meta-parameter search methods, we introduce MetaOptimize\nframework that dynamically adjusts meta-parameters, particularly step sizes\n(also known as learning rates), during training. More specifically,\nMetaOptimize can wrap around any first-order optimization algorithm, tuning\nstep sizes on the fly to minimize a specific form of regret that accounts for\nlong-term effect of step sizes on training, through a discounted sum of future\nlosses. We also introduce low complexity variants of MetaOptimize that, in\nconjunction with its adaptability to multiple optimization algorithms,\ndemonstrate performance competitive to those of best hand-crafted learning rate\nschedules across various machine learning applications.'}, {'Jumping Fluid Models and Delay Stability of Max-Weight Dynamics under\n  Heavy-Tailed Traffic': 'We say that a random variable is $light$-$tailed$ if moments of order\n$2+\\epsilon$ are finite for some $\\epsilon>0$; otherwise, we say that it is\n$heavy$-$tailed$. We study queueing networks that operate under the Max-Weight\nscheduling policy, for the case where some queues receive heavy-tailed and some\nreceive light-tailed traffic. Queues with light-tailed arrivals are often delay\nstable (that is, expected queue sizes are uniformly bounded over time) but can\nalso become delay unstable because of resource-sharing with other queues that\nreceive heavy-tailed arrivals.\n  Within this context, and for any given ""tail exponents"" of the input traffic,\nwe develop a necessary and sufficient condition under which a queue is robustly\ndelay stable, in terms of $jumping$ $fluid$ models - an extension of\ntraditional fluid models that allows for jumps along coordinates associated\nwith heavy-tailed flows. Our result elucidates the precise mechanism that leads\nto delay instability, through a coordination of multiple abnormally large\narrivals at possibly different times and queues and settles an earlier open\nquestion on the sufficiency of a particular fluid-based criterion. Finally, we\nexplore the power of Lyapunov functions in the study of delay stability.'}, {'Fluctuation Bounds for the Max-Weight Policy, with Applications to State\n  Space Collapse': 'We consider a multi-hop switched network operating under a Max-Weight (MW)\nscheduling policy, and show that the distance between the queue length process\nand a fluid solution remains bounded by a constant multiple of the deviation of\nthe cumulative arrival process from its average. We then exploit this result to\nprove matching upper and lower bounds for the time scale over which additive\nstate space collapse (SSC) takes place. This implies, as two special cases, an\nadditive SSC result in diffusion scaling under non-Markovian arrivals and, for\nthe case of i.i.d. arrivals, an additive SSC result over an exponential time\nscale.'}, {'Nonexpansive Piecewise Constant Hybrid Systems are Conservative': ""Consider a partition of $R^n$ into finitely many polyhedral regions $D_i$ and\nassociated drift vectors $\\mu_i\\in R^n$. We study ``hybrid'' dynamical systems\nwhose trajectories have a constant drift, $\\dot x=\\mu_i$, whenever $x$ is in\nthe interior of the $i$th region $D_i$, and behave consistently on the boundary\nbetween different regions. Our main result asserts that if such a system is\nnonexpansive (i.e., if the Euclidean distance between any pair of trajectories\nis a nonincreasing function of time), then the system must be conservative,\ni.e., its trajectories are the same as the trajectories of the negative\nsubgradient flow associated with a potential function. Furthermore, this\npotential function is necessarily convex, and is linear on each of the regions\n$D_i$. We actually establish a more general version of this result, by making\nseemingly weaker assumptions on the dynamical system of interest.""}, {'Step-size Optimization for Continual Learning': 'In continual learning, a learner has to keep learning from the data over its\nwhole life time. A key issue is to decide what knowledge to keep and what\nknowledge to let go. In a neural network, this can be implemented by using a\nstep-size vector to scale how much gradient samples change network weights.\nCommon algorithms, like RMSProp and Adam, use heuristics, specifically\nnormalization, to adapt this step-size vector. In this paper, we show that\nthose heuristics ignore the effect of their adaptation on the overall objective\nfunction, for example by moving the step-size vector away from better step-size\nvectors. On the other hand, stochastic meta-gradient descent algorithms, like\nIDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to\nthe overall objective function. On simple problems, we show that IDBD is able\nto consistently improve step-size vectors, where RMSProp and Adam do not. We\nexplain the differences between the two approaches and their respective\nlimitations. We conclude by suggesting that combining both approaches could be\na promising future direction to improve the performance of neural networks in\ncontinual learning.'}, {'Order Optimal One-Shot Distributed Learning': 'We consider distributed statistical optimization in one-shot setting, where\nthere are $m$ machines each observing $n$ i.i.d. samples. Based on its observed\nsamples, each machine then sends an $O(\\log(mn))$-length message to a server,\nat which a parameter minimizing an expected loss is to be estimated. We propose\nan algorithm called Multi-Resolution Estimator (MRE) whose expected error is no\nlarger than $\\tilde{O}\\big(m^{-{1}/{\\max(d,2)}} n^{-1/2}\\big)$, where $d$ is\nthe dimension of the parameter space. This error bound meets existing lower\nbounds up to poly-logarithmic factors, and is thereby order optimal. The\nexpected error of MRE, unlike existing algorithms, tends to zero as the number\nof machines ($m$) goes to infinity, even when the number of samples per machine\n($n$) remains upper bounded by a constant. This property of the MRE algorithm\nmakes it applicable in new machine learning paradigms where $m$ is much larger\nthan $n$.'}, {'Order Optimal Bounds for One-Shot Federated Learning over non-Convex\n  Loss Functions': 'We consider the problem of federated learning in a one-shot setting in which\nthere are $m$ machines, each observing $n$ sample functions from an unknown\ndistribution on non-convex loss functions. Let $F:[-1,1]^d\\to\\mathbb{R}$ be the\nexpected loss function with respect to this unknown distribution. The goal is\nto find an estimate of the minimizer of $F$. Based on its observations, each\nmachine generates a signal of bounded length $B$ and sends it to a server. The\nserver collects signals of all machines and outputs an estimate of the\nminimizer of $F$. We show that the expected loss of any algorithm is lower\nbounded by $\\max\\big(1/(\\sqrt{n}(mB)^{1/d}), 1/\\sqrt{mn}\\big)$, up to a\nlogarithmic factor. We then prove that this lower bound is order optimal in $m$\nand $n$ by presenting a distributed learning algorithm, called Multi-Resolution\nEstimator for Non-Convex loss function (MRE-NC), whose expected loss matches\nthe lower bound for large $mn$ up to polylogarithmic factors.'}]","Research Title: SPO: A Preference-Aware Approach to Reward-Free Alignment in Reinforcement Learning from Human Feedback

Abstract:
In the realm of reinforcement learning (RL) leveraging human preferences for model alignment, current approaches often struggle with noisy datasets and complex optimization landscapes. In this paper, we address these challenges by introducing SPO, a novel preference-aware RL technique that integrates a sophisticated preference loss with a regularization term, facilitating reward-free alignment via human feedback. Our objective is to improve alignment of learned policies with human intentions without relying on reward models, thereby enhancing performance and reducing the reliance on high-quality dataset samples. Innovations include a weighted preference loss to adaptively downplay noisy feedback and a regularization term to guide the generator policy towards human-compliant output spaces. Persistent Differential Augmentation (PD) enhances optimization dynamics, ensuring stability and efficiency throughout training. Evaluated in a story generation task using the TinyStories dataset, our approach achieves significantly higher alignment with target age groups, demonstrating generalizability and robustness in human intentions capture. SPO makes substantial strides in steering RL models towards preferred linguistic styles, crucial for applications in multi-objective text generation and personalized content creation. By avoiding dependence on reward modeling, SPO broadens the scope of human-in-the-loop RL applications, poised to reshape the landscape of aligned model deployment."
"Conventional recommendation systems (RSs) are typically optimized to enhance
performance metrics uniformly across all training samples.
  This makes it hard for data-driven RSs to cater to a diverse set of users due
to the varying properties of these users. The performance disparity among
various populations can harm the model's robustness with respect to
sub-populations. While recent works have shown promising results in adapting
large language models (LLMs) for recommendation to address hard samples, long
user queries from millions of users can degrade the performance of LLMs and
elevate costs, processing times and inference latency. This challenges the
practical applicability of LLMs for recommendations. To address this, we
propose a hybrid task allocation framework that utilizes the capabilities of
both LLMs and traditional RSs. By adopting a two-phase approach to improve
robustness to sub-populations, we promote a strategic assignment of tasks for
efficient and responsible adaptation of LLMs. Our strategy works by first
identifying the weak and inactive users that receive a suboptimal ranking
performance by RSs. Next, we use an in-context learning approach for such
users, wherein each user interaction history is contextualized as a distinct
ranking task and given to an LLM. We test our hybrid framework by incorporating
various recommendation algorithms -- collaborative filtering and
learning-to-rank recommendation models -- and two LLMs -- both open and
close-sourced. Our results on three real-world datasets show a significant
reduction in weak users and improved robustness of RSs to sub-populations
$(\approx12\%)$ and overall performance without disproportionately escalating
costs.","[{'On the flag curvature of homogeneous Finsler space with some special\n  $(α, β)$-metrics': 'In this paper, first we derive an explicit formula for the flag curvature of\na homogeneous Finsler space with infinite series $(\\alpha, \\beta)$-metric and\nexponential metric. Next, we deduce it for naturally reductive homogeneous\nFinsler space with the above mentioned metrics.'}, {'On the geodesics of homogeneous Finsler spaces with some special\n  $(α, β)$-metrics': 'In this paper, we study geodesics and geodesic vectors for homogeneous\nexponential Finsler space and homogeneous infinite series Finsler space.\nFurther, we find necessary and sufficient condition for a non-zero vector in\nthese homogeneous spaces to be a geodesic vector.'}, {'Homogeneous Finsler spaces with some special $(α, β)$-metrics': 'In this paper, first we prove the existence of invariant vector field on a\nhomogeneous Finsler space with infinite series $(\\alpha, \\beta)$-metric and\nexponential metric. Next, we deduce an explicit formula for the the\n$S$-curvature of homogeneous Finsler space with these metrics. Using this\nformula, we further derive the formula for mean Berwald curvature of the\nhomogeneous Finsler space with the above mentioned metrics.'}, {'Extending Quasi-Alternating Links': 'Champanerkar and Kofman introduced an interesting way to construct new\nexamples of quasi-alternating links from existing ones. Actually, they proved\nthat replacing a quasi-alternating crossing c in a quasi-alternating link by a\nrational tangle of same type yields a new quasi-alternating link. This\nconstruction has been extended to alternating algebraic tangles and applied to\ncharacterize all quasi-alternating Montesinos links. In this paper, we extend\nthis technique to any alternating tangle of same type as c. As an application,\nwe give new examples of quasi-alternating knots of 13 and 14 crossings.\nMoreover, we prove that the Jones polynomial of a quasi-alternating link that\nis obtained in this way has no gap if the original link has no gap in its Jones\npolynomial. This supports a conjecture introduced in arXiv:1810.11773\n[math.GT], which states that Jones polynomial of any prime quasi-alternating\nlink except (2; n)-torus link has no gap.'}, {'Some classes of projectively and dually flat Finsler spaces with Randers\n  change': 'In this paper, we consider Randers change of some special $ (\\alpha, \\beta)-\n$ metrics. First we find the fundamental metric tensor and Cartan tensor of\nthese Randers changed $ (\\alpha, \\beta)- $metrics. Next, we establish a general\nformula for inverse of fundamental metric tensors of these metrics. Finally, we\nfind the necessary and sufficient conditions under which the Randers change of\nthese $ (\\alpha, \\beta)- $ metrics are projectively and locally dually flat.'}, {'Two-variable polynomial invariants of virtual knots arising from flat\n  virtual knot invariants': 'We introduce two sequences of two-variable polynomials $\\{ L^n_K (t,\n\\ell)\\}_{n=1}^{\\infty}$ and $\\{ F^n_K (t, \\ell)\\}_{n=1}^{\\infty}$, expressed in\nterms of index value of a crossing and $n$-dwrithe value of a virtual knot $K$,\nwhere $t$ and $\\ell$ are variables. Basing on the fact that $n$-dwrithe is a\nflat virtual knot invariant we prove that $L^n_K$ and $F^n_K$ are virtual knot\ninvariants containing Kauffman affine index polynomial as a particular case.\nUsing $L^n_K$ we give sufficient conditions when virtual knot does not admit\ncosmetic crossing change.'}, {'An Unknotting Index for Virtual Links': 'Given a virtual link diagram $D$, we define its unknotting index $U(D)$ to be\nminimum among $(m, n)$ tuples, where $m$ stands for the number of crossings\nvirtualized and $n$ stands for the number of classical crossing changes, to\nobtain a trivial link diagram. By using span of a diagram and linking number of\na diagram we provide a lower bound for unknotting index of a virtual link. Then\nusing warping degree of a diagram, we obtain an upper bound. Both these bounds\nare applied to find unknotting index for virtual links obtained from pretzel\nlinks by virtualizing some crossings'}, {'Performance analysis of quantum harmonic Otto engine and refrigerator\n  under a trade-off figure of merit': 'We investigate the optimal performance of quantum Otto engine and\nrefrigeration cycles of a time-dependent harmonic oscillator under a trade-off\nfigure of merit for both adiabatic and nonadiabatic (sudden-switch) frequency\nmodulations. For heat engine (refrigerator), the chosen trade-off figure of\nmerit is an objective function defined by the product of efficiency\n(coefficient of performance) and work output (cooling load), thus representing\na compromise between them. We obtain analytical expressions for the efficiency\nand coefficient of performance of the harmonic Otto cycle for the optimal\nperformance of the thermal machine in various operational regimes.\nParticularly, in the sudden-switch regime, we discuss the implications of the\nnonadiabatic driving on the performance of the thermal machine under\nconsideration, and obtain analytic expressions for the maximum achievable\nefficiency and coefficient of performance of the harmonic Otto thermal machine.\nFurther, by carrying out a detailed comparative analysis of the heat engine\noperating under the chosen trade-off objective function with one operating at\nmaximum work output, we show that the trade-off objective functions have\ndesirable operation only for the adiabatic driving whereas for the sudden\nswitch operation, the choice of a trade-off objective function does not make\nmuch difference as the performance of the engine is dominated by frictional\neffects.'}, {'Unified trade-off optimization of a three-level quantum refrigerator': 'We study the optimal performance of a three-level quantum refrigerator using\na trade-off objective function, $\\Omega$ function, which represents a\ncompromise between the energy benefits and the energy losses of a thermal\ndevice. First, we optimize the performance of our refrigerator by employing a\ntwo-parameter optimization scheme and show that the first two-terms in the\nseries expansion of the obtained coefficient of performance (COP) match with\nthose of some classical models of refrigerator. Then, in the high-temperature\nlimit, optimizing with respect to one parameter while constraining the other\none, we obtain the lower and upper bounds on the COP for both strong as well as\nweak (intermediate) matter-field coupling conditions. In the strong\nmatter-field coupling regime, the obtained bounds on the COP exactly match with\nthe bounds already known for some models of classical refrigerators. Further\nfor weak matter-field coupling, we derive some new bounds on the the COP of the\nrefrigerator which lie beyond the range covered by bounds obtained for strong\nmatter-field coupling. Finally, in the parameter regime where both cooling\npower and $\\Omega$ function can be maximized, we compare the cooling power of\nthe quantum refrigerator at maximum $\\Omega$ function with the maximum cooling\npower.'}, {'Optimization analysis of an endoreversible quantum heat engine with\n  efficient power function': 'We study the optimal performance of an endoreversible quantum dot heat\nengine, in which the heat transfer between the system and baths is mediated by\nqubits, operating under the conditions of a trade-off objective function known\nas maximum efficient power function defined by the product of power and\nefficiency of the engine. First, we numerically study the optimization of the\nefficient power function for the engine under consideration. Then, we obtain\nsome analytic results by applying hightemperature limit and compare the\nperformance of the engine at maximum efficient power functionto the engine\noperating in the maximum power regime. We find that the engine operating at\nmaximum efficient power function produces at least 88.89% of the maximum power\noutput while at the same time reduces the power loss due to entropy production\nby considerable amount. We conclude by studying the stochastic simulations of\nthe efficiency of the engine in maximum power and maximum efficient power\nregime. We find that the engine operating at maximum power is subjected to less\npower fluctuations as compared to the on one operating at maximum efficient\npower function.'}]","Title: Efficient and Responsible Adaptation of Large Language Models for Robust Top-k Recommendations Conference

**Background**: Achieving effective, fair, and efficient personalized recommendations is pivotal in contemporary AI applications, such as content discovery, job recruitment, and online advertising. The incorporation of Large Language Models (LLMs) for recommendation tasks promises higher quality due to their renowned capabilities in coping with textual data. However, the processing of lengthy queries and high handling costs associated with adapting LLMs pose significant challenges.

**Objective**: The primary goal is to devise a task allocation strategy that leverages the strengths of traditional recommendation systems and LLMs via a hybrid framework. This aims to enhance ranking performance on sub-populations, particularly weak users, while addressing the high adaptation costs of LLMs.

**Innovations**: This work introduces a mechanism to identify weak users based on their interaction sparsity and performance with conventional recommendation systems, a strategy to generate tailored instructions for these users to prevent poor performance, and the selection of appropriate open- or closed-source models optimized for specific datasets.

**Methods**: A hybrid approach is implemented that categorizes users into strong and weak based on their recommendation performance and interaction sparsity. Weak users are then provided personalized instructions to their LLM models to enhance relevance. Various models, including traditional recommendation systems denoted by algorithms such as ItemKNN, NCF, and BPR, and LLMs such as Mixtral and GPT, are evaluated.

**Results**: The proposed framework resulted in significant improvements in ranking quality and robustness to sub-populations, especially for weak users, across different datasets. Notably, open-source models like Mixtral were shown to achieve performance comparable to or better than closed-source alternatives like GPT-3.5-turbo, indicating potential cost savings without sacrificing accuracy.

**Contributions**: The research introduces an algorithmically supported mechanism for the cost-effective use of LLMs in recommendation systems, proposes a classification scheme for addressing different user segments, and validates the efficacy of using semi-structured interactions for LLM instructions, merging the skills of traditional and modern AI models.

**Applications**: The devised strategy enriches the domain of collaborative filtering with more sophisticated user insights, enhances personalized content delivery, and optimizes resource allocation for AI model usage. It advances the scalability and efficiency of recommendation systems for businesses and technical organizations, while providing a structured approach to model adaptation and user segmentation.

In essence, this research paper collectively pertains to the advancement of hybrid LLM-based recommendation techniques, encompassing user segmentation, instruction formulation, and model selection, geared towards enhancing the overall system's performance while addressing economic and efficiency concerns."
"We study the problem of learning a binary classifier on the vertices of a
graph. In particular, we consider classifiers given by monophonic halfspaces,
partitions of the vertices that are convex in a certain abstract sense.
Monophonic halfspaces, and related notions such as geodesic halfspaces,have
recently attracted interest, and several connections have been drawn between
their properties(e.g., their VC dimension) and the structure of the underlying
graph $G$. We prove several novel results for learning monophonic halfspaces in
the supervised, online, and active settings. Our main result is that a
monophonic halfspace can be learned with near-optimal passive sample complexity
in time polynomial in $n = |V(G)|$. This requires us to devise a
polynomial-time algorithm for consistent hypothesis checking, based on several
structural insights on monophonic halfspaces and on a reduction to
$2$-satisfiability. We prove similar results for the online and active
settings. We also show that the concept class can be enumerated with delay
$\operatorname{poly}(n)$, and that empirical risk minimization can be performed
in time $2^{\omega(G)}\operatorname{poly}(n)$ where $\omega(G)$ is the clique
number of $G$. These results answer open questions from the literature
(Gonz\'alez et al., 2020), and show a contrast with geodesic halfspaces, for
which some of the said problems are NP-hard (Seiffarth et al., 2023).","[{'Efficient and near-optimal algorithms for sampling small connected\n  subgraphs': 'We study the following problem: given an integer $k \\ge 3$ and a simple graph\n$G$, sample a connected induced $k$-node subgraph of $G$ uniformly at random.\nThis is a fundamental graph mining primitive with applications in social\nnetwork analysis, bioinformatics, and more. Surprisingly, no efficient\nalgorithm is known for uniform sampling; the only somewhat efficient algorithms\navailable yield samples that are only approximately uniform, with running times\nthat are unclear or suboptimal. In this work we provide: (i) a near-optimal\nmixing time bound for a well-known random walk technique, (ii) the first\nefficient algorithm for truly uniform graphlet sampling, and (iii) the first\nsublinear-time algorithm for $\\epsilon$-uniform graphlet sampling.'}, {'Faster algorithms for counting subgraphs in sparse graphs': 'Given a $k$-node pattern graph $H$ and an $n$-node host graph $G$, the\nsubgraph counting problem asks to compute the number of copies of $H$ in $G$.\nIn this work we address the following question: can we count the copies of $H$\nfaster if $G$ is sparse? We answer in the affirmative by introducing a novel\ntree-like decomposition for directed acyclic graphs, inspired by the classic\ntree decomposition for undirected graphs. This decomposition gives a dynamic\nprogram for counting the homomorphisms of $H$ in $G$ by exploiting the\ndegeneracy of $G$, which allows us to beat the state-of-the-art subgraph\ncounting algorithms when $G$ is sparse enough. For example, we can count the\ninduced copies of any $k$-node pattern $H$ in time $2^{O(k^2)} O(n^{0.25k + 2}\n\\log n)$ if $G$ has bounded degeneracy, and in time $2^{O(k^2)} O(n^{0.625k +\n1} \\log n)$ if $G$ has bounded average degree. These bounds are instantiations\nof a more general result, parameterized by the degeneracy of $G$ and the\nstructure of $H$, which generalizes classic bounds on counting cliques and\ncomplete bipartite graphs. We also give lower bounds based on the Exponential\nTime Hypothesis, showing that our results are actually a characterization of\nthe complexity of subgraph counting in bounded-degeneracy graphs.'}, {'Simple set cardinality estimation through random sampling': 'We present a simple algorithm that estimates the cardinality $n$ of a set $V$\nwhen allowed to sample elements of $V$ uniformly and independently at random.\nOur algorithm with probability $(1-\\delta)$ returns a\n$(1\\pm\\epsilon)-$approximation of $n$ drawing $O\\big(\\sqrt{n} \\cdot\n\\epsilon^{-1}\\sqrt{\\log(\\delta^{-1})}\\big)$ samples (for\n$\\epsilon^{-1}\\sqrt{\\log(\\delta^{-1})} = O(\\sqrt{n})$).'}, {'Fully-Dynamic Approximate Decision Trees With Worst-Case Update Time\n  Guarantees': 'We give the first algorithm that maintains an approximate decision tree over\nan arbitrary sequence of insertions and deletions of labeled examples, with\nstrong guarantees on the worst-case running time per update request. For\ninstance, we show how to maintain a decision tree where every vertex has Gini\ngain within an additive $\\alpha$ of the optimum by performing\n$O\\Big(\\frac{d\\,(\\log n)^4}{\\alpha^3}\\Big)$ elementary operations per update,\nwhere $d$ is the number of features and $n$ the maximum size of the active set\n(the net result of the update requests). We give similar bounds for the\ninformation gain and the variance gain. In fact, all these bounds are\ncorollaries of a more general result, stated in terms of decision rules --\nfunctions that, given a set $S$ of labeled examples, decide whether to split\n$S$ or predict a label. Decision rules give a unified view of greedy decision\ntree algorithms regardless of the example and label domains, and lead to a\ngeneral notion of $\\epsilon$-approximate decision trees that, for natural\ndecision rules such as those used by ID3 or C4.5, implies the gain\napproximation guarantees above. The heart of our work provides a deterministic\nalgorithm that, given any decision rule and any $\\epsilon > 0$, maintains an\n$\\epsilon$-approximate tree using $O\\!\\left(\\frac{d\\, f(n)}{n}\n\\operatorname{poly}\\frac{h}{\\epsilon}\\right)$ operations per update, where\n$f(n)$ is the complexity of evaluating the rule over a set of $n$ examples and\n$h$ is the maximum height of the maintained tree.'}, {'The Power of Local Information in PageRank': 'How large a fraction of a graph must one explore to rank a small set of nodes\naccording to their PageRank scores? We show that the answer is quite nuanced,\nand depends crucially on the interplay between the correctness guarantees one\nrequires and the way one can access the graph. On the one hand, assuming the\ngraph can be accessed only via ""natural"" exploration queries that reveal small\npieces of its topology, we prove that deterministic and Las Vegas algorithms\nmust in the worst case perform $n - o(n)$ queries and explore essentially the\nentire graph, independently of the specific types of query employed. On the\nother hand we show that, depending on the types of query available, Monte Carlo\nalgorithms can perform asymptotically better: if allowed to both explore the\nlocal topology around single nodes and access nodes at random in the graph they\nneed $\\Omega(n^{2/3})$ queries in the worst case, otherwise they still need\n$\\Omega(n)$ queries similarly to Las Vegas algorithms. All our bounds\ngeneralize and tighten those already known, cover the different types of graph\nexploration queries appearing in the literature, and immediately apply also to\nthe problem of approximating the PageRank score of single nodes.'}, {'On approximating the stationary distribution of time-reversible Markov\n  chains': 'Approximating the stationary probability of a state in a Markov chain through\nMarkov chain Monte Carlo techniques is, in general, inefficient. Standard\nrandom walk approaches require $\\tilde{O}(\\tau/\\pi(v))$ operations to\napproximate the probability $\\pi(v)$ of a state $v$ in a chain with mixing time\n$\\tau$, and even the best available techniques still have complexity\n$\\tilde{O}(\\tau^{1.5}/\\pi(v)^{0.5})$, and since these complexities depend\ninversely on $\\pi(v)$, they can grow beyond any bound in the size of the chain\nor in its mixing time. In this paper we show that, for time-reversible Markov\nchains, there exists a simple randomized approximation algorithm that breaks\nthis ""small-$\\pi(v)$ barrier"".'}, {'Fully-Dynamic Decision Trees': 'We develop the first fully dynamic algorithm that maintains a decision tree\nover an arbitrary sequence of insertions and deletions of labeled examples.\nGiven $\\epsilon > 0$ our algorithm guarantees that, at every point in time,\nevery node of the decision tree uses a split with Gini gain within an additive\n$\\epsilon$ of the optimum. For real-valued features the algorithm has an\namortized running time per insertion/deletion of $O\\big(\\frac{d \\log^3\nn}{\\epsilon^2}\\big)$, which improves to $O\\big(\\frac{d \\log^2\nn}{\\epsilon}\\big)$ for binary or categorical features, while it uses space $O(n\nd)$, where $n$ is the maximum number of examples at any point in time and $d$\nis the number of features. Our algorithm is nearly optimal, as we show that any\nalgorithm with similar guarantees uses amortized running time $\\Omega(d)$ and\nspace $\\tilde{\\Omega} (n d)$. We complement our theoretical results with an\nextensive experimental evaluation on real-world data, showing the effectiveness\nof our algorithm.'}, {'Sublinear algorithms for local graph centrality estimation': 'We study the complexity of local graph centrality estimation, with the goal\nof approximating the centrality score of a given target node while exploring\nonly a sublinear number of nodes/arcs of the graph and performing a sublinear\nnumber of elementary operations. We develop a technique, that we apply to the\nPageRank and Heat Kernel centralities, for building a low-variance score\nestimator through a local exploration of the graph. We obtain an algorithm\nthat, given any node in any graph of $m$ arcs, with probability $(1-\\delta)$\ncomputes a multiplicative $(1\\pm\\epsilon)$-approximation of its score by\nexamining only $\\tilde{O}(\\min(m^{2/3} \\Delta^{1/3} d^{-2/3},\\, m^{4/5}\nd^{-3/5}))$ nodes/arcs, where $\\Delta$ and $d$ are respectively the maximum and\naverage outdegree of the graph (omitting for readability\n$\\operatorname{poly}(\\epsilon^{-1})$ and $\\operatorname{polylog}(\\delta^{-1})$\nfactors). A similar bound holds for computational complexity. We also prove a\nlower bound of $\\Omega(\\min(m^{1/2} \\Delta^{1/2} d^{-1/2}, \\, m^{2/3}\nd^{-1/3}))$ for both query complexity and computational complexity. Moreover,\nour technique yields a $\\tilde{O}(n^{2/3})$ query complexity algorithm for the\ngraph access model of [Brautbar et al., 2010], widely used in social network\nmining; we show this algorithm is optimal up to a sublogarithmic factor. These\nare the first algorithms yielding worst-case sublinear bounds for general\ndirected graphs and any choice of the target node.'}, {'Exact Recovery of Clusters in Finite Metric Spaces Using Oracle Queries': 'We investigate the problem of exact cluster recovery using oracle queries.\nPrevious results show that clusters in Euclidean spaces that are convex and\nseparated with a margin can be reconstructed exactly using only $O(\\log n)$\nsame-cluster queries, where $n$ is the number of input points. In this work, we\nstudy this problem in the more challenging non-convex setting. We introduce a\nstructural characterization of clusters, called $(\\beta,\\gamma)$-convexity,\nthat can be applied to any finite set of points equipped with a metric (or even\na semimetric, as the triangle inequality is not needed). Using\n$(\\beta,\\gamma)$-convexity, we can translate natural density properties of\nclusters (which include, for instance, clusters that are strongly non-convex in\n$\\mathbb{R}^d$) into a graph-theoretic notion of convexity. By exploiting this\nconvexity notion, we design a deterministic algorithm that recovers\n$(\\beta,\\gamma)$-convex clusters using $O(k^2 \\log n + k^2\n(6/\\beta\\gamma)^{dens(X)})$ same-cluster queries, where $k$ is the number of\nclusters and $dens(X)$ is the density dimension of the semimetric. We show that\nan exponential dependence on the density dimension is necessary, and we also\nshow that, if we are allowed to make $O(k^2 + k\\log n)$ additional queries to a\n""cluster separation"" oracle, then we can recover clusters that have different\nand arbitrary scales, even when the scale of each cluster is unknown.'}, {'Exact Recovery of Mangled Clusters with Same-Cluster Queries': 'We study the cluster recovery problem in the semi-supervised active\nclustering framework. Given a finite set of input points, and an oracle\nrevealing whether any two points lie in the same cluster, our goal is to\nrecover all clusters exactly using as few queries as possible. To this end, we\nrelax the spherical $k$-means cluster assumption of Ashtiani et al.\\ to allow\nfor arbitrary ellipsoidal clusters with margin. This removes the assumption\nthat the clustering is center-based (i.e., defined through an optimization\nproblem), and includes all those cases where spherical clusters are\nindividually transformed by any combination of rotations, axis scalings, and\npoint deletions. We show that, even in this much more general setting, it is\nstill possible to recover the latent clustering exactly using a number of\nqueries that scales only logarithmically with the number of input points. More\nprecisely, we design an algorithm that, given $n$ points to be partitioned into\n$k$ clusters, uses $O(k^3 \\ln k \\ln n)$ oracle queries and $\\tilde{O}(kn +\nk^3)$ time to recover the clustering with zero misclassification error. The\n$O(\\cdot)$ notation hides an exponential dependence on the dimensionality of\nthe clusters, which we show to be necessary thus characterizing the query\ncomplexity of the problem. Our algorithm is simple, easy to implement, and can\nalso learn the clusters using low-stretch separators, a class of ellipsoids\nwith additional theoretical guarantees. Experiments on large synthetic datasets\nconfirm that we can reconstruct clusterings exactly and efficiently.'}]","Context: Graph-based machines learning techniques are pivotal for social network analysis, bioinformatics and other real-world large-scale networks, yet the classification of nodes in such complex structures remains an intriguing challenge. Our study focuses on classifying nodes as monophonic halfspaces—a convexity-based approach pertinent to concepts in Euclidean space adapted for graphs rather than relying on homophily.    

Objective: The principal goal is to develop and analyze efficient algorithms for learning binary classifiers on graph vertices when class memberships adhere to monophonic halfspace criteria.  

Innovations: We introduce polynomial-time algorithms for consistency checking, empirical risk minimization, active learning, and online learning, tackling computational complexity issues common in graph machine learning. Specifically, we present a polynomial-time consistency checker and an empirical risk minimization algorithm, both underpinning efficient machine learning strategies on graph data.

Methods: The algorithms are designed for the realizable case using standard empirical risk minimization techniques. To ensure queryset tractability and polynomial time efficiency, we implement machine learning strategies tailored to the monophonic halfspace nature of graph classifiers.

Results: Our consistency checker significantly improves upon existing NP-hard complexity bounds through a novel polynomial-time technique. Empirical risk minimization yields efficient learning algorithms with optimal theoretical guarantees for training monophonic halfspace classifiers on graph vertices.

Contributions: The outcomes notably advance theoretical foundations and practical applications in efficient graph machine learning. By achieving polynomial time bounds, we enable effective large-scale node classification in networks.

Applications: The enhanced algorithms have potential applicability in social network analysis, disease propagation studies, cybersecurity strategies, computer vision tasks, drug discovery, and more, improving analytical precision and efficiency in these domains.

This research introduces a novel and efficient graph machine learning framework with broad implications for the future of complex data analysis across disciplines."
"Multiple choice questions (MCQs) are a popular method for evaluating
students' knowledge due to their efficiency in administration and grading.
Crafting high-quality math MCQs is a labor-intensive process that requires
educators to formulate precise stems and plausible distractors. Recent advances
in large language models (LLMs) have sparked interest in automating MCQ
creation, but challenges persist in ensuring mathematical accuracy and
addressing student errors. This paper introduces a prototype tool designed to
facilitate collaboration between LLMs and educators for streamlining the math
MCQ generation process. We conduct a pilot study involving math educators to
investigate how the tool can help them simplify the process of crafting
high-quality math MCQs. We found that while LLMs can generate well-formulated
question stems, their ability to generate distractors that capture common
student errors and misconceptions is limited. Nevertheless, a human-AI
collaboration has the potential to enhance the efficiency and effectiveness of
MCQ generation.","[{'SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and\n  Visual Cues': 'In second language vocabulary learning, existing works have primarily focused\non either the learning interface or scheduling personalized retrieval practices\nto maximize memory retention. However, the learning content, i.e., the\ninformation presented on flashcards, has mostly remained constant. Keyword\nmnemonic is a notable learning strategy that relates new vocabulary to existing\nknowledge by building an acoustic and imagery link using a keyword that sounds\nalike. Beyond that, producing verbal and visual cues associated with the\nkeyword to facilitate building these links requires a manual process and is not\nscalable. In this paper, we explore an opportunity to use large language models\nto automatically generate verbal and visual cues for keyword mnemonics. Our\napproach, an end-to-end pipeline for auto-generating verbal and visual cues,\ncan automatically generate highly memorable cues. We investigate the\neffectiveness of our approach via a human participant experiment by comparing\nit with manually generated cues.'}, {'Bridged Adversarial Training': 'Adversarial robustness is considered as a required property of deep neural\nnetworks. In this study, we discover that adversarially trained models might\nhave significantly different characteristics in terms of margin and smoothness,\neven they show similar robustness. Inspired by the observation, we investigate\nthe effect of different regularizers and discover the negative effect of the\nsmoothness regularizer on maximizing the margin. Based on the analyses, we\npropose a new method called bridged adversarial training that mitigates the\nnegative effect by bridging the gap between clean and adversarial examples. We\nprovide theoretical and empirical evidence that the proposed method provides\nstable and better robustness, especially for large perturbations.'}, {'GradDiv: Adversarial Robustness of Randomized Neural Networks via\n  Gradient Diversity Regularization': 'Deep learning is vulnerable to adversarial examples. Many defenses based on\nrandomized neural networks have been proposed to solve the problem, but fail to\nachieve robustness against attacks using proxy gradients such as the\nExpectation over Transformation (EOT) attack. We investigate the effect of the\nadversarial attacks using proxy gradients on randomized neural networks and\ndemonstrate that it highly relies on the directional distribution of the loss\ngradients of the randomized neural network. We show in particular that proxy\ngradients are less effective when the gradients are more scattered. To this\nend, we propose Gradient Diversity (GradDiv) regularizations that minimize the\nconcentration of the gradients to build a robust randomized neural network. Our\nexperiments on MNIST, CIFAR10, and STL10 show that our proposed GradDiv\nregularizations improve the adversarial robustness of randomized neural\nnetworks against a variety of state-of-the-art attack methods. Moreover, our\nmethod efficiently reduces the transferability among sample models of\nrandomized neural networks.'}, {'Enhancing Data Efficiency and Feature Identification for Lithium-Ion\n  Battery Lifespan Prediction by Deciphering Interpretation of Temporal\n  Patterns and Cyclic Variability Using Attention-Based Models': 'Accurately predicting the lifespan of lithium-ion batteries is crucial for\noptimizing operational strategies and mitigating risks. While numerous studies\nhave aimed at predicting battery lifespan, few have examined the\ninterpretability of their models or how such insights could improve\npredictions. Addressing this gap, we introduce three innovative models that\nintegrate shallow attention layers into a foundational model from our previous\nwork, which combined elements of recurrent and convolutional neural networks.\nUtilizing a well-known public dataset, we showcase our methodology\'s\neffectiveness. Temporal attention is applied to identify critical timesteps and\nhighlight differences among test cell batches, particularly underscoring the\nsignificance of the ""rest"" phase. Furthermore, by applying cyclic attention via\nself-attention to context vectors, our approach effectively identifies key\ncycles, enabling us to strategically decrease the input size for quicker\npredictions. Employing both single- and multi-head attention mechanisms, we\nhave systematically minimized the required input from 100 to 50 and then to 30\ncycles, refining this process based on cyclic attention scores. Our refined\nmodel exhibits strong regression capabilities, accurately forecasting the\ninitiation of rapid capacity fade with an average deviation of only 58 cycles\nby analyzing just the initial 30 cycles of easily accessible input data.'}, {'Understanding Catastrophic Overfitting in Single-step Adversarial\n  Training': 'Although fast adversarial training has demonstrated both robustness and\nefficiency, the problem of ""catastrophic overfitting"" has been observed. This\nis a phenomenon in which, during single-step adversarial training, the robust\naccuracy against projected gradient descent (PGD) suddenly decreases to 0%\nafter a few epochs, whereas the robust accuracy against fast gradient sign\nmethod (FGSM) increases to 100%. In this paper, we demonstrate that\ncatastrophic overfitting is very closely related to the characteristic of\nsingle-step adversarial training which uses only adversarial examples with the\nmaximum perturbation, and not all adversarial examples in the adversarial\ndirection, which leads to decision boundary distortion and a highly curved loss\nsurface. Based on this observation, we propose a simple method that not only\nprevents catastrophic overfitting, but also overrides the belief that it is\ndifficult to prevent multi-step adversarial attacks with single-step\nadversarial training.'}, {'Comment on Transferability and Input Transformation with Additive Noise': 'Adversarial attacks have verified the existence of the vulnerability of\nneural networks. By adding small perturbations to a benign example, adversarial\nattacks successfully generate adversarial examples that lead misclassification\nof deep learning models. More importantly, an adversarial example generated\nfrom a specific model can also deceive other models without modification. We\ncall this phenomenon ``transferability"". Here, we analyze the relationship\nbetween transferability and input transformation with additive noise by\nmathematically proving that the modified optimization can produce more\ntransferable adversarial examples.'}, {'Improving the Utility of Differentially Private Clustering through\n  Dynamical Processing': 'This study aims to alleviate the trade-off between utility and privacy in the\ntask of differentially private clustering. Existing works focus on simple\nclustering methods, which show poor clustering performance for non-convex\nclusters. By utilizing Morse theory, we hierarchically connect the Gaussian\nsub-clusters to fit complex cluster distributions. Because differentially\nprivate sub-clusters are obtained through the existing methods, the proposed\nmethod causes little or no additional privacy loss. We provide a theoretical\nbackground that implies that the proposed method is inductive and can achieve\nany desired number of clusters. Experiments on various datasets show that our\nframework achieves better clustering performance at the same privacy level,\ncompared to the existing methods.'}, {'Neural Networks vs. Splines: Advances in Numerical Extruder Design': ""We present a novel application of neural networks to design improved mixing\nelements for single-screw extruders. Specifically, we propose to use neural\nnetworks in numerical shape optimization to parameterize geometries. Geometry\nparameterization is crucial in enabling efficient shape optimization as it\nallows for optimizing complex shapes using only a few design variables. Recent\napproaches often utilize CAD data in conjunction with spline-based methods\nwhere the spline's control points serve as design variables. Consequently,\nthese approaches rely on the same design variables as specified by the human\ndesigner. While this choice is convenient, it either restricts the design to\nsmall modifications of given, initial design features - effectively prohibiting\ntopological changes - or yields undesirably many design variables. In this\nwork, we step away from CAD and spline-based approaches and construct an\nartificial, feature-dense yet low-dimensional optimization space using a\ngenerative neural network. Using the neural network for the geometry\nparameterization extends state-of-the-art methods in that the resulting design\nspace is not restricted to user-prescribed modifications of certain basis\nshapes. Instead, within the same optimization space, we can interpolate between\nand explore seemingly unrelated designs. To show the performance of this new\napproach, we integrate the developed shape parameterization into our numerical\ndesign framework for dynamic mixing elements in plastics extrusion. Finally, we\nchallenge the novel method in a competitive setting against current free-form\ndeformation-based approaches and demonstrate the method's performance even at\nthis early stage.""}, {'Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond': 'We study convergence lower bounds of without-replacement stochastic gradient\ndescent (SGD) for solving smooth (strongly-)convex finite-sum minimization\nproblems. Unlike most existing results focusing on final iterate lower bounds\nin terms of the number of components $n$ and the number of epochs $K$, we seek\nbounds for arbitrary weighted average iterates that are tight in all factors\nincluding the condition number $\\kappa$. For SGD with Random Reshuffling, we\npresent lower bounds that have tighter $\\kappa$ dependencies than existing\nbounds. Our results are the first to perfectly close the gap between lower and\nupper bounds for weighted average iterates in both strongly-convex and convex\ncases. We also prove weighted average iterate lower bounds for arbitrary\npermutation-based SGD, which apply to all variants that carefully choose the\nbest permutation. Our bounds improve the existing bounds in factors of $n$ and\n$\\kappa$ and thereby match the upper bounds shown for a recently proposed\nalgorithm called GraB.'}, {'Differentially Private Sharpness-Aware Training': 'Training deep learning models with differential privacy (DP) results in a\ndegradation of performance. The training dynamics of models with DP show a\nsignificant difference from standard training, whereas understanding the\ngeometric properties of private learning remains largely unexplored. In this\npaper, we investigate sharpness, a key factor in achieving better\ngeneralization, in private learning. We show that flat minima can help reduce\nthe negative effects of per-example gradient clipping and the addition of\nGaussian noise. We then verify the effectiveness of Sharpness-Aware\nMinimization (SAM) for seeking flat minima in private learning. However, we\nalso discover that SAM is detrimental to the privacy budget and computational\ntime due to its two-step optimization. Thus, we propose a new sharpness-aware\ntraining method that mitigates the privacy-optimization trade-off. Our\nexperimental results demonstrate that the proposed method improves the\nperformance of deep learning models with DP from both scratch and fine-tuning.\nCode is available at https://github.com/jinseongP/DPSAT.'}]","Title: Enhancing Math MCQ Generation with Human-Enhanced Distractor Generation Engine (HEDGE)

Abstract

As educational environments seek more effective methods to streamline assessment and learning, a critical and time-demanding task is crafting high-quality math multiple choice questions (MCQs). This research introduces the Human Enhanced Distractor Generation Engine (HEDGE), a tool developed to alleviate the constraints in math MCQogenesis, especially concerning the generation of appropriate misconceptions and corresponding distractors. The core objective of HEDGE is to elicit the expertise of math educators by providing an infrastructure that leverages an AI language model for all but the crucial step of anomaly detection, correction, and explanation refinement.

**Innovations**: HEDGE is an AI-driven framework that prompts language models to generate math MCQs along with likely student misconceptions and distractors, which can then be edited and refined by math educators. This process not only streamlines MCQ creation but also ensures a more nuanced understanding of common student error patterns.

**Methods**: Employing a structured two-step process, HEDGE begins with the AI-generated question stem, key answer, and explanation tailored to specific math teaching objectives (key competencies or KCs). The educators then edit this content, including any marked misconceptions the AI has identified, and assign feedback to aid error resolution. The second step involves refining distractor generation to align with the refined question components and educator feedback for each MCQ.

**Results**: The pilot study, involving four math education professionals, demonstrated a 70% validity rate for question stem, key, and explanation generation by HEDGE. However, only 37% of the generated misconceptions, distractors, and feedback were considered valid. Noteworthy was the explicit failure of the AI to predict the nature of some common student errors and misconceptions, highlighting the necessity for human correction.

**Contributions**: The primary contribution of this research is the demonstration of a computationally augmented, educator-led system for generating math MCQs that includes both intelligence-powered and human-crafted misconceptions, ultimately enhancing the system's ability to identify student errors while reducing the cognitive load on educators.

**Applications**: The HEDGE system provides a practical solution to the challenges of crafting math MCQs by integrating human expertise with AI-generated content, thereby accelerating the development of assessments and potentially improving student learning outcomes. Its application can be widened to other subject areas requiring nuanced understanding of common errors and misconceptions.

In summary, this research proposes a novel approach to math MCQ generation by combining the AI-based automation of question formulation with the human correction and refinement of potential student error patterns, aiming to significantly reduce the time and effort required in educational assessment development."
"Multi-agent reinforcement learning (MARL) algorithms often struggle to find
strategies close to Pareto optimal Nash Equilibrium, owing largely to the lack
of efficient exploration. The problem is exacerbated in sparse-reward settings,
caused by the larger variance exhibited in policy learning. This paper
introduces MESA, a novel meta-exploration method for cooperative multi-agent
learning. It learns to explore by first identifying the agents' high-rewarding
joint state-action subspace from training tasks and then learning a set of
diverse exploration policies to ""cover"" the subspace. These trained exploration
policies can be integrated with any off-policy MARL algorithm for test-time
tasks. We first showcase MESA's advantage in a multi-step matrix game.
Furthermore, experiments show that with learned exploration policies, MESA
achieves significantly better performance in sparse-reward tasks in several
multi-agent particle environments and multi-agent MuJoCo environments, and
exhibits the ability to generalize to more challenging tasks at test time.","[{'Sparse Feedback Controller: From Open-loop Solution to Closed-loop\n  Realization': 'In this paper, we explore the discrete time sparse feedback control for a\nlinear invariant system, where the proposed optimal feedback controller enjoys\ninput sparsity by using a dynamic linear compensator, i.e., the components of\nfeedback control signal having the smallest possible nonzero values. The\nresulting augmented dynamics ensures closed-loop stability, which infers sparse\nfeedback controller from open-loop solution to closed-loop realization. In\nparticular, we show that the implemented sparse feedback (closed-loop) control\nsolution is equivalent to the original sparse (open-loop) control solution\nunder a specified basis. We then extend the dynamic compensator to a\nfeedforward tracking control problem. Finally, numerical examples demonstrate\nthe effectiveness of proposed control approach.'}, {'Atomicity in Distributed Quantum Computing': 'Atomicity is a ubiquitous assumption in distributed computing, under which\nactions are indivisible and appear sequential. In classical computing, this\nassumption has several theoretical and practical guarantees. In quantum\ncomputing, although atomicity is still commonly assumed, it has not been\nseriously studied, and a rigorous basis for it is missing. Classical results on\natomicity do not directly carry over to distributed quantum computing, due to\nnew challenges caused by quantum entanglement and the measurement problem from\nthe underlying quantum mechanics.\n  In this paper, we initiate the study of atomicity in distributed quantum\ncomputing. A formal model of (non-atomic) distributed quantum system is\nestablished. Based on the Dijkstra-Lamport condition, the system dynamics and\nobservable dynamics of a distributed quantum system are defined, which\ncorrespond to the quantum state of and classically observable events in the\nsystem, respectively. Within this framework, we prove that local actions can be\nregarded as if they were atomic, up to the observable dynamics of the system.'}, {'Quantum Recursive Programming with Quantum Case Statements': 'We introduce a novel scheme of quantum recursive programming, in which large\nunitary transformations, i.e. quantum gates, can be recursively defined using\nquantum case statements, which are quantum counterparts of conditionals and\ncase statements extensively used in classical programming. A simple programming\nlanguage for supporting this kind of quantum recursion is defined, and its\nsemantics is formally described. A series of examples are presented to show\nthat some quantum algorithms can be elegantly written as quantum recursive\nprograms.'}, {'Verification of Recursively Defined Quantum Circuits': 'Recursive techniques have recently been introduced into quantum programming\nso that a variety of large quantum circuits and algorithms can be elegantly and\neconomically programmed. In this paper, we present a proof system for formal\nverification of the correctness of recursively defined quantum circuits. The\nsoundness and (relative) completeness of the proof system are established. To\ndemonstrating its effectiveness, a series of application examples of the proof\nsystem are given, including (multi-qubit) controlled gates, a quantum circuit\ngenerating (multi-qubit) GHZ (Greenberger-Horne-Zeilinger) states, recursive\ndefinition of quantum Fourier transform, quantum state preparation, and quantum\nrandom-access memories (QRAM).'}, {'Fast Quantum Algorithms for Trace Distance Estimation': 'In quantum information, trace distance is a basic metric of\ndistinguishability between quantum states. However, there is no known efficient\napproach to estimate the value of trace distance in general. In this paper, we\npropose efficient quantum algorithms for estimating the trace distance within\nadditive error $\\varepsilon$ between mixed quantum states of rank $r$.\nSpecifically, we first provide a quantum algorithm using $r \\cdot \\widetilde\nO(1/\\varepsilon^2)$ queries to the quantum circuits that prepare the\npurifications of quantum states. Then, we modify this quantum algorithm to\nobtain another algorithm using $\\widetilde O(r^2/\\varepsilon^5)$ samples of\nquantum states, which can be applied to quantum state certification. These\nalgorithms have query/sample complexities that are independent of the dimension\n$N$ of quantum states, and their time complexities only incur an extra $O(\\log\n(N))$ factor. In addition, we show that the decision version of low-rank trace\ndistance estimation is $\\mathsf{BQP}$-complete.'}, {'Quantum Lower Bounds by Sample-to-Query Lifting': ""The polynomial method by Beals, Buhrman, Cleve, Mosca, and de Wolf (FOCS\n1998) and the adversary method by Ambainis (STOC 2000) have been shown to be\npowerful in proving quantum query lower bounds for a wide variety of problems.\nIn this paper, we propose an arguably new method for proving quantum query\nlower bounds by a quantum sample-to-query lifting theorem, which is from an\ninformation theory perspective. Using this method, we obtain the following new\nresults:\n  1. A quadratic relation between quantum sample and query complexities\nregarding quantum property testing, which is optimal and saturated by quantum\nstate discrimination.\n  2. A matching lower bound $\\widetilde \\Omega(\\beta)$ for quantum Gibbs\nsampling at inverse temperature $\\beta$, showing that the quantum Gibbs sampler\nby Gily\\'en, Su, Low, and Wiebe (STOC 2019) is optimal.\n  3. A new lower bound $\\widetilde \\Omega(1/\\sqrt{\\Delta})$ for the\nentanglement entropy problem with gap $\\Delta$, which was recently studied by\nShe and Yuen (ITCS 2023).\n  4. A series of quantum query lower bounds for matrix spectrum testing, based\non the sample lower bounds for quantum state spectrum testing by O'Donnell and\nWright (STOC 2015).\n  In addition, we also provide unified proofs for some known lower bounds that\nhave been proven previously via different techniques, including those for\nphase/amplitude estimation and Hamiltonian simulation.""}, {'Time-Efficient Quantum Entropy Estimator via Samplizer': 'Entropy is a measure of the randomness of a system. Estimating the entropy of\na quantum state is a basic problem in quantum information. In this paper, we\nintroduce a time-efficient quantum approach to estimating the von Neumann\nentropy $S(\\rho)$ and R\\\'enyi entropy $S_\\alpha(\\rho)$ of an $N$-dimensional\nquantum state $\\rho$, given access to independent samples of $\\rho$.\nSpecifically, we provide the following quantum estimators.\n  1. A quantum estimator for $S(\\rho)$ with time complexity $\\widetilde\nO(N^2)$, improving the prior best time complexity $\\widetilde O (N^6)$ by\nAcharya, Issa, Shende, and Wagner (2020) and Bavarian, Mehraba, and Wright\n(2016).\n  2. A quantum estimator for $S_\\alpha(\\rho)$ with time complexity $\\widetilde\nO(N^{4/\\alpha-2})$ for $0 < \\alpha < 1$ and $\\widetilde O(N^{4-2/\\alpha})$ for\n$\\alpha > 1$, improving the prior best time complexity $\\widetilde\nO(N^{6/\\alpha})$ for $0 < \\alpha < 1$ and $\\widetilde O(N^6)$ for $\\alpha > 1$\nby Acharya, Issa, Shende, and Wagner (2020), though at a cost of a slightly\nlarger sample complexity.\n  Moreover, these estimators are naturally extensible to the low-rank case.\n  Technically, our method is quite different from the previous ones that are\nbased on weak Schur sampling and Young diagrams. At the heart of our\nconstruction, is a novel tool called samplizer, which can ""samplize"" a quantum\nquery algorithm to a quantum algorithm with similar behavior using only samples\nof quantum states; this suggests a unified framework for estimating quantum\nentropies. Specifically, when a quantum oracle $U$ block-encodes a mixed\nquantum state $\\rho$, any quantum query algorithm using $Q$ queries to $U$ can\nbe samplized to a $\\delta$-close (in the diamond norm) quantum algorithm using\n$\\widetilde \\Theta(Q^2/\\delta)$ samples of $\\rho$. Moreover, this samplization\nis proven to be optimal, up to a polylogarithmic factor.'}, {'Geometric Approach to Mirabolic Schur-Weyl Duality of Type A': 'We commence by constructing the mirabolic quantum Schur algebra, utilizing\nthe convolution algebra defined on the variety of triples of two $n$-step\npartial flags and a vector. Subsequently, we employ a stabilization procedure\nto derive the mirabolic quantum $\\mathfrak{gl}_n$. Then we present the\ngeometric approach of the mirabolic Schur-Weyl duality of type $A$.'}, {'The Bulk-boundary Correspondence in Non-Hermitian Hopf-link Exceptional\n  Line Semimetals': 'We consider a 3-dimensional (3D) non-Hermitian exceptional line semimetal\nmodel and take open boundary conditions in x, y, and z directions separately.\nIn each case, we calculate the parameter regions where the bulk-boundary\ncorrespondence is broken. The breakdown of the bulk-boundary correspondence is\nmanifested by the deviation from unit circles of generalized Brillouin zones\n(GBZ) and the discrepancy between spectra calculated with open boundary\nconditions (OBC) and periodic boundary conditions (PBC). The consistency\nbetween OBC and PBC spectra can be recovered if the PBC spectra are calculated\nwith GBZs. We use both unit-circle Brillouin zones (BZ) and GBZs to plot the\ntopological phase diagrams. The systematic analysis of the differences between\nthe two phase diagrams suggests that it is necessary to use GBZ to characterize\nthe bulk-boundary correspondence of non-Hermitian models.'}, {'Noise-like Pulses from an All-Normal-Dispersion Fiber Laser with\n  Weakened Spectrum Filtering': 'Noise-like pulses (NLP) are extremely sought after in many fields. Here, we\nexperimentally and numerically investigated the generation of noise-like pulses\nin an all-normal-dispersion fiber laser with weak spectrum filtering. With the\ninsertion of the grating as a tunable spectrum filter, the laser operates at a\nstable dissipative soliton state with a 3.84 ps duration. Replacing the grating\nwith a mirror, NLPs with double-scale intensity autocorrelation trace is\nultimately attained. Numerical simulations are performed in detail and\ndemonstrated that with the absence of a spectrum filter, the stable state\ncannot be established but form the random pulse cluster. The random pulse\ncluster achieves dynamic stability with suitable feedback, and the NLP is\nultimately generated. The NLP here is directly evolved by the initial noise,\nand no other states occur during its evolution. These explorations could deepen\nthe understanding of NLP and enrich the complex dynamics of the ANDi ultrafast\nfiber laser.'}]","A Meta-learning Approach to Autonomous Exploration in Multi-Agent Systems

Background: Research on efficient exploration in multi-agent systems (MAS) is crucial due to the exponential growth of the joint state-action space, challenging the effectiveness of traditional exploration methods. Recent advancements have mostly focused on creating task-specific or uniform exploration policies, but their reliance on task agnostic rewards and failure to leverage intrinsic task structure impedes their success in complex MAS settings.

Objective: This paper introduces a meta-learning framework and corresponding exploration policies aimed at autonomous exploration in multi-agent systems. The primary goal is to develop structured exploration strategies that are context-aware and learn to steer the agent towards optimal outcomes autonomously.

Innovations: The framework utilizes meta-learning to adapt exploration policies to new tasks, simultaneously considering the intrinsic structure of each task. Utilizing a reward matrix and specific learning objectives enables autonomous selection of exploration strategies that efficiently navigate the action space.

Methods: The proposed exploration policies adopt a straightforward matrix game framework to demonstrate and theoretically explain the shortcomings of conventional exploration strategies. Through empirical evaluation in matrix climb games, discrete- and continuous-fidelity multi-agent environments, the meta-trained policies outperform task-specific and uniform exploration schemes.

Results: The meta-trained exploration policies effectively reduce inefficiencies associated with randomization and coordinate well in environments with significant Pareto optimal solutions. Across both multi-level and continuous environments, the policies rapidly learn optimal actions with minimal steps.

Contributions: The research reveals the power of meta-learning in enhancing exploration capabilities for autonomous agents without changes to the underlying agent algorithms. This approach significantly streamlines the process of learning from sparse and reward-confusing environments by deducing optimal actions directly, offering a scalable solution to exploration challenges in multi-agent systems.

Applications: The findings have important implications for improving the learning efficiency of agents in cooperative missions, enabling more intelligent decision-making in harsh or deceptive environments, and advancing the autonomy of artificial life forms in complex multi-agent tasks. The demonstrated capabilities make it a promising tool for real-world applications where reinforcement learning agents must operate in multi-agent settings.

The presented meta-learning approach to exploration not only broadens the applicability of reinforcement learning in complex research areas but also facilitates more responsive and efficient adaptive decision-making in the eyes of autonomous agents."
"In this study, we introduce Generative Manufacturing Systems (GMS) as a novel
approach to effectively manage and coordinate autonomous manufacturing assets,
thereby enhancing their responsiveness and flexibility to address a wide array
of production objectives and human preferences. Deviating from traditional
explicit modeling, GMS employs generative AI, including diffusion models and
ChatGPT, for implicit learning from envisioned futures, marking a shift from a
model-optimum to a training-sampling decision-making. Through the integration
of generative AI, GMS enables complex decision-making through interactive
dialogue with humans, allowing manufacturing assets to generate multiple
high-quality global decisions that can be iteratively refined based on human
feedback. Empirical findings showcase GMS's substantial improvement in system
resilience and responsiveness to uncertainties, with decision times reduced
from seconds to milliseconds. The study underscores the inherent creativity and
diversity in the generated solutions, facilitating human-centric
decision-making through seamless and continuous human-machine interactions.","[{'Asymptotic behavior of Nernst-Planck equation': 'This paper is devoted to the Nernst-Planck system of equations with an\nexternal potential of confinement. The main result is concerned with the\nasymptotic behaviour of the solution of the Cauchy problem. We will prove that\nthe optimal exponential rate of convergence of the solution to the unique\nstationary solution is determined by the spectral gap of the linearized problem\naround the minimizer of the free energy. The key issue is to consider an\nadapted notion of scalar product.'}, {'Blind stain separation using model-aware generative learning and its\n  applications on fluorescence microscopy images': ""Multiple stains are usually used to highlight biological substances in\nbiomedical image analysis. To decompose multiple stains for co-localization\nquantification, blind source separation is usually performed. Prior model-based\nstain separation methods usually rely on stains' spatial distributions over an\nimage and may fail to solve the co-localization problem. With the advantage of\nmachine learning, deep generative models are used for this purpose. Since prior\nknowledge of imaging models is ignored in purely data-driven solutions, these\nmethods may be sub-optimal. In this study, a novel learning-based blind source\nseparation framework is proposed, where the physical model of biomedical\nimaging is incorporated to regularize the learning process. The introduced\nmodel-relevant adversarial loss couples all generators in the framework and\nlimits the capacities of the generative models. Further more, a training\nalgorithm is innovated for the proposed framework to avoid inter-generator\nconfusion during learning. This paper particularly takes fluorescence unmixing\nin fluorescence microscopy images as an application example of the proposed\nframework. Qualitative and quantitative experimentation on a public\nfluorescence microscopy image set demonstrates the superiority of the proposed\nmethod over both prior model-based approaches and learning-based methods.""}, {'Phase transition and asymptotic behaviour of flocking Cucker-Smale model': 'In this paper, we study a continuous ocking Cucker-Smale model with noise,\nwhich has isotropic and polarized stationary solutions depending on the\nintensity of the noise. The first result establishes the threshold value of the\nnoise parameter which drives the phase transition. This threshold value is used\nto classify all stationary solutions and their linear stability properties.\nUsing an entropy, these stability properties are extended to the non-linear\nregime. The second result is concerned with the asymptotic behaviour of the\nsolutions of the evolution problem. In several cases, we prove that stable\nsolutions attract the other solutions with an optimal exponential rate of\nconvergence determined by the spectral gap of the linearized problem around the\nstable solutions. The spectral gap has to be computed in a norm adapted to the\nnon-local term.'}, {'Large Time Asymptotic Behaviors of Two Types of Fast Diffusion Equations': 'We consider two types of non linear fast diffusion equations in R^N:(1)\nExternal drift type equation with general external potential. It is a natural\nextension of the harmonic potential case, which has been studied in many\npapers. In this paper we can prove the large time asymptotic behavior to the\nstationary state by using entropy methods.(2) Mean-field type equation with the\nconvolution term. The stationary solution is the minimizer of the free energy\nfunctional, which has direct relation with reverse Hardy-Littlewood-Sobolev\ninequalities. In this paper, we prove that for some special cases, it also\nexists large time asymptotic behavior to the stationary state.'}, {'CoverTheFace: face covering monitoring and demonstrating using deep\n  learning and statistical shape analysis': 'Wearing a mask is a strong protection against the COVID-19 pandemic, even\nthough the vaccine has been successfully developed and is widely available.\nHowever, many people wear them incorrectly. This observation prompts us to\ndevise an automated approach to monitor the condition of people wearing masks.\nUnlike previous studies, our work goes beyond mask detection; it focuses on\ngenerating a personalized demonstration on proper mask-wearing, which helps\npeople use masks better through visual demonstration rather than text\nexplanation. The pipeline starts from the detection of face covering. For\nimages where faces are improperly covered, our mask overlay module incorporates\nstatistical shape analysis (SSA) and dense landmark alignment to approximate\nthe geometry of a face and generates corresponding face-covering examples. Our\nresults show that the proposed system successfully identifies images with faces\ncovered properly. Our ablation study on mask overlay suggests that the SSA\nmodel helps to address variations in face shapes, orientations, and scales. The\nfinal face-covering examples, especially half profile face images, surpass\nprevious arts by a noticeable margin.'}, {'G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima': 'Deep neural networks (DNNs) have demonstrated promising results in various\ncomplex tasks. However, current DNNs encounter challenges with\nover-parameterization, especially when there is limited training data\navailable. To enhance the generalization capability of DNNs, the Mixup\ntechnique has gained popularity. Nevertheless, it still produces suboptimal\noutcomes. Inspired by the successful Sharpness-Aware Minimization (SAM)\napproach, which establishes a connection between the sharpness of the training\nloss landscape and model generalization, we propose a new learning framework\ncalled Generalized-Mixup, which combines the strengths of Mixup and SAM for\ntraining DNN models. The theoretical analysis provided demonstrates how the\ndeveloped G-Mix framework enhances generalization. Additionally, to further\noptimize DNN performance with the G-Mix framework, we introduce two novel\nalgorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the\ntraining data into two subsets based on the sharpness-sensitivity of each\nexample to address the issue of ""manifold intrusion"" in Mixup. Both theoretical\nexplanations and experimental results reveal that the proposed BG-Mix and\nDG-Mix algorithms further enhance model generalization across multiple datasets\nand models, achieving state-of-the-art performance.'}, {'Generalized logarithmic Hardy-Littlewood-Sobolev inequality': 'This paper is devoted to logarithmic Hardy-Littlewood-Sobolev inequalities in\nthe two-dimensional Euclidean space, in presence of an external potential with\nlogarithmic growth. The coupling with the potential introduces a new parameter,\nwith two regimes. The attractive regime reflects the standard logarithmic\nHardy-Littlewood-Sobolev inequality. The second regime corresponds to a reverse\ninequality, with the opposite sign in the convolution term, that allows us to\nbound the free energy of a drift-diffusion-Poisson system from below. Our\nmethod is based on an extension of an entropy method proposed by E. Carlen, J.\nCarrillo and M. Loss, and on a nonlinear diffusion equation.'}, {'Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE': ""The recent success of SimCSE has greatly advanced state-of-the-art sentence\nrepresentations. However, the original formulation of SimCSE does not fully\nexploit the potential of hard negative samples in contrastive learning. This\nstudy introduces an unsupervised contrastive learning framework that combines\nSimCSE with hard negative mining, aiming to enhance the quality of sentence\nembeddings. The proposed focal-InfoNCE function introduces self-paced\nmodulation terms in the contrastive objective, downweighting the loss\nassociated with easy negatives and encouraging the model focusing on hard\nnegatives. Experimentation on various STS benchmarks shows that our method\nimproves sentence embeddings in terms of Spearman's correlation and\nrepresentation alignment and uniformity.""}, {'Patterns in a Smoluchowski Equation': 'We analyze the dynamics of concentrated polymer solutions modeled by a 2D\nSmoluchowski equation. We describe the long time behavior of the polymer\nsuspensions in a fluid. \\par When the flow influence is neglected the equation\nhas a gradient structure. The presence of a simple flow introduces significant\nstructural changes in the dynamics. We study the case of an externally imposed\nflow with homogeneous gradient. We show that the equation is still dissipative\nbut new phenomena appear. The dynamics depend on both the concentration\nintensity and the structure of the flow. In certain {\\it limit cases} the\nequation has a gradient structure, in an appropriate reference frame, and the\nsolutions evolve to either a steady state or a tumbling wave. For small\nperturbations of the gradient structure we show that some features of the\ngradient dynamics survive: for small concentrations the solutions evolve in the\nlong time limit to a steady state and for high concentrations there is a\ntumbling wave.'}, {'Phi-entropies for Fokker-Planck and kinetic Fokker-Planck equations': 'This paper is devoted to $\\phi$-entropies applied to Fokker-Planck and\nkinetic Fokker-Planck equations in the whole space, with confinement. The\nso-called $\\phi$-entropies are Lyapunov functionals which typically interpolate\nbetween Gibbs entropies and L2 estimates. We review some of their properties in\nthe case of diffusion equations of Fokker-Planck type, give new and simplified\nproofs, and then adapt these methods to a kinetic Fokker-Planck equation acting\non a phase space with positions and velocities. At kinetic level, since the\ndiffusion only acts on the velocity variable, the transport operator plays an\nessential role in the relaxation process. Here we adopt the H1 point of view\nand establish a sharp decay rate. Rather than giving general but quantitatively\nvague estimates, our goal here is to consider simple cases, benchmark available\nmethods and obtain sharp estimates on a key example. Some $\\phi$-entropies give\nrise to improved entropy -- entropy production inequalities and, as a\nconsequence, to faster decay rates for entropy estimates of solutions to\nnon-degenerate diffusion equations. Our main result is to prove that faster\nentropy decay also holds at kinetic level and that optimal decay rates are\nachieved only in asymptotic regimes.'}]","Title: Resilient and Human-Centric Generation of Manufacturing System Configurations Using Guided Diffusion Models

### Background
The manufacturing sector confronts unprecedented challenges due to technological advancements, regulatory changes, and operational uncertainties such as production disruptions, prompting the necessity for more flexible and adaptive systems. This paper addresses the core issue of creating novel, efficient, and scalable manufacturing system configurations that can swiftly accommodate variable requirements and constraints.

### Objective
The main objective is to develop a methodology that leverages AI to dynamically generate and optimize manufacturing system configurations in real-time, enabling them to adapt seamlessly to diverse operational demands and human skill levels.

### Innovations
Key innovations include the utilization of randomization techniques for human skill assignments, deploying a dedicated daydreaming process for iterative skill optimization, and the implementation of diffusion models in Python for iterative generation of system configurations. A unique feature is the integration of guidance elements within the learning model, adding a new dimension to traditional model-based approaches.

### Methods
The procedure involves random skill allocation, followed by a diffusive modeling process that simulates and refines system configurations. The models process a vast amount of data, predicting and testing configurations against predefined criteria, which is then optimized using CPLEX for efficiency. Through multiple generations, the system learns and adapts to achieve optimal performance.

### Results
The research demonstrates significant improvements in generating configurations that efficiently match human skill levels, desired throughput, and asset composition, typically achieving high accuracy and diversity in configurations. Performance analysis reveals enhanced responsiveness to uncertainties and a notable reduction in decision variability.

### Contributions
The paper contributes to the field by introducing an automated mechanism that streamlines the configuration generation process, reducing human labor and error rates. The incorporation of iterative refinement and learning abilities affords systems the potential to self-adapt and improve continuously, fostering a more sustainable manufacturing environment.

### Applications
The primary application lies in the automation of manufacturing system decision-making, promising to boost productivity, reduce costs, and enhance flexibility. The findings can inform the design of future intelligent manufacturing systems capable of addressing real-time complexities and varying demands.

This research offers a significant advancement in the field of manufacturing systems through the development of a novel methodology and demonstrates its potential to revolutionize the adaptability and efficiency of modern manufacturing operations."
"One-on-one tutoring is widely acknowledged as an effective instructional
method, conditioned on qualified tutors. However, the high demand for qualified
tutors remains a challenge, often necessitating the training of novice tutors
(i.e., trainees) to ensure effective tutoring. Research suggests that providing
timely explanatory feedback can facilitate the training process for trainees.
However, it presents challenges due to the time-consuming nature of assessing
trainee performance by human experts. Inspired by the recent advancements of
large language models (LLMs), our study employed the GPT-4 model to build an
explanatory feedback system. This system identifies trainees' responses in
binary form (i.e., correct/incorrect) and automatically provides template-based
feedback with responses appropriately rephrased by the GPT-4 model. We
conducted our study on 410 responses from trainees across three training
lessons: Giving Effective Praise, Reacting to Errors, and Determining What
Students Know. Our findings indicate that: 1) using a few-shot approach, the
GPT-4 model effectively identifies correct/incorrect trainees' responses from
three training lessons with an average F1 score of 0.84 and an AUC score of
0.85; and 2) using the few-shot approach, the GPT-4 model adeptly rephrases
incorrect trainees' responses into desired responses, achieving performance
comparable to that of human experts.","[{'A Neural Network Based Explainable Recommender System': 'Recommendation system could help the companies to persuade users to visit or\nconsume at a particular place, which was based on many traditional methods such\nas the set of collaborative filtering algorithms. Most research discusses the\nmodel design or feature engineering methods to minimize the root mean square\nerror (RMSE) of rating prediction, but lacks exploring the ways to generate the\nreasons for recommendations. This paper proposed an integrated neural network\nbased model which integrates rating scores prediction and explainable words\ngeneration. Based on the experimental results, this model presented lower RMSE\ncompared with traditional methods, and generate the explanation of\nrecommendation to convince customers to visit the recommended place.'}, {'AI Chatbots as Multi-Role Pedagogical Agents: Transforming Engagement in\n  CS Education': ""This study investigates the use of Artificial Intelligence (AI)-powered,\nmulti-role chatbots as a means to enhance learning experiences and foster\nengagement in computer science education. Leveraging a design-based research\napproach, we develop, implement, and evaluate a novel learning environment\nenriched with four distinct chatbot roles: Instructor Bot, Peer Bot, Career\nAdvising Bot, and Emotional Supporter Bot. These roles, designed around the\ntenets of Self-Determination Theory, cater to the three innate psychological\nneeds of learners - competence, autonomy, and relatedness. Additionally, the\nsystem embraces an inquiry-based learning paradigm, encouraging students to ask\nquestions, seek solutions, and explore their curiosities.\n  We test this system in a higher education context over a period of one month\nwith 200 participating students, comparing outcomes with conditions involving a\nhuman tutor and a single chatbot. Our research utilizes a mixed-methods\napproach, encompassing quantitative measures such as chat log sequence\nanalysis, and qualitative methods including surveys and focus group interviews.\nBy integrating cutting-edge Natural Language Processing techniques such as\ntopic modelling and sentiment analysis, we offer an in-depth understanding of\nthe system's impact on learner engagement, motivation, and inquiry-based\nlearning.\n  This study, through its rigorous design and innovative approach, provides\nsignificant insights into the potential of AI-empowered, multi-role chatbots in\nreshaping the landscape of computer science education and fostering an\nengaging, supportive, and motivating learning environment.""}, {'3DG: A Framework for Using Generative AI for Handling Sparse Learner\n  Performance Data From Intelligent Tutoring Systems': ""Learning performance data (e.g., quiz scores and attempts) is significant for\nunderstanding learner engagement and knowledge mastery level. However, the\nlearning performance data collected from Intelligent Tutoring Systems (ITSs)\noften suffers from sparsity, impacting the accuracy of learner modeling and\nknowledge assessments. To address this, we introduce the 3DG framework\n(3-Dimensional tensor for Densification and Generation), a novel approach\ncombining tensor factorization with advanced generative models, including\nGenerative Adversarial Network (GAN) and Generative Pre-trained Transformer\n(GPT), for enhanced data imputation and augmentation. The framework operates by\nfirst representing the data as a three-dimensional tensor, capturing dimensions\nof learners, questions, and attempts. It then densifies the data through tensor\nfactorization and augments it using Generative AI models, tailored to\nindividual learning patterns identified via clustering. Applied to data from an\nAutoTutor lesson by the Center for the Study of Adult Literacy (CSAL), the 3DG\nframework effectively generated scalable, personalized simulations of learning\nperformance. Comparative analysis revealed GAN's superior reliability over\nGPT-4 in this context, underscoring its potential in addressing data sparsity\nchallenges in ITSs and contributing to the advancement of personalized\neducational technology.""}, {'Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise\n  Given to Students in Synthetic Dialogues': ""Research suggests that providing specific and timely feedback to human tutors\nenhances their performance. However, it presents challenges due to the\ntime-consuming nature of assessing tutor performance by human evaluators. Large\nlanguage models, such as the AI-chatbot ChatGPT, hold potential for offering\nconstructive feedback to tutors in practical settings. Nevertheless, the\naccuracy of AI-generated feedback remains uncertain, with scant research\ninvestigating the ability of models like ChatGPT to deliver effective feedback.\nIn this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a\ntutor-student setting. We use two different prompting approaches, the zero-shot\nchain of thought and the few-shot chain of thought, to identify specific\ncomponents of effective praise based on five criteria. These approaches are\nthen compared to the results of human graders for accuracy. Our goal is to\nassess the extent to which GPT-4 can accurately identify each praise criterion.\nWe found that both zero-shot and few-shot chain of thought approaches yield\ncomparable results. GPT-4 performs moderately well in identifying instances\nwhen the tutor offers specific and immediate praise. However, GPT-4\nunderperforms in identifying the tutor's ability to deliver sincere praise,\nparticularly in the zero-shot prompting scenario where examples of sincere\ntutor praise statements were not provided. Future work will focus on enhancing\nprompt engineering, developing a more general tutoring rubric, and evaluating\nour method using real-life tutoring dialogues.""}, {'Elucidating STEM Concepts through Generative AI: A Multi-modal\n  Exploration of Analogical Reasoning': ""This study explores the integration of generative artificial intelligence\n(AI), specifically large language models, with multi-modal analogical reasoning\nas an innovative approach to enhance science, technology, engineering, and\nmathematics (STEM) education. We have developed a novel system that utilizes\nthe capacities of generative AI to transform intricate principles in\nmathematics, physics, and programming into comprehensible metaphors. To further\naugment the educational experience, these metaphors are subsequently converted\ninto visual form. Our study aims to enhance the learners' understanding of STEM\nconcepts and their learning engagement by using the visual metaphors. We\nexamine the efficacy of our system via a randomized A/B/C test, assessing\nlearning gains and motivation shifts among the learners. Our study demonstrates\nthe potential of applying large language models to educational practice on STEM\nsubjects. The results will shed light on the design of educational system in\nterms of harnessing AI's potential to empower educational stakeholders.""}, {'Does Informativeness Matter? Active Learning for Educational Dialogue\n  Act Classification': 'Dialogue Acts (DAs) can be used to explain what expert tutors do and what\nstudents know during the tutoring process. Most empirical studies adopt the\nrandom sampling method to obtain sentence samples for manual annotation of DAs,\nwhich are then used to train DA classifiers. However, these studies have paid\nlittle attention to sample informativeness, which can reflect the information\nquantity of the selected samples and inform the extent to which a classifier\ncan learn patterns. Notably, the informativeness level may vary among the\nsamples and the classifier might only need a small amount of low informative\nsamples to learn the patterns. Random sampling may overlook sample\ninformativeness, which consumes human labelling costs and contributes less to\ntraining the classifiers. As an alternative, researchers suggest employing\nstatistical sampling methods of Active Learning (AL) to identify the\ninformative samples for training the classifiers. However, the use of AL\nmethods in educational DA classification tasks is under-explored. In this\npaper, we examine the informativeness of annotated sentence samples. Then, the\nstudy investigates how the AL methods can select informative samples to support\nDA classifiers in the AL sampling process. The results reveal that most\nannotated sentences present low informativeness in the training dataset and the\npatterns of these sentences can be easily captured by the DA classifier. We\nalso demonstrate how AL methods can reduce the cost of manual annotation in the\nAL sampling process.'}, {""Using Large Language Models to Assess Tutors' Performance in Reacting to\n  Students Making Math Errors"": ""Research suggests that tutors should adopt a strategic approach when\naddressing math errors made by low-efficacy students. Rather than drawing\ndirect attention to the error, tutors should guide the students to identify and\ncorrect their mistakes on their own. While tutor lessons have introduced this\npedagogical skill, human evaluation of tutors applying this strategy is arduous\nand time-consuming. Large language models (LLMs) show promise in providing\nreal-time assessment to tutors during their actual tutoring sessions, yet\nlittle is known regarding their accuracy in this context. In this study, we\ninvestigate the capacity of generative AI to evaluate real-life tutors'\nperformance in responding to students making math errors. By analyzing 50\nreal-life tutoring dialogues, we find both GPT-3.5-Turbo and GPT-4 demonstrate\nproficiency in assessing the criteria related to reacting to students making\nerrors. However, both models exhibit limitations in recognizing instances where\nthe student made an error. Notably, GPT-4 tends to overidentify instances of\nstudents making errors, often attributing student uncertainty or inferring\npotential errors where human evaluators did not. Future work will focus on\nenhancing generalizability by assessing a larger dataset of dialogues and\nevaluating learning transfer. Specifically, we will analyze the performance of\ntutors in real-life scenarios when responding to students' math errors before\nand after lesson completion on this crucial tutoring skill.""}, {'Predicting Learning Performance with Large Language Models: A Study in\n  Adult Literacy': 'Intelligent Tutoring Systems (ITSs) have significantly enhanced adult\nliteracy training, a key factor for societal participation, employment\nopportunities, and lifelong learning. Our study investigates the application of\nadvanced AI models, including Large Language Models (LLMs) like GPT-4, for\npredicting learning performance in adult literacy programs in ITSs. This\nresearch is motivated by the potential of LLMs to predict learning performance\nbased on its inherent reasoning and computational capabilities. By using\nreading comprehension datasets from the ITS, AutoTutor, we evaluate the\npredictive capabilities of GPT-4 versus traditional machine learning methods in\npredicting learning performance through five-fold cross-validation techniques.\nOur findings show that the GPT-4 presents the competitive predictive abilities\nwith traditional machine learning methods such as Bayesian Knowledge Tracing,\nPerformance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor\nfactorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained\non local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected\nXGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior\nperformance compared to local machine execution. Moreover, our investigation\ninto hyper-parameter tuning by GPT-4 versus grid-search suggests comparable\nperformance, albeit with less stability in the automated approach, using\nXGBoost as the case study. Our study contributes to the field by highlighting\nthe potential of integrating LLMs with traditional machine learning models to\nenhance predictive accuracy and personalize adult literacy education, setting a\nfoundation for future research in applying LLMs within ITSs.'}, {'Using Large Language Models to Provide Explanatory Feedback to Human\n  Tutors': 'Research demonstrates learners engaging in the process of producing\nexplanations to support their reasoning, can have a positive impact on\nlearning. However, providing learners real-time explanatory feedback often\npresents challenges related to classification accuracy, particularly in\ndomain-specific environments, containing situationally complex and nuanced\nresponses. We present two approaches for supplying tutors real-time feedback\nwithin an online lesson on how to give students effective praise. This\nwork-in-progress demonstrates considerable accuracy in binary classification\nfor corrective feedback of effective, or effort-based (F1 score = 0.811), and\nineffective, or outcome-based (F1 score = 0.350), praise responses. More\nnotably, we introduce progress towards an enhanced approach of providing\nexplanatory feedback using large language model-facilitated named entity\nrecognition, which can provide tutors feedback, not only while engaging in\nlessons, but can potentially suggest real-time tutor moves. Future work\ninvolves leveraging large language models for data augmentation to improve\naccuracy, while also developing an explanatory feedback interface.'}, {'Robust Educational Dialogue Act Classifiers with Low-Resource and\n  Imbalanced Datasets': 'Dialogue acts (DAs) can represent conversational actions of tutors or\nstudents that take place during tutoring dialogues. Automating the\nidentification of DAs in tutoring dialogues is significant to the design of\ndialogue-based intelligent tutoring systems. Many prior studies employ machine\nlearning models to classify DAs in tutoring dialogues and invest much effort to\noptimize the classification accuracy by using limited amounts of training data\n(i.e., low-resource data scenario). However, beyond the classification\naccuracy, the robustness of the classifier is also important, which can reflect\nthe capability of the classifier on learning the patterns from different class\ndistributions. We note that many prior studies on classifying educational DAs\nemploy cross entropy (CE) loss to optimize DA classifiers on low-resource data\nwith imbalanced DA distribution. The DA classifiers in these studies tend to\nprioritize accuracy on the majority class at the expense of the minority class\nwhich might not be robust to the data with imbalanced ratios of different DA\nclasses. To optimize the robustness of classifiers on imbalanced class\ndistributions, we propose to optimize the performance of the DA classifier by\nmaximizing the area under the ROC curve (AUC) score (i.e., AUC maximization).\nThrough extensive experiments, our study provides evidence that (i) by\nmaximizing AUC in the training process, the DA classifier achieves significant\nperformance improvement compared to the CE approach under low-resource data,\nand (ii) AUC maximization approaches can improve the robustness of the DA\nclassifier under different class imbalance ratios.'}]","Title: Enhancing Tutor Training Through AI-Powered Feedback Systems

Background: Effective feedback is a cornerstone of educational improvement, yet designing and delivering high-quality feedback is challenging. AI systems, particularly those pretrained on diverse datasets, offer the potential to automate feedback generation, making it scalable and universally applicable.

Objective: Develop a methodology for training binary classifiers adept at discerning correct from incorrect tutor responses, followed by using AI models, specifically GPT-4, to rephrase incorrect responses into more instructive feedback.

Innovations: This paper introduces the use of zero-shot and few-shot learning strategies to train binary classifiers, and advanced methods for rephrasing trainee responses incorporating both lesson principles and detailed scenarios.

Methods: We utilized a diverse set of three lessons from expert-designed training materials as the foundation. For binary classification, we laid out a prompting strategy leveraging lessons' principles. In response refinement, GPT-4 was equipped with context-specific scenarios and two example rephrasings to learn rephrasing rules.

Results: Binary classifiers operated with a high degree of accuracy, identifying the correct response type with a median F1 score of 0.761 in the zero-shot approach and an F1 score range of 0.900 – 0.942 from few-shot learning. GPT-4 rephrased responses showed a higher accuracy (M=0.86) in the zero-shot scenario and improved responsiveness in the few-shot approach without significantly altering original text structure.

Contributions: The research advances pedagogic feedback systems through AI technology, broadening the scope for scalable training of competent tutors. It introduces innovative application of GPT-4 to automate response rephrasing while maintaining robust structure endorsement.

Applications: The developed system can be integrated into tutor training programs to enhance feedback quality, making the learning process more efficient and effective. This technology opens up possibilities for more personalized and dynamic feedback mechanisms across various educational domains."
"Recent work in cross-language information retrieval (CLIR), where queries and
documents are in different languages, has shown the benefit of the
Translate-Distill framework that trains a cross-language neural dual-encoder
model using translation and distillation. However, Translate-Distill only
supports a single document language. Multilingual information retrieval (MLIR),
which ranks a multilingual document collection, is harder to train than CLIR
because the model must assign comparable relevance scores to documents in
different languages. This work extends Translate-Distill and propose
Multilingual Translate-Distill (MTD) for MLIR. We show that ColBERT-X models
trained with MTD outperform their counterparts trained ith Multilingual
Translate-Train, which is the previous state-of-the-art training approach, by
5% to 25% in nDCG@20 and 15% to 45% in MAP. We also show that the model is
robust to the way languages are mixed in training batches. Our implementation
is available on GitHub.","[{'TARexp: A Python Framework for Technology-Assisted Review Experiments': 'Technology-assisted review (TAR) is an important industrial application of\ninformation retrieval (IR) and machine learning (ML). While a small TAR\nresearch community exists, the complexity of TAR software and workflows is a\nmajor barrier to entry. Drawing on past open source TAR efforts, as well as\ndesign patterns from the IR and ML open source software, we present an open\nsource Python framework for conducting experiments on TAR algorithms. Key\ncharacteristics of this framework are declarative representations of workflows\nand experiment plans, the ability for components to play variable numbers of\nworkflow roles, and state maintenance and restart capabilities. Users can draw\non reference implementations of standard TAR algorithms while incorporating\nnovel components to explore their research interests. The framework is\navailable at https://github.com/eugene-yang/tarexp.'}, {'HLTCOE at TREC 2023 NeuCLIR Track': 'The HLTCOE team applied PLAID, an mT5 reranker, and document translation to\nthe TREC 2023 NeuCLIR track. For PLAID we included a variety of models and\ntraining techniques -- the English model released with ColBERT v2,\ntranslate-train~(TT), Translate Distill~(TD) and multilingual\ntranslate-train~(MTT). TT trains a ColBERT model with English queries and\npassages automatically translated into the document language from the MS-MARCO\nv1 collection. This results in three cross-language models for the track, one\nper language. MTT creates a single model for all three document languages by\ncombining the translations of MS-MARCO passages in all three languages into\nmixed-language batches. Thus the model learns about matching queries to\npassages simultaneously in all languages. Distillation uses scores from the mT5\nmodel over non-English translated document pairs to learn how to score\nquery-document pairs. The team submitted runs to all NeuCLIR tasks: the CLIR\nand MLIR news task as well as the technical documents task.'}, {'ToxCCIn: Toxic Content Classification with Interpretability': 'Despite the recent successes of transformer-based models in terms of\neffectiveness on a variety of tasks, their decisions often remain opaque to\nhumans. Explanations are particularly important for tasks like offensive\nlanguage or toxicity detection on social media because a manual appeal process\nis often in place to dispute automatically flagged content. In this work, we\npropose a technique to improve the interpretability of these models, based on a\nsimple and powerful assumption: a post is at least as toxic as its most toxic\nspan. We incorporate this assumption into transformer models by scoring a post\nbased on the maximum toxicity of its spans and augmenting the training process\nto identify correct spans. We find this approach effective and can produce\nexplanations that exceed the quality of those provided by Logistic Regression\nanalysis (often regarded as a highly-interpretable model), according to a human\nstudy.'}, {'Patapasco: A Python Framework for Cross-Language Information Retrieval\n  Experiments': 'While there are high-quality software frameworks for information retrieval\nexperimentation, they do not explicitly support cross-language information\nretrieval (CLIR). To fill this gap, we have created Patapsco, a Python CLIR\nframework. This framework specifically addresses the complexity that comes with\nrunning experiments in multiple languages. Patapsco is designed to be\nextensible to many language pairs, to be scalable to large document\ncollections, and to support reproducible experiments driven by a configuration\nfile. We include Patapsco results on standard CLIR collections using multiple\nsettings.'}, {'Extending Translate-Train for ColBERT-X to African Language CLIR': 'This paper describes the submission runs from the HLTCOE team at the CIRAL\nCLIR tasks for African languages at FIRE 2023. Our submissions use machine\ntranslation models to translate the documents and the training passages, and\nColBERT-X as the retrieval model. Additionally, we present a set of unofficial\nruns that use an alternative training procedure with a similar training\nsetting.'}, {'Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review': 'Technology-assisted review (TAR) refers to iterative active learning\nworkflows for document review in high recall retrieval (HRR) tasks. TAR\nresearch and most commercial TAR software have applied linear models such as\nlogistic regression to lexical features. Transformer-based models with\nsupervised tuning are known to improve effectiveness on many text\nclassification tasks, suggesting their use in TAR. We indeed find that the\npre-trained BERT model reduces review cost by 10% to 15% in TAR workflows\nsimulated on the RCV1-v2 newswire collection. In contrast, we likewise\ndetermined that linear models outperform BERT for simulated legal discovery\ntopics on the Jeb Bush e-mail collection. This suggests the match between\ntransformer pre-training corpora and the task domain is of greater significance\nthan generally appreciated. Additionally, we show that just-right language\nmodel fine-tuning on the task collection before starting active learning is\ncritical. Too little or too much fine-tuning hinders performance, worse than\nthat of linear models, even for a favorable corpus such as RCV1-v2.'}, {'On Minimizing Cost in Legal Document Review Workflows': 'Technology-assisted review (TAR) refers to human-in-the-loop machine learning\nworkflows for document review in legal discovery and other high recall review\ntasks. Attorneys and legal technologists have debated whether review should be\na single iterative process (one-phase TAR workflows) or whether model training\nand review should be separate (two-phase TAR workflows), with implications for\nthe choice of active learning algorithm. The relative cost of manual labeling\nfor different purposes (training vs. review) and of different documents\n(positive vs. negative examples) is a key and neglected factor in this debate.\nUsing a novel cost dynamics analysis, we show analytically and empirically that\nthese relative costs strongly impact whether a one-phase or two-phase workflow\nminimizes cost. We also show how category prevalence, classification task\ndifficulty, and collection size impact the optimal choice not only of workflow\ntype, but of active learning method and stopping point.'}, {'Heuristic Stopping Rules For Technology-Assisted Review': 'Technology-assisted review (TAR) refers to human-in-the-loop active learning\nworkflows for finding relevant documents in large collections. These workflows\noften must meet a target for the proportion of relevant documents found (i.e.\nrecall) while also holding down costs. A variety of heuristic stopping rules\nhave been suggested for striking this tradeoff in particular settings, but none\nhave been tested against a range of recall targets and tasks. We propose two\nnew heuristic stopping rules, Quant and QuantCI based on model-based estimation\ntechniques from survey research. We compare them against a range of proposed\nheuristics and find they are accurate at hitting a range of recall targets\nwhile substantially reducing review costs.'}, {'Certifying One-Phase Technology-Assisted Reviews': 'Technology-assisted review (TAR) workflows based on iterative active learning\nare widely used in document review applications. Most stopping rules for\none-phase TAR workflows lack valid statistical guarantees, which has\ndiscouraged their use in some legal contexts. Drawing on the theory of quantile\nestimation, we provide the first broadly applicable and statistically valid\nsample-based stopping rules for one-phase TAR. We further show theoretically\nand empirically that overshooting a recall target, which has been treated as\ninnocuous or desirable in past evaluations of stopping rules, is a major source\nof excess cost in one-phase TAR workflows. Counterintuitively, incurring a\nlarger sampling cost to reduce excess recall leads to lower total cost in\nalmost all scenarios.'}, {'TAR on Social Media: A Framework for Online Content Moderation': 'Content moderation (removing or limiting the distribution of posts based on\ntheir contents) is one tool social networks use to fight problems such as\nharassment and disinformation. Manually screening all content is usually\nimpractical given the scale of social media data, and the need for nuanced\nhuman interpretations makes fully automated approaches infeasible. We consider\ncontent moderation from the perspective of technology-assisted review (TAR): a\nhuman-in-the-loop active learning approach developed for high recall retrieval\nproblems in civil litigation and other fields. We show how TAR workflows, and a\nTAR cost model, can be adapted to the content moderation problem. We then\ndemonstrate on two publicly available content moderation data sets that a TAR\nworkflow can reduce moderation costs by 20% to 55% across a variety of\nconditions.'}]","Abstract:

The field of Multilingual Information Retrieval (MLIR) proposes search over multilingual collections of documents in diverse languages. This research introduces a novel framework, MTD (Multi-lingual Teach-and-Translate Dataset), that trains language-agnostic models directly on multilingual datasets rather than translating all content into a single language. This method ensures the retrieval system must evaluate documents based on query relevance independently of language. MTD integrates distillation from monolingual models into multilingual settings, creating a robust, language-neutral method for enhancing MLIR. The effectiveness of MTD is demonstrated across four datasets using a dual-encoder model, showing superior performance compared to traditional multilingual training (non-distillation). MTD facilitates the development of comprehensive and efficient multilingual systems, significantly contributing to the advancement of information retrieval technologies. The robustness and efficiency of MTD suggest broad applications for handling multilingual queries in real-time information systems."
"Multilingual information retrieval (MLIR) considers the problem of ranking
documents in several languages for a query expressed in a language that may
differ from any of those languages. Recent work has observed that approaches
such as combining ranked lists representing a single document language each or
using multilingual pretrained language models demonstrate a preference for one
language over others. This results in systematic unfair treatment of documents
in different languages. This work proposes a language fairness metric to
evaluate whether documents across different languages are fairly ranked through
statistical equivalence testing using the Kruskal-Wallis test. In contrast to
most prior work in group fairness, we do not consider any language to be an
unprotected group. Thus our proposed measure, PEER (Probability of
EqualExpected Rank), is the first fairness metric specifically designed to
capture the language fairness of MLIR systems. We demonstrate the behavior of
PEER on artificial ranked lists. We also evaluate real MLIR systems on two
publicly available benchmarks and show that the PEER scores align with prior
analytical findings on MLIR fairness. Our implementation is compatible with
ir-measures and is available at http://github.com/hltcoe/peer_measure.","[{'TARexp: A Python Framework for Technology-Assisted Review Experiments': 'Technology-assisted review (TAR) is an important industrial application of\ninformation retrieval (IR) and machine learning (ML). While a small TAR\nresearch community exists, the complexity of TAR software and workflows is a\nmajor barrier to entry. Drawing on past open source TAR efforts, as well as\ndesign patterns from the IR and ML open source software, we present an open\nsource Python framework for conducting experiments on TAR algorithms. Key\ncharacteristics of this framework are declarative representations of workflows\nand experiment plans, the ability for components to play variable numbers of\nworkflow roles, and state maintenance and restart capabilities. Users can draw\non reference implementations of standard TAR algorithms while incorporating\nnovel components to explore their research interests. The framework is\navailable at https://github.com/eugene-yang/tarexp.'}, {'HLTCOE at TREC 2023 NeuCLIR Track': 'The HLTCOE team applied PLAID, an mT5 reranker, and document translation to\nthe TREC 2023 NeuCLIR track. For PLAID we included a variety of models and\ntraining techniques -- the English model released with ColBERT v2,\ntranslate-train~(TT), Translate Distill~(TD) and multilingual\ntranslate-train~(MTT). TT trains a ColBERT model with English queries and\npassages automatically translated into the document language from the MS-MARCO\nv1 collection. This results in three cross-language models for the track, one\nper language. MTT creates a single model for all three document languages by\ncombining the translations of MS-MARCO passages in all three languages into\nmixed-language batches. Thus the model learns about matching queries to\npassages simultaneously in all languages. Distillation uses scores from the mT5\nmodel over non-English translated document pairs to learn how to score\nquery-document pairs. The team submitted runs to all NeuCLIR tasks: the CLIR\nand MLIR news task as well as the technical documents task.'}, {'ToxCCIn: Toxic Content Classification with Interpretability': 'Despite the recent successes of transformer-based models in terms of\neffectiveness on a variety of tasks, their decisions often remain opaque to\nhumans. Explanations are particularly important for tasks like offensive\nlanguage or toxicity detection on social media because a manual appeal process\nis often in place to dispute automatically flagged content. In this work, we\npropose a technique to improve the interpretability of these models, based on a\nsimple and powerful assumption: a post is at least as toxic as its most toxic\nspan. We incorporate this assumption into transformer models by scoring a post\nbased on the maximum toxicity of its spans and augmenting the training process\nto identify correct spans. We find this approach effective and can produce\nexplanations that exceed the quality of those provided by Logistic Regression\nanalysis (often regarded as a highly-interpretable model), according to a human\nstudy.'}, {'Patapasco: A Python Framework for Cross-Language Information Retrieval\n  Experiments': 'While there are high-quality software frameworks for information retrieval\nexperimentation, they do not explicitly support cross-language information\nretrieval (CLIR). To fill this gap, we have created Patapsco, a Python CLIR\nframework. This framework specifically addresses the complexity that comes with\nrunning experiments in multiple languages. Patapsco is designed to be\nextensible to many language pairs, to be scalable to large document\ncollections, and to support reproducible experiments driven by a configuration\nfile. We include Patapsco results on standard CLIR collections using multiple\nsettings.'}, {'Extending Translate-Train for ColBERT-X to African Language CLIR': 'This paper describes the submission runs from the HLTCOE team at the CIRAL\nCLIR tasks for African languages at FIRE 2023. Our submissions use machine\ntranslation models to translate the documents and the training passages, and\nColBERT-X as the retrieval model. Additionally, we present a set of unofficial\nruns that use an alternative training procedure with a similar training\nsetting.'}, {'Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review': 'Technology-assisted review (TAR) refers to iterative active learning\nworkflows for document review in high recall retrieval (HRR) tasks. TAR\nresearch and most commercial TAR software have applied linear models such as\nlogistic regression to lexical features. Transformer-based models with\nsupervised tuning are known to improve effectiveness on many text\nclassification tasks, suggesting their use in TAR. We indeed find that the\npre-trained BERT model reduces review cost by 10% to 15% in TAR workflows\nsimulated on the RCV1-v2 newswire collection. In contrast, we likewise\ndetermined that linear models outperform BERT for simulated legal discovery\ntopics on the Jeb Bush e-mail collection. This suggests the match between\ntransformer pre-training corpora and the task domain is of greater significance\nthan generally appreciated. Additionally, we show that just-right language\nmodel fine-tuning on the task collection before starting active learning is\ncritical. Too little or too much fine-tuning hinders performance, worse than\nthat of linear models, even for a favorable corpus such as RCV1-v2.'}, {'On Minimizing Cost in Legal Document Review Workflows': 'Technology-assisted review (TAR) refers to human-in-the-loop machine learning\nworkflows for document review in legal discovery and other high recall review\ntasks. Attorneys and legal technologists have debated whether review should be\na single iterative process (one-phase TAR workflows) or whether model training\nand review should be separate (two-phase TAR workflows), with implications for\nthe choice of active learning algorithm. The relative cost of manual labeling\nfor different purposes (training vs. review) and of different documents\n(positive vs. negative examples) is a key and neglected factor in this debate.\nUsing a novel cost dynamics analysis, we show analytically and empirically that\nthese relative costs strongly impact whether a one-phase or two-phase workflow\nminimizes cost. We also show how category prevalence, classification task\ndifficulty, and collection size impact the optimal choice not only of workflow\ntype, but of active learning method and stopping point.'}, {'Heuristic Stopping Rules For Technology-Assisted Review': 'Technology-assisted review (TAR) refers to human-in-the-loop active learning\nworkflows for finding relevant documents in large collections. These workflows\noften must meet a target for the proportion of relevant documents found (i.e.\nrecall) while also holding down costs. A variety of heuristic stopping rules\nhave been suggested for striking this tradeoff in particular settings, but none\nhave been tested against a range of recall targets and tasks. We propose two\nnew heuristic stopping rules, Quant and QuantCI based on model-based estimation\ntechniques from survey research. We compare them against a range of proposed\nheuristics and find they are accurate at hitting a range of recall targets\nwhile substantially reducing review costs.'}, {'Certifying One-Phase Technology-Assisted Reviews': 'Technology-assisted review (TAR) workflows based on iterative active learning\nare widely used in document review applications. Most stopping rules for\none-phase TAR workflows lack valid statistical guarantees, which has\ndiscouraged their use in some legal contexts. Drawing on the theory of quantile\nestimation, we provide the first broadly applicable and statistically valid\nsample-based stopping rules for one-phase TAR. We further show theoretically\nand empirically that overshooting a recall target, which has been treated as\ninnocuous or desirable in past evaluations of stopping rules, is a major source\nof excess cost in one-phase TAR workflows. Counterintuitively, incurring a\nlarger sampling cost to reduce excess recall leads to lower total cost in\nalmost all scenarios.'}, {'TAR on Social Media: A Framework for Online Content Moderation': 'Content moderation (removing or limiting the distribution of posts based on\ntheir contents) is one tool social networks use to fight problems such as\nharassment and disinformation. Manually screening all content is usually\nimpractical given the scale of social media data, and the need for nuanced\nhuman interpretations makes fully automated approaches infeasible. We consider\ncontent moderation from the perspective of technology-assisted review (TAR): a\nhuman-in-the-loop active learning approach developed for high recall retrieval\nproblems in civil litigation and other fields. We show how TAR workflows, and a\nTAR cost model, can be adapted to the content moderation problem. We then\ndemonstrate on two publicly available content moderation data sets that a TAR\nworkflow can reduce moderation costs by 20% to 55% across a variety of\nconditions.'}]","Title: Assessing Language Fairness in Multilingual Information Retrieval

Abstract: 

As multilingual information retrieval ecosystems become ubiquitous, promoting equitable access across languages is critical. This research introduces a novel methodology for analyzing fairness in multilingual search results, measuring how publications in different languages relate within retrieved sequences. Using the proposed PEER (Pairwise Evaluation of Equity for Retrieval), the study demonstrates systematic biases and quantifies these in real-world language retrieval systems. Through experimental evaluation on two collections—CLEF 2003 and NeuCLIR 2022—the techniques of fair and unfair data distribution are contrasted, illustrating the disclosure of notable language preferences in learned ranking models. Above-mentioned PEER supplements traditional fairness metrics, offering a comprehensive tool to understand systematic disparities. The findings open avenues for researchers and practitioners to design and improve multilingual AI systems that are not only accurate but also equitable in their outputs. By mitigating language biases, these systems can provide more balanced and inclusive information to users, facilitating smoother cross-lingual knowledge discovery."
"Large Language Models (LLMs) have enabled new ways to satisfy information
needs. Although great strides have been made in applying them to settings like
document ranking and short-form text generation, they still struggle to compose
complete, accurate, and verifiable long-form reports. Reports with these
qualities are necessary to satisfy the complex, nuanced, or multi-faceted
information needs of users. In this perspective paper, we draw together
opinions from industry and academia, and from a variety of related research
areas, to present our vision for automatic report generation, and -- critically
-- a flexible framework by which such reports can be evaluated. In contrast
with other summarization tasks, automatic report generation starts with a
detailed description of an information need, stating the necessary background,
requirements, and scope of the report. Further, the generated reports should be
complete, accurate, and verifiable. These qualities, which are desirable -- if
not required -- in many analytic report-writing settings, require rethinking
how to build and evaluate systems that exhibit these qualities. To foster new
efforts in building these systems, we present an evaluation framework that
draws on ideas found in various evaluations. To test completeness and accuracy,
the framework uses nuggets of information, expressed as questions and answers,
that need to be part of any high-quality generated report. Additionally,
evaluation of citations that map claims made in the report to their source
documents ensures verifiability.","[{'Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking': 'Cross-language entity linking grounds mentions in multiple languages to a\nsingle-language knowledge base. We propose a neural ranking architecture for\nthis task that uses multilingual BERT representations of the mention and the\ncontext in a neural network. We find that the multilingual ability of BERT\nleads to robust performance in monolingual and multilingual settings.\nFurthermore, we explore zero-shot language transfer and find surprisingly\nrobust performance. We investigate the zero-shot degradation and find that it\ncan be partially mitigated by a proposed auxiliary training objective, but that\nthe remaining error can best be attributed to domain shift rather than language\ntransfer.'}, {'Patapasco: A Python Framework for Cross-Language Information Retrieval\n  Experiments': 'While there are high-quality software frameworks for information retrieval\nexperimentation, they do not explicitly support cross-language information\nretrieval (CLIR). To fill this gap, we have created Patapsco, a Python CLIR\nframework. This framework specifically addresses the complexity that comes with\nrunning experiments in multiple languages. Patapsco is designed to be\nextensible to many language pairs, to be scalable to large document\ncollections, and to support reproducible experiments driven by a configuration\nfile. We include Patapsco results on standard CLIR collections using multiple\nsettings.'}, {'Extending Translate-Train for ColBERT-X to African Language CLIR': 'This paper describes the submission runs from the HLTCOE team at the CIRAL\nCLIR tasks for African languages at FIRE 2023. Our submissions use machine\ntranslation models to translate the documents and the training passages, and\nColBERT-X as the retrieval model. Additionally, we present a set of unofficial\nruns that use an alternative training procedure with a similar training\nsetting.'}, {'Improving Zero-Shot Multi-Lingual Entity Linking': 'Entity linking -- the task of identifying references in free text to relevant\nknowledge base representations -- often focuses on single languages. We\nconsider multilingual entity linking, where a single model is trained to link\nreferences to same-language knowledge bases in several languages. We propose a\nneural ranker architecture, which leverages multilingual transformer\nrepresentations of text to be easily applied to a multilingual setting. We then\nexplore how a neural ranker trained in one language (e.g. English) transfers to\nan unseen language (e.g. Chinese), and find that while there is a consistent\nbut not large drop in performance. How can this drop in performance be\nalleviated? We explore adding an adversarial objective to force our model to\nlearn language-invariant representations. We find that using this approach\nimproves recall in several datasets, often matching the in-language\nperformance, thus alleviating some of the performance loss occurring from\nzero-shot transfer.'}, {'HLTCOE at TREC 2023 NeuCLIR Track': 'The HLTCOE team applied PLAID, an mT5 reranker, and document translation to\nthe TREC 2023 NeuCLIR track. For PLAID we included a variety of models and\ntraining techniques -- the English model released with ColBERT v2,\ntranslate-train~(TT), Translate Distill~(TD) and multilingual\ntranslate-train~(MTT). TT trains a ColBERT model with English queries and\npassages automatically translated into the document language from the MS-MARCO\nv1 collection. This results in three cross-language models for the track, one\nper language. MTT creates a single model for all three document languages by\ncombining the translations of MS-MARCO passages in all three languages into\nmixed-language batches. Thus the model learns about matching queries to\npassages simultaneously in all languages. Distillation uses scores from the mT5\nmodel over non-English translated document pairs to learn how to score\nquery-document pairs. The team submitted runs to all NeuCLIR tasks: the CLIR\nand MLIR news task as well as the technical documents task.'}, {'Interactive Knowledge Base Population': 'Most work on building knowledge bases has focused on collecting entities and\nfacts from as large a collection of documents as possible. We argue for and\ndescribe a new paradigm where the focus is on a high-recall extraction over a\nsmall collection of documents under the supervision of a human expert, that we\ncall Interactive Knowledge Base Population (IKBP).'}, {'Improving Neural Named Entity Recognition with Gazetteers': 'The goal of this work is to improve the performance of a neural named entity\nrecognition system by adding input features that indicate a word is part of a\nname included in a gazetteer. This article describes how to generate gazetteers\nfrom the Wikidata knowledge graph as well as how to integrate the information\ninto a neural NER system. Experiments reveal that the approach yields\nperformance gains in two distinct languages: a high-resource, word-based\nlanguage, English and a high-resource, character-based language, Chinese.\nExperiments were also performed in a low-resource language, Russian on a newly\nannotated Russian NER corpus from Reddit tagged with four core types and twelve\nextended types. This article reports a baseline score. It is a longer version\nof a paper in the 33rd FLAIRS conference (Song et al. 2020).'}, {'HC4: A New Suite of Test Collections for Ad Hoc CLIR': 'HC4 is a new suite of test collections for ad hoc Cross-Language Information\nRetrieval (CLIR), with Common Crawl News documents in Chinese, Persian, and\nRussian, topics in English and in the document languages, and graded relevance\njudgments. New test collections are needed because existing CLIR test\ncollections built using pooling of traditional CLIR runs have systematic gaps\nin their relevance judgments when used to evaluate neural CLIR methods. The HC4\ncollections contain 60 topics and about half a million documents for each of\nChinese and Persian, and 54 topics and five million documents for Russian.\nActive learning was used to determine which documents to annotate after being\nseeded using interactive search and judgment. Documents were judged on a\nthree-grade relevance scale. This paper describes the design and construction\nof the new test collections and provides baseline results for demonstrating\ntheir utility for evaluating systems.'}, {'Neural Approaches to Multilingual Information Retrieval': 'Providing access to information across languages has been a goal of\nInformation Retrieval (IR) for decades. While progress has been made on Cross\nLanguage IR (CLIR) where queries are expressed in one language and documents in\nanother, the multilingual (MLIR) task to create a single ranked list of\ndocuments across many languages is considerably more challenging. This paper\ninvestigates whether advances in neural document translation and pretrained\nmultilingual neural language models enable improvements in the state of the art\nover earlier MLIR techniques. The results show that although combining neural\ndocument translation with neural ranking yields the best Mean Average Precision\n(MAP), 98% of that MAP score can be achieved with an 84% reduction in indexing\ntime by using a pretrained XLM-R multilingual language model to index documents\nin their native language, and that 2% difference in effectiveness is not\nstatistically significant. Key to achieving these results for MLIR is to\nfine-tune XLM-R using mixed-language batches from neural translations of MS\nMARCO passages.'}, {'Translate-Distill: Learning Cross-Language Dense Retrieval by\n  Translation and Distillation': 'Prior work on English monolingual retrieval has shown that a cross-encoder\ntrained using a large number of relevance judgments for query-document pairs\ncan be used as a teacher to train more efficient, but similarly effective,\ndual-encoder student models. Applying a similar knowledge distillation approach\nto training an efficient dual-encoder model for Cross-Language Information\nRetrieval (CLIR), where queries and documents are in different languages, is\nchallenging due to the lack of a sufficiently large training collection when\nthe query and document languages differ. The state of the art for CLIR thus\nrelies on translating queries, documents, or both from the large English MS\nMARCO training set, an approach called Translate-Train. This paper proposes an\nalternative, Translate-Distill, in which knowledge distillation from either a\nmonolingual cross-encoder or a CLIR cross-encoder is used to train a\ndual-encoder CLIR student model. This richer design space enables the teacher\nmodel to perform inference in an optimized setting, while training the student\nmodel directly for CLIR. Trained models and artifacts are publicly available on\nHuggingface.'}]","Abstract

Stratified Reporting for Automated Report Generation: An Evaluation Framework on Comprehensive Needs

The research proposes a novel evaluation framework, Automated Report Generation Under Evaluation (ARGUE), for automated report generation systems. Inspired by lessons from information retrieval, summarization, and text generation research, ARGUE aims to address the evolving needs of highly complex, multi-faceted information requests.

The core objective is to develop an assessment system capable of evaluating the quality and utility of reports, crafted through advanced AI technologies, to ensure they comprehensively address the multifaceted needs of diversified audiences. The framework innovatively employs the concept of 'information nuggets'—a distinctive approach in summarization literature—to capture the essential content that reports must encapsulate.

To operationalize this concept, ARGUE focuses on two main components: 'report requests' that define the content needs, and 'document collections' acting as a citable resource to verify the accuracy and completeness of the reports. The contribution of this research is manifest in the systematic decomposition of complex reporting tasks into simpler, targeted sub-tasks via the creation of stipulated information nuggets. This enhances navigability and enables the evaluation of the reports based on the metric of their content-coverage, leading to a more precise assessment of task satisfaction, accuracy, and reference credibility.

The application of this framework is particularly suited for developmental assessments of Natural Language Generation (NLG) systems, advancing the field's understanding of sudden shifts in information needs across domains. Its potential extends to a wide array of domains, from legal documentation, where ARGUE could refine automated summary generation of complex legal documents, to scientific research, aiding the throughput of comprehensive, accurately referenced reports.

In conclusion, ARGUE represents a foundational step toward a more sophisticated and effective evaluation of automated report generation systems, capable of delivering reports that address complex multi-faceted information requests, with applications that support a wide array of industries' reporting needs."
"Identifying layers within text-to-image models which control visual
attributes can facilitate efficient model editing through closed-form updates.
Recent work, leveraging causal tracing show that early Stable-Diffusion
variants confine knowledge primarily to the first layer of the CLIP
text-encoder, while it diffuses throughout the UNet.Extending this framework,
we observe that for recent models (e.g., SD-XL, DeepFloyd), causal tracing
fails in pinpointing localized knowledge, highlighting challenges in model
editing. To address this issue, we introduce the concept of Mechanistic
Localization in text-to-image models, where knowledge about various visual
attributes (e.g., ""style"", ""objects"", ""facts"") can be mechanistically localized
to a small fraction of layers in the UNet, thus facilitating efficient model
editing. We localize knowledge using our method LocoGen which measures the
direct effect of intermediate layers to output generation by performing
interventions in the cross-attention layers of the UNet. We then employ
LocoEdit, a fast closed-form editing method across popular open-source
text-to-image models (including the latest SD-XL)and explore the possibilities
of neuron-level model editing. Using Mechanistic Localization, our work offers
a better view of successes and failures in localization-based text-to-image
model editing. Code will be available at
https://github.com/samyadeepbasu/LocoGen.","[{'Membership Model Inversion Attacks for Deep Networks': 'With the increasing adoption of AI, inherent security and privacy\nvulnerabilities formachine learning systems are being discovered. One such\nvulnerability makes itpossible for an adversary to obtain private information\nabout the types of instancesused to train the targeted machine learning model.\nThis so-called model inversionattack is based on sequential leveraging of\nclassification scores towards obtaininghigh confidence representations for\nvarious classes. However, for deep networks,such procedures usually lead to\nunrecognizable representations that are uselessfor the adversary. In this\npaper, we introduce a more realistic definition of modelinversion, where the\nadversary is aware of the general purpose of the attackedmodel (for instance,\nwhether it is an OCR system or a facial recognition system),and the goal is to\nfind realistic class representations within the corresponding lower-dimensional\nmanifold (of, respectively, general symbols or general faces). To thatend, we\nleverage properties of generative adversarial networks for constructinga\nconnected lower-dimensional manifold, and demonstrate the efficiency of\nourmodel inversion attack that is carried out within that manifold.'}, {'On Second-Order Group Influence Functions for Black-Box Predictions': 'With the rapid adoption of machine learning systems in sensitive\napplications, there is an increasing need to make black-box models explainable.\nOften we want to identify an influential group of training samples in a\nparticular test prediction for a given machine learning model. Existing\ninfluence functions tackle this problem by using first-order approximations of\nthe effect of removing a sample from the training set on model parameters. To\ncompute the influence of a group of training samples (rather than an individual\npoint) in model predictions, the change in optimal model parameters after\nremoving that group from the training set can be large. Thus, in such cases,\nthe first-order approximation can be loose. In this paper, we address this\nissue and propose second-order influence functions for identifying influential\ngroups in test-time predictions. For linear models, across different sizes and\ntypes of groups, we show that using the proposed second-order influence\nfunction improves the correlation between the computed influence values and the\nground truth ones. We also show that second-order influence functions could be\nused with optimization techniques to improve the selection of the most\ninfluential group for a test-sample.'}, {'On Hard Episodes in Meta-Learning': ""Existing meta-learners primarily focus on improving the average task accuracy\nacross multiple episodes. Different episodes, however, may vary in hardness and\nquality leading to a wide gap in the meta-learner's performance across\nepisodes. Understanding this issue is particularly critical in industrial\nfew-shot settings, where there is limited control over test episodes as they\nare typically uploaded by end-users. In this paper, we empirically analyse the\nbehaviour of meta-learners on episodes of varying hardness across three\nstandard benchmark datasets: CIFAR-FS, mini-ImageNet, and tiered-ImageNet.\nSurprisingly, we observe a wide gap in accuracy of around 50% between the\nhardest and easiest episodes across all the standard benchmarks and\nmeta-learners. We additionally investigate various properties of hard episodes\nand highlight their connection to catastrophic forgetting during meta-training.\nTo address the issue of sub-par performance on hard episodes, we investigate\nand benchmark different meta-training strategies based on adversarial training\nand curriculum learning. We find that adversarial training strategies are much\nmore powerful than curriculum learning in improving the prediction performance\non hard episodes.""}, {'Influence Functions in Deep Learning Are Fragile': 'Influence functions approximate the effect of training samples in test-time\npredictions and have a wide variety of applications in machine learning\ninterpretability and uncertainty estimation. A commonly-used (first-order)\ninfluence function can be implemented efficiently as a post-hoc method\nrequiring access only to the gradients and Hessian of the model. For linear\nmodels, influence functions are well-defined due to the convexity of the\nunderlying loss function and are generally accurate even across difficult\nsettings where model changes are fairly large such as estimating group\ninfluences. Influence functions, however, are not well-understood in the\ncontext of deep learning with non-convex loss functions. In this paper, we\nprovide a comprehensive and large-scale empirical study of successes and\nfailures of influence functions in neural network models trained on datasets\nsuch as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments,\nwe show that the network architecture, its depth and width, as well as the\nextent of model parameterization and regularization techniques have strong\neffects in the accuracy of influence functions. In particular, we find that (i)\ninfluence estimates are fairly accurate for shallow networks, while for deeper\nnetworks the estimates are often erroneous; (ii) for certain network\narchitectures and datasets, training with weight-decay regularization is\nimportant to get high-quality influence estimates; and (iii) the accuracy of\ninfluence estimates can vary significantly depending on the examined test\npoints. These results suggest that in general influence functions in deep\nlearning are fragile and call for developing improved influence estimation\nmethods to mitigate these issues in non-convex setups.'}, {'Strong Baselines for Parameter Efficient Few-Shot Fine-tuning': ""Few-shot classification (FSC) entails learning novel classes given only a few\nexamples per class after a pre-training (or meta-training) phase on a set of\nbase classes. Recent works have shown that simply fine-tuning a pre-trained\nVision Transformer (ViT) on new test classes is a strong approach for FSC.\nFine-tuning ViTs, however, is expensive in time, compute and storage. This has\nmotivated the design of parameter efficient fine-tuning (PEFT) methods which\nfine-tune only a fraction of the Transformer's parameters. While these methods\nhave shown promise, inconsistencies in experimental conditions make it\ndifficult to disentangle their advantage from other experimental factors\nincluding the feature extractor architecture, pre-trained initialization and\nfine-tuning algorithm, amongst others. In our paper, we conduct a large-scale,\nexperimentally consistent, empirical analysis to study PEFTs for few-shot image\nclassification. Through a battery of over 1.8k controlled experiments on\nlarge-scale few-shot benchmarks including Meta-Dataset (MD) and ORBIT, we\nuncover novel insights on PEFTs that cast light on their efficacy in\nfine-tuning ViTs for few-shot classification. Through our controlled empirical\nstudy, we have two main findings: (i) Fine-tuning just the LayerNorm parameters\n(which we call LN-Tune) during few-shot adaptation is an extremely strong\nbaseline across ViTs pre-trained with both self-supervised and supervised\nobjectives, (ii) For self-supervised ViTs, we find that simply learning a set\nof scaling parameters for each attention matrix (which we call AttnScale) along\nwith a domain-residual adapter (DRA) module leads to state-of-the-art\nperformance (while being $\\sim\\!$ 9$\\times$ more parameter-efficient) on MD.\nOur extensive empirical findings set strong baselines and call for rethinking\nthe current design of PEFT methods for FSC.""}, {'Localizing and Editing Knowledge in Text-to-Image Generative Models': 'Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have\nachieved unprecedented quality of photorealism with state-of-the-art FID scores\non MS-COCO and other generation benchmarks. Given a caption, image generation\nrequires fine-grained knowledge about attributes such as object structure,\nstyle, and viewpoint amongst others. Where does this information reside in\ntext-to-image generative models? In our paper, we tackle this question and\nunderstand how knowledge corresponding to distinct visual attributes is stored\nin large-scale text-to-image diffusion models. We adapt Causal Mediation\nAnalysis for text-to-image models and trace knowledge about distinct visual\nattributes to various (causal) components in the (i) UNet and (ii) text-encoder\nof the diffusion model. In particular, we show that unlike generative\nlarge-language models, knowledge about different attributes is not localized in\nisolated components, but is instead distributed amongst a set of components in\nthe conditional UNet. These sets of components are often distinct for different\nvisual attributes. Remarkably, we find that the CLIP text-encoder in public\ntext-to-image models such as Stable-Diffusion contains only one causal state\nacross different visual attributes, and this is the first self-attention layer\ncorresponding to the last subject token of the attribute in the caption. This\nis in stark contrast to the causal states in other language models which are\noften the mid-MLP layers. Based on this observation of only one causal state in\nthe text-encoder, we introduce a fast, data-free model editing method\nDiff-QuickFix which can effectively edit concepts in text-to-image models.\nDiffQuickFix can edit (ablate) concepts in under a second with a closed-form\nupdate, providing a significant 1000x speedup and comparable editing\nperformance to existing fine-tuning based editing methods.'}, {'Privacy Leakage Avoidance with Switching Ensembles': 'We consider membership inference attacks, one of the main privacy issues in\nmachine learning. These recently developed attacks have been proven successful\nin determining, with confidence better than a random guess, whether a given\nsample belongs to the dataset on which the attacked machine learning model was\ntrained. Several approaches have been developed to mitigate this privacy\nleakage but the tradeoff performance implications of these defensive mechanisms\n(i.e., accuracy and utility of the defended machine learning model) are not\nwell studied yet. We propose a novel approach of privacy leakage avoidance with\nswitching ensembles (PASE), which both protects against current membership\ninference attacks and does that with very small accuracy penalty, while\nrequiring acceptable increase in training and inference time. We test our PASE\nmethod, along with the the current state-of-the-art PATE approach, on three\ncalibration image datasets and analyze their tradeoffs.'}, {'Topic Segmentation in the Wild: Towards Segmentation of Semi-structured\n  & Unstructured Chats': 'Breaking down a document or a conversation into multiple contiguous segments\nbased on its semantic structure is an important and challenging problem in NLP,\nwhich can assist many downstream tasks. However, current works on topic\nsegmentation often focus on segmentation of structured texts. In this paper, we\ncomprehensively analyze the generalization capabilities of state-of-the-art\ntopic segmentation models on unstructured texts. We find that: (a) Current\nstrategies of pre-training on a large corpus of structured text such as\nWiki-727K do not help in transferability to unstructured texts. (b) Training\nfrom scratch with only a relatively small-sized dataset of the target\nunstructured domain improves the segmentation results by a significant margin.'}, {'Distilling Knowledge from Text-to-Image Generative Models Improves\n  Visio-Linguistic Reasoning in CLIP': ""Image-text contrastive models like CLIP have wide applications in zero-shot\nclassification, image-text retrieval, and transfer learning. However, they\noften struggle on compositional visio-linguistic tasks (e.g., attribute-binding\nor object-relationships) where their performance is no better than random\nchance. To address this, we introduce SDS-CLIP, a lightweight and\nsample-efficient distillation method to enhance CLIP's compositional\nvisio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation\nobjective borrowed from large text-to-image generative models like\nStable-Diffusion, which are known for their strong visio-linguistic reasoning\nabilities. On the challenging Winoground benchmark, SDS-CLIP improves the\nvisio-linguistic performance of various CLIP models by up to 7%, while on the\nARO dataset, it boosts performance by up to 3%. This work underscores the\npotential of well-designed distillation objectives from generative models to\nenhance contrastive image-text models with improved visio-linguistic reasoning\ncapabilities.""}, {'Rethinking Artistic Copyright Infringements in the Era of Text-to-Image\n  Generative Models': 'Recent text-to-image generative models such as Stable Diffusion are extremely\nadept at mimicking and generating copyrighted content, raising concerns amongst\nartists that their unique styles may be improperly copied. Understanding how\ngenerative models copy ""artistic style"" is more complex than duplicating a\nsingle image, as style is comprised by a set of elements (or signature) that\nfrequently co-occurs across a body of work, where each individual work may vary\nsignificantly. In our paper, we first reformulate the problem of ""artistic\ncopyright infringement"" to a classification problem over image sets, instead of\nprobing image-wise similarities. We then introduce ArtSavant, a practical\n(i.e., efficient and easy to understand) tool to (i) determine the unique style\nof an artist by comparing it to a reference dataset of works from 372 artists\ncurated from WikiArt, and (ii) recognize if the identified style reappears in\ngenerated images. We leverage two complementary methods to perform artistic\nstyle classification over image sets, includingTagMatch, which is a novel\ninherently interpretable and attributable method, making it more suitable for\nbroader use by non-technical stake holders (artists, lawyers, judges, etc).\nLeveraging ArtSavant, we then perform a large-scale empirical study to provide\nquantitative insight on the prevalence of artistic style copying across 3\npopular text-to-image generative models. Namely, amongst a dataset of prolific\nartists (including many famous ones), only 20% of them appear to have their\nstyles be at a risk of copying via simple prompting of today\'s popular\ntext-to-image generative models.'}]","Title: Layer Localization Model Editing in Text-to-Image Generation: LOCOGEN and LOCOEDIT for Knowledge Tracing, Manipulation, and Updating

Background: Text-to-image generation models, a critical component of AI systems, assimilate textual instructions into visual outputs, presenting challenges in interpretability and control. Leveraging these models to manipulate specific visual attributes, such as style, objects, and facts, necessitates effective methods of intervention.

Objective: The core aim is to enhance interpretability and control in text-to-image models by identifying and directly influencing the layers responsible for embedding visual attribute knowledge, with a focus on content like style, objects, and factual content.

Innovations: This research introduces two key algorithms - LOCOGEN and LOCOEDIT. LOCOGEN generates a 'controlling layer' set responsible for a specific visual attribute from manipulated inputs, facilitating targeted knowledge tracing. LOCOEDIT allows direct modifications of these knowledge locations within the layers to change the model output without needing detailed knowledge of internal mechanisms.

Methods: The algorithms work by comparing manipulated inputs with original prompt assertions under altered conditions for key and value matrices of cross-attention layers. LOCOGEN extracts the causal layer sets, while LOCOEDIT updates these modules to manipulate visual attributes without altering underlying model architecture.

Results: Across popular open-source text-to-image models, including Stable-Diffusion variants, Open-Journey, and SD-XL, the study demonstrated the effective identification and manipulation of knowledge locations for varying visual attributes like style, objects, and facts. Notably, manipulation took place at small numbers of layers, indicating a compact impact on the model.

Contributions: The paper significantly advances model interpretation by pinpointing specific layers for visual attribute control, thus enabling easy modification without invasive alterations. This facilitates nuanced interventions in the model's decision-making process, improving overall control and interpretability.

Applications: This method offers applications in diverse fields requiring text-to-image generation, such as enhancing creativity in image synthesis, updating factual content, and controlling stylistic attributes, paving the way for more responsible and interpretable AI systems in areas like media, digital art, and education."
"Next basket recommendation (NBR) is a special type of sequential
recommendation that is increasingly receiving attention. So far, most NBR
studies have focused on optimizing the accuracy of the recommendation, whereas
optimizing for beyond-accuracy metrics, e.g., item fairness and diversity
remains largely unexplored. Recent studies into NBR have found a substantial
performance difference between recommending repeat items and explore items.
Repeat items contribute most of the users' perceived accuracy compared with
explore items. Informed by these findings, we identify a potential ""short-cut""
to optimize for beyond-accuracy metrics while maintaining high accuracy. To
leverage and verify the existence of such short-cuts, we propose a
plug-and-play two-step repetition-exploration (TREx) framework that treats
repeat items and explores items separately, where we design a simple yet highly
effective repetition module to ensure high accuracy, while two exploration
modules target optimizing only beyond-accuracy metrics. Experiments are
performed on two widely-used datasets w.r.t. a range of beyond-accuracy
metrics, viz. five fairness metrics and three diversity metrics. Our
experimental results verify the effectiveness of TREx. Prima facie, this
appears to be good news: we can achieve high accuracy and improved
beyond-accuracy metrics at the same time. However, we argue that the real-world
value of our algorithmic solution, TREx, is likely to be limited and reflect on
the reasonableness of the evaluation setup. We end up challenging existing
evaluation paradigms, particularly in the context of beyond-accuracy metrics,
and provide insights for researchers to navigate potential pitfalls and
determine reasonable metrics to consider when optimizing for accuracy and
beyond-accuracy metrics.","[{'On equivalence theorems of Minkowski spaces and applications in Finsler\n  geometry': 'In this paper, we first establish an equivalence theorem of Minkowski spaces\nby using results in centro-affine differential geometry. As an application in\nFinsler geometry, we gives some new characterizations of Berwald spaces.'}, {'Simple2Complex: Global Optimization by Gradient Descent': 'A method named simple2complex for modeling and training deep neural networks\nis proposed. Simple2complex train deep neural networks by smoothly adding more\nand more layers to the shallow networks, as the learning procedure going on,\nthe network is just like growing. Compared with learning by end2end,\nsimple2complex is with less possibility trapping into local minimal, namely,\nowning ability for global optimization. Cifar10 is used for verifying the\nsuperiority of simple2complex.'}, {'Semi-conformal structure on certain vertex superalgebras associated to\n  vertex superalgebroids': 'In this paper, we first give the definiton of a vertex superalgebroid. Then\nwe construct a family of vertex superalgebras associated to vertex\nsuperalgebroids. As a main result, we find a sufficient and necessary condition\nthat this vertex superalgebras are semi-conformal. In addition, we give an\nconcrete example of this vertex superalgebras and apply our results to this\nsuperalgebra.'}, {'On isotropic Berwald scalar curvature': 'In this short paper, we establish a closer relation between the Berwald\nscalar curvature and the $S$-curvature. In fact, we prove that a Finsler metric\nhas isotropic Berwald scalar curvature if and only if it has weakly isotropic\n$S$-curvature. For Finsler metrics of scalar flag curvature and of weakly\nisotropic $S$-curvature, they have almost isotropic $S$-curvature if and only\nif the flag curvature is weakly isotropic.'}, {'A Schur type lemma for the Mean Berwald curvature in Finsler geometry': 'In this short paper, we study a symmetric covariant tensor in Finsler\ngeometry, which is called the mean Berwald curvature. We first investigate the\ngeometry of the fibres as the submanifolds of the tangent sphere bundle on a\nFinsler manifold. Then we prove that if the mean Berwald curvature is isotropic\nalong fibres, then the Berwald scalar curvature is constant along fibres.'}, {'Revisiting the Gluon Spectrum in the Boost-Invariant Glasma from a\n  Semi-Analytic Approach': 'In high energy heavy-ion collisions, the degrees of freedom at the very early\nstage can be effectively represented by strong classical gluonic fields within\nthe Color Glass Condensate framework. As the system expands, the strong gluonic\nfields eventually become weak such that an equivalent description using the\ngluonic particle degrees of freedom starts to become valid. We revisit the\nspectrum of these gluonic particles by solving the classical Yang-Mills\nequations semi-analytically with the solutions having the form of power series\nexpansions in the proper time. We propose a different formula for the gluon\nspectrum which is consistent with energy density during the whole time\nevolution. We find that the chromo-electric fields have larger contributions to\nthe gluon spectrum than the chromo-magnetic fields do. Furthermore, the large\nmomentum modes take less time to reach the weak-field regime while smaller\nmomentum modes take more time. The resulting functional form of the gluon\nspectrum is exponential in nature and the spectrum is close to a thermal\ndistrubtion with effective temperatures around $0.6$ to $0.9\\, Q_s$ late in the\nGlasma evolution. The sensitiveness of the gluon spectrum to the infrared and\nthe ultraviolet cut-offs are discussed.'}, {'Theory of vibrators with variable-order fractional forces': 'In this paper, we present a theory of six classes of vibrators with\nvariable-order fractional forces of inertia, damping, and restoration. The\nnovelty and contributions of the present theory are reflected in six aspects.\n1) Equivalent motion equations of those variable-order fractional vibrators are\nproposed. 2) The analytical expressions of the effective mass, damping, and\nstiffness of those variable-order fractional vibrators are presented. 3) The\nasymptotic properties of the effective mass, damping, and stiffness of a class\nof variable-order fractional vibrators are given. 4) The restricted effective\nparameters (damping ratio, damping free natural frequency, damped natural\nfrequency, frequency ratio) of the variable-order fractional vibrators are put\nforward. 5) We bring forward the analytical representations of the free\nresponses, the impulse responses, and the frequency transfer functions of those\nvariable-order fractional vibrators. 6) We propose a solution to an open\nproblem of how to mathematically explain the Rayleigh damping assumption based\non the present theory of variable-order fractional vibrations.'}, {'Gluon saturation in proton and its contribution to single inclusive soft\n  gluon production in high energy proton-nucleus collisions': 'The leading order single inclusive soft gluon production in high energy\nproton-nucleus (pA) collisions has been studied by various approaches for more\nthan two decades. The first correction due to the gluon saturation in proton\nwas analytically attempted recently through a diagrammatic approach in which\nonly partial results were obtained. An important feature of the first\nsaturation correction is that it includes both initial state and final state\ninteractions. In this paper, we present the complete result derived from the\nColor Glass Condensate framework. Our approach is to analytically solve the\nclassical Yang-Mills equations in the dilute-dense regime and then use the\nLehmann-Symanzik-Zimmermann (LSZ) reduction formula to obtain gluon production\nfrom classical gluon fields.'}, {'A Time-Varying Endogenous Random Coefficient Model with an Application\n  to Production Functions': 'This paper proposes a random coefficient panel model where the regressors are\ncorrelated with the time-varying random coefficients in each period, a critical\nfeature in many economic applications. We model the random coefficients as\nunknown functions of a fixed effect of arbitrary dimensions, a time-varying\nrandom shock that affects the choice of regressors, and an exogenous\nidiosyncratic shock. A sufficiency argument is used to control for the fixed\neffect, which enables one to construct a feasible control function for the\nrandom shock and subsequently identify the moments of the random coefficients.\nWe propose a three-step series estimator and prove an asymptotic normality\nresult. Simulation results show that the method can accurately estimate both\nthe mean and the dispersion of the random coefficients. As an application, we\nestimate the average output elasticities for a sample of Chinese manufacturing\nfirms.'}, {'CaFT: Clustering and Filter on Tokens of Transformer for Weakly\n  Supervised Object Localization': 'Weakly supervised object localization (WSOL) is a challenging task to\nlocalize the object by only category labels. However, there is contradiction\nbetween classification and localization because accurate classification network\ntends to pay attention to discriminative region of objects rather than the\nentirety. We propose this discrimination is caused by handcraft threshold\nchoosing in CAM-based methods. Therefore, we propose Clustering and Filter of\nTokens (CaFT) with Vision Transformer (ViT) backbone to solve this problem in\nanother way. CaFT first sends the patch tokens of the image split to ViT and\ncluster the output tokens to generate initial mask of the object. Secondly,\nCaFT considers the initial mask as pseudo labels to train a shallow convolution\nhead (Attention Filter, AtF) following backbone to directly extract the mask\nfrom tokens. Then, CaFT splits the image into parts, outputs masks respectively\nand merges them into one refined mask. Finally, a new AtF is trained on the\nrefined masks and used to predict the box of object. Experiments verify that\nCaFT outperforms previous work and achieves 97.55\\% and 69.86\\% localization\naccuracy with ground-truth class on CUB-200 and ImageNet-1K respectively. CaFT\nprovides a fresh way to think about the WSOL task.'}]","Title: Leveraging the ""Short-Cut"" Strategy in Next Basket Recommendation: Imbalanced Performance on Beyond-Accuracy Metrics

Abstract:

This research paper critically examines the ""short-cut"" strategy in Next Basket Recommendation (NBR), where the accuracy of exploration is sacrificed to enhance beyond-accuracy metrics. The context is that terms like diversity and fairness in recommender systems are frequently discussed but hard to define precisely. This study investigates and validates the hypothesis that by prioritizing these beyond-accuracy metrics, it can lead to superior performance in cases where these metrics strongly correlate with accuracy metrics. The objective is to uncover the underlying issues within existing evaluation paradigms that may favor such ""short-cut"" strategies merely due to their alignment with certain beyond-accuracy metrics, even at the cost of overall prediction accuracy. 

Innovations of this study include the PHYTOR framework, an improved version of the REPTR model, which uses position information to enhance recommendation accuracy and assess the impact on beyond-accuracy metrics. Specifically, the framework develops a method to better evaluate the relationship between accuracies and the choice of beyond-accuracy metrics, such as diversity and fairness, providing insights into the trade-offs between prediction accuracy and metric-specific performance.

Methodology involves utilizing publicly available datasets from common domains like e-commerce platforms, with varying user-item interaction patterns. The datasets were processed to form a recommendation sequence for each user, and the effectiveness of the ""short-cut"" strategy was empirically tested through comparison with alternative NBR models and beyond-accuracy baselines.

The results showed that while the ""short-cut"" strategy can facilitate performances superior to those of standard NBR models in specific scenarios when beyond-accuracy metrics significantly correlate with precision, an emphasis on accuracy retains superior overall performance in datasets with well-connected metrics like item exposure or utility. Furthermore, the study identified flaws in the current evaluation paradigm that may perpetuate the reliance on ""short-cut"" strategies when formal accuracy and beyond-accuracy metrics diverge in correlation strength. 

The contribution of the study lies in proposing an empirical assessment framework that encourages a nuanced discussion around the ""short-cut"" performance trade-off, thereby highlighting the importance of a welldefined validation strategy in NBR. The findings promote a more critical evaluation approach that considers both the immediate impact and the macro-level consequences of prioritizing diverse metrics over standard accuracy metrics.

Potential applications of this research include the enhancement of current NBR systems used in e-commerce, entertainment platforms, and subscription services, improving user satisfaction and engagement by addressing the limitations of existing evaluation paradigms. By offering an alternative perspective on achieving 'better' beyond-accuracy performance, the study guides future developments towards more balanced and equitable recommendation systems that prioritize accuracy while attempting to satisfy diverse user needs."
"Large Language Models (LLMs) have achieved remarkable success across diverse
tasks, yet they remain vulnerable to adversarial attacks, notably the
well-documented \textit{jailbreak} attack. Recently, the Greedy Coordinate
Gradient (GCG) attack has demonstrated efficacy in exploiting this
vulnerability by optimizing adversarial prompts through a combination of
gradient heuristics and greedy search. However, the efficiency of this attack
has become a bottleneck in the attacking process. To mitigate this limitation,
in this paper we rethink the generation of adversarial prompts through an
optimization lens, aiming to stabilize the optimization process and harness
more heuristic insights from previous iterations. Specifically, we introduce
the \textbf{M}omentum \textbf{A}ccelerated G\textbf{C}G (\textbf{MAC}) attack,
which incorporates a momentum term into the gradient heuristic. Experimental
results showcase the notable enhancement achieved by MAP in gradient-based
attacks on aligned language models. Our code is available at
https://github.com/weizeming/momentum-attack-llm.","[{'Using Z3 for Formal Modeling and Verification of FNN Global Robustness': 'While Feedforward Neural Networks (FNNs) have achieved remarkable success in\nvarious tasks, they are vulnerable to adversarial examples. Several techniques\nhave been developed to verify the adversarial robustness of FNNs, but most of\nthem focus on robustness verification against the local perturbation\nneighborhood of a single data point. There is still a large research gap in\nglobal robustness analysis. The global-robustness verifiable framework\nDeepGlobal has been proposed to identify \\textit{all} possible Adversarial\nDangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In\nthis paper, we propose a complete specification and implementation of\nDeepGlobal utilizing the SMT solver Z3 for more explicit definition, and\npropose several improvements to DeepGlobal for more efficient verification. To\nevaluate the effectiveness of our implementation and improvements, we conduct\nextensive experiments on a set of benchmark datasets. Visualization of our\nexperiment results shows the validity and effectiveness of the approach.'}, {'Towards General Conceptual Model Editing via Adversarial Representation\n  Engineering': 'Since the development of Large Language Models (LLMs) has achieved remarkable\nsuccess, understanding and controlling their internal complex mechanisms has\nbecome an urgent problem. Recent research has attempted to interpret their\nbehaviors through the lens of inner representation. However, developing\npractical and efficient methods for applying these representations for general\nand flexible model editing remains challenging. In this work, we explore how to\nuse representation engineering methods to guide the editing of LLMs by\ndeploying a representation sensor as an oracle. We first identify the\nimportance of a robust and reliable sensor during editing, then propose an\nAdversarial Representation Engineering (ARE) framework to provide a unified and\ninterpretable approach for conceptual model editing without compromising\nbaseline performance. Experiments on multiple model editing paradigms\ndemonstrate the effectiveness of ARE in various settings. Code and data are\navailable at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.'}, {'A Front-End for Dense Monocular SLAM using a Learned Outlier Mask Prior': 'Recent achievements in depth prediction from a single RGB image have powered\nthe new research area of combining convolutional neural networks (CNNs) with\nclassical simultaneous localization and mapping (SLAM) algorithms. The depth\nprediction from a CNN provides a reasonable initial point in the optimization\nprocess in the traditional SLAM algorithms, while the SLAM algorithms further\nimprove the CNN prediction online. However, most of the current CNN-SLAM\napproaches have only taken advantage of the depth prediction but not yet other\nproducts from a CNN. In this work, we explore the use of the outlier mask, a\nby-product from unsupervised learning of depth from video, as a prior in a\nclassical probability model for depth estimate fusion to step up the\noutlier-resistant tracking performance of a SLAM front-end. On the other hand,\nsome of the previous CNN-SLAM work builds on feature-based sparse SLAM methods,\nwasting the per-pixel dense prediction from a CNN. In contrast to these sparse\nmethods, we devise a dense CNN-assisted SLAM front-end that is implementable\nwith TensorFlow and evaluate it on both indoor and outdoor datasets.'}, {'Bootstrapped Self-Supervised Training with Monocular Video for Semantic\n  Segmentation and Depth Estimation': 'For a robot deployed in the world, it is desirable to have the ability of\nautonomous learning to improve its initial pre-set knowledge. We formalize this\nas a bootstrapped self-supervised learning problem where a system is initially\nbootstrapped with supervised training on a labeled dataset and we look for a\nself-supervised training method that can subsequently improve the system over\nthe supervised training baseline using only unlabeled data. In this work, we\nleverage temporal consistency between frames in monocular video to perform this\nbootstrapped self-supervised training. We show that a well-trained\nstate-of-the-art semantic segmentation network can be further improved through\nour method. In addition, we show that the bootstrapped self-supervised training\nframework can help a network learn depth estimation better than pure supervised\ntraining or self-supervised training.'}, {'Weighted Automata Extraction and Explanation of Recurrent Neural\n  Networks for Natural Language Tasks': 'Recurrent Neural Networks (RNNs) have achieved tremendous success in\nprocessing sequential data, yet understanding and analyzing their behaviours\nremains a significant challenge. To this end, many efforts have been made to\nextract finite automata from RNNs, which are more amenable for analysis and\nexplanation. However, existing approaches like exact learning and compositional\napproaches for model extraction have limitations in either scalability or\nprecision. In this paper, we propose a novel framework of Weighted Finite\nAutomata (WFA) extraction and explanation to tackle the limitations for natural\nlanguage tasks. First, to address the transition sparsity and context loss\nproblems we identified in WFA extraction for natural language tasks, we propose\nan empirical method to complement missing rules in the transition diagram, and\nadjust transition matrices to enhance the context-awareness of the WFA. We also\npropose two data augmentation tactics to track more dynamic behaviours of RNN,\nwhich further allows us to improve the extraction precision. Based on the\nextracted model, we propose an explanation method for RNNs including a word\nembedding method -- Transition Matrix Embeddings (TME) and TME-based task\noriented explanation for the target RNN. Our evaluation demonstrates the\nadvantage of our method in extraction precision than existing approaches, and\nthe effectiveness of TME-based explanation method in applications to\npretraining and adversarial example generation.'}, {'A Tale of Santa Claus, Hypergraphs and Matroids': ""A well-known problem in scheduling and approximation algorithms is the Santa\nClaus problem. Suppose that Santa Claus has a set of gifts, and he wants to\ndistribute them among a set of children so that the least happy child is made\nas happy as possible. Here, the value that a child $i$ has for a present $j$ is\nof the form $p_{ij} \\in \\{ 0,p_j\\}$. A polynomial time algorithm by Annamalai\net al. gives a $12.33$-approximation and is based on a modification of Haxell's\nhypergraph matching argument.\n  In this paper, we introduce a matroid version of the Santa Claus problem. Our\nalgorithm is also based on Haxell's augmenting tree, but with the introduction\nof the matroid structure we solve a more general problem with cleaner methods.\nOur result can then be used as a blackbox to obtain a\n$(4+\\varepsilon)$-approximation for Santa Claus. This factor also compares\nagainst a natural, compact LP for Santa Claus.""}, {'Sharpness-Aware Minimization Alone can Improve Adversarial Robustness': 'Sharpness-Aware Minimization (SAM) is an effective method for improving\ngeneralization ability by regularizing loss sharpness. In this paper, we\nexplore SAM in the context of adversarial robustness. We find that using only\nSAM can achieve superior adversarial robustness without sacrificing clean\naccuracy compared to standard training, which is an unexpected benefit. We also\ndiscuss the relation between SAM and adversarial training (AT), a popular\nmethod for improving the adversarial robustness of DNNs. In particular, we show\nthat SAM and AT differ in terms of perturbation strength, leading to different\naccuracy and robustness trade-offs. We provide theoretical evidence for these\nclaims in a simplified model. Finally, while AT suffers from decreased clean\naccuracy and computational overhead, we suggest that SAM can be regarded as a\nlightweight substitute for AT under certain requirements. Code is available at\nhttps://github.com/weizeming/SAM_AT.'}, {'Deep reinforcement learning with a particle dynamics environment applied\n  to emergency evacuation of a room with obstacles': 'A very successful model for simulating emergency evacuation is the\nsocial-force model. At the heart of the model is the self-driven force that is\napplied to an agent and is directed towards the exit. However, it is not clear\nif the application of this force results in optimal evacuation, especially in\ncomplex environments with obstacles. Here, we develop a deep reinforcement\nlearning algorithm in association with the social force model to train agents\nto find the fastest evacuation path. During training, we penalize every step of\nan agent in the room and give zero reward at the exit. We adopt the Dyna-Q\nlearning approach. We first show that in the case of a room without obstacles\nthe resulting self-driven force points directly towards the exit as in the\nsocial force model and that the median exit time intervals calculated using the\ntwo methods are not significantly different. Then, we investigate evacuation of\na room with one obstacle and one exit. We show that our method produces similar\nresults with the social force model when the obstacle is convex. However, in\nthe case of concave obstacles, which sometimes can act as traps for agents\ngoverned purely by the social force model and prohibit complete room\nevacuation, our approach is clearly advantageous since it derives a policy that\nresults in object avoidance and complete room evacuation without additional\nassumptions. We also study evacuation of a room with multiple exits. We show\nthat agents are able to evacuate efficiently from the nearest exit through a\nshared network trained for a single agent. Finally, we test the robustness of\nthe Dyna-Q learning approach in a complex environment with multiple exits and\nobstacles. Overall, we show that our model can efficiently simulate emergency\nevacuation in complex environments with multiple room exits and obstacles where\nit is difficult to obtain an intuitive rule for fast evacuation.'}, {'Scheduling with Communication Delays via LP Hierarchies and Clustering': ""We consider the classic problem of scheduling jobs with precedence\nconstraints on identical machines to minimize makespan, in the presence of\ncommunication delays. In this setting, denoted by $\\mathsf{P} \\mid\n\\mathsf{prec}, c \\mid C_{\\mathsf{max}}$, if two dependent jobs are scheduled on\ndifferent machines, then at least $c$ units of time must pass between their\nexecutions. Despite its relevance to many applications, this model remains one\nof the most poorly understood in scheduling theory. Even for a special case\nwhere an unlimited number of machines is available, the best known\napproximation ratio is $2/3 \\cdot (c+1)$, whereas Graham's greedy list\nscheduling algorithm already gives a $(c+1)$-approximation in that setting. An\noutstanding open problem in the top-10 list by Schuurman and Woeginger and its\nrecent update by Bansal asks whether there exists a constant-factor\napproximation algorithm.\n  In this work we give a polynomial-time $O(\\log c \\cdot \\log m)$-approximation\nalgorithm for this problem, where $m$ is the number of machines and $c$ is the\ncommunication delay. Our approach is based on a Sherali-Adams lift of a linear\nprogramming relaxation and a randomized clustering of the semimetric space\ninduced by this lift.""}, {'A deep reinforcement learning model based on deterministic policy\n  gradient for collective neural crest cell migration': 'Modeling cell interactions such as co-attraction and contact-inhibition of\nlocomotion is essential for understanding collective cell migration. Here, we\npropose a novel deep reinforcement learning model for collective neural crest\ncell migration. We apply the deep deterministic policy gradient algorithm in\nassociation with a particle dynamics simulation environment to train agents to\ndetermine the migration path. Because of the different migration mechanisms of\nleader and follower neural crest cells, we train two types of agents (leaders\nand followers) to learn the collective cell migration behavior. For a leader\nagent, we consider a linear combination of a global task, resulting in the\nshortest path to the target source, and a local task, resulting in a\ncoordinated motion along the local chemoattractant gradient. For a follower\nagent, we consider only the local task. First, we show that the self-driven\nforces learned by the leader cell point approximately to the placode, which\nmeans that the agent is able to learn to follow the shortest path to the\ntarget. To validate our method, we compare the total time elapsed for agents to\nreach the placode computed using the proposed method and the time computed\nusing an agent-based model. The distributions of the migration time intervals\ncalculated using the two methods are shown to not differ significantly. We then\nstudy the effect of co-attraction and contact-inhibition of locomotion to the\ncollective leader cell migration. We show that the overall leader cell\nmigration for the case with co-attraction is slower because the co-attraction\nmitigates the source-driven effect. In addition, we find that the leader and\nfollower agents learn to follow a similar migration behavior as in experimental\nobservations. Overall, our proposed method provides useful insight on how to\napply reinforcement learning techniques to simulate collective cell migration.'}]","Title: Momentum Accelerated GCG Attack: Efficacious and Robust Jailbreaking of Large Language Models

UNLeASHING AUGMENTED GRADIENT OPTIMIZATION FOR ENHANCED LANGUAGEMODEL ATTACK

**Abstract:**
In the era of large language models (LLMs) becoming instrumental AI components, ensuring their safety and ethical deployment is paramount due to the growing concerns around jailbreaking and unintended content generation. A recent upsurge highlights vulnerabilities within these models, particularly when maliciously manipulated prompts induce harmful outputs. Motivated by the effectiveness of gradient-based attacks, this paper introduces Momentum Accelerated Greedy Coordinate Gradient (MAC) attack. It refines the existing individual and multiple prompt attack strategies by integrating exponential decay into the gradient descent process, thereby enhancing optimization stability without compromising efficiency. MAC employs a batch of greedy coordinate ascent steps with adaptive momentum for each adversarial suffix, significantly boosting the attack's ability to identify the optimal jailbreak prompt. Comparative analyses on aligned language models reveal that MAC outperforms vanilla GCG by elevating attack success rates in various scenarios, notably those demanding stability and robustness. This approach not only expands the arsenal of defense strategies against LLMs but also strengthens our understanding of the vulnerabilities and potential uses of human-aligned AI systems, thus guiding future research and development in this critical field. The implications of this novel attack methodology underscore the importance of continuous development in AI safety mechanisms, optimizing these tools to detect and mitigate risks efficiently."
"Diffusion models have emerged as dominant performers for image generation. To
support training large diffusion models, this paper studies pipeline parallel
training of diffusion models and proposes DiffusionPipe, a synchronous pipeline
training system that advocates innovative pipeline bubble filling technique,
catering to structural characteristics of diffusion models. State-of-the-art
diffusion models typically include trainable (the backbone) and non-trainable
(e.g., frozen input encoders) parts. We first unify optimal stage partitioning
and pipeline scheduling of single and multiple backbones in representative
diffusion models with a dynamic programming approach. We then propose to fill
the computation of non-trainable model parts into idle periods of the pipeline
training of the backbones by an efficient greedy algorithm, thus achieving high
training throughput. Extensive experiments show that DiffusionPipe can achieve
up to 1.41x speedup over pipeline parallel methods and 1.28x speedup over data
parallel training on popular diffusion models.","[{'Congruent Numbers and Heegner Points': ""Mohammed Ben Alhocain, in an Arab manuscript of the tenth century, stated\nthat the principal object of the theory of rational right triangles is to find\na square which when increased or diminished by a certain number $m$ becomes a\nsquare (see Dickson). In modern language, this object is to find a rational\npoint of infinite order on the elliptic curve $my^2=x^3-x$. Heegner constructed\n(see also Monsky) such rational points in the case that $m$ are primes\ncongruent to 5, 7 modulo 8 or twice primes congruent to 6 modulo 8. We extend\nHeegner's result to integers $m$ with many prime divisors.""}, {'A Multilayer Correlated Topic Model': ""We proposed a novel multilayer correlated topic model (MCTM) to analyze how\nthe main ideas inherit and vary between a document and its different segments,\nwhich helps understand an article's structure. The variational\nexpectation-maximization (EM) algorithm was derived to estimate the posterior\nand parameters in MCTM. We introduced two potential applications of MCTM,\nincluding the paragraph-level document analysis and market basket data\nanalysis. The effectiveness of MCTM in understanding the document structure has\nbeen verified by the great predictive performance on held-out documents and\nintuitive visualization. We also showed that MCTM could successfully capture\ncustomers' popular shopping patterns in the market basket analysis.""}, {'The even parity Goldfeld conjecture: congruent number elliptic curves': 'In 1979 Goldfeld conjectured: 50\\% of the quadratic twists of an elliptic\ncurve defined over the rationals have analytic rank zero. In this expository\narticle we present a few recent developments towards the conjecture, especially\nits first instance - the congruent number elliptic curves.'}, {'Genus Periods, Genus Points and Congruent Number Problem': 'In this paper, based on an ideal of Tian, we establish a new sufficient\ncondition for a positive integer to be a congruent number in terms of Legendre\nsymbols of prime factors of the positive integer. Our criterion generalizes\nprevious criterions of Heegner, and Birch--Stephens, Monsky, and Tian, and\nconjecturally provides a list of positive density of congruent numbers. Our\nmethod of proving our criterion is to give formulae for the analytic\nTate--Shafarevich number in terms of the so-called genus periods and genus\npoints. These formulae are derived from the Waldspurger formula and the\ngeneralized Gross--Zagier formula of Yuan-Zhang-Zhang.'}, {'Explicit Gross-Zagier and Waldspurger Formulae': 'We give an explicit form of Gross-Zagier formula on Shimura Curves and an\nexplicit form of Waldspurger formula.'}, {'Relaying for Multiuser Networks in the Absence of Codebook Information': 'This work considers relay assisted transmission for multiuser networks when\nthe relay has no access to the codebooks used by the transmitters. The relay is\ncalled oblivious for this reason. Of particular interest is the generalized\ncompress-and-forward (GCF) strategy, where the destinations jointly decode the\ncompression indices and the transmitted messages, and their optimality in this\nsetting. The relay-to-destination links are assumed to be out-of-band with\nfinite capacity. Two models are investigated: the multiple access relay channel\n(MARC) and the interference relay channel (IFRC). For the MARC with an\noblivious relay, a new outerbound is derived and it is shown to be tight by\nmeans of achievability of the capacity region using GCF scheme. For the IFRC\nwith an oblivious relay, a new strong interference condition is established,\nunder which the capacity region is found by deriving a new outerbound and\nshowing that it is achievable using GCF scheme. The result is further extended\nto establish the capacity region of M-user MARC with an oblivious relay, and\nmulticast networks containing M sources and K destinations with an oblivious\nrelay.'}, {'Degrees of Freedom for the MIMO Multi-way Relay Channel': 'This paper investigates the degrees of freedom (DoF) of the L-cluster, K-user\nMIMO multi-way relay channel, where users in each cluster wish to exchange\nmessages within the cluster, and they can only communicate through the relay. A\nnovel DoF upper bound is derived by providing users with carefully designed\ngenie information. Achievable DoF is identified using signal space alignment\nand multiple-access transmission. For the two-cluster MIMO multi-way relay\nchannel with two users in each cluster, DoF is established for the general case\nwhen users and the relay have arbitrary number of antennas, and it is shown\nthat the DoF upper bound can be achieved using signal space alignment or\nmultiple-access transmission, or a combination of both. The result is then\ngeneralized to the three user case. For the L-cluster K-user MIMO multi-way\nrelay channel in the symmetric setting, conditions under which the DoF upper\nbound can be achieved are established. In addition to being shown to be tight\nin a variety of scenarios of interests of the multi-way relay channel, the\nnewly derived upperbound also establishes the optimality of several previously\nestablished achievable DoF results for multiuser relay channels that are\nspecial cases of the multi-way relay channel.'}, {'Downstream Model Design of Pre-trained Language Model for Relation\n  Extraction Task': 'Supervised relation extraction methods based on deep neural network play an\nimportant role in the recent information extraction field. However, at present,\ntheir performance still fails to reach a good level due to the existence of\ncomplicated relations. On the other hand, recently proposed pre-trained\nlanguage models (PLMs) have achieved great success in multiple tasks of natural\nlanguage processing through fine-tuning when combined with the model of\ndownstream tasks. However, original standard tasks of PLM do not include the\nrelation extraction task yet. We believe that PLMs can also be used to solve\nthe relation extraction problem, but it is necessary to establish a specially\ndesigned downstream task model or even loss function for dealing with\ncomplicated relations. In this paper, a new network architecture with a special\nloss function is designed to serve as a downstream model of PLMs for supervised\nrelation extraction. Experiments have shown that our method significantly\nexceeded the current optimal baseline models across multiple public datasets of\nrelation extraction.'}, {'RaSE: A Variable Screening Framework via Random Subspace Ensembles': 'Variable screening methods have been shown to be effective in dimension\nreduction under the ultra-high dimensional setting. Most existing screening\nmethods are designed to rank the predictors according to their individual\ncontributions to the response. As a result, variables that are marginally\nindependent but jointly dependent with the response could be missed. In this\nwork, we propose a new framework for variable screening, Random Subspace\nEnsemble (RaSE), which works by evaluating the quality of random subspaces that\nmay cover multiple predictors. This new screening framework can be naturally\ncombined with any subspace evaluation criterion, which leads to an array of\nscreening methods. The framework is capable to identify signals with no\nmarginal effect or with high-order interaction effects. It is shown to enjoy\nthe sure screening property and rank consistency. We also develop an iterative\nversion of RaSE screening with theoretical support. Extensive simulation\nstudies and real-data analysis show the effectiveness of the new screening\nframework.'}, {'Transfer Learning under High-dimensional Generalized Linear Models': ""In this work, we study the transfer learning problem under high-dimensional\ngeneralized linear models (GLMs), which aim to improve the fit on target data\nby borrowing information from useful source data. Given which sources to\ntransfer, we propose a transfer learning algorithm on GLM, and derive its\n$\\ell_1/\\ell_2$-estimation error bounds as well as a bound for a prediction\nerror measure. The theoretical analysis shows that when the target and source\nare sufficiently close to each other, these bounds could be improved over those\nof the classical penalized estimator using only target data under mild\nconditions. When we don't know which sources to transfer, an algorithm-free\ntransferable source detection approach is introduced to detect informative\nsources. The detection consistency is proved under the high-dimensional GLM\ntransfer learning setting. We also propose an algorithm to construct confidence\nintervals of each coefficient component, and the corresponding theories are\nprovided. Extensive simulations and a real-data experiment verify the\neffectiveness of our algorithms. We implement the proposed GLM transfer\nlearning algorithms in a new R package glmtrans, which is available on CRAN.""}]","Abstract:

In response to the recent surge in interest in large diffusion models for content generation across diverse modalities, this research paper introduces DiffusionPipe: a novel pipeline training method aimed at enhancing the efficiency of distributed training for diffusion models, traversing the complexities posed by their dual trainable and static component architecture. 

The primary objective of our work is to develop a scalable pipeline training solution that mitigates the limitations of conventional data-parallel approaches, particularly in addressing synchronization and memory constraints when allocating resources across multiple devices. We achieve this through the adoption of bidirectional pipeline execution, where multiple model stages are organized for each diffusion model, synchronized with the stable, forward-pass-dominated epoch to optimize memory usage and iteration time.

Essentially, DiffusionPipe combines insights from pipeline architecture and synchronous training techniques, particularly focusing on the strategic placement of synchronization barriers to enable more efficient micro-batch processing, thereby leveraging the underutilized ""pipeline bubbles"" for non-trainable model computing components, typically not optimized in traditional pipeline systems.

Our experiments validate that DiffusionPipe enables an efficient utilization of local GPU memory and accelerates training, outperforming well-established baselines such as GPipe and DeepSpeed in various diffusion models at large batch sizes. Specifically, when applied to Stable Diffusion v2.1 and ControlNet v1.0 on NVIDIA A100 GPUs, DiffusionPipe showed significant speedups, ranging from 1.16x to 1.44x and 1.28x to 1.41x, respectively.

A primary contribution of this research is the introduction of an effective method for handling the layers of cascaded diffusion models that require prolonged stability and anchored concurrency control, ensuring that the pipeline scheduling does not compromise the stability or correctness of computations within each model stage during training. In addition, we highlight potential applications of DiffusionPipe in scaling up high-quality content generation across diverse public sectors, including but not limited to visual creativity, digital media production, health and medical research, and query-based automated reporting systems, thereby broadening the scope of automated content generation technologies."
"Non-autoregressive (NAR) language models are known for their low latency in
neural machine translation (NMT). However, a performance gap exists between NAR
and autoregressive models due to the large decoding space and difficulty in
capturing dependency between target words accurately. Compounding this,
preparing appropriate training data for NAR models is a non-trivial task, often
exacerbating exposure bias. To address these challenges, we apply reinforcement
learning (RL) to Levenshtein Transformer, a representative edit-based NAR
model, demonstrating that RL with self-generated data can enhance the
performance of edit-based NAR models. We explore two RL approaches: stepwise
reward maximization and episodic reward maximization. We discuss the respective
pros and cons of these two approaches and empirically verify them. Moreover, we
experimentally investigate the impact of temperature setting on performance,
confirming the importance of proper temperature setting for NAR models'
training.","[{'A Posteriori Error Estimates for Energy-Based Quasicontinuum\n  Approximations of a Periodic Chain': 'We present a posteriori error estimates for a recently developed\natomistic/continuum coupling method, the Consistent Energy-Based QC Coupling\nmethod. The error estimate of the deformation gradient combines a residual\nestimate and an a posteriori stability analysis. The residual is decomposed\ninto the residual due to the approximation of the stored energy and that due to\nthe approximation of the external force, and are bounded in negative Sobolev\nnorms. In addition, the error estimate of the total energy using the error\nestimate of the deformation gradient is also presented. Finally, numerical\nexperiments are provided to illustrate our analysis.'}, {'Two New Algorithms for Solving Covariance Graphical Lasso Based on\n  Coordinate Descent and ECM': 'Covariance graphical lasso applies a lasso penalty on the elements of the\ncovariance matrix. This method is useful because it not only produces sparse\nestimation of covariance matrix but also discovers marginal independence\nstructures by generating zeros in the covariance matrix. We propose and explore\ntwo new algorithms for solving the covariance graphical lasso problem. Our new\nalgorithms are based on coordinate descent and ECM. We show that these two\nalgorithms are more attractive than the only existing competing algorithm of\nBien and Tibshirani (2011) in terms of simplicity, speed and stability. We also\ndiscuss convergence properties of our algorithms.'}, {'Detection of fraudulent users in P2P financial market': 'Financial fraud detection is one of the core technological assets of Fintech\ncompanies. It saves tens of millions of money fro m Chinese Fintech companies\nsince the bad loan rate is more than 10%. HC Financial Service Group is the 3rd\nlargest company in the Chinese P2P financial market. In this paper we\nillustrate how we tackle the fraud detection problem at HC Financial. We\nutilize two powerful workhorses in the machine learning field - random forest\nand gradient boosting decision tree to detect fraudulent users . We demonstrate\nthat by carefully select features and tune model parameters , we could\neffectively filter out fraudulent users in the P2P market.'}, {'Theoretically Accurate Regularization Technique for Matrix Factorization\n  based Recommender Systems': 'Regularization is a popular technique to solve the overfitting problem of\nmachine learning algorithms. Most regularization technique relies on parameter\nselection of the regularization coefficient. Plug-in method and\ncross-validation approach are two most common parameter selection approaches\nfor regression methods such as Ridge Regression, Lasso Regression and Kernel\nRegression. Matrix factorization based recommendation system also has heavy\nreliance on the regularization technique. Most people select a single scalar\nvalue to regularize the user feature vector and item feature vector\nindependently or collectively. In this paper, we prove that such approach of\nselecting regularization coefficient is invalid, and we provide a theoretically\naccurate method that outperforms the most widely used approach in both accuracy\nand fairness metrics.'}, {'Does ""Fans Economy"" Work for Chinese Pop Music Industry?': 'China has become one of the largest entertainment markets in the world in\nrecent years. Due to the success of Xiaomi, many Chinese pop music industry\nentrepreneurs believe ""Fans Economy"" works in the pop music industry. ""Fans\nEconomy"" is based on the assumption that pop music consumer market could be\nsegmented based on artists. Each music artist has its own exclusive loyal fans.\nIn this paper, we provide an insightful study of the pop music artists and fans\nsocial network. Particularly, we segment the pop music consumer market and pop\nmusic artists respectively. Our results show that due to the Matthew Effect and\nlimited diversity of consumer market, ""Fans Economy"" does not work for the\nChinese pop music industry.'}, {'Que Bian: An Electronic Medical Record Management System on Blockchain': ""Medical Record Management System is an important information management\nsystem in healthcare centers and hospitals. Information kept in such systems\nneed to be clean, correct and tamper-proof. In this paper, we take advantage of\nblockchains' tamper-proof and decentralization properties to develop a robust\nand secure electronic medical record management system. In particular we choose\nHyperLedger Fabric as our underlying technical architecture. HyperLedger Fabric\nyields higher throughput and lower latency compared with other blockchains,\nwhich is a perfect candidate for enterprise software development. Our system is\na novel innovation that can serve as an ideal replacement for conventional\nMedical Record Management System.""}, {'Zipf Matrix Factorization : Matrix Factorization with Matthew Effect\n  Reduction': ""Recommender system recommends interesting items to users based on users' past\ninformation history. Researchers have been paying attention to improvement of\nalgorithmic performance such as MAE and precision@K. Major techniques such as\nmatrix factorization and learning to rank are optimized based on such\nevaluation metrics. However, the intrinsic Matthew Effect problem poses great\nthreat to the fairness of the recommender system, and the unfairness problem\ncannot be resolved by optimization of traditional metrics. In this paper, we\npropose a novel algorithm that incorporates Matthew Effect reduction with the\nmatrix factorization framework. We demonstrate that our approach can boost the\nfairness of the algorithm and enhances performance evaluated by traditional\nmetrics.""}, {'KL-Mat : Fair Recommender System via Information Geometry': 'Recommender system has intrinsic problems such as sparsity and fairness.\nAlthough it has been widely adopted for the past decades, research on fairness\nof recommendation algorithms has been largely neglected until recently. One\nimportant paradigm for resolving the issue is regularization. However,\nresearchers have not been able to come up with a consensusly agreed\nregularization term like regularization framework in other fields such as Lasso\nor Ridge Regression. In this paper, we borrow concepts from information\ngeometry and propose a new regularization-based fair algorithm called KL-Mat.\nThe algorithmic technique leads to a more robust performance in accuracy\nperformance such as MAE. More importantly, the algorithm produces much fairer\nresults than vanilla matrix factorization approach. KL-Mat is fast,\neasy-to-implement and explainable.'}, {'DotMat: Solving Cold-start Problem and Alleviating Sparsity Problem for\n  Recommender Systems': 'Cold-start and sparsity problem are two key intrinsic problems to recommender\nsystems. During the past two decades, researchers and industrial practitioners\nhave spent considerable amount of efforts trying to solve the problems.\nHowever, for cold-start problem, most research relies on importing side\ninformation to transfer knowledge. A notable exception is ZeroMat, which uses\nno extra input data. Sparsity is a lesser noticed problem. In this paper, we\npropose a new algorithm named DotMat that relies on no extra input data, but is\ncapable of solving cold-start and sparsity problems. In experiments, we prove\nthat like ZeroMat, DotMat can achieve competitive results with recommender\nsystems with full data, such as the classic matrix factorization algorithm.'}, {'PoissonMat: Remodeling Matrix Factorization using Poisson Distribution\n  and Solving the Cold Start Problem without Input Data': 'Matrix Factorization is one of the most successful recommender system\ntechniques over the past decade. However, the classic probabilistic theory\nframework for matrix factorization is modeled using normal distributions. To\nfind better probabilistic models, algorithms such as RankMat, ZeroMat and\nDotMat have been invented in recent years. In this paper, we model the user\nrating behavior in recommender system as a Poisson process, and design an\nalgorithm that relies on no input data to solve the recommendation problem and\nthe cold start issue at the same time. We prove the superiority of our\nalgorithm in comparison with matrix factorization, random placement, Zipf\nplacement, ZeroMat, DotMat, etc.'}]","In the context of Neural Architecture Representations (NARs), a comparative gap exists between reinforcement learning (RL) and traditional autoregressive (AR) methods. This paper aims to bridge this gap, specifically focusing on RL’s application in NARS, an area previously underexplored. We introduce innovative methods, particularly a variant of the REINFORCE algorithm for reducing variance in RL training of NARS by fixing predicted words and sampling additional positions. This enhancement, however, faces limitations with edit-based models. To address these challenges, we experiment with Levenshtein Transformer, a distinctive NARA. Our exploration includes a dual-policy learning mechanism utilizing a supervised approach to augment data generation and train models while mitigating concerns over exposure bias. In employing RL, we highlight the importance of suitable temperature control in navigating the complexities of edit operations within the model’s decoding space.

The primary contributions of this research lie in the development of novel RL strategies for enhancing NARs, which include the introduction of a tailored, variance-reduced REINFORCE adaptation and an efficient dual-policy learning mechanism. Additionally, we delineate the impact of temperature control in RL training, demonstrating its critical role and exploring the dynamics of varying temperature settings on model performance and learning efficiency.

Potential applications span various fields where NARS are pivotal, including but not limited to natural language processing, artificial intelligence, and machine learning environments. Our research not only advances the state-of-the-art in NAR studies, particularly in the RL domain but also opens new avenues for processing and developing sophisticated models for bead-based tasks, thereby bolstering the capabilities of autonomous systems and enhancing their adaptability in complex learning scenarios."
"Bridging the significant gap between large language model's English and
non-English performance presents a great challenge. While some previous studies
attempt to mitigate this gap with translated training data, the recently
proposed question alignment approach leverages the model's English expertise to
improve multilingual performance with minimum usage of expensive, error-prone
translation. In this paper, we explore how broadly this method can be applied
by examining its effects in reasoning with executable code and reasoning with
common sense. We also explore how to apply this approach efficiently to
extremely large language models using proxy-tuning. Experiment results on
multilingual reasoning benchmarks mGSM, mSVAMP and xCSQA demonstrate that the
question alignment approach can be used to boost multilingual performance
across diverse reasoning scenarios, model families, and sizes. For instance,
when applied to the LLaMA2 models, our method brings an average accuracy
improvements of 12.2% on mGSM even with the 70B model. To understand the
mechanism of its success, we analyze representation space, chain-of-thought and
translation data scales, which reveals how question translation training
strengthens language alignment within LLMs and shapes their working patterns.","[{'Stepping Back to SMILES Transformers for Fast Molecular Representation\n  Inference': 'In the intersection of molecular science and deep learning, tasks like\nvirtual screening have driven the need for a high-throughput molecular\nrepresentation generator on large chemical databases. However, as SMILES\nstrings are the most common storage format for molecules, using deep graph\nmodels to extract molecular feature from raw SMILES data requires an\nSMILES-to-graph conversion, which significantly decelerates the whole process.\nDirectly deriving molecular representations from SMILES is feasible, yet there\nexists a performance gap between the existing unpretrained SMILES-based models\nand graph-based models at large-scale benchmark results, while pretrain models\nare resource-demanding at training. To address this issue, we propose ST-KD, an\nend-to-end \\textbf{S}MILES \\textbf{T}ransformer for molecular representation\nlearning boosted by \\textbf{K}nowledge \\textbf{D}istillation. In order to\nconduct knowledge transfer from graph Transformers to ST-KD, we have redesigned\nthe attention layers and introduced a pre-transformation step to tokenize the\nSMILES strings and inject structure-based positional embeddings. Without\nexpensive pretraining, ST-KD shows competitive results on latest standard\nmolecular datasets PCQM4M-LSC and QM9, with $3\\text{-}14\\times$ inference speed\ncompared with existing graph models.'}, {'A family of transformed copulas with singular component': 'In this paper, we present a family of bivariate copulas by transforming a\ngiven copula function with two increasing functions, named as transformed\ncopula. One distinctive characteristic of the transformed copula is its\nsingular component along the main diagonal. Conditions guaranteeing the\ntransformed function to be a copula function are provided, and several classes\nof the transformed copulas are given. The singular component along the main\ndiagonal of the transformed copula is verified, and the tail dependence\ncoefficients of the transformed copulas are obtained. Finally, some properties\nof the transformed copula are discussed, such as the totally positive of order\n2 and the concordance order.'}, {'Gradient Episodic Memory with a Soft Constraint for Continual Learning': 'Catastrophic forgetting in continual learning is a common destructive\nphenomenon in gradient-based neural networks that learn sequential tasks, and\nit is much different from forgetting in humans, who can learn and accumulate\nknowledge throughout their whole lives. Catastrophic forgetting is the fatal\nshortcoming of a large decrease in performance on previous tasks when the model\nis learning a novel task. To alleviate this problem, the model should have the\ncapacity to learn new knowledge and preserve learned knowledge. We propose an\naverage gradient episodic memory (A-GEM) with a soft constraint $\\epsilon \\in\n[0, 1]$, which is a balance factor between learning new knowledge and\npreserving learned knowledge; our method is called gradient episodic memory\nwith a soft constraint $\\epsilon$ ($\\epsilon$-SOFT-GEM). $\\epsilon$-SOFT-GEM\noutperforms A-GEM and several continual learning benchmarks in a single\ntraining epoch; additionally, it has state-of-the-art average accuracy and\nefficiency for computation and memory, like A-GEM, and provides a better\ntrade-off between the stability of preserving learned knowledge and the\nplasticity of learning new knowledge.'}, {'What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain\n  Adaptation': 'kNN-MT presents a new paradigm for domain adaptation by building an external\ndatastore, which usually saves all target language token occurrences in the\nparallel corpus. As a result, the constructed datastore is usually large and\npossibly redundant. In this paper, we investigate the interpretability issue of\nthis approach: what knowledge does the NMT model need? We propose the notion of\nlocal correctness (LAC) as a new angle, which describes the potential\ntranslation correctness for a single entry and for a given neighborhood.\nEmpirical study shows that our investigation successfully finds the conditions\nwhere the NMT model could easily fail and need related knowledge. Experiments\non six diverse target domains and two language-pairs show that pruning\naccording to local correctness brings a light and more explainable memory for\nkNN-MT domain adaptation.'}, {'Hierarchical Transformer for Scalable Graph Learning': 'Graph Transformer is gaining increasing attention in the field of machine\nlearning and has demonstrated state-of-the-art performance on benchmarks for\ngraph representation learning. However, as current implementations of Graph\nTransformer primarily focus on learning representations of small-scale graphs,\nthe quadratic complexity of the global self-attention mechanism presents a\nchallenge for full-batch training when applied to larger graphs. Additionally,\nconventional sampling-based methods fail to capture necessary high-level\ncontextual information, resulting in a significant loss of performance. In this\npaper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a\nsolution to these challenges. HSGT successfully scales the Transformer\narchitecture to node representation learning tasks on large-scale graphs, while\nmaintaining high performance. By utilizing graph hierarchies constructed\nthrough coarsening techniques, HSGT efficiently updates and stores multi-scale\ninformation in node embeddings at different levels. Together with\nsampling-based training methods, HSGT effectively captures and aggregates\nmulti-level information on the hierarchical graph using only Transformer\nblocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-art\nperformance on large-scale benchmarks with graphs containing millions of nodes\nwith high efficiency.'}, {'INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation': 'Neural machine translation has achieved promising results on many translation\ntasks. However, previous studies have shown that neural models induce a\nnon-smooth representation space, which harms its generalization results.\nRecently, kNN-MT has provided an effective paradigm to smooth the prediction\nbased on neighbor representations during inference. Despite promising results,\nkNN-MT usually requires large inference overhead. We propose an effective\ntraining framework INK to directly smooth the representation space via\nadjusting representations of kNN neighbors with a small number of new\nparameters. The new parameters are then used to refresh the whole\nrepresentation datastore to get new kNN knowledge asynchronously. This loop\nkeeps running until convergence. Experiments on four benchmark datasets show\nthat \\method achieves average gains of 1.99 COMET and 1.0 BLEU, outperforming\nthe state-of-the-art kNN-MT system with 0.02x memory space and 1.9x inference\nspeedup.'}, {'On Structural Expressive Power of Graph Transformers': ""Graph Transformer has recently received wide attention in the research\ncommunity with its outstanding performance, yet its structural expressive power\nhas not been well analyzed. Inspired by the connections between\nWeisfeiler-Lehman (WL) graph isomorphism test and graph neural network (GNN),\nwe introduce \\textbf{SEG-WL test} (\\textbf{S}tructural \\textbf{E}ncoding\nenhanced \\textbf{G}lobal \\textbf{W}eisfeiler-\\textbf{L}ehman test), a\ngeneralized graph isomorphism test algorithm as a powerful theoretical tool for\nexploring the structural discriminative power of graph Transformers. We\ntheoretically prove that the SEG-WL test is an expressivity upper bound on a\nwide range of graph Transformers, and the representational power of SEG-WL test\ncan be approximated by a simple Transformer network arbitrarily under certain\nconditions. With the SEG-WL test, we show how graph Transformers' expressive\npower is determined by the design of structural encodings, and present\nconditions that make the expressivity of graph Transformers beyond WL test and\nGNNs. Moreover, motivated by the popular shortest path distance encoding, we\nfollow the theory-oriented principles and develop a provably stronger\nstructural encoding method, Shortest Path Induced Subgraph (\\textit{SPIS})\nencoding. Our theoretical findings provide a novel and practical paradigm for\ninvestigating the expressive power of graph Transformers, and extensive\nsynthetic and real-world experiments empirically verify the strengths of our\nproposed methods.""}, {'SIFTER: A Task-specific Alignment Strategy for Enhancing Sentence\n  Embeddings': ""The paradigm of pre-training followed by fine-tuning on downstream tasks has\nbecome the mainstream method in natural language processing tasks. Although\npre-trained models have the advantage of generalization, their performance may\nstill vary significantly across different domain tasks. This is because the\ndata distribution in different domains varies. For example, the different parts\nof the sentence 'He married Smt. Dipali Ghosh in 1947 and led a very happy\nmarried life' may have different impact for downstream tasks. For similarity\ncalculations, words such as 'led' and 'life' are more important. On the other\nhand, for sentiment analysis, the word 'happy' is crucial. This indicates that\ndifferent downstream tasks have different levels of sensitivity to sentence\ncomponents. Our starting point is to scale information of the model and data\naccording to the specifics of downstream tasks, enhancing domain information of\nrelevant parts for these tasks and reducing irrelevant elements for different\ndomain tasks, called SIFTER. In the experimental part, we use the SIFTER to\nimprove SimCSE by constructing positive sample pairs based on enhancing the\nsentence stem and reducing the unimportant components in the sentence, and\nmaximize the similarity between three sentences. Similarly, SIFTER can improve\nthe gate mechanism of the LSTM model by short-circuiting the input gate of\nimportant words so that the LSTM model remembers the important parts of the\nsentence. Our experiments demonstrate that SIFTER outperforms the SimCSE and\nLSTM baselines.""}, {'kNN-BOX: A Unified Framework for Nearest Neighbor Generation': 'Augmenting the base neural model with a token-level symbolic datastore is a\nnovel generation paradigm and has achieved promising results in machine\ntranslation (MT). In this paper, we introduce a unified framework kNN-BOX,\nwhich enables quick development and interactive analysis for this novel\nparadigm. kNN-BOX decomposes the datastore-augmentation approach into three\nmodules: datastore, retriever and combiner, thus putting diverse kNN generation\nmethods into a unified way. Currently, kNN-BOX has provided implementation of\nseven popular kNN-MT variants, covering research from performance enhancement\nto efficiency optimization. It is easy for users to reproduce these existing\nworks or customize their own models. Besides, users can interact with their kNN\ngeneration systems with kNN-BOX to better understand the underlying inference\nprocess in a visualized way. In the experiment section, we apply kNN-BOX for\nmachine translation and three other seq2seq generation tasks, namely, text\nsimplification, paraphrase generation and question generation. Experiment\nresults show that augmenting the base neural model with kNN-BOX leads to a\nlarge performance improvement in all these tasks. The code and document of\nkNN-BOX is available at https://github.com/NJUNLP/knn-box.'}, {'Beyond Generic: Enhancing Image Captioning with Real-World Knowledge\n  using Vision-Language Pre-Training Model': 'Current captioning approaches tend to generate correct but ""generic""\ndescriptions that lack real-world knowledge, e.g., named entities and\ncontextual information. Considering that Vision-Language Pre-Training (VLP)\nmodels master massive such knowledge from large-scale web-harvested data, it is\npromising to utilize the generalizability of VLP models to incorporate\nknowledge into image descriptions. However, using VLP models faces challenges:\nzero-shot inference suffers from knowledge hallucination that leads to\nlow-quality descriptions, but the generic bias in downstream task fine-tuning\nhinders the VLP model from expressing knowledge. To address these concerns, we\npropose a simple yet effective method called Knowledge-guided Replay\n(K-Replay), which enables the retention of pre-training knowledge during\nfine-tuning. Our approach consists of two parts: (1) a knowledge prediction\ntask on automatically collected replay exemplars to continuously awaken the VLP\nmodel\'s memory about knowledge, thus preventing the model from collapsing into\nthe generic pattern; (2) a knowledge distillation constraint to improve the\nfaithfulness of generated descriptions hence alleviating the knowledge\nhallucination. To evaluate knowledge-enhanced descriptions, we construct a\nnovel captioning benchmark KnowCap, containing knowledge of landmarks, famous\nbrands, special foods and movie characters. Experimental results show that our\napproach effectively incorporates knowledge into descriptions, outperforming\nstrong VLP baseline by 20.9 points (78.7->99.6) in CIDEr score and 20.5\npercentage points (34.0%->54.5%) in knowledge recognition accuracy. Our code\nand data is available at https://github.com/njucckevin/KnowCap.'}]","Title: Enabling Large Language Models to Enhance Multilingual Mathematical Reasoning

Abstract:
Despite their impressive capabilities, Large Language Models require advancements to effectively tackle multilingual mathematical reasoning tasks, particularly in low-resource languages. Our study innovates by introducing uniquely tailored approaches to augment the reasoning ability of language models across ten languages, including English, using the Question Alignment (QAlign) method. This approach leverages publicly available translation data to unify semantic spaces across languages, facilitating consistent performance regardless of language base. We detail an efficient proxy-tuning technique that enables these models to reach near-fine-tuning performance in both non-English and English mathematical reasoning tasks, with minimal computational cost. Our study identifies the importance of selecting the right small models for proxy-tuning to achieve a significant performance boost akin to full fine-tuning. We demonstrate that incorporating English-foreign language translations optimizes the model's reasoning path consistency notably, but this comes with a slight trade-off in accuracy theorectically. The constructed models exhibit a remarkable average improvement of 11.1% in reasoning performance in non-English languages, pushing the boundaries for multilingual mathematical problem-solving. The scalable method of our QAlign approach and the proxy-tuning strategy are pivotal contributions to the field, offering a flexible toolkit for researchers to extend reasoning capabilities of large language models to meet diverse, multilingual needs.

This research potentially transforms how AI systems are deployed in education, STEM fields, and global consultations. Employing advanced dialogue capabilities and seamless language interoperability, the enhanced mathematics reasoning ability of large language models will accelerate research processes, expand accessibility to educational materials, and facilitate cross-cultural knowledge exchange, fundamentally advocating for a more inclusive technological future in mathematical reasoning."
"A lexicographic maximum of a set $X \subseteq \mathbb{R}^n$ is a vector in
$X$ whose smallest component is as large as possible, and subject to that
requirement, whose second smallest component is as large as possible, and so on
for the third smallest component, etc. Lexicographic maximization has numerous
practical and theoretical applications, including fair resource allocation,
analyzing the implicit regularization of learning algorithms, and
characterizing refinements of game-theoretic equilibria. We prove that a
minimizer in $X$ of the exponential loss function $L_c(\mathbf{x}) = \sum_i
\exp(-c x_i)$ converges to a lexicographic maximum of $X$ as $c \rightarrow
\infty$, provided that $X$ is stable in the sense that a well-known iterative
method for finding a lexicographic maximum of $X$ cannot be made to fail simply
by reducing the required quality of each iterate by an arbitrarily tiny degree.
Our result holds for both near and exact minimizers of the exponential loss,
while earlier convergence results made much stronger assumptions about the set
$X$ and only held for the exact minimizer. We are aware of no previous results
showing a connection between the iterative method for computing a lexicographic
maximum and exponential loss minimization. We show that every convex polytope
is stable, but that there exist compact, convex sets that are not stable. We
also provide the first analysis of the convergence rate of an exponential loss
minimizer (near or exact) and discover a curious dichotomy: While the two
smallest components of the vector converge to the lexicographically maximum
values very quickly (at roughly the rate $\frac{\log n}{c}$), all other
components can converge arbitrarily slowly.","[{'Blackwell Approachability and Low-Regret Learning are Equivalent': 'We consider the celebrated Blackwell Approachability Theorem for two-player\ngames with vector payoffs. We show that Blackwell\'s result is equivalent, via\nefficient reductions, to the existence of ""no-regret"" algorithms for Online\nLinear Optimization. Indeed, we show that any algorithm for one such problem\ncan be efficiently converted into an algorithm for the other. We provide a\nuseful application of this reduction: the first efficient algorithm for\ncalibrated forecasting.'}, {'Faster Convex Optimization: Simulated Annealing with an Efficient\n  Universal Barrier': 'This paper explores a surprising equivalence between two seemingly-distinct\nconvex optimization methods. We show that simulated annealing, a well-studied\nrandom walk algorithms, is directly equivalent, in a certain sense, to the\ncentral path interior point algorithm for the the entropic universal barrier\nfunction. This connection exhibits several benefits. First, we are able improve\nthe state of the art time complexity for convex optimization under the\nmembership oracle model. We improve the analysis of the randomized algorithm of\nKalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii that\nunderly the central path following interior point algorithm. We are able to\ntighten the temperature schedule for simulated annealing which gives an\nimproved running time, reducing by square root of the dimension in certain\ninstances. Second, we get an efficient randomized interior point method with an\nefficiently computable universal barrier for any convex set described by a\nmembership oracle. Previously, efficiently computable barriers were known only\nfor particular convex sets.'}, {'Linear Separation via Optimism': 'Binary linear classification has been explored since the very early days of\nthe machine learning literature. Perhaps the most classical algorithm is the\nPerceptron, where a weight vector used to classify examples is maintained, and\nadditive updates are made as incorrect examples are discovered. The Perceptron\nhas been thoroughly studied and several versions have been proposed over many\ndecades. The key theoretical fact about the Perceptron is that, so long as a\nperfect linear classifier exists with some margin $\\gamma > 0$, the number of\nrequired updates to find such a perfect linear separator is bounded by\n$\\frac{1}{\\gamma^2}$. What has never been fully addressed is: does there exist\nan algorithm that can achieve this with fewer updates? In this paper we answer\nthis in the affirmative: we propose the Optimistic Perceptron algorithm, a\nsimple procedure that finds a separating hyperplane in no more than\n$\\frac{1}{\\gamma}$ updates. We also show experimentally that this procedure can\nsignificantly outperform Perceptron.'}, {'Online Linear Optimization via Smoothing': 'We present a new optimization-theoretic approach to analyzing\nFollow-the-Leader style algorithms, particularly in the setting where\nperturbations are used as a tool for regularization. We show that adding a\nstrongly convex penalty function to the decision rule and adding stochastic\nperturbations to data correspond to deterministic and stochastic smoothing\noperations, respectively. We establish an equivalence between ""Follow the\nRegularized Leader"" and ""Follow the Perturbed Leader"" up to the smoothness\nproperties. This intuition leads to a new generic analysis framework that\nrecovers and improves the previous known regret bounds of the class of\nalgorithms commonly known as Follow the Perturbed Leader.'}, {'Fighting Bandits with a New Kind of Smoothness': 'We define a novel family of algorithms for the adversarial multi-armed bandit\nproblem, and provide a simple analysis technique based on convex smoothing. We\nprove two main results. First, we show that regularization via the\n\\emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the\n$\\Theta(\\sqrt{TN})$ minimax regret. Second, we show that a wide class of\nperturbation methods achieve a near-optimal regret as low as $O(\\sqrt{TN \\log\nN})$ if the perturbation distribution has a bounded hazard rate. For example,\nthe Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this\nkey property.'}, {'An Optimization-Based Framework for Automated Market-Making': 'Building on ideas from online convex optimization, we propose a general\nframework for the design of efficient securities markets over very large\noutcome spaces. The challenge here is computational. In a complete market, in\nwhich one security is offered for each outcome, the market institution can not\nefficiently keep track of the transaction history or calculate security prices\nwhen the outcome space is large. The natural solution is to restrict the space\nof securities to be much smaller than the outcome space in such a way that\nsecurities can be priced efficiently. Recent research has focused on searching\nfor spaces of securities that can be priced efficiently by existing mechanisms\ndesigned for complete markets. While there have been some successes, much of\nthis research has led to hardness results. In this paper, we take a drastically\ndifferent approach. We start with an arbitrary space of securities with bounded\npayoff, and establish a framework to design markets tailored to this space. We\nprove that any market satisfying a set of intuitive conditions must price\nsecurities via a convex potential function and that the space of reachable\nprices must be precisely the convex hull of the security payoffs. We then show\nhow the convex potential function can be defined in terms of an optimization\nover the convex hull of the security payoffs. The optimal solution to the\noptimization problem gives the security prices. Using this framework, we\nprovide an efficient market for predicting the landing location of an object on\na sphere. In addition, we show that we can relax our ""no-arbitrage"" condition\nto design a new efficient market maker for pair betting, which is known to be\n#P-hard to price using existing mechanisms. This relaxation also allows the\nmarket maker to charge transaction fees so that the depth of the market can be\ndynamically increased as the number of trades increases.'}, {'A Collaborative Mechanism for Crowdsourcing Prediction Problems': 'Machine Learning competitions such as the Netflix Prize have proven\nreasonably successful as a method of ""crowdsourcing"" prediction tasks. But\nthese competitions have a number of weaknesses, particularly in the incentive\nstructure they create for the participants. We propose a new approach, called a\nCrowdsourced Learning Mechanism, in which participants collaboratively ""learn""\na hypothesis for a given prediction task. The approach draws heavily from the\nconcept of a prediction market, where traders bet on the likelihood of a future\nevent. In our framework, the mechanism continues to publish the current\nhypothesis, and participants can modify this hypothesis by wagering on an\nupdate. The critical incentive property is that a participant will profit an\namount that scales according to how much her update improves performance on a\nreleased test set.'}, {'Minimax Option Pricing Meets Black-Scholes in the Limit': ""Option contracts are a type of financial derivative that allow investors to\nhedge risk and speculate on the variation of an asset's future market price. In\nshort, an option has a particular payout that is based on the market price for\nan asset on a given date in the future. In 1973, Black and Scholes proposed a\nvaluation model for options that essentially estimates the tail risk of the\nasset price under the assumption that the price will fluctuate according to\ngeometric Brownian motion. More recently, DeMarzo et al., among others, have\nproposed more robust valuation schemes, where we can even assume an adversary\nchooses the price fluctuations. This framework can be considered as a\nsequential two-player zero-sum game between the investor and Nature. We analyze\nthe value of this game in the limit, where the investor can trade at smaller\nand smaller time intervals. Under weak assumptions on the actions of Nature (an\nadversary), we show that the minimax option price asymptotically approaches\nexactly the Black-Scholes valuation. The key piece of our analysis is showing\nthat Nature's minimax optimal dual strategy converges to geometric Brownian\nmotion in the limit.""}, {'Information Aggregation in Exponential Family Markets': 'We consider the design of prediction market mechanisms known as automated\nmarket makers. We show that we can design these mechanisms via the mold of\n\\emph{exponential family distributions}, a popular and well-studied probability\ndistribution template used in statistics. We give a full development of this\nrelationship and explore a range of benefits. We draw connections between the\ninformation aggregation of market prices and the belief aggregation of learning\nagents that rely on exponential family distributions. We develop a very natural\nanalysis of the market behavior as well as the price equilibrium under the\nassumption that the traders exhibit risk aversion according to exponential\nutility. We also consider similar aspects under alternative models, such as\nwhen traders are budget constrained.'}, {'Spectral Smoothing via Random Matrix Perturbations': 'We consider stochastic smoothing of spectral functions of matrices using\nperturbations commonly studied in random matrix theory. We show that a spectral\nfunction remains spectral when smoothed using a unitarily invariant\nperturbation distribution. We then derive state-of-the-art smoothing bounds for\nthe maximum eigenvalue function using the Gaussian Orthogonal Ensemble (GOE).\nSmoothing the maximum eigenvalue function is important for applications in\nsemidefinite optimization and online learning. As a direct consequence of our\nGOE smoothing results, we obtain an $O((N \\log N)^{1/4} \\sqrt{T})$ expected\nregret bound for the online variance minimization problem using an algorithm\nthat performs only a single maximum eigenvector computation per time step. Here\n$T$ is the number of rounds and $N$ is the matrix dimension. Our algorithm and\nits analysis also extend to the more general online PCA problem where the\nlearner has to output a rank $k$ subspace. The algorithm just requires\ncomputing $k$ maximum eigenvectors per step and enjoys an $O(k (N \\log N)^{1/4}\n\\sqrt{T})$ expected regret bound.'}]","Title: Exact and Approximate Lexicographic Maximization Algorithms: Analysis and Convergence Rates

Abstract:

To address issues of fairness and resource distribution, this paper introduces an in-depth exploration of lexicographic maximization (LM) algorithms. Primary results gauge the optimal conditions for convergence and performance of these algorithms when used as alternative optimization techniques, differing from traditional solutions. The investigation aims to comprehend the behavior of lexicographic max problems under varying constraints and approximation tolerances, focusing particularly on the interplay between the exponential loss minimization procedure and the lexicographic maximization goals. Our novel contributions include the design and specification of an algorithm, whose exact calculations yield infinite accuracy but which also presents approximation methods with bounded drawbacks. These bounds are meticulously calculated through thorough analysis. Insights highlight the balance between algorithmic efficiency and precision, delineating an optimal point for utilizing exact vs approximative lexicographic maximization. An application case specifically connects the linkage concept within lexicographic maximization to fundamental principles in fair queueing, showcasing the algorithm's versatile practical utility. Furthermore, the paper provides a/Theorem/expliciting bounds on the convergence rates for both exact and approximate lexicographic maximization, answering a critical question regarding the optimization progress under various algorithm configurations. This analysis selects the efficacy parameters for placing a mark on the convergence phase, highlighting the structured progression towards the lexicographically maximum and providing a theoretical foundation for its practical implementation. Provided lexicographic maximization is pertinent for ensuring equitable distribution and optimum performance in systems with sequential order-dependent objectives, this work contributes valuable insight for researchers and practitioners aiming to leverage these algorithms in real-world applications."
"Large 2D vision-language models (2D-LLMs) have gained significant attention
by bridging Large Language Models (LLMs) with images using a simple projector.
Inspired by their success, large 3D point cloud-language models (3D-LLMs) also
integrate point clouds into LLMs. However, directly aligning point clouds with
LLM requires expensive training costs, typically in hundreds of GPU-hours on
A100, which hinders the development of 3D-LLMs. In this paper, we introduce
MiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA
results while training for only 27 hours on one RTX 3090. Specifically, we
propose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which
can leverage the similarity between 2D and 3D visual information. We introduce
a novel four-stage training strategy for modality alignment in a cascaded way,
and a mixture of query experts module to adaptively aggregate features with
high efficiency. Moreover, we utilize parameter-efficient fine-tuning methods
LoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which
is up to 260x fewer than existing methods. Extensive experiments show that
MiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with
significantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12
increase on GPT-4 evaluation score for the challenging object captioning task
compared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.
We are the first to explore the efficient 3D-LLM, offering new insights to the
community. Code and weights are available at
https://github.com/TangYuan96/MiniGPT-3D.","[{'Autoplotly - Automatic Generation of Interactive Visualizations for\n  Popular Statistical Results': 'The autoplotly package provides functionalities to automatically generate\ninteractive visualizations for many popular statistical results supported by\nggfortify package with plotly and ggplot2 style. The generated visualizations\ncan also be easily extended using ggplot2 and plotly syntax while staying\ninteractive.'}, {""TF.Learn: TensorFlow's High-level Module for Distributed Machine\n  Learning"": ""TF.Learn is a high-level Python module for distributed machine learning\ninside TensorFlow. It provides an easy-to-use Scikit-learn style interface to\nsimplify the process of creating, configuring, training, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide range of\nstate-of-art machine learning algorithms built on top of TensorFlow's low level\nAPIs for small to large-scale supervised and unsupervised problems. This module\nfocuses on bringing machine learning to non-specialists using a general-purpose\nhigh-level language as well as researchers who want to implement, benchmark,\nand compare their new methods in a structured environment. Emphasis is put on\nease of use, performance, documentation, and API consistency.""}, {'Improving the Space-Time Efficiency of Processor-Oblivious Matrix\n  Multiplication Algorithms': ""Classic cache-oblivious parallel matrix multiplication algorithms achieve\noptimality either in time or space, but not both, which promotes lots of\nresearch on the best possible balance or tradeoff of such algorithms. We study\nmodern processor-oblivious runtime systems and figure out several ways to\nimprove algorithm's time bound while still bounding space and cache\nrequirements to be asymptotically optimal. By our study, we give out sublinear\ntime, optimal work, space and cache algorithms for both general matrix\nmultiplication on a semiring and Strassen-like fast algorithm. Our experiments\nalso show such algorithms have empirical advantages over classic counterparts.\nOur study provides new insights and research angles on how to optimize\ncache-oblivious parallel algorithms from both theoretical and empirical\nperspectives.""}, {'Nested Dataflow Algorithms for Dynamic Programming Recurrences with more\n  than O(1) Dependency': 'Dynamic programming problems have wide applications in real world and have\nbeen studied extensively in both serial and parallel settings. In 1994, Galil\nand Park developed work-efficient and sublinear-time algorithms for several\nimportant dynamic programming problems based on the closure method and matrix\nproduct method. However, in the same paper, they raised an open question\nwhether such an algorithm exists for the general GAP problem. % In this paper,\nwe answer their question by developing the first work-efficient and\nsublinear-time GAP algorithm based on the closure method and Nested Dataflow\nmethod. % We also improve the time bounds of classic work-efficient,\ncache-oblivious and cache-efficient algorithms for the 1D problem and GAP\nproblem, respectively.'}, {'A Reexamination of the COnfLUX 2.5D LU Factorization Algorithm': 'This article conducts a reexamination of the research conducted by\nKwasniewski et al., focusing on their adaptation of the 2.5D LU factorization\nalgorithm with tournament pivoting, known as \\func{COnfLUX}. Our reexamination\nreveals potential concerns regarding the upper bound, empirical investigation\nmethods, and lower bound, despite the original study providing a theoretical\nfoundation and an instantiation of the proposed algorithm. This paper offers a\nreexamination of these matters, highlighting probable shortcomings in the\noriginal investigation. Our observations are intended to enhance the\ndevelopment and comprehension of parallel matrix factorization algorithms.'}, {'A Reexamination of the Communication Bandwidth Cost Analysis of A\n  Parallel Recursive Algorithm for Solving Triangular Systems of Linear\n  Equations': 'This paper presents a reexamination of the research paper titled\n""Communication-Avoiding Parallel Algorithms for \\proc{TRSM}"" by Wicky et al. We\nfocus on the communication bandwidth cost analysis presented in the original\nwork and identify potential issues that require clarification or revision. The\nproblem at hand is the need to address inconsistencies and miscalculations\nfound in the analysis, particularly in the categorization of costs into three\nscenarios based on the relationship between matrix dimensions and processor\ncount. Our findings contribute to the ongoing discourse in the field and pave\nthe way for further improvements in this area of research.'}, {'lfda: An R Package for Local Fisher Discriminant Analysis and\n  Visualization': 'Local Fisher discriminant analysis is a localized variant of Fisher\ndiscriminant analysis and it is popular for supervised dimensionality reduction\nmethod. lfda is an R package for performing local Fisher discriminant analysis,\nincluding its variants such as kernel local Fisher discriminant analysis and\nsemi-supervised local Fisher discriminant analysis. It also provides\nvisualization functions to easily visualize the dimension reduction results by\nusing either rgl for 3D visualization or ggfortify for 2D visualization in\nggplot2 style.'}, {'Balanced Partitioning of Several Cache-Oblivious Algorithms': ""Frigo et al. proposed an ideal cache model and a recursive technique to\ndesign sequential cache-efficient algorithms in a cache-oblivious fashion.\nBallard et al. pointed out that it is a fundamental open problem to extend the\ntechnique to an arbitrary architecture. Ballard et al. raised another open\nquestion on how to parallelize Strassen's algorithm exactly and efficiently on\nan arbitrary number of processors.\n  We propose a novel way of partitioning a cache-oblivious algorithm to achieve\nperfect strong scaling on an arbitrary number, even a prime number, of\nprocessors within a certain range in a shared-memory setting. Our approach is\nProcessor-Aware but Cache-Oblivious (PACO). We demonstrate our approach on\nseveral important cache-oblivious algorithms, including LCS, 1D, GAP, classic\nrectangular matrix multiplication on a semiring, and Strassen's algorithm. We\ndiscuss how to extend our approach to a distributed-memory architecture, or\neven a heterogeneous computing system. Hence, our work may provide a new\nperspective on the fundamental open problem of extending the recursive\ncache-oblivious technique to an arbitrary architecture. We provide an almost\nexact solution to the open problem on parallelizing Strassen. Our approach may\nprovide a new perspective on extending the recursive cache-oblivious technique\nto an arbitrary architecture. All our algorithms demonstrate better scalability\nor better overall parallel cache complexities than the best known algorithms.\nPreliminary experiments justify our theoretical prediction that the PACO\nalgorithms can outperform significantly state-of-the-art Processor-Oblivious\n(PO) and Processor-Aware (PA) counterparts.""}, {'Superdiffusion of quantized vortices uncovering scaling behavior of\n  quantum turbulence': ""Generic scaling laws, such as the Kolmogorov's 5/3-law, are milestone\nachievements of turbulence research in classical fluids. For quantum fluids\nsuch as atomic Bose-Einstein condensates, superfluid helium, and superfluid\nneutron stars, turbulence can also exist in the presence of a chaotic tangle of\nevolving quantized vortex lines. However, due to the lack of suitable\nexperimental tools to directly probe the vortex-tangle motion, so far little is\nknown about possible scaling laws that characterize the velocity correlations\nand trajectory statistics of the vortices in quantum-fluid turbulence (QT).\nAcquiring such knowledge could greatly benefit the development of advanced\nstatistical models of QT. Here we report an experiment where a tangle of\nvortices in superfluid $^4$He are decorated with solidified deuterium tracer\nparticles. Under experimental conditions where these tracers follow the motion\nof the vortices, we observed an apparent superdiffusion of the vortices. Our\nanalysis shows that this superdiffusion is not due to L\\'{e}vy flights, i.e.,\nlong-distance hops that are known to be responsible for superdiffusion of\nrandom walkers. Instead, a previously unknown power-law scaling of the\nvortex-velocity temporal correlation is uncovered as the cause. This finding\nmay motivate future research on hidden scaling laws in QT.""}, {'Mamba3D: Enhancing Local Features for 3D Point Cloud Analysis via State\n  Space Model': 'Existing Transformer-based models for point cloud analysis suffer from\nquadratic complexity, leading to compromised point cloud resolution and\ninformation loss. In contrast, the newly proposed Mamba model, based on state\nspace models (SSM), outperforms Transformer in multiple areas with only linear\ncomplexity. However, the straightforward adoption of Mamba does not achieve\nsatisfactory performance on point cloud tasks. In this work, we present\nMamba3D, a state space model tailored for point cloud learning to enhance local\nfeature extraction, achieving superior performance, high efficiency, and\nscalability potential. Specifically, we propose a simple yet effective Local\nNorm Pooling (LNP) block to extract local geometric features. Additionally, to\nobtain better global features, we introduce a bidirectional SSM (bi-SSM) with\nboth a token forward SSM and a novel backward SSM that operates on the feature\nchannel. Extensive experimental results show that Mamba3D surpasses\nTransformer-based counterparts and concurrent works in multiple tasks, with or\nwithout pre-training. Notably, Mamba3D achieves multiple SoTA, including an\noverall accuracy of 92.6% (train from scratch) on the ScanObjectNN and 95.1%\n(with single-modal pre-training) on the ModelNet40 classification task, with\nonly linear complexity.'}]","### Abstract

Navigating the gap between 2D and 3D languages, this research introduces MiniGPT-3D, a novel language model that expertise in understanding and generating 3D object captions. The main goal of this study is to uncover and chart the potential of LLMs in grasping the vast yet nuanced 3D world, far outweighing limitations posed by their predominance in 2D entities. The innovative contribution lies in combining a 2D-to-3D knowledge bridge (2D-LLM) with multimodal transformers to equip MiniGPT-3D with superior semantic transition capabilities across dimensions. Through a series of evaluations, including accuracy-enhancing strategies, 2D priors assimilation, and stage-wise semi-supervised training, the model markedly attains supremacy in 3D object captioning tasks over state-of-the-art methods.

Key methodologies involve contextual interaction between a two-dimensional language model and a three-dimensional language model, culminating in a semi-supervised training process divided into multiple stages. This results in a model that can interpret, reason about, and generate captions for 3D objects with unprecedented precision. Derived from a comparative analysis of different stage compositions, the optimal training sequence maximizes both accuracy and the model's ability to understand fine details in three-dimensional space.

MiniGPT-3D significantly advances the field by elucidating the limitations of current 3D-LLMs, offering an incremental approach with a clear demonstration of its efficacy in structuring 3D data. This research not only overcomes existing challenges in 3D language processing but also sets new standards for the interpretation and description of 3D entities, unlocking new avenues for applications in augmented reality, virtual reality, biomechanics, and robotics, where fluent interaction with 3D objects is quintessential. Hence, the development of MiniGPT-3D catapults the future of interactions with the digital 3D world into a promising era, marked by enhanced realism and efficiency."
"Traditionally, natural language processing (NLP) models often use a rich set
of features created by linguistic expertise, such as semantic representations.
However, in the era of large language models (LLMs), more and more tasks are
turned into generic, end-to-end sequence generation problems. In this paper, we
investigate the question: what is the role of semantic representations in the
era of LLMs? Specifically, we investigate the effect of Abstract Meaning
Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven
chain-of-thought prompting method, which we call AMRCoT, and find that it
generally hurts performance more than it helps. To investigate what AMR may
have to offer on these tasks, we conduct a series of analysis experiments. We
find that it is difficult to predict which input examples AMR may help or hurt
on, but errors tend to arise with multi-word expressions, named entities, and
in the final inference step where the LLM must connect its reasoning over the
AMR to its prediction. We recommend focusing on these areas for future work in
semantic representations for LLMs. Our code:
https://github.com/causalNLP/amr_llm.","[{'Natural Language Processing for Policymaking': 'Language is the medium for many political activities, from campaigns to news\nreports. Natural language processing (NLP) uses computational tools to parse\ntext into key information that is needed for policymaking. In this chapter, we\nintroduce common methods of NLP, including text classification, topic modeling,\nevent extraction, and text scaling. We then overview how these methods can be\nused for policymaking through four major applications including data collection\nfor evidence-based policymaking, interpretation of political decisions, policy\ncommunication, and investigation of policy effects. Finally, we highlight some\npotential limitations and ethical concerns when using NLP for policymaking.\n  This text is from Chapter 7 (pages 141-162) of the Handbook of Computational\nSocial Science for Policy (2023). Open Access on Springer:\nhttps://doi.org/10.1007/978-3-031-16624-2'}, {'3D Traffic Simulation for Autonomous Vehicles in Unity and Python': 'Over the recent years, there has been an explosion of studies on autonomous\nvehicles. Many collected large amount of data from human drivers. However,\ncompared to the tedious data collection approach, building a virtual simulation\nof traffic makes the autonomous vehicle research more flexible, time-saving,\nand scalable. Our work features a 3D simulation that takes in real time\nposition information parsed from street cameras. The simulation can easily\nswitch between a global bird view of the traffic and a local perspective of a\ncar. It can also filter out certain objects in its customized camera, creating\nvarious channels for objects of different categories. This provides alternative\nsupervised or unsupervised ways to train deep neural networks. Another\nadvantage of the 3D simulation is its conformation to physical laws. Its\nnaturalness to accelerate and collide prepares the system for potential deep\nreinforcement learning needs.'}, {'Relation of the Relations: A New Paradigm of the Relation Extraction\n  Problem': 'In natural language, often multiple entities appear in the same text.\nHowever, most previous works in Relation Extraction (RE) limit the scope to\nidentifying the relation between two entities at a time. Such an approach\ninduces a quadratic computation time, and also overlooks the interdependency\nbetween multiple relations, namely the relation of relations (RoR). Due to the\nsignificance of RoR in existing datasets, we propose a new paradigm of RE that\nconsiders as a whole the predictions of all relations in the same context.\nAccordingly, we develop a data-driven approach that does not require\nhand-crafted rules but learns by itself the RoR, using Graph Neural Networks\nand a relation matrix transformer. Experiments show that our model outperforms\nthe state-of-the-art approaches by +1.12\\% on the ACE05 dataset and +2.55\\% on\nSemEval 2018 Task 7.2, which is a substantial improvement on the two\ncompetitive benchmarks.'}, {'Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion': 'Recent years have seen the rapid development of large generative models for\ntext; however, much less research has explored the connection between text and\nanother ""language"" of communication -- music. Music, much like text, can convey\nemotions, stories, and ideas, and has its own unique structure and syntax. In\nour work, we bridge text and music via a text-to-music generation model that is\nhighly efficient, expressive, and can handle long-term structure. Specifically,\nwe develop Mo\\^usai, a cascading two-stage latent diffusion model that can\ngenerate multiple minutes of high-quality stereo music at 48kHz from textual\ndescriptions. Moreover, our model features high efficiency, which enables\nreal-time inference on a single consumer GPU with a reasonable speed. Through\nexperiments and property analyses, we show our model\'s competence over a\nvariety of criteria compared with existing music generation models. Lastly, to\npromote the open-source culture, we provide a collection of open-source\nlibraries with the hope of facilitating future work in the field. We\nopen-source the following: Codes:\nhttps://github.com/archinetai/audio-diffusion-pytorch; music samples for this\npaper: http://bit.ly/44ozWDH; all music samples for all models:\nhttps://bit.ly/audio-diffusion.'}, {'Slangvolution: A Causal Analysis of Semantic Change and Frequency\n  Dynamics in Slang': 'Languages are continuously undergoing changes, and the mechanisms that\nunderlie these changes are still a matter of debate. In this work, we approach\nlanguage evolution through the lens of causality in order to model not only how\nvarious distributional factors associate with language change, but how they\ncausally affect it. In particular, we study slang, which is an informal\nlanguage that is typically restricted to a specific group or social setting. We\nanalyze the semantic change and frequency shift of slang words and compare them\nto those of standard, nonslang words. With causal discovery and causal\ninference techniques, we measure the effect that word type (slang/nonslang) has\non both semantic change and frequency shift, as well as its relationship to\nfrequency, polysemy and part of speech. Our analysis provides some new insights\nin the study of language change, e.g., we show that slang words undergo less\nsemantic change but tend to have larger frequency shifts over time.'}, {""Editing a Woman's Voice"": 'Prior work shows that men and women speak with different levels of\nconfidence, though it\'s often assumed that these differences are innate or are\nlearned in early childhood. Using academic publishing as a setting, we find\nthat language differences across male and female authors are initially\nnegligible: in first drafts of academic manuscripts, men and women write with\nsimilar levels of uncertainty. However, when we trace those early drafts to\ntheir published versions, a substantial gender gap in linguistic uncertainty\narises. That is, women increase their use of cautionary language through the\npublication process more than men. We show this increase in the linguistic\ngender gap varies substantially based on editor assignment. Specifically, our\nauthor-to-editor matched dataset allows us to estimate editor-specific fixed\neffects, capturing how specific editors impact the change in linguistic\nuncertainty for female authors relative to male authors (the editor\'s\nauthor-gender gap). Editors\' author-gender gaps vary widely, and correlate with\nobservable editor characteristics such as societal norms in their\ncountry-of-origin, their work history, and the year that they obtained their\nPhD. Overall, our study suggests that a woman\'s ""voice"" is partially shaped by\nexternal forces, and it highlights the critical role of editors in shaping how\nfemale academics communicate.'}, {'Deep Learning for Text Style Transfer: A Survey': 'Text style transfer is an important task in natural language generation,\nwhich aims to control certain attributes in the generated text, such as\npoliteness, emotion, humor, and many others. It has a long history in the field\nof natural language processing, and recently has re-gained significant\nattention thanks to the promising performance brought by deep neural models. In\nthis paper, we present a systematic survey of the research on neural text style\ntransfer, spanning over 100 representative articles since the first neural text\nstyle transfer work in 2017. We discuss the task formulation, existing datasets\nand subtasks, evaluation, as well as the rich methodologies in the presence of\nparallel and non-parallel data. We also provide discussions on a variety of\nimportant topics regarding the future development of this task. Our curated\npaper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey'}, {'IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and\n  Translation': 'Text attribute transfer aims to automatically rewrite sentences such that\nthey possess certain linguistic attributes, while simultaneously preserving\ntheir semantic content. This task remains challenging due to a lack of\nsupervised parallel data. Existing approaches try to explicitly disentangle\ncontent and attribute information, but this is difficult and often results in\npoor content-preservation and ungrammaticality. In contrast, we propose a\nsimpler approach, Iterative Matching and Translation (IMaT), which: (1)\nconstructs a pseudo-parallel corpus by aligning a subset of semantically\nsimilar sentences from the source and the target corpora; (2) applies a\nstandard sequence-to-sequence model to learn the attribute transfer; (3)\niteratively improves the learned transfer function by refining imperfections in\nthe alignment. In sentiment modification and formality transfer tasks, our\nmethod outperforms complex state-of-the-art systems by a large margin. As an\nauxiliary contribution, we produce a publicly-available test set with\nhuman-generated transfer references.'}, {'A Simple Baseline to Semi-Supervised Domain Adaptation for Machine\n  Translation': 'State-of-the-art neural machine translation (NMT) systems are data-hungry and\nperform poorly on new domains with no supervised data. As data collection is\nexpensive and infeasible in many cases, domain adaptation methods are needed.\nIn this work, we propose a simple but effect approach to the semi-supervised\ndomain adaptation scenario of NMT, where the aim is to improve the performance\nof a translation model on the target domain consisting of only non-parallel\ndata with the help of supervised source domain data. This approach iteratively\ntrains a Transformer-based NMT model via three training objectives: language\nmodeling, back-translation, and supervised translation. We evaluate this method\non two adaptation settings: adaptation between specific domains and adaptation\nfrom a general domain to specific domains, and on two language pairs: German to\nEnglish and Romanian to English. With substantial performance improvement\nachieved---up to +19.31 BLEU over the strongest baseline, and +47.69 BLEU\nimprovement over the unadapted model---we present this method as a simple but\ntough-to-beat baseline in the field of semi-supervised domain adaptation for\nNMT.'}, {'Is BERT Really Robust? A Strong Baseline for Natural Language Attack on\n  Text Classification and Entailment': 'Machine learning algorithms are often vulnerable to adversarial examples that\nhave imperceptible alterations from the original counterparts but can fool the\nstate-of-the-art models. It is helpful to evaluate or even improve the\nrobustness of these models by exposing the maliciously crafted adversarial\nexamples. In this paper, we present TextFooler, a simple but strong baseline to\ngenerate natural adversarial text. By applying it to two fundamental natural\nlanguage tasks, text classification and textual entailment, we successfully\nattacked three target models, including the powerful pre-trained BERT, and the\nwidely used convolutional and recurrent neural networks. We demonstrate the\nadvantages of this framework in three ways: (1) effective---it outperforms\nstate-of-the-art attacks in terms of success rate and perturbation rate, (2)\nutility-preserving---it preserves semantic content and grammaticality, and\nremains correctly classified by humans, and (3) efficient---it generates\nadversarial text with computational complexity linear to the text length. *The\ncode, pre-trained target models, and test examples are available at\nhttps://github.com/jind11/TextFooler.'}]","Abstract

In the era of large language models (LLMs), the ability of models to encode semantic representations and the subsequent impact of these semantic structures on performance are becoming increasingly important research topics. This paper explores the integration of language-specific semantic structures, namely Abstract Meaning Representation (AMR), within the chains-of-thought (CoT) framework to improve NLP model performance on tasks such as Paraphrase Detection, WMT Translation, Logic Fallacy Detection, PubMed45 Event Extraction, and Text-to-SQL Query Generation. The research contrasted two prompting strategies: AMRCOT, which incorporates both the original text and its semantically annotated AMR in the human-in-the-loop instruction, and BASE, which solely uses raw text. 

To achieve our objectives, we innovatively leveraged semantically augmented data through the Blank Thoughts and Composed AMR datasets, and applied a linguistic feature analysis to evaluate the relevance of specific text characteristics in enhancing NLM model performance. The methodology involved training and evaluating a series of models on the augmented datasets, followed by an in-depth analysis of the linguistic features which significantly influenced model performance.

The results show that AMRCOT partially outperforms BASE, improving performance by between 2.6g3 F1 points in average for the Paraphrase Detection Task, with notable exceptions in the translation and event extraction tasks showing degradation. The linguistic feature analysis, incorporating this specific semantic annotation, revealed that the type and structure of the AMR had non-trivial effects on model performance, indicating that semantic representation can indeed be leveraged to complement and enhance model capabilities through CoT prompting. These findings provide new insights into the potential for combining human expertise in linguistic annotation with automation in model prompting for NLP tasks that could have significant applications in real-world settings such as interpreting multilingual documents, writing helper scripts, and improving reasoning capabilities across various tasks.

In summary, this research contributes to the knowledge of how integrating semantically enriched data into the input of LLMs can be achieved and optimized for various NLP tasks, and highlights the need for further investigation into the balance between human expertise and automation in the realm of AI. The outcomes suggest promising possibilities for enhancing the performance of contemporary AI models with the inclusion of domain-specific semantic representations like AMR, thereby guiding future research towards refining the methodologies which enable the best symbiotic interaction between human-designed languages and AI-driven language processing."
"In the current digital era, the rapid spread of misinformation on online
platforms presents significant challenges to societal well-being, public trust,
and democratic processes, influencing critical decision making and public
opinion. To address these challenges, there is a growing need for automated
fake news detection mechanisms. Pre-trained large language models (LLMs) have
demonstrated exceptional capabilities across various natural language
processing (NLP) tasks, prompting exploration into their potential for
verifying news claims. Instead of employing LLMs in a non-agentic way, where
LLMs generate responses based on direct prompts in a single shot, our work
introduces FactAgent, an agentic approach of utilizing LLMs for fake news
detection. FactAgent enables LLMs to emulate human expert behavior in verifying
news claims without any model training, following a structured workflow. This
workflow breaks down the complex task of news veracity checking into multiple
sub-steps, where LLMs complete simple tasks using their internal knowledge or
external tools. At the final step of the workflow, LLMs integrate all findings
throughout the workflow to determine the news claim's veracity. Compared to
manual human verification, FactAgent offers enhanced efficiency. Experimental
studies demonstrate the effectiveness of FactAgent in verifying claims without
the need for any training process. Moreover, FactAgent provides transparent
explanations at each step of the workflow and during final decision-making,
offering insights into the reasoning process of fake news detection for end
users. FactAgent is highly adaptable, allowing for straightforward updates to
its tools that LLMs can leverage within the workflow, as well as updates to the
workflow itself using domain knowledge. This adaptability enables FactAgent's
application to news verification across various domains.","[{'Percolative properties of Brownian interlacements and its vacant set': 'In this article we investigate the percolative properties of Brownian\ninterlacements, a model introduced by Alain-Sol Sznitman in arXiv:1209.4531,\nand show that: the interlacement set is ""well-connected"", i.e., any two\n""sausages"" in $d$-dimensional Brownian interlacements, $d\\geq 3$, can be\nconnected via no more than $\\lceil (d-4)/2 \\rceil$ intermediate sausages almost\nsurely; while the vacant set undergoes a non-trivial percolation phase\ntransition when the level parameter varies.'}, {'A lower bound for disconnection by simple random walk': 'We consider simple random walk on Z^d, d bigger or equal to 3. Motivated by\nthe work of A.-S. Sznitman and the author in arXiv:1304.7477 and\narXiv:1310.2177, we investigate the asymptotic behaviour of the probability\nthat a large body gets disconnected from infinity by the set of points visited\nby a simple random walk. We derive asymptotic lower bounds that bring into play\nrandom interlacements. Although open at the moment, some of the lower bounds\nthat we obtain possibly match the asymptotic upper bounds obtained in a recent\narticle of A.-S. Sznitman. This potentially yields special significance to the\ntilted walks that we use in this work, and to the strategy that we employ to\nimplement disconnection.'}, {'The Hölder continuity of the scaling limit of three-dimensional\n  loop-erased random walk': 'Let $\\beta$ be the growth exponent of the loop-erased random walk (LERW) in\nthree dimensions. We prove that the scaling limit of 3D LERW is $h$-H\\""older\ncontinuous almost surely for all $h < 1/\\beta$, while not $1/\\beta$-H\\""older\ncontinuous almost surely.'}, {'One-point function estimates for loop-erased random walk in three\n  dimensions': 'In this work, we consider loop-erased random walk (LERW) in three dimensions\nand give an asymptotic estimate on the one-point function for LERW and the\nnon-intersection probability of LERW and simple random walk in three dimensions\nfor dyadic scales. These estimates will be crucial to the characterization of\nthe convergence of LERW to its scaling limit in natural parametrization. As a\nstep in the proof, we also obtain a coupling of two pairs of LERW and SRW with\ndifferent starting points conditioned to avoid each other.'}, {'Convergence of three-dimensional loop-erased random walk in the natural\n  parametrization': 'In this work we consider loop-erased random walk (LERW) and its scaling limit\nin three dimensions, and prove that 3D LERW parametrized by renormalized length\nconverges to its scaling limit parametrized by some suitable measure with\nrespect to the uniform convergence topology in the lattice size scaling limit.\nOur result greatly improves the work (Acta Math. 199(1):29-152) of Gady Kozma\nwhich establishes the weak convergence of the rescaled trace of 3D LERW towards\na random compact set with respect to the Hausdorff distance.'}, {'On large deviations and intersection of random interlacements': 'We investigate random interlacements on $\\mathbb{Z}^d$ with $d \\geq 3$, and\nderive the large deviation rate for the probability that the capacity of the\ninterlacement set in a macroscopic box is much smaller than that of the box. As\nan application, we obtain the large deviation rate for the probability that two\nindependent interlacements have empty intersections in a macroscopic box. We\nalso prove that conditioning on this event, one of them will be sparse in the\nbox in terms of capacity. This result is an example of the entropic repulsion\nphenomenon for random interlacements.'}, {'The intermediate level-sets of the four-dimensional membrane model': 'In this paper, we consider the discrete membrane model in four dimensions. We\nconfirm the existence of the scaling limit of the intermediate (i.e., a\nmultiple of the expected maximum) level-sets of the model, and show that it is\nequal in law to the sub-critical Gaussian multiplicative chaos (GMC) measure of\nthe continuum membrane model.'}, {'On the Robustness of Multi-View Rotation Averaging': 'Rotation averaging is a synchronization process on single or multiple\nrotation groups, and is a fundamental problem in many computer vision tasks\nsuch as multi-view structure from motion (SfM). Specifically, rotation\naveraging involves the recovery of an underlying pose-graph consistency from\npairwise relative camera poses. Specifically, given pairwise motion in rotation\ngroups, especially 3-dimensional rotation groups (\\eg, $\\mathbb{SO}(3)$), one\nis interested in recovering the original signal of multiple rotations with\nrespect to a fixed frame. In this paper, we propose a robust framework to solve\nmultiple rotation averaging problem, especially in the cases that a significant\namount of noisy measurements are present. By introducing the $\\epsilon$-cycle\nconsistency term into the solver, we enable the robust initialization scheme to\nbe implemented into the IRLS solver. Instead of conducting the costly edge\nremoval, we implicitly constrain the negative effect of erroneous measurements\nby weight reducing, such that IRLS failures caused by poor initialization can\nbe effectively avoided. Experiment results demonstrate that our proposed\napproach outperforms state of the arts on various benchmarks.'}, {'Hybrid Camera Pose Estimation with Online Partitioning for SLAM': 'This paper presents a hybrid real-time camera pose estimation framework with\na novel partitioning scheme and introduces motion averaging to monocular\nSimultaneous Localization and Mapping (SLAM) systems. Breaking through the\nlimitations of fixed-size temporal partitioning in many conventional SLAM\npipelines, our approach significantly improves the accuracy of local bundle\nadjustment by gathering spatially-strongly-connected cameras into each block.\nWith the dynamic initialization using intermediate computation values, \\XL{we\nimprove the Levenberg-Marquardt solver to further enhance the efficiency of the\nlocal optimization.} Moreover, the dense data association between blocks by our\nco-visibility-based partitioning enables us to explore and implement motion\naveraging to efficiently align the blocks globally, updating camera motion\nestimations on-the-fly. Experiments on benchmarks convincingly demonstrate the\npracticality and robustness of our proposed approach by significantly\noutperforming conventional approaches.'}, {'TransCamP: Graph Transformer for 6-DoF Camera Pose Estimation': 'Camera pose estimation or camera relocalization is the centerpiece in\nnumerous computer vision tasks such as visual odometry, structure from motion\n(SfM) and SLAM. In this paper we propose a neural network approach with a graph\ntransformer backbone, namely TransCamP, to address the camera relocalization\nproblem. In contrast with prior work where the pose regression is mainly guided\nby photometric consistency, TransCamP effectively fuses the image features,\ncamera pose information and inter-frame relative camera motions into encoded\ngraph attributes and is trained towards the graph consistency and accuracy\ninstead, yielding significantly higher computational efficiency. By leveraging\ngraph transformer layers with edge features and enabling tensorized adjacency\nmatrix, TransCamP dynamically captures the global attention and thus endows the\npose graph with evolving structures to achieve improved robustness and\naccuracy. In addition, optional temporal transformer layers actively enhance\nthe spatiotemporal inter-frame relation for sequential inputs. Evaluation of\nthe proposed network on various public benchmarks demonstrates that TransCamP\noutperforms state-of-the-art approaches.'}]","Title: FactAgent: An Agentic Large Language Model for Cost-Effective Fake News Detection

Background: The prolific dissemination of misinformation, or fake news, challenges the maintenance of accurate information in the digital age. The urgent need for reliable solutions to combat fake news demands innovative tools that leverage advancements in artificial intelligence to enhance detection accuracy efficiently.

Objective: The core aim of this research is to develop and evaluate an advanced system, FactAgent, utilizing large language models (LLMs) in an agentic manner to optimize fake news detection with minimal resources.

Innovations: FactAgent introduces a structured expert workflow, incorporating external search tools and strategic decision making guided by manually designed decision rules. By exploiting LLM capabilities while minimizing resource requirements, the proposed approach evaluates news veracity through a thoughtful process that includes evidence collection, analysis, and verification.

Methods: The approach involves collecting news through a corpus of expert-designed tools, including an external search engine for related articles, a standing tool focused on politics for political news, and a separate URL tool. The LLM autonomously compares the gathered evidence to make the final decision, with the exclusion of certain tools based on the news content. Performance is analyzed across three datasets, and an ablation study is conducted to elucidate the impact of various workflow components.

Results: FactAgent demonstrates superior performance compared to baseline models like LSTM, BERT, and supervised approaches in detecting fake news across the evaluated datasets. Notably, it achieves this without the extensive training and parameter tuning required by supervised LLM-based systems, making it a cost-effective solution for widespread implementation.

Contributions: This research introduces FactAgent, a pioneering agentic LLM framework for efficient fake news detection, grounded in manually designed workflows that optimize decision-making with minimal data. The findings advocate for a more adaptive and less data-intensive approach to ensuring information integrity on the internet.

Applications: FactAgent's accuracy in authenticating news across multiple topics signifies its broader applicability in domains ranging from social media management to academic research and political communication. By providing reliable tools for information validation, FactAgent fosters a more informed and trustworthy digital ecosystem."
"We present SAUNAS (Selective Amplification of Ultra Noisy Astronomical
Signal), a pipeline designed for detecting diffuse X-ray emission in the data
obtained with the Advanced CCD Imaging Spectrometer (ACIS) of the Chandra X-ray
Observatory. SAUNAS queries the available observations in the Chandra archive,
performs photometric calibration, PSF (point spread function) modeling, and
deconvolution, point-source removal, adaptive smoothing, and background
correction. This pipeline builds on existing and well-tested software including
CIAO, VorBin, and LIRA. We characterize the performance of SAUNAS through
several quality performance tests, and demonstrate the broad applications and
capabilities of SAUNAS using two galaxies already known to show X-ray emitting
structures. SAUNAS successfully detects the 30 kpc X-ray super-wind of NGC 3079
using Chandra/ACIS datasets, matching the spatial distribution detected with
more sensitive XMM-Newton observations. The analysis performed by SAUNAS
reveals an extended low surface brightness source in the field of UGC 5101 in
the 0.3-1.0 keV and 1.0-2.0 keV bands. This source is potentially a background
galaxy cluster or a hot gas plume associated with UGC 5101. SAUNAS demonstrates
its ability to recover previously undetected structures in archival data,
expanding exploration into the low surface brightness X-ray universe with
Chandra/ACIS.","[{'The galaxy ""missing dark matter"" NGC1052-DF4 is undergoing tidal\n  disruption': 'The existence of long-lived galaxies lacking dark matter represents a\nchallenge to our understanding of how galaxies form. Here, we present evidence\nthat explains the lack of dark matter in one of such galaxies: NGC1052-DF4.\nDeep optical imaging of the system has detected tidal tails in this object\ncaused by its interaction with its neighbouring galaxy NGC1035. As stars are\nmore centrally concentrated than the dark matter, the tidal stripping will\nremove a significant percentage of the dark matter before affecting the stars\nof the galaxy. Only ~7% of the stellar mass of the galaxy is in the tidal\ntails, suggesting that the stars of NGC1052-DF4 are starting only now to be\naffected by the interaction, while the percentage of remaining dark matter is\n<1%. This naturally explains the low content of dark matter inferred for this\ngalaxy and reconciles these type of galaxies with our current models of galaxy\nformation.'}, {'A disk and no signatures of tidal distortion in the galaxy ""lacking""\n  dark matter NGC 1052-DF2': 'Using ultra-deep imaging ($\\mu_g = 30.4$ mag/arcsec$^2$; 3$\\sigma$, 10""x10""),\nwe probed the surroundings of the first galaxy ""lacking"" dark matter\nKKS2000[04] (NGC 1052-DF2). Signs of tidal stripping in this galaxy would\nexplain its claimed low content of dark matter. However, we find no evidence of\ntidal tails. In fact, the galaxy remains undisturbed down to a radial distance\nof 80 arcsec. This radial distance triples previous spatial explorations of the\nstellar distribution of this galaxy. In addition, the distribution of its\nglobular clusters (GCs) is not extended in relation to the bulk of the galaxy\n(the radius containing half of the GCs is 21 arcsec). We also found that the\nsurface brightness radial profiles of this galaxy in the g and r bands decline\nexponentially from 35 to 80 arcsec. That, together with a constant ellipticity\nand position angle in the outer parts of the galaxy strongly suggests the\npresence of a low-inclination disk. This is consistent with the evidence of\nrotation found for this object. This finding implies that the dynamical mass of\nthis galaxy is a factor of 2 higher than previously reported, bringing the dark\nmatter content of this galaxy in line with galaxies of similar stellar mass.'}, {'The truncation of the disk of NGC 4565: Detected up to z=4 kpc, with\n  star formation, and affected by the warp': 'Context: The hierarchical model of galaxy formation suggests that galaxies\nare continuously growing. However, our position inside the Milky Way prevents\nus from studying the disk edge. Truncations are low surface brightness features\nlocated in the disk outskirts of external galaxies. They indicate where the\ndisk brightness abruptly drops and their location is thought to change\ndynamically. In previous analyses of Milky Way-like galaxies, truncations were\ndetected up to 3 kpc above the mid-plane but whether they remain present beyond\nthat height remains unclear.\n  Aims: Our goal is to determine whether truncations can be detected above 3\nkpc height in the Milky Way-like galaxy NGC 4565, thus establishing the actual\ndisk thickness. We also aim to study how the truncation relates to disk\nproperties such as star formation activity or the warp.\n  Methods: We perform a vertical study of the disk of NGC 4565 edge in\nunprecedented detail. We explore the truncation radius at different heights\nabove/below the disk mid-plane (0<z<8 kpc) and at different wavelengths. We use\nnew ultra-deep optical data ($\\mu_{g,\\rm{lim}}=30.5$ mag arcsec$^{-2}$; $3\n\\sigma$ within $10 \\times 10$ arcsec$^{2}$ boxes) in the $g$, $r$ and $i$ broad\nbands, along with near- and far-ultraviolet, H$\\alpha$, and \\ion{H}{i}\nobservations.\n  Results: We detect the truncation up to 4 kpc in the $g$, $r$ and $i$\nultra-deep bands which is 1 kpc higher than in any previous study for any\ngalaxy. The radial position of the truncation remains constant up to 3 kpc\nwhile higher up it is located at a smaller radius. This result is independent\nof the wavelength but is affected by the presence of the warp.\n  Conclusions: We propose an inside-out growth scenario for the formation of\nthe disk of NGC 4565. Our results point towards the truncation feature being\nlinked to a star-forming threshold and to the onset of the disk warp.'}, {'Extragalactic Magnetism with SOFIA (Legacy Program) -- I: The magnetic\n  field in the multi-phase interstellar medium of M51': 'The recent availability of high-resolution far-infrared (FIR) polarization\nobservations of galaxies using HAWC+/SOFIA has facilitated studies of\nextragalactic magnetic fields in the cold and dense molecular disks.We\ninvestigate if any significant structural differences are detectable in the\nkpc-scale magnetic field of the grand design face-on spiral galaxy M51 when\ntraced within the diffuse (radio) and the dense and cold (FIR) interstellar\nmedium (ISM). Our analysis reveals a complex scenario where radio and FIR\npolarization observations do not necessarily trace the same magnetic field\nstructure. We find that the magnetic field in the arms is wrapped tighter at\n154um than at 3 and 6 cm; statistically significant lower values for the\nmagnetic pitch angle are measured at FIR in the outskirts (R > 7 kpc) of the\ngalaxy. This difference is not detected in the interarm region. We find strong\ncorrelations of the polarization fraction and total intensity at FIR and radio\nwith the gas column density and 12CO(1-0) velocity dispersion. We conclude that\nthe arms show a relative increase of small-scale turbulent B-fields at regions\nwith increasing column density and dispersion velocities of the molecular gas.\nNo correlations are found with HI neutral gas. The star formation rate shows a\nclear correlation with the radio polarized intensity, which is not found in\nFIR, pointing to a small-scale dynamo-driven B-field amplification scenario.\nThis work shows that multi-wavelength polarization observations are key to\ndisentangling the interlocked relation between star formation, magnetic fields,\nand gas kinematics in the multi-phase ISM.'}, {'Extragalactic magnetism with SOFIA (SALSA Legacy Program) -- V: First\n  results on the magnetic field orientation of galaxies': 'We present the analysis of the magnetic field ($B$-field) structure of\ngalaxies measured with far-infrared (FIR) and radio (3 and 6 cm) polarimetric\nobservations. We use the first data release of the Survey on extragALactic\nmagnetiSm with SOFIA (SALSA) of 14 nearby ($<20$ Mpc) galaxies with resolved (5\narcsec-18 arcsec; $90$ pc--$1$ kpc) imaging polarimetric observations using\nHAWC+/SOFIA from $53$ to $214$ \\um. We compute the magnetic pitch angle\n($\\Psi_{B}$) profiles as a function of the galactrocentric radius. We introduce\na new magnetic alignment parameter ($\\zeta$) to estimate the\ndisordered-to-ordered $B$-field ratio in spiral $B$-fields. We find FIR and\nradio wavelengths to not generally trace the same $B$-field morphology in\ngalaxies. The $\\Psi_{B}$ profiles tend to be more ordered with galactocentric\nradius in radio ($\\zeta_{\\rm{6cm}} = 0.93\\pm0.03$) than in FIR\n($\\zeta_{\\rm{154\\mu m}} = 0.84\\pm0.14$). For spiral galaxies, FIR $B$-fields\nare $2-75$\\% more turbulent than the radio $B$-fields. For starburst galaxies,\nwe find that FIR polarization is a better tracer of the $B$-fields along the\ngalactic outflows than radio polarization. Our results suggest that the\n$B$-fields associated with dense, dusty, turbulent star-forming regions, those\ntraced at FIR, are less ordered than warmer, less-dense regions, those traced\nat radio, of the interstellar medium. The FIR $B$-fields seem to be more\nsensitive to the activity of the star-forming regions and the morphology of the\nmolecular clouds within a vertical height of few hundred pc in the disk of\nspiral galaxies than the radio $B$-fields.'}, {'Extragalactic magnetism with SOFIA (Legacy Program) -- II: A\n  magnetically-driven flow in the starburst ring of NGC 1097': ""Galactic bars are frequent in disk galaxies and they may support the transfer\nof matter towards the central engine of active nuclei. The barred galaxy NGC\n1097 has magnetic forces controlling the gas flow at several kpc scales, which\nsuggest that magnetic fields (B-fields) are dynamically important along the bar\nand nuclear ring. However, the effect of the B-field on the gas flows in the\ncentral kpc scale has not been characterized. Using thermal polarized emission\nat $89$ $\\mu$m with HAWC+/SOFIA, here, we measure that the polarized flux is\nspatially located at the contact regions of the outer-bar with the starburst\nring. The linear polarization decomposition analysis shows that the $89$ $\\mu$m\nand radio ($3.5$ and $6.2$ cm) polarization traces two different modes, $m$, of\nthe B-field: a constant B-field orientation and dominated by $m=0$ at $89$\n$\\mu$m, and a spiral B-field dominated by $m=2$ at radio. We show that the\nB-field at 89 $\\mu$m is concentrated in the warmest region of a shock driven by\nthe galactic-bar dynamics in the contact regions between the outer-bar with the\nstarburst ring. Radio polarization traces a superposition of the spiral B-field\noutside and within the starburst ring. According to Faraday rotation measures\nbetween $3.5$ and $6.2$ cm, the radial component of the B-field along the\ncontact regions points toward the galaxy's center on both sides. We conclude\nthat gas streams outside and within the starburst ring follow the B-field,\nwhich feeds the black hole with matter from the host galaxy.""}, {'Extragalactic magnetism with SOFIA (SALSA Legacy Program). VI. The\n  magnetic fields in the multi-phase interstellar medium of the Antennae\n  galaxies': 'Mergers are thought to be a fundamental channel for galaxy growth, perturbing\nthe gas dynamics and the magnetic fields (B-fields) in the interstellar medium\n(ISM). However, the mechanisms that amplify and dissipate B-fields during a\nmerger remain unclear. We characterize the morphology of the ordered B-fields\nin the multi-phase ISM of the closest merger of two spiral galaxies, the\nAntennae galaxies. We compare the inferred B-fields using $154~\\mu$m thermal\ndust and $11$ cm radio synchrotron emission polarimetric observations. We find\nthat the $154~\\mu$m B-fields are more ordered across the Antennae galaxies than\nthe $11$ cm B-fields. The turbulent-to-ordered $154~\\mu$m B-field increases at\nthe galaxy cores and star-forming regions. The relic spiral arm has an ordered\nspiral $154~\\mu$m B-field, while the $11$ cm B-field is radial. The $154~\\mu$m\nB-field may be dominated by turbulent dynamos with high $^{12}$CO(1-0) velocity\ndispersion driven by star-forming regions, while the $11$ cm B-field is\ncospatial with high HI velocity dispersion driven by galaxy interaction. This\nresult shows the dissociation between the warm gas mainly disturbed by the\nmerger, and the dense gas still following the dynamics of the relic spiral arm.\nWe find a $\\sim8.9$ kpc scale ordered B-field connecting the two galaxies. The\nbase of the tidal tail is cospatial with the HI and $^{12}$CO(1-0) emission and\nhas compressed and/or sheared $154~\\mu$m and $11$ cm B-fields driven by the\nmerger. We suggest that amplify B-fields, with respect to the rest of the\nsystem and other spiral galaxies, may be supporting the gas flow between both\ngalaxies and the tidal tail.'}, {'Extragalactic Magnetism with SOFIA (SALSA Legacy Program). VII. A\n  Tomographic View of Far-infrared and Radio Polarimetric Observations through\n  MHD Simulations of Galaxies': 'The structure of magnetic fields in galaxies remains poorly constrained,\ndespite the importance of magnetism in the evolution of galaxies. Radio\nsynchrotron and far-infrared (FIR) polarization and polarimetric observations\nare the best methods to measure galactic scale properties of magnetic fields in\ngalaxies beyond the Milky Way. We use synthetic polarimetric observations of a\nsimulated galaxy to identify and quantify the regions, scales, and interstellar\nmedium (ISM) phases probed at FIR and radio wavelengths. Our studied suite of\nmagnetohydrodynamical cosmological zoom-in simulations features\nhigh-resolutions (10 pc full-cell size) and multiple magnetization models. Our\nsynthetic observations have a striking resemblance to those of observed\ngalaxies. We find that the total and polarized radio emission extends to\napproximately double the altitude above the galactic disk (half-intensity disk\nthickness of $h_\\text{I radio} \\sim h_\\text{PI radio} = 0.23 \\pm 0.03$ kpc)\nrelative to the total FIR and polarized emission that are concentrated in the\ndisk midplane ($h_\\text{I FIR} \\sim h_\\text{PI FIR} = 0.11 \\pm 0.01$ kpc).\nRadio emission traces magnetic fields at scales of $\\gtrsim 300$ pc, whereas\nFIR emission probes magnetic fields at the smallest scales of our simulations.\nThese scales are comparable to our spatial resolution and well below the\nspatial resolution ($<300$ pc) of existing FIR polarimetric measurements.\nFinally, we confirm that synchrotron emission traces a combination of the warm\nneutral and cold neutral gas phases, whereas FIR emission follows the densest\ngas in the cold neutral phase in the simulation. These results are independent\nof the ISM magnetic field strength. The complementarity we measure between\nradio and FIR wavelengths motivates future multiwavelength polarimetric\nobservations to advance our knowledge of extragalactic magnetism.'}, {'Extragalactic magnetism with SOFIA (SALSA Legacy Program) -- IV: Program\n  overview and first results on the polarization fraction': 'We present the first data release of the Survey on extragALactic magnetiSm\nwith SOFIA (SALSA Legacy Program) with a set of 14 nearby ($<20$ Mpc) galaxies\nwith resolved imaging polarimetric observations using HAWC+ from $53$ to $214$\n$\\mu$m at a resolution of $5-18$"" ($90$ pc $-$ $1$ kpc). We introduce the\ndefinitions and background on extragalactic magnetism, and present the\nscientific motivation and sample selection of the program. Here, we focus on\nthe general trends in the emissive polarization fraction. Far-infrared\npolarimetric observations trace the thermal polarized emission of magnetically\naligned dust grains across the galaxy disks with polarization fractions of\n$P=0-15$% in the cold, $T_{\\rm d} = [19,48]$ K, and dense, $\\log_{10}(N_{\\rm\nHI+H_{2}}) = [19.96,22.91]$, interstellar medium. The spiral galaxies show a\nmedian $\\langle P_{154\\mu m} \\rangle = 3.3\\pm0.9 $% across the disks. We report\nthe first polarized spectrum of starburst galaxies showing a minimum within\n$89-154$ $\\mu$m. The falling $53-154$ $\\mu$m polarized spectrum may be due to a\ndecrease in the dust grain alignment efficiency produced by variations in dust\ntemperatures along the line-of-sight in the galactic outflow. We find that the\nstarburst galaxies and the star-forming regions within normal galaxies have the\nlowest polarization fractions. We find that 50% (7 out of 14) of the galaxies\nrequire a broken power-law in the $P-N_{HI+H_{2}}$ and $P-T_{d}$ relations with\nthree different trends. Group 1 has a relative increase of anisotropic random\nB-fields produced by compression or shear of B-fields in the galactic outflows,\nstarburst rings, and inner-bar of galaxies; and Groups 2 and 3 have a relative\nincrease of isotropic random B-fields driven by star-forming regions in the\nspiral arms, and/or an increase of dust grain alignment efficiency caused by\nshock-driven regions or evolutionary stages of a galaxy.'}, {'Extragalactic magnetism with SOFIA (SALSA Legacy Program) -- III: First\n  data release and on-the-fly polarization mapping characterization': 'We describe the data processing of the Survey on extragALactic magnetiSm with\nSOFIA (SALSA Legacy Program). This first data release presents 33% (51.34h out\nof 155.7h, including overheads) of the total awarded time taken from January\n2020 to December 2021. Our observations were performed using the newly\nimplemented on-the-fly mapping (OTFMAP) technique in the polarimetric mode. We\npresent the pipeline steps to obtain homogeneously reduced high-level data\nproducts of polarimetric maps of galaxies for use in scientific analysis. Our\napproach has a general design and can be applied to sources smaller than the\nfield-of-view of the HAWC+ array in any given band. We estimate that the OTFMAP\npolarimetric mode offers a reduction of observing overheads by a factor 2.34,\nand an improvement in sensitivity by a factor 1.80 when compared to previously\nobtained polarimetric observations using the chopping and nodding mode. The\nOTFMAP is a significant optimization of the polarimetric mode of HAWC+ as it\nultimately reduces the cost of operations of SOFIA/HAWC+ by increasing the\nscience collected per hour of observation up to an overall factor of 2.49. The\nOTFMAP polarimetric mode is the standard observing strategy of SALSA. The\nresults and quantitative analysis of this first data release are presented in\nPapers IV and V of the series.'}]","Title: SAUNAS: A Novel Mosaic Pipeline for High-fidelity X-ray Source Detection and Analysis

Abstract:
**Background**: Research in astrophysics often requires detailed analysis of X-ray emission, which can reveal valuable information about celestial sources, such as star formation, gas dynamics, and high-energy events. Traditional methods for X-ray detection lack clarity in disentangling diffuse emission from sources, and few standardized pipelines are readily available for data analysis.

**Objective**: The primary objective of this paper is to present SAUNAS, an innovative software pipeline designed to enhance the quality of X-ray source detection and analysis in astronomical surveys, particularly in distinguishing diffuse emission from point sources.

**Innovations**: SAUNAS introduces several advancements over existing methodologies, including more precise PSF correction, optimal separation of emission types, and robust statistical algorithms that ensure high detection accuracy and minimal false positives. The pipeline also facilitates reproducibility with detailed documentation and publicly accessible code.

**Methods**: The pipeline employs CIAO for initial image preprocessing, adjusts exposure times, and performs background subtraction. SAUNAS then refines detection through recursive Voronoi binning, adaptive smoothing, and advanced PSF correction. A bootstrapping-LIRA step corrects for uncertainties, leading to background-subtracted, PSF-corrected, and adaptive surface brightness maps.

**Results**: The pipeline outperforms conventional methods in various quality tests, including a lower false positive rate, higher flux conservation, and superior background subtraction. It was successfully applied to model galaxies, demonstrating its versatility and effectiveness in different settings.

**Contributions**: SAUNAS contributes to the field by offering a more accurate, efficient, and standardized approach to X-ray astrophysical survey analysis. Its integration with existing data products enhances the credibility and reliability of detections.

**Applications**: The enhanced precision of SAUNAS can lead to more sophisticated studies in astrophysics, such as detailed analyses of galaxies, interactions between galaxies, and the characterization of specific extragalactic phenomena, contributing to a broader understanding of the universe."
"Originating from semantic bugs, Entity-Inconsistency Bugs (EIBs) involve
misuse of syntactically valid yet incorrect program entities, such as variable
identifiers and function names, which often have security implications. Unlike
straightforward syntactic vulnerabilities, EIBs are subtle and can remain
undetected for years. Traditional detection methods, such as static analysis
and dynamic testing, often fall short due to the versatile and
context-dependent nature of EIBs. However, with advancements in Large Language
Models (LLMs) like GPT-4, we believe LLM-powered automatic EIB detection
becomes increasingly feasible through these models' semantics understanding
abilities. This research first undertakes a systematic measurement of LLMs'
capabilities in detecting EIBs, revealing that GPT-4, while promising, shows
limited recall and precision that hinder its practical application. The primary
problem lies in the model's tendency to focus on irrelevant code snippets
devoid of EIBs. To address this, we introduce a novel, cascaded EIB detection
system named WitheredLeaf, which leverages smaller, code-specific language
models to filter out most negative cases and mitigate the problem, thereby
significantly enhancing the overall precision and recall. We evaluated
WitheredLeaf on 154 Python and C GitHub repositories, each with over 1,000
stars, identifying 123 new flaws, 45% of which can be exploited to disrupt the
program's normal operations. Out of 69 submitted fixes, 27 have been
successfully merged.","[{'DCL-SLAM: A Distributed Collaborative LiDAR SLAM Framework for a Robotic\n  Swarm': 'To execute collaborative tasks in unknown environments, a robotic swarm needs\nto establish a global reference frame and locate itself in a shared\nunderstanding of the environment. However, it faces many challenges in\nreal-world scenarios, such as the prior information about the environment being\nabsent and poor communication among the team members. This work presents\nDCL-SLAM, a fully distributed collaborative LiDAR SLAM framework intended for\nthe robotic swarm to simultaneously co-localize in an unknown environment with\nminimal information exchange. Based on ad-hoc wireless peer-to-peer\ncommunication (limited bandwidth and communication range), DCL-SLAM adopts the\nlightweight LiDAR-Iris descriptor for place recognition and does not require\nfull connectivity among teams. DCL-SLAM includes three main parts: a\nreplaceable single-robot front-end that produces LiDAR odometry results; a\ndistributed loop closure module that detects inter-robot loop closures with\nkeyframes; and a distributed back-end module that adapts distributed pose graph\noptimizer combined with a pairwise consistent measurement set maximization\nalgorithm to reject spurious inter-robot loop closures. We integrate our\nproposed framework with diverse open-source LiDAR odometry methods to show its\nversatility. The proposed system is extensively evaluated on benchmarking\ndatasets and field experiments over various scales and environments.\nExperimental result shows that DCL-SLAM achieves higher accuracy and lower\ncommunication bandwidth than other state-of-art multi-robot SLAM systems. The\nfull source code is available at https://github.com/zhongshp/DCL-SLAM.git.'}, {'Understanding TEE Containers, Easy to Use? Hard to Trust': ""As an emerging technique for confidential computing, trusted execution\nenvironment (TEE) receives a lot of attention. To better develop, deploy, and\nrun secure applications on a TEE platform such as Intel's SGX, both academic\nand industrial teams have devoted much effort to developing reliable and\nconvenient TEE containers. In this paper, we studied the isolation strategies\nof 15 existing TEE containers to protect secure applications from potentially\nmalicious operating systems (OS) or untrusted applications, using a\nsemi-automatic approach combining a feedback-guided analyzer with manual code\nreview. Our analysis reveals the isolation protection each of these TEE\ncontainers enforces, and their security weaknesses. We observe that none of the\nexisting TEE containers can fulfill the goal they set, due to various pitfalls\nin their design and implementation. We report the lessons learnt from our study\nfor guiding the development of more secure containers, and further discuss the\ntrend of TEE container designs. We also release our analyzer that helps\nevaluate the container middleware both from the enclave and from the kernel.""}, {'Hand-held 3D Photoacoustic Imager with GPS': 'As an emerging medical diagnostic technology, photoacoustic imaging has been\nimplemented for both preclinical and clinical applications. For clinical\nconvenience, a handheld free scan photoacoustic tomography (PAT) system\nproviding 3D imaging capability is essentially needed, which has potential for\nsurgical navigation and disease diagnosis. In this paper, we proposed a free\nscan 3D PAT (fsPAT) system based on a handheld linear array ultrasound probe. A\nglobal positioning system (GPS) is applied for ultrasound probes coordinate\nacquisition. The proposed fsPAT can simultaneously realize real time 2D\nimaging, and large field of view 3D volumetric imaging, which is reconstructed\nfrom the multiple 2D images with coordinate information acquired by the GPS. To\nform a high quality 3D image, a dedicated space transformation method and\nreconstruction algorithm are used and validated by the proposed system. Both\nsimulation and experimental studies have been performed to prove the\nfeasibility of the proposed fsPAT. To explore its clinical potential, in vivo\n3D imaging of human wrist vessels is also conducted, showing clear subcutaneous\nvessel network with high image contrast.'}, {'HySec-Flow: Privacy-Preserving Genomic Computing with SGX-based Big-Data\n  Analytics Framework': 'Trusted execution environments (TEE) such as Intel\'s Software Guard Extension\n(SGX) have been widely studied to boost security and privacy protection for the\ncomputation of sensitive data such as human genomics. However, a performance\nhurdle is often generated by SGX, especially from the small enclave memory. In\nthis paper, we propose a new Hybrid Secured Flow framework (called\n""HySec-Flow"") for large-scale genomic data analysis using SGX platforms. Here,\nthe data-intensive computing tasks can be partitioned into independent subtasks\nto be deployed into distinct secured and non-secured containers, therefore\nallowing for parallel execution while alleviating the limited size of Page\nCache (EPC) memory in each enclave. We illustrate our contributions using a\nworkflow supporting indexing, alignment, dispatching, and merging the execution\nof SGX- enabled containers. We provide details regarding the architecture of\nthe trusted and untrusted components and the underlying Scorn and Graphene\nsupport as generic shielding execution frameworks to port legacy code. We\nthoroughly evaluate the performance of our privacy-preserving reads mapping\nalgorithm using real human genome sequencing data. The results demonstrate that\nthe performance is enhanced by partitioning the time-consuming genomic\ncomputation into subtasks compared to the conventional execution of the\ndata-intensive reads mapping algorithm in an enclave. The proposed HySec-Flow\nframework is made available as an open-source and adapted to the data-parallel\ncomputation of other large-scale genomic tasks requiring security and scalable\ncomputational resources.'}, {'Assessing Bone Quality of Spine on Children with Scoliosis Using\n  Ultrasound Reflection FAI Method -- A Preliminary Study': 'Osteopenia is indicated as a common phenomenon in patients who have\nscoliosis. Quantitative ultrasound (QUS) has been used to assess skeletal\nstatus for decades, and recently ultrasound imaging using reflection signals\nfrom vertebra were as well applied to measure spinal curvatures on children\nwith scoliosis. The objectives of this study are to develop a new method which\ncan robustly extract a parameter from ultrasound spinal data for estimating\nbone quality of scoliotic patients and to investigate the potential for the\nparameter on predicting curve progression. The frequency amplitude index (FAI)\nwas calculated based on the spectrum of the original radio frequency (RF)\nsignals reflected from the tissue-vertebra interface. The correlation between\nFAI and reflection coefficient was validated using decalcified bovine bone\nsamples in vitro, and the FAIs of scoliotic subjects were investigated in vivo\nreferring to BMI, Cobb angles and curve progression status. The results showed\nthat the intra-rater measures were highly reliable between different trials\n(ICC=0.997). The FAI value was strongly correlated to the reflection\ncoefficient of bone tissue ($R^{2}=0.824$), and the lower FAI indicated the\nhigher risk of curve progression for the non-mild cases. This preliminary study\nreported that the FAI method can provide a feasible and promising approach to\nassess bone quality and monitor curve progression of the patients who have AIS.'}, {'RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse\n  Environments': 'LiDAR-based localization is valuable for applications like mining surveys and\nunderground facility maintenance. However, existing methods can struggle when\ndealing with uninformative geometric structures in challenging scenarios. This\npaper presents RELEAD, a LiDAR-centric solution designed to address\nscan-matching degradation. Our method enables degeneracy-free point cloud\nregistration by solving constrained ESIKF updates in the front end and\nincorporates multisensor constraints, even when dealing with outlier\nmeasurements, through graph optimization based on Graduated Non-Convexity\n(GNC). Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL)\nfor efficient GNC-based optimization. RELEAD has undergone extensive evaluation\nin degenerate scenarios and has outperformed existing state-of-the-art\nLiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods.'}, {'S3E: A Large-scale Multimodal Dataset for Collaborative SLAM': ""The burgeoning demand for collaborative robotic systems to execute complex\ntasks collectively has intensified the research community's focus on advancing\nsimultaneous localization and mapping (SLAM) in a cooperative context. Despite\nthis interest, the scalability and diversity of existing datasets for\ncollaborative trajectories remain limited, especially in scenarios with\nconstrained perspectives where the generalization capabilities of Collaborative\nSLAM (C-SLAM) are critical for the feasibility of multi-agent missions.\nAddressing this gap, we introduce S3E, an expansive multimodal dataset.\nCaptured by a fleet of unmanned ground vehicles traversing four distinct\ncollaborative trajectory paradigms, S3E encompasses 13 outdoor and 5 indoor\nsequences. These sequences feature meticulously synchronized and spatially\ncalibrated data streams, including 360-degree LiDAR point cloud,\nhigh-resolution stereo imagery, high-frequency inertial measurement units\n(IMU), and Ultra-wideband (UWB) relative observations. Our dataset not only\nsurpasses previous efforts in scale, scene diversity, and data intricacy but\nalso provides a thorough analysis and benchmarks for both collaborative and\nindividual SLAM methodologies. For access to the dataset and the latest\ninformation, please visit our repository at https://pengyu-team.github.io/S3E.""}, {'CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic\n  Swarms': 'Collaborative state estimation using different heterogeneous sensors is a\nfundamental prerequisite for robotic swarms operating in GPS-denied\nenvironments, posing a significant research challenge. In this paper, we\nintroduce a centralized system to facilitate collaborative\nLiDAR-ranging-inertial state estimation, enabling robotic swarms to operate\nwithout the need for anchor deployment. The system efficiently distributes\ncomputationally intensive tasks to a central server, thereby reducing the\ncomputational burden on individual robots for local odometry calculations. The\nserver back-end establishes a global reference by leveraging shared data and\nrefining joint pose graph optimization through place recognition, global\noptimization techniques, and removal of outlier data to ensure precise and\nrobust collaborative state estimation. Extensive evaluations of our system,\nutilizing both publicly available datasets and our custom datasets, demonstrate\nsignificant enhancements in the accuracy of collaborative SLAM estimates.\nMoreover, our system exhibits remarkable proficiency in large-scale missions,\nseamlessly enabling ten robots to collaborate effectively in performing SLAM\ntasks. In order to contribute to the research community, we will make our code\nopen-source and accessible at \\url{https://github.com/PengYu-team/Co-LRIO}.'}, {'Automatic Diagnosis of Carotid Atherosclerosis Using a Portable Freehand\n  3D Ultrasound Imaging System': ""The objective of this study is to develop a deep-learning based detection and\ndiagnosis technique for carotid atherosclerosis using a portable freehand 3D\nultrasound (US) imaging system. A total of 127 3D carotid artery scans were\nacquired using a portable 3D US system which consisted of a handheld US scanner\nand an electromagnetic tracking system. A U-Net segmentation network was\nfirstly applied to extract the carotid artery on 2D transverse frame, then a\nnovel 3D reconstruction algorithm using fast dot projection (FDP) method with\nposition regularization was proposed to reconstruct the carotid artery volume.\nFurthermore, a convolutional neural network was used to classify healthy and\ndiseased cases qualitatively. 3D volume analysis methods including longitudinal\nimage acquisition and stenosis grade measurement were developed to obtain the\nclinical metrics quantitatively. The proposed system achieved sensitivity of\n0.714, specificity of 0.851 and accuracy of 0.803 respectively for diagnosis of\ncarotid atherosclerosis. The automatically measured stenosis grade illustrated\ngood correlation (r=0.762) with the experienced expert measurement. The\ndeveloped technique based on 3D US imaging can be applied to the automatic\ndiagnosis of carotid atherosclerosis. The proposed deep-learning based\ntechnique was specially designed for a portable 3D freehand US system, which\ncan provide more convenient carotid atherosclerosis examination and decrease\nthe dependence on clinician's experience.""}, {'Confidential Attestation: Efficient in-Enclave Verification of Privacy\n  Policy Compliance': 'A trusted execution environment (TEE) such as Intel Software Guard Extension\n(SGX) runs a remote attestation to prove to a data owner the integrity of the\ninitial state of an enclave, including the program to operate on her data. For\nthis purpose, the data-processing program is supposed to be open to the owner,\nso its functionality can be evaluated before trust can be established. However,\nincreasingly there are application scenarios in which the program itself needs\nto be protected. So its compliance with privacy policies as expected by the\ndata owner should be verified without exposing its code.\n  To this end, this paper presents CAT, a new model for TEE-based confidential\nattestation. Our model is inspired by Proof-Carrying Code, where a code\ngenerator produces proof together with the code and a code consumer verifies\nthe proof against the code on its compliance with security policies. Given that\nthe conventional solutions do not work well under the resource-limited and\nTCB-frugal TEE, we propose a new design that allows an untrusted out-enclave\ngenerator to analyze the source code of a program when compiling it into binary\nand a trusted in-enclave consumer efficiently verifies the correctness of the\ninstrumentation and the presence of other protection before running the binary.\nOur design strategically moves most of the workload to the code generator,\nwhich is responsible for producing well-formatted and easy-to-check code, while\nkeeping the consumer simple. Also, the whole consumer can be made public and\nverified through a conventional attestation. We implemented this model on Intel\nSGX and demonstrate that it introduces a very small part of TCB. We also\nthoroughly evaluated its performance on micro- and macro- benchmarks and\nreal-world applications, showing that the new design only incurs a small\noverhead when enforcing several categories of security policies.'}]","Abstract

In the era of large-scale software development, Semantic bugs (EIBs) pose significant threats to system reliability and legal obligations, often overlooked by existing static and dynamic analysis tools. This paper addresses the critical gap in detecting these subtle inconsistencies in code functionalities. Our research introduces WitheredLeaf, a groundbreaking pipeline integrating pre-filtering, post-filtering, and leverages Large Language Models (LLMs), notably CodeBERT and GPT-4, tackling various challenges related to entity inconsistency.

The core objective is to devise an effective method for identifying and mitigating entity inconsistency bugs (EIBs) in software code. The innovative aspects include our pre-filtering technique with CodeBERT tailored to address its max-token length limitation, a multi-stage approach employing novel masked code concatenation and entity scoring. 

The proposed pipeline utilizes pre-filter CodeBERT and post-filter Code Llama for efficient EIB detection. Evaluation on Python and C code reveals that WitheredLeaf successfully identifies a significant number of EIBs, achieving acceptable detection costs.

Contributions lie in transforming pre-existing LLM models for code understanding and entity prediction, enhancing capabilities beyond general programming tasks by addressing the nuanced intricacies of semantic bugs. The results demonstrate the feasibility of deploying such a framework within existing practices, opening avenues for more automated, efficient detection methods in software development processes.

With WitheredLeaf, the software industry can prioritize and focus resources on bugs that threaten the system's integrity more efficiently, thereby leading to more secure and reliable software products. This research significantly advances the field of automated software testing and bug detection, offering practical solutions and opportunities for further exploration.

This abstract encapsulates the pivotal moments: from identifying the pressing need for EIB detection, to proposing WitheredLeaf as an innovative solution, alongside presenting promising performance outcomes."
"Machine learning systems have been widely used to make decisions about
individuals who may best respond and behave strategically to receive favorable
outcomes, e.g., they may genuinely improve the true labels or manipulate
observable features directly to game the system without changing labels.
Although both behaviors have been studied (often as two separate problems) in
the literature, most works assume individuals can (i) perfectly foresee the
outcomes of their behaviors when they best respond; (ii) change their features
arbitrarily as long as it is affordable, and the costs they need to pay are
deterministic functions of feature changes. In this paper, we consider a
different setting and focus on imitative strategic behaviors with unforeseeable
outcomes, i.e., individuals manipulate/improve by imitating the features of
those with positive labels, but the induced feature changes are unforeseeable.
We first propose a Stackelberg game to model the interplay between individuals
and the decision-maker, under which we examine how the decision-maker's ability
to anticipate individual behavior affects its objective function and the
individual's best response. We show that the objective difference between the
two can be decomposed into three interpretable terms, with each representing
the decision-maker's preference for a certain behavior. By exploring the roles
of each term, we further illustrate how a decision-maker with adjusted
preferences can simultaneously disincentivize manipulation, incentivize
improvement, and promote fairness.","[{'A combinatorial algorithm for constrained assortment optimization under\n  nested logit model': 'We consider the assortment optimization problem with disjoint-cardinality\nconstraints under two-level nested logit model. To solve this problem, we first\nidentify a candidate set with $O(mn^2)$ assortments and show that at least one\noptimal assortment is included in this set. Based on this observation, a fast\nalgorithm, which runs in $O(m n^2 \\log mn)$ time, is proposed to find an\noptimal assortment.'}, {'Optimizing Medication Decisions for Patients with Atrial Fibrillation\n  through Path Development Network': 'Atrial fibrillation (AF) is a common cardiac arrhythmia characterized by\nrapid and irregular contractions of the atria. It significantly elevates the\nrisk of strokes due to slowed blood flow in the atria, especially in the left\natrial appendage, which is prone to blood clot formation. Such clots can\nmigrate into cerebral arteries, leading to ischemic stroke. To assess whether\nAF patients should be prescribed anticoagulants, doctors often use the\nCHA2DS2-VASc scoring system. However, anticoagulant use must be approached with\ncaution as it can impact clotting functions. This study introduces a machine\nlearning algorithm that predicts whether patients with AF should be recommended\nanticoagulant therapy using 12-lead ECG data. In this model, we use STOME to\nenhance time-series data and then process it through a Convolutional Neural\nNetwork (CNN). By incorporating a path development layer, the model achieves a\nspecificity of 30.6% under the condition of an NPV of 1. In contrast, LSTM\nalgorithms without path development yield a specificity of only 2.7% under the\nsame NPV condition.'}, {'Crystal Graph Convolutional Neural Networks for an Accurate and\n  Interpretable Prediction of Material Properties': 'The use of machine learning methods for accelerating the design of\ncrystalline materials usually requires manually constructed feature vectors or\ncomplex transformation of atom coordinates to input the crystal structure,\nwhich either constrains the model to certain crystal types or makes it\ndifficult to provide chemical insights. Here, we develop a crystal graph\nconvolutional neural networks framework to directly learn material properties\nfrom the connection of atoms in the crystal, providing a universal and\ninterpretable representation of crystalline materials. Our method provides a\nhighly accurate prediction of density functional theory calculated properties\nfor eight different properties of crystals with various structure types and\ncompositions after being trained with $10^4$ data points. Further, our\nframework is interpretable because one can extract the contributions from local\nchemical environments to global properties. Using an example of perovskites, we\nshow how this information can be utilized to discover empirical rules for\nmaterials design.'}, {'Hierarchical Visualization of Materials Space with Graph Convolutional\n  Neural Networks': 'The combination of high throughput computation and machine learning has led\nto a new paradigm in materials design by allowing for the direct screening of\nvast portions of structural, chemical, and property space. The use of these\npowerful techniques leads to the generation of enormous amounts of data, which\nin turn calls for new techniques to efficiently explore and visualize the\nmaterials space to help identify underlying patterns. In this work, we develop\na unified framework to hierarchically visualize the compositional and\nstructural similarities between materials in an arbitrary material space with\nrepresentations learned from different layers of graph convolutional neural\nnetworks. We demonstrate the potential for such a visualization approach by\nshowing that patterns emerge automatically that reflect similarities at\ndifferent scales in three representative classes of materials: perovskites,\nelemental boron, and general inorganic crystals, covering material spaces of\ndifferent compositions, structures, and both. For perovskites, elemental\nsimilarities are learned that reflects multiple aspects of atom properties. For\nelemental boron, structural motifs emerge automatically showing characteristic\nboron local environments. For inorganic crystals, the similarity and stability\nof local coordination environments are shown combining different center and\nneighbor atoms. The method could help transition to a data-centered exploration\nof materials space in automated materials design.'}, {'GraphHop: An Enhanced Label Propagation Method for Node Classification': ""A scalable semi-supervised node classification method on graph-structured\ndata, called GraphHop, is proposed in this work. The graph contains attributes\nof all nodes but labels of a few nodes. The classical label propagation (LP)\nmethod and the emerging graph convolutional network (GCN) are two popular\nsemi-supervised solutions to this problem. The LP method is not effective in\nmodeling node attributes and labels jointly or facing a slow convergence rate\non large-scale graphs. GraphHop is proposed to its shortcoming. With proper\ninitial label vector embeddings, each iteration of GraphHop contains two steps:\n1) label aggregation and 2) label update. In Step 1, each node aggregates its\nneighbors' label vectors obtained in the previous iteration. In Step 2, a new\nlabel vector is predicted for each node based on the label of the node itself\nand the aggregated label information obtained in Step 1. This iterative\nprocedure exploits the neighborhood information and enables GraphHop to perform\nwell in an extremely small label rate setting and scale well for very large\ngraphs. Experimental results show that GraphHop outperforms state-of-the-art\ngraph learning methods on a wide range of tasks (e.g., multi-label and\nmulti-class classification on citation networks, social graphs, and commodity\nconsumption graphs) in graphs of various sizes. Our codes are publicly\navailable on GitHub (https://github.com/TianXieUSC/GraphHop).""}, {'Label Efficient Regularization and Propagation for Graph Node\n  Classification': 'An enhanced label propagation (LP) method called GraphHop was proposed\nrecently. It outperforms graph convolutional networks (GCNs) in the\nsemi-supervised node classification task on various networks. Although the\nperformance of GraphHop was explained intuitively with joint node attribute and\nlabel signal smoothening, its rigorous mathematical treatment is lacking. In\nthis paper, we propose a label efficient regularization and propagation (LERP)\nframework for graph node classification, and present an alternate optimization\nprocedure for its solution. Furthermore, we show that GraphHop only offers an\napproximate solution to this framework and has two drawbacks. First, it\nincludes all nodes in the classifier training without taking the reliability of\npseudo-labeled nodes into account in the label update step. Second, it provides\na rough approximation to the optimum of a subproblem in the label aggregation\nstep. Based on the LERP framework, we propose a new method, named the LERP\nmethod, to solve these two shortcomings. LERP determines reliable pseudo-labels\nadaptively during the alternate optimization and provides a better\napproximation to the optimum with computational efficiency. Theoretical\nconvergence of LERP is guaranteed. Extensive experiments are conducted to\ndemonstrate the effectiveness and efficiency of LERP. That is, LERP outperforms\nall benchmarking methods, including GraphHop, consistently on five test\ndatasets and an object recognition task at extremely low label rates (i.e., 1,\n2, 4, 8, 16, and 20 labeled samples per class).'}, {'L2-Relaxation: With Applications to Forecast Combination and Portfolio\n  Analysis': 'This paper tackles forecast combination with many forecasts or minimum\nvariance portfolio selection with many assets. A novel convex problem called\nL2-relaxation is proposed. In contrast to standard formulations, L2-relaxation\nminimizes the squared Euclidean norm of the weight vector subject to a set of\nrelaxed linear inequality constraints. The magnitude of relaxation, controlled\nby a tuning parameter, balances the bias and variance. When the\nvariance-covariance (VC) matrix of the individual forecast errors or financial\nassets exhibits latent group structures -- a block equicorrelation matrix plus\na VC for idiosyncratic noises, the solution to L2-relaxation delivers roughly\nequal within-group weights. Optimality of the new method is established under\nthe asymptotic framework when the number of the cross-sectional units $N$\npotentially grows much faster than the time dimension $T$. Excellent finite\nsample performance of our method is demonstrated in Monte Carlo simulations.\nIts wide applicability is highlighted in three real data examples concerning\nempirical applications of microeconomics, macroeconomics, and finance.'}, {'Boosting Retailer Revenue by Generated Optimized Combined Multiple\n  Digital Marketing Campaigns': ""Campaign is a frequently employed instrument in lifting up the GMV (Gross\nMerchandise Volume) of retailer in traditional marketing. As its counterpart in\nonline context, digital-marketing-campaign (DMC) has being trending in recent\nyears with the rapid development of the e-commerce. However, how to empower\nmassive sellers on the online retailing platform the capacity of applying\ncombined multiple digital marketing campaigns to boost their shops' revenue, is\nstill a novel topic. In this work, a comprehensive solution of generating\noptimized combined multiple DMCs is presented. Firstly, a potential\npersonalized DMC pool is generated for every retailer by a newly proposed\nneural network model, i.e. the DMCNet (Digital-Marketing-Campaign Net).\nSecondly, based on the sub-modular optimization theory and the DMC pool by\nDMCNet, the generated combined multiple DMCs are ranked with respect to their\nrevenue generation strength then the top three ranked campaigns are returned to\nthe sellers' back-end management system, so that retailers can set combined\nmultiple DMCs for their online shops just in one-shot. Real online A/B-test\nshows that with the integrated solution, sellers of the online retailing\nplatform increase their shops' GMVs with approximately 6$\\%$.""}, {'Adaptive Joint Estimation of Temporal Vertex and Edge Signals': 'The adaptive estimation of coexisting temporal vertex (node) and edge signals\non graphs is a critical task when a change in edge signals influences the\ntemporal dynamics of the vertex signals. However, the current Graph Signal\nProcessing algorithms mostly consider only the signals existing on the graph\nvertices and have neglected the fact that signals can reside on the edges. We\npropose an Adaptive Joint Vertex-Edge Estimation (AJVEE) algorithm for jointly\nestimating time-varying vertex and edge signals through a time-varying\nregression, incorporating both vertex signal filtering and edge signal\nfiltering. Accompanying AJVEE is a newly proposed Adaptive Least Mean Square\nprocedure based on the Hodge Laplacian (ALMS-Hodge), which is inspired by\nclassical adaptive filters combining simplicial filtering and simplicial\nregression. AJVEE is able to operate jointly on the vertices and edges by\nmerging two ALMS-Hodge algorithms specified on the vertices and edges into a\nunified formulation. A more generalized case extending AJVEE beyond the\nvertices and edges is being discussed. Experimenting on real-world traffic\nnetworks and population mobility networks, we have confirmed that our proposed\nAJVEE algorithm could accurately and jointly track time-varying vertex and edge\nsignals on graphs.'}, {'Inverse Design of Next-generation Superconductors Using Data-driven Deep\n  Generative Models': 'Finding new superconductors with a high critical temperature ($T_c$) has been\na challenging task due to computational and experimental costs. We present a\ndiffusion model inspired by the computer vision community to generate new\nsuperconductors with unique structures and chemical compositions. Specifically,\nwe used a crystal diffusion variational autoencoder (CDVAE) along with\natomistic line graph neural network (ALIGNN) pretrained models and the Joint\nAutomated Repository for Various Integrated Simulations (JARVIS)\nsuperconducting database of density functional theory (DFT) calculations to\ngenerate new superconductors with a high success rate. We started with a DFT\ndataset of $\\approx$1000 superconducting materials to train the diffusion\nmodel. We used the model to generate 3000 new structures, which along with\npre-trained ALIGNN screening results in 61 candidates. For the top candidates,\nwe performed DFT calculations for validation. Such approaches go beyond the\nfunnel-like materials design approaches and allow for the inverse design of\nnext-generation materials.'}]","**Abstract**

In the realm of machine learning decision-making under strategic behavior, individuals often modify their features to secure preferential outcomes, posing a challenge for fair and robust decision systems. This paper advances the field by analyzing the interplay between strategic manipulation and improvement in a controlled decision-making context. We aim to develop optimization strategies that are resilient to strategic behavior while incentivizing genuine improvements, promoting fairness and efficiency.

Our innovation lies in modeling manipulation as a distinct strategy from improvement, accounting for the varying impact of each action on decision-makers. We derive conditions under which strategic decision-makers can simultaneously disincentivize manipulation and boost improvement, ensuring the fairness and explainability of their policies. 

Utilizing beta and normal distributions, we conduct experiments on synthetic and real-world datasets (such as FICO scores) to demonstrate the effectiveness of our proposed adjustments in enhancing policy outcomes. Results show a compensation mechanism that aligns improved future labels and feature distributions with decision-makers' objectives, diverging our actions from previous studies that relied on assumptions about perfect foresight and deterministic cost functions for manipulative or improving behaviors.

This research contributes significantly to the body of work in accountable, fair, and socially responsible AI decision-making processes, showcasing a novel path to address algorithmic manipulation and inefficiencies. Our findings have practical implications for numerous sectors aiming to optimize decision-making systems, from financial services to educational and employment arenas, fostering an environment that acknowledges and leverages the complexities of human strategic behavior to enhance overall outcomes."
"This paper aims to efficiently enable large language models (LLMs) to use
external knowledge and goal guidance in conversational recommender system (CRS)
tasks. Advanced LLMs (e.g., ChatGPT) are limited in domain-specific CRS tasks
for 1) generating grounded responses with recommendation-oriented knowledge, or
2) proactively leading the conversations through different dialogue goals. In
this work, we first analyze those limitations through a comprehensive
evaluation, showing the necessity of external knowledge and goal guidance which
contribute significantly to the recommendation accuracy and language quality.
In light of this finding, we propose a novel ChatCRS framework to decompose the
complex CRS task into several sub-tasks through the implementation of 1) a
knowledge retrieval agent using a tool-augmented approach to reason over
external Knowledge Bases and 2) a goal-planning agent for dialogue goal
prediction. Experimental results on two multi-goal CRS datasets reveal that
ChatCRS sets new state-of-the-art benchmarks, improving language quality of
informativeness by 17% and proactivity by 27%, and achieving a tenfold
enhancement in recommendation accuracy.","[{'Naturalness, dark matter, and the muon anomalous magnetic moment in\n  supersymmetric extensions of the standard model with a pseudo-Dirac gluino': 'We study the naturalness, dark matter, and muon anomalous magnetic moment in\nthe Supersymmetric Standard Models (SSMs) with a pseudo-Dirac gluino (PDGSSMs)\nfrom hybrid $F-$ and $D-$term supersymmetry (SUSY) breakings. To obtain the\nobserved dark matter relic density and explain the muon anomalous magnetic\nmoment, we find that the low energy fine-tuning measures are larger than about\n30 due to strong constraints from the LUX and PANDAX experiments. Thus, to\nstudy the natural PDGSSMs, we consider multi-component dark matter and then the\nrelic density of the lighest supersymmetric particle (LSP) neutralino is\nsmaller than the correct value. We classify our models into six kinds: (i) Case\nA is a general case, which has small low energy fine-tuning measure and can\nexplain the anomalous magnetic moment of the muon; (ii) Case B with the LSP\nneutralino and light stau coannihilation; (iii) Case C with Higgs funnel; (iv)\nCase D with Higgsino LSP; (v) Case E with light stau coannihilation and\nHiggsino LSP; (vi) Case F with Higgs funnel and Higgsino LSP. We study these\nCases in details, and show that our models can be natural and consistent with\nthe LUX and PANDAX experiments, as well as explain the muon anomalous magnetic\nmoment. In particular, all these cases except the stau coannihilation can even\nhave low energy fine-tuning measures around 10.'}, {'Driving many distant atoms into high-fidelity steady state entanglement\n  via Lyapunov control': 'Based on Lyapunov control theory in closed and open systems, we propose a\nscheme to generate W state of many distant atoms in the cavity-fiber-cavity\nsystem. In the closed system, the W state is generated successfully even when\nthe coupling strength between the cavity and fiber is extremely weak. In the\npresence of atomic spontaneous emission or cavity and fiber decay, the\nphoton-measurement and quantum feedback approaches are proposed to improve the\nfidelity, which enable efficient generation of high-fidelity W state in the\ncase of large dissipation. Furthermore, the time-optimal Lyapunov control is\ninvestigated to shorten the evolution time and improve the fidelity in open\nsystems.'}, {""Comments on the expanded Maxwell's equations for moving charged media\n  system"": ""In the recent work~\\cite{Wang:2021p2}, the author proposed the expanded\nMaxwell's equations for moving charged media system, which seems subtle.\nConsidering a very short time, we can approximately define the inertial frame\nof reference. If we assume all the physical quantities are defined in the same\nreference frame by default, Maxwell's equations for the static media system and\nmoving media system are definitely the traditional Maxwell's equations, which\nare covariant and consistent with the two fundamental postulates of special\nrelativity. We even prove the covariance of Maxwell's equations explicitly by\nconsidering the Lorentz transformation under the ${\\cal O} (v)$ order\napproximation and the Galileo approximation, respectively. Therefore, it seems\nto us that the fields in the expanded Maxwell's equations cannot be in the same\nreference frame. Defining the fields in the lab and co-moving frames\nexplicitly, we derive the expanded Maxwell's equations for moving media system.\nFurthermore, we discuss another possible variant of Maxwell's equations, which\nhas an additional coefficient $\\alpha$ related to the media. However, it is\nstill subtle from theoretical point of view.""}, {'Evaluating Cognitive and Neuropsychological Assessments -- A\n  Comprehensive Review': ""Cognitive impairments in older adults represent a significant public health\nconcern, necessitating accurate diagnostic and monitoring strategies. In this\nstudy, the principal cognitive and neuropsychological evaluations employed for\nthe diagnosis and longitudinal observation of cognitive deficits in the elderly\nare investigated. An analytical review of instruments including the Mini-Mental\nState Examination (MMSE), Digit Symbol Substitution Test (DSST), Montreal\nCognitive Assessment (MoCA), and Trail Making Test (TMT) is conducted. This\nexamination encompasses an assessment of each instrument's methodology,\nefficacy, advantages, and limitations. The objective is to enhance\ncomprehension of these assessments for the early identification and effective\nmanagement of conditions such as dementia and mild cognitive impairment,\nthereby contributing to the advancement of cognitive health within the\ngeriatric population.""}, {'SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin\n  Transformer and LSTM': 'Integrating CNNs and RNNs to capture spatiotemporal dependencies is a\nprevalent strategy for spatiotemporal prediction tasks. However, the property\nof CNNs to learn local spatial information decreases their efficiency in\ncapturing spatiotemporal dependencies, thereby limiting their prediction\naccuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which\nintegrates Swin Transformer blocks and the simplified LSTM, an extension that\nreplaces the convolutional structure in ConvLSTM with the self-attention\nmechanism. Furthermore, we construct a network with SwinLSTM cell as the core\nfor spatiotemporal prediction. Without using unique tricks, SwinLSTM\noutperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and\nKTH datasets. In particular, it exhibits a significant improvement in\nprediction accuracy compared to ConvLSTM. Our competitive experimental results\ndemonstrate that learning global spatial dependencies is more advantageous for\nmodels to capture spatiotemporal dependencies. We hope that SwinLSTM can serve\nas a solid baseline to promote the advancement of spatiotemporal prediction\naccuracy. The codes are publicly available at\nhttps://github.com/SongTang-x/SwinLSTM.'}, {'Intrinsic Andreev $π$-reflection and Josephson $π$-junction for\n  centrosymmetric spin-triplet superconductors': 'In this work, we systematically study two phases, called Andreev $\\pi$-phase\nand orbital-phase, and their influence on the Josephson effect. When the system\nis time-reversal invariant and centrosymmetric, these two phases only appear in\nthe odd-parity pairings. The Andreev $\\pi$-phase has nothing to do with the\nspecific form of the odd-parity pairings and means an intrinsic $\\pi$-phase\nbetween the spin-triplet Cooper pairs entering and leaving CTSCs in the Andreev\nreflections. The orbital-phase corresponds to the phase difference between the\nspin-triplet Cooper pairs with opposite spin polarization and depends on the\nspecific form of the odd-parity gap functions. When the normal region of the\nJosephson junction contacts the same side of the CTSCs with some specific\nodd-parity parings, the competition between the two phases can lead to the\nJosephson $\\pi$-junction. Note that this junction is different from that of the\nconventional Josephson junction (JJ) and is dubbed a U-shaped junction\naccording to its geometry. Meanwhile, in a conventional JJ, the interplay of\nthese two phases causes their impact on the CPR to be completely canceled out.\nTherefore no matter what kind of pairing symmetries the CTSC has, it will lead\nto Josephson 0-junction in this case. We obtain our results based on the model\nof the M$_{x}$Bi$_2$Se$_3$ family where M may be Cu, Sr, or Nb. Therefore, we\npropose to detect the pairing symmetry of M$_{x}$Bi$_2$Se$_3$ through a\nsuperconducting quantum interference device containing a U-shaped Josephson\njunction.'}, {'Hexagonal warping effect to Majorana zero modes at ends of\n  superconducting vortex line in doped 3D strong topological insulators': 'In a superconducting topological insulator, superconducting vortex line may\ntrap 1-dimensional topological band with Majorana zero modes (MZMs) localized\nat its ends. In this work, we study the effect of hexagonal warping to the\nvortex phase transition. We carry out both analytical calculations based on a\nsemiclassical formula and numerical calculations based on full quantum\nmechanics of Bogliubov de Gennes equation. We find that the hexagonal warping\nterm enlarges the topological region of the vortex line as the chemical\npotential changes, and leads to the MZMs even in the absence of topological\nsurface states.'}, {'UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking': ""Previous zero-shot dialogue state tracking (DST) methods only apply transfer\nlearning, ignoring unlabelled data in the target domain. We transform zero-shot\nDST into few-shot DST by utilising such unlabelled data via joint and\nself-training methods. Our method incorporates auxiliary tasks that generate\nslot types as inverse prompts for main tasks, creating slot values during joint\ntraining. Cycle consistency between these two tasks enables the generation and\nselection of quality samples in unknown target domains for subsequent\nfine-tuning. This approach also facilitates automatic label creation, thereby\noptimizing the training and fine-tuning of DST models. We demonstrate this\nmethod's effectiveness on general language models in zero-shot scenarios,\nimproving average joint goal accuracy by 8% across all domains in MultiWOZ.""}, {'Natural explanation for 21cm absorption signals via axion-induced\n  cooling': 'The EDGES Collaboration has reported an anomalously strong 21cm absorption\nfeature corresponding to the era of first star formation, which may indirectly\nbetray the influence of dark matter during this epoch. We demonstrate that, by\nvirtue of the ability to mediate cooling processes whilst in the condensed\nphase, a small amount of axion dark matter can explain these observations\nwithin the context of standard models of axions and axion-like-particles. The\nEDGES best-fit result favours an axion-like-particles mass in the (10, 450) meV\nrange, which can be compressed for the QCD axion to (100, 450) meV in the\nabsence of fine tuning. Future experiments and large scale surveys,\nparticularly the International Axion Observatory (IAXO) and EUCLID, should have\nthe capability to directly test this scenario.'}, {'Utilizing Computer Vision for Continuous Monitoring of Vaccine Side\n  Effects in Experimental Mice': 'The demand for improved efficiency and accuracy in vaccine safety assessments\nis increasing. Here, we explore the application of computer vision technologies\nto automate the monitoring of experimental mice for potential side effects\nafter vaccine administration. Traditional observation methods are\nlabor-intensive and lack the capability for continuous monitoring. By deploying\na computer vision system, our research aims to improve the efficiency and\naccuracy of vaccine safety assessments. The methodology involves training\nmachine learning models on annotated video data of mice behaviors pre- and\npost-vaccination. Preliminary results indicate that computer vision effectively\nidentify subtle changes, signaling possible side effects. Therefore, our\napproach has the potential to significantly enhance the monitoring process in\nvaccine trials in animals, providing a practical solution to the limitations of\nhuman observation.'}]","Title: Enhancing Conversational Recommendation Systems via External Knowledge Integration: A Multi-agent System Approach

Abstract:
In recent years, conversational recommendation systems (CRS) have gained prominence for their ability to tailor recommendations based on interactive dialogue. Despite the advent of advanced large language models (LLMs) such as ChatGPT, their efficacy in various CRS tasks has been predominantly assessed under limited circumstances. This study addresses the crucial issue arising from the scarcity of internal knowledge in domains such as Chinese movies, where LLMs exhibit notably inferior performance.

To tackle this multifaceted challenge, we introduce an innovative approach, the Multi-Agent Task-Delegation Conversational Recommendation System (ChatCRS), which integrates external knowledge through a unified framework. This framework encompasses a goal planning agent and a knowledge retrieval agent that collaborates with the LLM agent. ChatCRS employs a novel knowledge retrieval method that leverages predefined entity-relation-entity triples, thereby enabling efficient and targeted information acquisition, both critical components for effective CRS.

During the response generation, ChatCRS leverages ICL prompts based on unique examples. For the recommendation task, it incorporates N-shot In-context Learning to understand and fulfill user preferences effectively. Consequently, ChatCRS surpasses existing methods, achieving state-of-the-art results in both generating fluent, coherent, informative, and proactive responses and delivering accurate item recommendations.

By integrating external knowledge, ChatCRS significantly enhances LLM capabilities, curving the desperately needed advancements for ubiquitous, effective CRS. The findings underscore the paramount potential for external inputs in augmenting LLM-based CRS models, holding promise across various domains. This work paves the way for the development of more sophisticated and versatile conversational systems, profoundly impacting personalization, user engagement, and decision-making in online interactive platforms.

In summary, the research advances the state-of-the-art understanding of conversational recommendation systems by providing a novel blueprint for external knowledge integration in LLM-based systems, offering a significant contribution to the fields of natural language processing and information retrieval. The practical applications are broad, with potential impacts on streaming platforms, e-commerce, and personalized content delivery systems."
"We study the asymptotics of a two-dimensional stochastic differential system
with a degenerate diffusion matrix. This system describes the dynamics of a
population where individuals contribute to the degradation of their environment
through two different behaviors. We exploit the almost one-dimensional form of
the dynamical system to compute explicitly the Freidlin-Wentzell action
functional. That allows to give conditions under which the small noise regime
of the invariant measureis concentrated around the equilibrium of the dynamical
system having the smallest diffusion coefficient.","[{'On the Complexity of some Geometrical Objects': 'We recall the definition of the $\\epsilon$-distortion complexity of a set\ndefined in \\cite{bcc} and the results obtained in this paper for Cantor sets of\nthe interval defined by iterated function systems. We state an analogous\ndefinition for measures which may be more useful when dealing with dynamical\nsystems. We prove a new lower bound in the case of Cantor sets of the interval\ndefined by analytic iterated function systems. We also give an upper bound the\n$\\epsilon$-distortion complexity of invariant sets of uniformly hyperbolic\ndynamical systems.'}, {'Chains of infinite order, chains with memory of variable length, and\n  maps of the interval': 'We show how to construct a topological Markov map of the interval whose\ninvariant probability measure is the stationary law of a given stochastic chain\nof infinite order. In particular we caracterize the maps corresponding to\nstochastic chains with memory of variable length. The problem treated here is\nthe converse of the classical construction of the Gibbs formalism for Markov\nexpanding maps of the interval.'}, {'Loss of memory of hidden Markov models and Lyapunov exponents': 'In this paper we prove that the asymptotic rate of exponential loss of memory\nof a finite state hidden Markov model is bounded above by the difference of the\nfirst two Lyapunov exponents of a certain product of matrices. We also show\nthat this bound is in fact realized, namely for almost all realizations of the\nobserved process we can find symbols where the asymptotic exponential rate of\nloss of memory attains the difference of the first two Lyapunov exponents.\nThese results are derived in particular for the observed process and for the\nfilter; that is, for the distribution of the hidden state conditioned on the\nobserved sequence. We also prove similar results in total variation.'}, {'Wave localization does not affect the breakdown of a Schrödinger-type\n  amplifier driven by the square of a Gaussian field': 'We study the divergence of the solution to a Schr\\""odinger-type amplifier\ndriven by the square of a Gaussian noise in presence of a random potential. We\nfollow the same approach as Mounaix, Collet, and Lebowitz (MCL) in terms of a\ndistributional formulation of the amplified field and the use of the\nPaley-Wiener theorem [Commun. Math. Phys. {\\bf 264}, 741-758 (2006) and {\\bf\n280}, 281-283 (2008)]. Our results show that the divergence is not affected by\nthe random potential, in the sense that it occurs at exactly the same coupling\nconstant as what was found by MCL without a potential. It follows {\\it a\nfortiori} that the breakdown of the amplifier is not affected by the possible\nexistence of a localized regime in the amplification free limit.'}, {'Anomaly detection and motif discovery in symbolic representations of\n  time series': 'The advent of the Big Data hype and the consistent recollection of event logs\nand real-time data from sensors, monitoring software and machine configuration\nhas generated a huge amount of time-varying data in about every sector of the\nindustry. Rule-based processing of such data has ceased to be relevant in many\nscenarios where anomaly detection and pattern mining have to be entirely\naccomplished by the machine. Since the early 2000s, the de-facto standard for\nrepresenting time series has been the Symbolic Aggregate approXimation (SAX).In\nthis document, we present a few algorithms using this representation for\nanomaly detection and motif discovery, also known as pattern mining, in such\ndata. We propose a benchmark of anomaly detection algorithms using data from\nCloud monitoring software.'}, {'Complexity for extended dynamical systems': 'We consider dynamical systems for which the spatial extension plays an\nimportant role. For these systems, the notions of attractor, epsilon-entropy\nand topological entropy per unit time and volume have been introduced\npreviously. In this paper we use the notion of Kolmogorov complexity to\nintroduce, for extended dynamical systems, a notion of complexity per unit time\nand volume which plays the same role as the metric entropy for classical\ndynamical systems. We introduce this notion as an almost sure limit on orbits\nof the system. Moreover we prove a kind of variational principle for this\ncomplexity.'}, {'The Number of Large Graphs with a Positive Density of Triangles': 'We give upper and lower bounds on the number of graphs of fixed degree which\nhave a positive density of triangles. In particular, we show that there are\nvery few such graphs, when compared to the number of graphs without this\nrestriction. We also show that in this case the triangles seem to cluster even\nat low density.'}, {'Dynamics of Triangulations': 'We study a few problems related to Markov processes of flipping\ntriangulations of the sphere. We show that these processes are ergodic and\nmixing, but find a natural example which does not satisfy detailed balance. In\nthis example, the expected distribution of the degrees of the nodes seems to\nfollow the power law $d^{-4}$.'}, {'Liapunov Multipliers and Decay of Correlations in Dynamical Systems': 'The essential decorrelation rate of a hyperbolic dynamical system is the\ndecay rate of time-correlations one expects to see stably for typical\nobservables once resonances are projected out. We define and illustrate these\nnotions and study the conjecture that for observables in $C^1$, the essential\ndecorrelation rate is never faster than what is dictated by the {\\em smallest}\nunstable Liapunov multiplier.'}, {'A Model of Heat Conduction': ""We define a deterministic ``scattering'' model for heat conduction which is\ncontinuous in space, and which has a Boltzmann type flavor, obtained by a\nclosure based on memory loss between collisions. We prove that this model has,\nfor stochastic driving forces at the boundary, close to Maxwellians, a unique\nnon-equilibrium steady state.""}]","Title: Analysis of a Climate-Driven Pro-Environmental Behavior Mathematical Model and Stochastic Dynamics

Abstract:

This paper delves into a sophisticated mathematical model that quantifies counteractions between individuals’ inconsistent environmental behaviors and social interactions. The core objective is to analyze the population-wide dynamics of behaviors influenced by perceived environmental states, exploring effects caused by global climate change. Innovations include adapting traditional models to address pro-environmental behavior dynamics under climate challenges, emphasizing interactions' counteracting influence on erratic behavior. 

Methods involve a two-dimensional mathematical model that analyzes the coupled dynamics between the environment's status over time and the distribution of population behavior between two distinct actions. A stochastically described process is instrumented, leveraging a pure jump Markov process to model behavior changes affected by environmental perception, complemented by a stochastic differential system approximating large populations dynamics. This study uncovers the connection between environmental states, behavioral dynamics, and social forces in mitigating climate-induced inconsistencies.

The key results demonstrate that social interactions and collective behavior significantly counteract the tendency towards erratic environmental behaviors, facilitating more stable pro-environmental outcomes. Moreover, the model elucidates the long-term dynamics of population behavior transitions between different levels of environmental disturbance, providing insights into how social feedback mechanisms can stabilize individual actions towards more sustainable practices.

Contributions include a novel mathematical framework that elucidates the co-evolution of environmental perception with behavior and social dynamics, offering a valuable resource for designing effective interventions in pro-environmental behavior. This research advocates for the incorporation of dynamic social interactions as critical factors in the design of climate adaptation and mitigation policies.

The findings underscore the potential for social mechanisms to stabilize and promote consistently pro-environmental actions, highlighting the significance of understanding and leveraging these dynamics in the context of global climate change. Implications for policy design, public engagement strategies, and the implementation of adaptive technological solutions in environmental management are profound. The study provides a foundational understanding essential for the development of effective coordination mechanisms across diverse social, economic, and ecological scales to mitigate climate change's impacts."
"The multi-grid reaction-diffusion master equation (mgRDME) provides a
generalization of stochastic compartment-based reaction-diffusion modelling
described by the standard reaction-diffusion master equation (RDME). By
enabling different resolutions on lattices for biochemical species with
different diffusion constants, the mgRDME approach improves both accuracy and
efficiency of compartment-based reaction-diffusion simulations. The mgRDME
framework is examined through its application to morphogen gradient formation
in stochastic reaction-diffusion scenarios, using both an analytically
tractable first-order reaction network and a model with a second-order
reaction. The results obtained by the mgRDME modelling are compared with the
standard RDME model and with the (more detailed) particle-based Brownian
dynamics simulations. The dependence of error and numerical cost on the
compartment sizes is defined and investigated through a multi-objective
optimization problem.","[{'Coupling all-atom molecular dynamics simulations of ions in water with\n  Brownian dynamics': 'Molecular dynamics (MD) simulations of ions (K$^+$, Na$^+$, Ca$^{2+}$ and\nCl$^-$) in aqueous solutions are investigated. Water is described using the\nSPC/E model. A stochastic coarse-grained description for ion behaviour is\npresented and parameterized using MD simulations. It is given as a system of\ncoupled stochastic and ordinary differential equations, describing the ion\nposition, velocity and acceleration. The stochastic coarse-grained model\nprovides an intermediate description between all-atom MD simulations and\nBrownian dynamics (BD) models. It is used to develop a multiscale method which\nuses all-atom MD simulations in parts of the computational domain and (less\ndetailed) BD simulations in the remainder of the domain.'}, {'From Molecular Dynamics to Brownian Dynamics': 'Three coarse-grained molecular dynamics (MD) models are investigated with the\naim of developing and analyzing multiscale methods which use MD simulations in\nparts of the computational domain and (less detailed) Brownian dynamics (BD)\nsimulations in the remainder of the domain. The first MD model is formulated in\none spatial dimension. It is based on elastic collisions of heavy molecules\n(e.g. proteins) with light point particles (e.g. water molecules). Two\nthree-dimensional MD models are then investigated. The obtained results are\napplied to a simplified model of protein binding to receptors on the cellular\nmembrane. It is shown that modern BD simulators of intracellular processes can\nbe used in the bulk and accurately coupled with a (more detailed) MD model of\nprotein binding which is used close to the membrane.'}, {'Coarse-graining molecular dynamics: stochastic models with non-Gaussian\n  force distributions': 'Incorporating atomistic and molecular information into models of cellular\nbehaviour is challenging because of a vast separation of spatial and temporal\nscales between processes happening at the atomic and cellular levels.\nMultiscale or multi-resolution methodologies address this difficulty by using\nmolecular dynamics (MD) and coarse-grained models in different parts of the\ncell. Their applicability depends on the accuracy and properties of the\ncoarse-grained model which approximates the detailed MD description. A family\nof stochastic coarse-grained (SCG) models, written as relatively\nlow-dimensional systems of nonlinear stochastic differential equations, is\npresented. The nonlinear SCG model incorporates the non-Gaussian force\ndistribution which is observed in MD simulations and which cannot be described\nby linear models. It is shown that the nonlinearities can be chosen in such a\nway that they do not complicate parametrization of the SCG description by\ndetailed MD simulations. The solution of the SCG model is found in terms of\ngamma functions.'}, {'Limiting stochastic processes of shift-periodic dynamical systems': ""A shift-periodic map is a one-dimensional map from the real line to itself\nwhich is periodic up to a linear translation and allowed to have singularities.\nIt is shown that iterative sequences $x_{n+1}=F(x_n)$ generated by such maps\ndisplay rich dynamical behaviour. The integer parts $\\lfloor x_n \\rfloor$ give\na discrete-time random walk for a suitable initial distribution of $x_0$ and\nconverge in certain limits to Brownian motion or more general L\\'evy processes.\nFurthermore, for certain shift-periodic maps with small holes on $[0,1]$,\nconvergence of trajectories to a continuous-time random walk is shown in a\nlimit.""}, {'On the counting function of semiprimes': 'A semiprime is a natural number which can be written as the product of two\nprimes. The asymptotic behaviour of the function $\\pi_2(x)$, the number of\nsemiprimes less than or equal to $x$, is studied. Using a combinatorial\nargument, asymptotic series of $\\pi_2(x)$ is determined, with all the terms\nexplicitly given. An algorithm for the calculation of the constants involved in\nthe asymptotic series is presented and the constants are computed to 20\nsignificant digits. The errors of the partial sums of the asymptotic series are\ninvestigated. A generalization of this approach to products of $k$ primes, for\n$k\\geq 3$, is also proposed.'}, {'Symmetries of many-body systems imply distance-dependent potentials': 'Considering interatomic potential $U({\\mathbf q})$ where ${\\mathbf q} =\n[{\\mathbf q}_1, {\\mathbf q}_2, \\dots, {\\mathbf q}_N] \\in {\\mathbb R}^{3N}$ is a\nvector describing positions, $\\mathbf{q}_i \\in {\\mathbb R}^3$, it is shown that\n$U$ can be defined as a function of the interatomic distance variables $r_{ij}\n= |{\\mathbf q}_i - {\\mathbf q}_j |$, provided that the potential $U$ satisfies\nsome symmetry assumptions. Moreover, the potential $U$ can be defined as a\nfunction of a proper subset of the distance variables $r_{ij}$, provided that\n$N > 5$, with the number of distance variables used scaling linearly with the\nnumber of atoms, $N$.'}, {'On chemisorption of polymers to solid surfaces': 'The irreversible adsorption of polymers to a two-dimensional solid surface is\nstudied. An operator formalism is introduced for chemisorption from a\npolydisperse solution of polymers which transforms the analysis of the\nadsorption process to a set of combinatorial problems on a two-dimensional\nlattice. The time evolution of the number of polymers attached and the surface\narea covered are calculated via a series expansion. The dependence of the final\ncoverage on the parameters of the model (i.e. the parameters of the\ndistribution of polymer lengths in the solution) is studied. Various methods\nfor accelerating the convergence of the resulting infinite series are\nconsidered. To demonstrate the accuracy of the truncated series approach, the\nseries expansion results are compared with the results of stochastic\nsimulation.'}, {'Multi-resolution polymer Brownian dynamics with hydrodynamic\n  interactions': 'A polymer model given in terms of beads, interacting through Hookean springs\nand hydrodynamic forces, is studied. Brownian dynamics description of this\nbead-spring polymer model is extended to multiple resolutions. Using this\nmultiscale approach, a modeller can efficiently look at different regions of\nthe polymer in different spatial and temporal resolutions with scalings given\nfor the number of beads, statistical segment length and bead radius in order to\nmaintain macro-scale properties of the polymer filament. The Boltzmann\ndistribution of a Gaussian chain for differing statistical segment lengths\ngives a Langevin equation for the multi-resolution model with a mobility tensor\nfor different bead sizes. Using the pre-averaging approximation, the\ntranslational diffusion coefficient is obtained as a function of the inverse of\na matrix and then in closed form in the long-chain limit. This is then\nconfirmed with numerical experiments.'}, {'On standardised moments of force distribution in simple liquids': 'The force distribution of a tagged atom in a Lennard-Jones fluid in the\ncanonical ensemble is studied with a focus on its dependence on inherent\nphysical parameters: number density ($n$) and temperature ($T$). Utilising\nstructural information from molecular dynamics simulations of the Lennard-Jones\nfluid, explicit analytical expressions for the dependence of standardised force\nmoments on $n$ and $T$ are derived. Leading order behaviour of standardised\nmoments of the force distribution are obtained in the limiting cases of small\ndensity ($n \\rightarrow 0$) and low temperature ($T \\rightarrow 0$), while the\nvariations in the standardised moments are probed for general $n$ and $T$ using\nmolecular dynamics simulations. Clustering effects are seen in molecular\ndynamics simulations and their effect on these standardised moments is\ndiscussed.'}, {'Error Analysis of Diffusion Approximation Methods for Multiscale Systems\n  in Reaction Kinetics': 'Several different methods exist for efficient approximation of paths in\nmultiscale stochastic chemical systems. Another approach is to use bursts of\nstochastic simulation to estimate the parameters of a stochastic differential\nequation approximation of the paths. In this paper, multiscale methods for\napproximating paths are used to formulate different strategies for estimating\nthe dynamics by diffusion processes. We then analyse how efficient and accurate\nthese methods are in a range of different scenarios, and compare their\nrespective advantages and disadvantages to other methods proposed to analyse\nmultiscale chemical networks.'}]","### Abstract

#### Background and Context
The research landscape of spatially structured models in reaction-diffusion systems is a crucial domain for studying pattern formation in biological, chemical, and physical systems. The dynamics in these systems, particularly when incorporating stochastic processes and spatial correlations, pose significant challenges to traditional analytical methods. The standard reaction-diffusion master equation (RDME) has been widely used in this context, yet it simplifies the spatial dynamics by assuming grid-boundary symmetries that can introduce errors, especially when higher-order reactions are considered.

#### Objective
This study aims to develop and analyze a generalized reaction-diffusion master equation (mgRDME) that accommodates a coarser spatial discretization of one species while maintaining accuracy in multidimensional spaces. This approach seeks to reduce simulation complexity, computational costs, and lower the discrepancies associated with second-order reactions when species move non-uniformly in space.

#### Innovations
The key innovation revolves around a method introduced to adjust the compartment-based models, especially for species with significantly different diffusivity properties. By allowing for distinct grid sizes for different chemical species, this research addresses the limitations of the standard RDME and bridges its difficulty in accurately simulating complex systems with spatial heterogeneity.

#### Methods
The mgRDME is developed through a systematic decomposition of the standard RDME components for interactions of species with unequal diffusivities. The method employs a one-dimensional scaling to first separate diﬀusion and reaction processes, then dictates the refinement in the domain discretization for the faster moving species, while preserving accuracy for all species in the system.

#### Results
Simulation results on a morphogen gradient system with fast-slow components and dimerization kinetics indicate the mgRDME accurately captures the steady-state and time-evolving dynamics. The approach shows reduced computational complexity without significant loss of accuracy, particularly in systems where the diffusivity comparison is substantial.

#### Contributions
The research makes several significant contributions to the field. Firstly, it demonstrates a quasi-asymptotic approximation that outperforms the standard RDME for complex system simulations, by accounting for species-specific spatial discretization with a bounded false negative rate in reaction encounters. Secondly, it reveals an intriguing parameter regime enabling the mgRDME to parameterize first-order reactions when diffusives encounter are dependent on diffusivities and allows some second-order interactions to be resolved.

#### Applications
These findings have broad implications for modeling chemical reactions in complex environments such as cell-suspensions, chemical kinetics under spatially-dependent conditions, and the simulation of morphogen gradient formation in developing tissues. The method's potential for reducing computational resources while preserving biological detail makes it particularly valuable for detailed systems biology studies.

The proposed method enables researchers to achieve more accurate, computationally efficient modeling of spatially structured reactions, enhancing the scope for detailed exploration into pattern formation processes across various scientific disciplines."
"The rapid development of Large Language Models (LLMs) has led to a surge in
applications that facilitate collaboration among multiple agents, assisting
humans in their daily tasks. However, a significant gap remains in assessing to
what extent LLM-powered applications genuinely enhance user experience and task
execution efficiency. This highlights the need to verify utility of LLM-powered
applications, particularly by ensuring alignment between the application's
functionality and end-user needs. We introduce AgentEval, a novel framework
designed to simplify the utility verification process by automatically
proposing a set of criteria tailored to the unique purpose of any given
application. This allows for a comprehensive assessment, quantifying the
utility of an application against the suggested criteria. We present a
comprehensive analysis of the effectiveness and robustness of AgentEval for two
open source datasets including Math Problem solving and ALFWorld House-hold
related tasks. For reproducibility purposes, we make the data, code and all the
logs publicly available at https://bit.ly/3w3yKcS .","[{'Fréchet Distance for Offline Evaluation of Information Retrieval\n  Systems with Sparse Labels': ""The rapid advancement of natural language processing, information retrieval\n(IR), computer vision, and other technologies has presented significant\nchallenges in evaluating the performance of these systems. One of the main\nchallenges is the scarcity of human-labeled data, which hinders the fair and\naccurate assessment of these systems. In this work, we specifically focus on\nevaluating IR systems with sparse labels, borrowing from recent research on\nevaluating computer vision tasks. taking inspiration from the success of using\nFr\\'echet Inception Distance (FID) in assessing text-to-image generation\nsystems. We propose leveraging the Fr\\'echet Distance to measure the distance\nbetween the distributions of relevant judged items and retrieved results. Our\nexperimental results on MS MARCO V1 dataset and TREC Deep Learning Tracks query\nsets demonstrate the effectiveness of the Fr\\'echet Distance as a metric for\nevaluating IR systems, particularly in settings where a few labels are\navailable. This approach contributes to the advancement of evaluation\nmethodologies in real-world scenarios such as the assessment of generative IR\nsystems.""}, {'A Comparison of Methods for Evaluating Generative IR': ""Information retrieval systems increasingly incorporate generative components.\nFor example, in a retrieval augmented generation (RAG) system, a retrieval\ncomponent might provide a source of ground truth, while a generative component\nsummarizes and augments its responses. In other systems, a large language model\n(LLM) might directly generate responses without consulting a retrieval\ncomponent. While there are multiple definitions of generative information\nretrieval (Gen-IR) systems, in this paper we focus on those systems where the\nsystem's response is not drawn from a fixed collection of documents or\npassages. The response to a query may be entirely new text. Since traditional\nIR evaluation methods break down under this model, we explore various methods\nthat extend traditional offline evaluation approaches to the Gen-IR context.\nOffline IR evaluation traditionally employs paid human assessors, but\nincreasingly LLMs are replacing human assessment, demonstrating capabilities\nsimilar or superior to crowdsourced labels. Given that Gen-IR systems do not\ngenerate responses from a fixed set, we assume that methods for Gen-IR\nevaluation must largely depend on LLM-generated labels. Along with methods\nbased on binary and graded relevance, we explore methods based on explicit\nsubtopics, pairwise preferences, and embeddings. We first validate these\nmethods against human assessments on several TREC Deep Learning Track tasks; we\nthen apply these methods to evaluate the output of several purely generative\nsystems. For each method we consider both its ability to act autonomously,\nwithout the need for human labels or other input, and its ability to support\nhuman auditing. To trust these methods, we must be assured that their results\nalign with human assessments. In order to do so, evaluation criteria must be\ntransparent, so that outcomes can be audited by human assessors.""}, {'Retrieving Supporting Evidence for LLMs Generated Answers': 'Current large language models (LLMs) can exhibit near-human levels of\nperformance on many natural language tasks, including open-domain question\nanswering. Unfortunately, they also convincingly hallucinate incorrect answers,\nso that responses to questions must be verified against external sources before\nthey can be accepted at face value. In this paper, we report a simple\nexperiment to automatically verify generated answers against a corpus. After\npresenting a question to an LLM and receiving a generated answer, we query the\ncorpus with the combination of the question + generated answer. We then present\nthe LLM with the combination of the question + generated answer + retrieved\nanswer, prompting it to indicate if the generated answer can be supported by\nthe retrieved answer. We base our experiment on questions and passages from the\nMS MARCO (V1) test collection, exploring three retrieval approaches ranging\nfrom standard BM25 to a full question answering stack, including a reader based\non the LLM. For a large fraction of questions, we find that an LLM is capable\nof verifying its generated answer if appropriate supporting material is\nprovided. However, with an accuracy of 70-80%, this approach cannot be fully\nrelied upon to detect hallucinations.'}, {'Predicting Efficiency/Effectiveness Trade-offs for Dense vs. Sparse\n  Retrieval Strategy Selection': 'Over the last few years, contextualized pre-trained transformer models such\nas BERT have provided substantial improvements on information retrieval tasks.\nRecent approaches based on pre-trained transformer models such as BERT,\nfine-tune dense low-dimensional contextualized representations of queries and\ndocuments in embedding space. While these dense retrievers enjoy substantial\nretrieval effectiveness improvements compared to sparse retrievers, they are\ncomputationally intensive, requiring substantial GPU resources, and dense\nretrievers are known to be more expensive from both time and resource\nperspectives. In addition, sparse retrievers have been shown to retrieve\ncomplementary information with respect to dense retrievers, leading to\nproposals for hybrid retrievers. These hybrid retrievers leverage low-cost,\nexact-matching based sparse retrievers along with dense retrievers to bridge\nthe semantic gaps between query and documents. In this work, we address this\ntrade-off between the cost and utility of sparse vs dense retrievers by\nproposing a classifier to select a suitable retrieval strategy (i.e., sparse\nvs. dense vs. hybrid) for individual queries. Leveraging sparse retrievers for\nqueries which can be answered with sparse retrievers decreases the number of\ncalls to GPUs. Consequently, while utility is maintained, query latency\ndecreases. Although we use less computational resources and spend less time, we\nstill achieve improved performance. Our classifier can select between sparse\nand dense retrieval strategies based on the query alone. We conduct experiments\non the MS MARCO passage dataset demonstrating an improved range of\nefficiency/effectiveness trade-offs between purely sparse, purely dense or\nhybrid retrieval strategies, allowing an appropriate strategy to be selected\nbased on a target latency and resource budget.'}, {'Unsupervised Question Clarity Prediction Through Retrieved Item\n  Coherency': 'Despite recent progress on conversational systems, they still do not perform\nsmoothly and coherently when faced with ambiguous requests. When questions are\nunclear, conversational systems should have the ability to ask clarifying\nquestions, rather than assuming a particular interpretation or simply\nresponding that they do not understand. Previous studies have shown that users\nare more satisfied when asked a clarifying question, rather than receiving an\nunrelated response. While the research community has paid substantial attention\nto the problem of predicting query ambiguity in traditional search contexts,\nresearchers have paid relatively little attention to predicting when this\nambiguity is sufficient to warrant clarification in the context of\nconversational systems. In this paper, we propose an unsupervised method for\npredicting the need for clarification. This method is based on the measured\ncoherency of results from an initial answer retrieval step, under the\nassumption that a less ambiguous query is more likely to retrieve more coherent\nresults when compared to an ambiguous query. We build a graph from retrieved\nitems based on their context similarity, treating measures of graph\nconnectivity as indicators of ambiguity. We evaluate our approach on two\nrecently released open-domain conversational question answering datasets,\nClariQ and AmbigNQ, comparing it with neural and non-neural baselines. Our\nunsupervised approach performs as well as supervised approaches while providing\nbetter generalization.'}, {'Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers': 'Large language models can now directly generate answers to many factual\nquestions without referencing external sources. Unfortunately, relatively\nlittle attention has been paid to methods for evaluating the quality and\ncorrectness of these answers, for comparing the performance of one model to\nanother, or for comparing one prompt to another. In addition, the quality of\ngenerated answers are rarely directly compared to the quality of retrieved\nanswers. As models evolve and prompts are modified, we have no systematic way\nto measure improvements without resorting to expensive human judgments. To\naddress this problem we adapt standard retrieval benchmarks to evaluate answers\ngenerated by large language models. Inspired by the BERTScore metric for\nsummarization, we explore two approaches. In the first, we base our evaluation\non the benchmark relevance judgments. We empirically run experiments on how\ninformation retrieval relevance judgments can be utilized as an anchor to\nevaluating the generated answers. In the second, we compare generated answers\nto the top results retrieved by a diverse set of retrieval models, ranging from\ntraditional approaches to advanced methods, allowing us to measure improvements\nwithout human judgments. In both cases, we measure the similarity between an\nembedded representation of the generated answer and an embedded representation\nof a known, or assumed, relevant passage from the retrieval benchmark.'}, {'Generative Information Retrieval Evaluation': 'This paper is a draft of a chapter intended to appear in a forthcoming book\non generative information retrieval, co-edited by Chirag Shah and Ryen White.\nIn this chapter, we consider generative information retrieval evaluation from\ntwo distinct but interrelated perspectives. First, large language models (LLMs)\nthemselves are rapidly becoming tools for evaluation, with current research\nindicating that LLMs may be superior to crowdsource workers and other paid\nassessors on basic relevance judgement tasks. We review past and ongoing\nrelated research, including speculation on the future of shared task\ninitiatives, such as TREC, and a discussion on the continuing need for human\nassessments. Second, we consider the evaluation of emerging LLM-based\ngenerative information retrieval (GenIR) systems, including retrieval augmented\ngeneration (RAG) systems. We consider approaches that focus both on the\nend-to-end evaluation of GenIR systems and on the evaluation of a retrieval\ncomponent as an element in a RAG system. Going forward, we expect the\nevaluation of GenIR systems to be at least partially based on LLM-based\nassessment, creating an apparent circularity, with a system seemingly\nevaluating its own output. We resolve this apparent circularity in two ways: 1)\nby viewing LLM-based assessment as a form of ""slow search"", where a slower IR\nsystem is used for evaluation and training of a faster production IR system;\nand 2) by recognizing a continuing need to ground evaluation in human\nassessment, even if the characteristics of that human assessment must change.'}, {'Early Stage Sparse Retrieval with Entity Linking': 'Despite the advantages of their low-resource settings, traditional sparse\nretrievers depend on exact matching approaches between high-dimensional\nbag-of-words (BoW) representations of both the queries and the collection. As a\nresult, retrieval performance is restricted by semantic discrepancies and\nvocabulary gaps. On the other hand, transformer-based dense retrievers\nintroduce significant improvements in information retrieval tasks by exploiting\nlow-dimensional contextualized representations of the corpus. While dense\nretrievers are known for their relative effectiveness, they suffer from lower\nefficiency and lack of generalization issues, when compared to sparse\nretrievers. For a lightweight retrieval task, high computational resources and\ntime consumption are major barriers encouraging the renunciation of dense\nmodels despite potential gains. In this work, we propose boosting the\nperformance of sparse retrievers by expanding both the queries and the\ndocuments with linked entities in two formats for the entity names: 1) explicit\nand 2) hashed. We employ a zero-shot end-to-end dense entity linking system for\nentity recognition and disambiguation to augment the corpus. By leveraging the\nadvanced entity linking methods, we believe that the effectiveness gap between\nsparse and dense retrievers can be narrowed. We conduct our experiments on the\nMS MARCO passage dataset. Since we are concerned with the early stage retrieval\nin cascaded ranking architectures of large information retrieval systems, we\nevaluate our results using recall@1000. Our approach is also capable of\nretrieving documents for query subsets judged to be particularly difficult in\nprior work. We further demonstrate that the non-expanded and the expanded runs\nwith both explicit and hashed entities retrieve complementary results.\nConsequently, we adopt a run fusion approach to maximize the benefits of entity\nlinking.'}, {'Query Performance Prediction: From Ad-hoc to Conversational Search': 'Query performance prediction (QPP) is a core task in information retrieval.\nThe QPP task is to predict the retrieval quality of a search system for a query\nwithout relevance judgments. Research has shown the effectiveness and\nusefulness of QPP for ad-hoc search. Recent years have witnessed considerable\nprogress in conversational search (CS). Effective QPP could help a CS system to\ndecide an appropriate action to be taken at the next turn. Despite its\npotential, QPP for CS has been little studied. We address this research gap by\nreproducing and studying the effectiveness of existing QPP methods in the\ncontext of CS. While the task of passage retrieval remains the same in the two\nsettings, a user query in CS depends on the conversational history, introducing\nnovel QPP challenges. In particular, we seek to explore to what extent findings\nfrom QPP methods for ad-hoc search generalize to three CS settings: (i)\nestimating the retrieval quality of different query rewriting-based retrieval\nmethods, (ii) estimating the retrieval quality of a conversational dense\nretrieval method, and (iii) estimating the retrieval quality for top ranks vs.\ndeeper-ranked lists. Our findings can be summarized as follows: (i) supervised\nQPP methods distinctly outperform unsupervised counterparts only when a\nlarge-scale training set is available; (ii) point-wise supervised QPP methods\noutperform their list-wise counterparts in most cases; and (iii) retrieval\nscore-based unsupervised QPP methods show high effectiveness in assessing the\nconversational dense retrieval method, ConvDR.'}, {'Retrieving Supporting Evidence for Generative Question Answering': 'Current large language models (LLMs) can exhibit near-human levels of\nperformance on many natural language-based tasks, including open-domain\nquestion answering. Unfortunately, at this time, they also convincingly\nhallucinate incorrect answers, so that responses to questions must be verified\nagainst external sources before they can be accepted at face value. In this\npaper, we report two simple experiments to automatically validate generated\nanswers against a corpus. We base our experiments on questions and passages\nfrom the MS MARCO (V1) test collection, and a retrieval pipeline consisting of\nsparse retrieval, dense retrieval and neural rerankers. In the first\nexperiment, we validate the generated answer in its entirety. After presenting\na question to an LLM and receiving a generated answer, we query the corpus with\nthe combination of the question + generated answer. We then present the LLM\nwith the combination of the question + generated answer + retrieved answer,\nprompting it to indicate if the generated answer can be supported by the\nretrieved answer. In the second experiment, we consider the generated answer at\na more granular level, prompting the LLM to extract a list of factual\nstatements from the answer and verifying each statement separately. We query\nthe corpus with each factual statement and then present the LLM with the\nstatement and the corresponding retrieved evidence. The LLM is prompted to\nindicate if the statement can be supported and make necessary edits using the\nretrieved material. With an accuracy of over 80%, we find that an LLM is\ncapable of verifying its generated answer when a corpus of supporting material\nis provided. However, manual assessment of a random sample of questions reveals\nthat incorrect generated answers are missed by this verification process. While\nthis verification process can reduce hallucinations, it can not entirely\neliminate them.'}]","Title: AgentEval: Leveraging LLM-powered Agents to Assess Utility in Various Applications

Abstract:
In the rapidly evolving landscape of large language models (LLMs), the necessity for scalable and cost-effective methods to evaluate their performance and utility becomes paramount. In addressing this pressing demand, this paper introduces AgentEval, a novel framework that employs LLM-powered agents to assess the utility of applications across different domains. The main goal of AgentEval is to provide insights into how current flows can be characterized for the development of arbitrary applications, ensuring they respond effectively to user needs. This research is pivotal in developing tailored solutions for diverse tasks, including but not limited to code generation, healthcare, and everyday utilities. By combining the capabilities of CriticAgent, QuantifierAgent, and VerifierAgent, AgentEval provides a comprehensive, autonomous, and reliable evaluation methodology that complements traditional human evaluations. The paper showcases the application of AgentEval on a dataset of 12,500 high school mathematics problems from the MATH dataset and a series of ALFWorld tasks, illustrating the formulation of 5 diverse criteria and its utility for evaluating solution utilities across various models, such as AutoGen, Langchain ReAct, and VanillaSolver. The contributions of this research extend beyond offering a new evaluation methodology for LLM-powered applications. It introduces potential for replicable insights and improvements, aimed at enhancing the performance of LLMs across a wide range of applications. The findings of this study provide valuable guidance for application developers and researchers on how to harness the full potential of LLMs to align their functionalities with the needs of the end-users effectively.

Keywords: Large Language Models, Evaluation, AgentEval, LLM-powered applications, User Utility Assessment"
"Predictive Coding (PC) is a theoretical framework in cognitive science
suggesting that the human brain processes cognition through spatiotemporal
prediction of the visual world. Existing studies have developed spatiotemporal
prediction neural networks based on the PC theory, emulating its two core
mechanisms: Correcting predictions from residuals and hierarchical learning.
However, these models do not show the enhancement of prediction skills on
real-world forecasting tasks and ignore the Precision Weighting mechanism of PC
theory. The precision weighting mechanism posits that the brain allocates more
attention to signals with lower precision, contributing to the cognitive
ability of human brains. This work introduces the Cognitive Diffusion
Probabilistic Models (CogDPM), which demonstrate the connection between
diffusion probabilistic models and PC theory. CogDPM features a precision
estimation method based on the hierarchical sampling capabilities of diffusion
models and weight the guidance with precision weights estimated by the inherent
property of diffusion models. We experimentally show that the precision weights
effectively estimate the data predictability. We apply CogDPM to real-world
prediction tasks using the United Kindom precipitation and ERA surface wind
datasets. Our results demonstrate that CogDPM outperforms both existing
domain-specific operational models and general deep prediction models by
providing more proficient forecasting.","[{'Thermal analysis of dual-phase-lag model in a two-dimensional plate\n  subjected to a heat source moving along elliptical trajectories': ""In this paper, we focus on the study of heat transfer behavior for the\ndual-phase-lag heat conduction model, which describes the evolution of\ntemperature in a two-dimensional rectangular plate caused by the activity of a\npoint heat source moving along elliptical trajectories. At first, Green's\nfunction approach is applied to derive the analytical solution of temperature\nfor the given model. Based on the series representation of this analytical\nsolution, the thermal responses for the underlying heat transfer problem,\nincluding the relations between the moving heat source and the concomitant\ntemperature peak, the influences of the pair of phase lags and the angular\nvelocity of heat source on temperature, are then investigated, analyzed and\ndiscussed in detail for three different movement trajectories. Compared with\nthe results revealed for the common situation that the heat source moves in a\nstraight line with a constant speed, the present results show quite distinctive\nthermal behaviors for all cases, which subsequently can help us to better\nunderstand the internal mechanism of the dual-phase-lag heat transfer subjected\nto a moving heat source with curved trajectory.""}, {'Dual-phase-lag heat conduction analysis of a three-dimensional finite\n  medium heated by a moving laser beam with circular or annular cross-section': ""We analyze the non-Fourier dual-phase-lag heat conduction process in a\nthree-dimensional medium heated by a moving circular or annular laser beam,\nwhich is modeled by a set of point heat sources in the cross-section. In order\nto solve the model, Green's function approach is first used to obtain an\nanalytical solution for the temperature distribution over the medium subjected\nto a single point heat source. Then the temperature distribution on the medium\nsubjected to the laser beam can be obtained by the superposition method.\nAccording to this solution, the dependence between the heat conduction process\nand the cross-section of the heat source is investigated. Based on the\ncomparison of the temperature distribution of the medium under Fourier's law\nand non-Fourier's law, the effect of the phase lag parameter is revealed. In\naddition, the effects of laser spot size and laser moving speed on the\ntemperature distribution are also analyzed. The discovered properties provide\ntheoretical support for the application of moving laser heat sources in various\nfields under the dual-phase-lag model.""}, {'Spectroscopic Confirmation of Two X-ray Diffuse and Massive Galaxy\n  Clusters at Low Redshift': 'We present MMT spectroscopic observations of two massive galaxy cluster\ncandidates at redshift $z\\sim0.07$ that show extended and diffuse X-ray\nemission in the ROSAT All Sky Survey (RASS) images. The targets were selected\nfrom a previous catalog of 303 newly-identified cluster candidates with the\nsimilar properties using the intra-cluster medium emission. Using the new MMT\nHectospec data and SDSS archival spectra, we identify a number of member\ngalaxies for the two targets and confirm that they are galaxy clusters at\n$z=0.079$ and 0.067, respectively. The size of the two clusters, calculated\nfrom the distribution of the member galaxies, is roughly 2 Mpc in radius. We\nestimate cluster masses using three methods based on their galaxy number\noverdensities, galaxy velocity dispersions, and X-ray emission. The\noverdensity-based masses are $(6\\sim8) \\rm \\times10^{14}\\ M_\\odot$, comparable\nto the masses of large clusters at low redshift. The masses derived from\nvelocity dispersions are significantly lower, likely due to their diffuse and\nlow concentration features. Our result suggests the existence of a population\nof large clusters with very diffuse X-ray emission that have been missed by\nmost previous searches using the RASS images. If most of the 303 candidates in\nthe previous catalog are confirmed to be real clusters, this may help to reduce\nthe discrepancy of cosmological results between the CMB and galaxy cluster\nmeasurements.'}, {'FogROS G: Enabling Secure, Connected and Mobile Fog Robotics with Global\n  Addressability': 'Fog Robotics renders networked robots with greater mobility, on-demand\ncompute capabilities and better energy efficiency by offloading heavy robotics\nworkloads to nearby Edge and distant Cloud data centers. However, as the\nde-facto standard for implementing fog robotics applications, Robot Operating\nSystem (ROS) and its successor ROS2 fail to provide fog robots with a\nmobile-friendly and secure communication infrastructure.\n  In this work, we present FogROS G, a secure routing framework that connects\nrobotics software components from different physical locations, networks, Data\nDistribution Service (DDS) and ROS distributions. FogROS G indexes networked\nrobots with globally unique 256-bit names that remains constant even if the\nrobot roams between multiple administrative network domains. FogROS G leverages\nGlobal Data Plane, a global and secure peer-to-peer routing infrastructure\nbetween the names, guaranteeing that only authenticated party can send to or\nreceive from the robot. FogROS G adopts a proxy-based design that connect nodes\nfrom ROS1 and ROS2 with mainstream DDS vendors; this can be done without any\nchanges to the application code.'}, {'FogROS2-Config: Optimizing Latency and Cost for Multi-Cloud Robot\n  Applications': 'Cloud service providers provide over 50,000 distinct and dynamically changing\nset of cloud server options. To help roboticists make cost-effective decisions,\nwe present FogROS2-Config, an open toolkit that takes ROS2 nodes as input and\nautomatically runs relevant benchmarks to quickly return a menu of cloud\ncompute services that tradeoff latency and cost. Because it is infeasible to\ntry every hardware configuration, FogROS2-Config quickly samples tests a small\nset of edge case servers. We evaluate FogROS2-Config on three robotics\napplication tasks: visual SLAM, grasp planning. and motion planning.\nFogROS2-Config can reduce the cost by up to 20x. By comparing with a Pareto\nfrontier for cost and latency by running the application task on feasible\nserver configurations, we evaluate cost and latency models and confirm that\nFogROS2-Config selects efficient hardware configurations to balance cost and\nlatency.'}, {'FogROS2-SGC: A ROS2 Cloud Robotics Platform for Secure Global\n  Connectivity': 'The Robot Operating System (ROS2) is the most widely used software platform\nfor building robotics applications. FogROS2 extends ROS2 to allow robots to\naccess cloud computing on demand. However, ROS2 and FogROS2 assume that all\nrobots are locally connected and that each robot has full access and control of\nthe other robots. With applications like distributed multi-robot systems,\nremote robot control, and mobile robots, robotics increasingly involves the\nglobal Internet and complex trust management. Existing approaches for\nconnecting disjoint ROS2 networks lack key features such as security,\ncompatibility, efficiency, and ease of use. We introduce FogROS2-SGC, an\nextension of FogROS2 that can effectively connect robot systems across\ndifferent physical locations, networks, and Data Distribution Services (DDS).\nWith globally unique and location-independent identifiers, FogROS2-SGC securely\nand efficiently routes data between robotics components around the globe.\nFogROS2-SGC is agnostic to the ROS2 distribution and configuration, is\ncompatible with non-ROS2 software, and seamlessly extends existing ROS2\napplications without any code modification. Experiments suggest FogROS2-SGC is\n19x faster than rosbridge (a ROS2 package with comparable features, but lacking\nsecurity). We also apply FogROS2-SGC to 4 robots and compute nodes that are\n3600km apart. Videos and code are available on the project website\nhttps://sites.google.com/view/fogros2-sgc.'}, {'SCL: A Secure Concurrency Layer For Paranoid Stateful Lambdas': 'We propose a federated Function-as-a-Service (FaaS) execution model that\nprovides secure and stateful execution in both Cloud and Edge environments. The\nFaaS workers, called Paranoid Stateful Lambdas (PSLs), collaborate with one\nanother to perform large parallel computations. We exploit cryptographically\nhardened and mobile bundles of data, called DataCapsules, to provide persistent\nstate for our PSLs, whose execution is protected using hardware-secured TEEs.\nTo make PSLs easy to program and performant, we build the familiar Key-Value\nStore interface on top of DataCapsules in a way that allows amortization of\ncryptographic operations. We demonstrate PSLs functioning in an edge\nenvironment running on a group of Intel NUCs with SGXv2.\n  As described, our Secure Concurrency Layer (SCL), provides\neventually-consistent semantics over written values using untrusted and\nunordered multicast. All SCL communication is encrypted, unforgeable, and\nprivate. For durability, updates are recorded in replicated DataCapsules, which\nare append-only cryptographically-hardened blockchain with confidentiality,\nintegrity, and provenance guarantees. Values for inactive keys are stored in a\nlog-structured merge-tree (LSM) in the same DataCapsule. SCL features a variety\nof communication optimizations, such as an efficient message passing framework\nthat reduces the latency up to 44x from the Intel SGX SDK, and an actor-based\ncryptographic processing architecture that batches cryptographic operations and\nincreases throughput by 81x.'}, {'Phase evolution and thermal stability of high Curie temperature\n  BiScO$_3$-PbTiO$_3$-Pb(Cd${1/3}$Nb$_{2/3}$)O$_3$ ceramics near MPB': 'Piezoelectric and ferroelectric ceramics with a high Curie temperature (Tc)\nhave attracted a growing attention owning to their applications under severe\nenvironments. In this work, phase structure, dielectric, ferroelectric and\npiezoelectric properties of (0.975-x)BiScO3-xPbTiO3-0.025Pb(Cd1/3Nb2/3)O3\nceramics (x = 0.58-0.64) were studied. A composition-induced structural\ntransformation occurs from rhombohedral phase to tetragonal phase through an\nintermediate monoclinic phase with the increasing PT concentration. The\nrelationship between structure and electrical properties of the system were\ndiscussed. The BS-xPT-PCN system near the morphotropic phase boundary (x =\n0.62) exhibits excellent piezoelectric and ferroelectric performances with d33\n= 508 pC/N, kp = 56%, and Pr = 40 uC/cm2. The high-temperature piezoelectricity\nof the sample with MPB (x = 0.62) was characterized by an in situ XRD. The\nexcellent thermal stability of the crystal structure and the piezoelectric\nproperty indicate that the BS-xPT-PCN system is a promising candidate for high\ntemperature piezoelectric applications.'}, {'Dynamic behavior of polar nanoregions in re-entrant relaxor\n  0.6Bi(Mg1/2Ti1/2)O3-0.4PbTiO3': 'The existence of polar nanoregions is the most important characteristic of\nferroelectric relaxors, however, the size determination and dynamic of PNRs\nremains uncertain. We reveal a re-entrant relaxor behavior and\nferroelectric-paraelectric transition coexists in complex perovskite oxide\n0.6Bi(Mg1/2Ti1/2)O3-0.4PbTiO3. Two dielectric anomalies (i) the low-temperature\nre-entrant relaxor transition and (ii) the high-temperature diffuse phase\ntransition (DPT) were described by the phenomenological statistical model. The\nsizes of the two kinds of polar nanoregions (PNRs) corresponding to two\nferroelectric states were obtained. The dynamic of PNRs were analyzed using\nisothermal electrical modulus, which shows three critical temperatures\nassociated with the diffuse phase transition, the formation and freezing of\nPNRs, respectively. The temperature evolution of the PNRs evolution depends on\nthe stoichiometry of bismuth. The results provide new insights into the dynamic\nbehavior of PNRs and the modification way of re-entrant relaxor behavior.'}, {'FogROS2: An Adaptive Platform for Cloud and Fog Robotics Using ROS 2': 'Mobility, power, and price points often dictate that robots do not have\nsufficient computing power on board to run contemporary robot algorithms at\ndesired rates. Cloud computing providers such as AWS, GCP, and Azure offer\nimmense computing power and increasingly low latency on demand, but tapping\ninto that power from a robot is non-trivial. We present FogROS2, an open-source\nplatform to facilitate cloud and fog robotics that is included in the Robot\nOperating System 2 (ROS 2) distribution. FogROS2 is distinct from its\npredecessor FogROS1 in 9 ways, including lower latency, overhead, and startup\ntimes; improved usability, and additional automation, such as region and\ncomputer type selection. Additionally, FogROS2 gains performance, timing, and\nadditional improvements associated with ROS 2. In common robot applications,\nFogROS2 reduces SLAM latency by 50 %, reduces grasp planning time from 14 s to\n1.2 s, and speeds up motion planning 45x. When compared to FogROS1, FogROS2\nreduces network utilization by up to 3.8x, improves startup time by 63 %, and\nnetwork round-trip latency by 97 % for images using video compression. The\nsource code, examples, and documentation for FogROS2 are available at\nhttps://github.com/BerkeleyAutomation/FogROS2, and is available through the\nofficial ROS 2 repository at https://index.ros.org/p/fogros2/.'}]","Title: CogDPM: Diffusion Probabilistic Models via Cognitive Predictive Coding

**Abstract:**

Overcoming the limitations of traditional models in capturing complex spatiotemporal dynamics, ""CogDPM: Diffusion Probabilistic Models via Cognitive Predictive Coding"" introduces a pioneering approach that merges the predictive power of Diffusion Probabilistic Models (DPMs) with Cognitive Predictive Coding (CPC). Deliberately aiming to enhance predictions in critical temporal sequences, the method targets advanced forecasting scenarios, notably in weather systems where precision is paramount. 

**Objective:** This study synthesizes CPC and DPMs to develop a probabilistic forecasting model, CogDPM, intended for applications where time-consuming and manual recalibration is impractical. Central to its design are the trigonometric operations that facilitate the model's precision in handling sequential data.

**Innovations:** Unlike existing models that often rely on simplistic data processing techniques, CogDPM leverages CPC mechanisms to adjust prediction outcomes dynamically along different spatiotemporal scales. This results in more accurate and responsive predictions, particularly in rapidly changing conditions, thus paving the way for dynamic adjustments and enhanced forecast efficiency.

**Methods:** CogDPM is constructed through three core components: inverse precision mapping, which categorizes data importance for variable temporal adjustments; multilayer inference, which enables the model to learn from sequential predictions through hierarchical error minimization; and a unique coupling of generative and perceptual models that assesses uncertainty and improves interpretability.

**Results:** Evaluations on moving MNIST and turbulence datasets demonstrate CogDPM’s superiority in forecasting precision, assessed through metrics like CRPS, CSI, and FSS. Detailed predictions align closely with ground truths, even in turbulent scenarios, showcasing the method’s predictive robustness.

**Contributions:** By integrating CPC principles with DPMs, CogDPM advances the predictive forecasting field, specifically in handling moving bodies, with a unique capability to glean and visualize uncertainty through precision weights. This novel framework enhances the interpretability and practicality of probabilistic predictions, offering a significant leap in model forecasting accuracy and reliability.

**Applications:** This model opens up new avenues for real-world applications, from enhancing weather forecasting capabilities to optimizing autonomous driving systems by predicting vehicle trajectories more accurately. Integrated within dynamic environments, CogDPM demonstrates substantial potential for improving decision-making processes across sectors requiring precise predictions."
"Prediction of flow to boreholes or excavations in fractured low-permeability
rocks is important for resource extraction and disposal or sequestration
activities. Analytical solutions for fluid pressure and flowrate, when
available, are powerful, insightful, and efficient tools enabling parameter
estimation and uncertainty quantification. A flexible porous media flow
solution for arbitrary physical dimension is derived and extended to double
porosity for converging radial flow when permeability and porosity decrease
radially as a power law away from a borehole or opening. This distribution can
arise from damage accumulation due to stress relief associated with drilling or
mining. The single-porosity graded conductivity solution was initially found
for heat conduction, the arbitrary dimension flow solution comes from
hydrology, and the solution with both arbitrary dimension and graded
permeability distribution appeared in reservoir engineering. These existing
solutions are here combined and extended to two implementations of the
double-porosity conceptual model, for both a simpler thin-film mass transfer
and more physically realistic diffusion between fracture and matrix. This work
presents a new specified-flowrate solution with wellbore storage for the
simpler double-porosity model, and a new more physically realistic solution for
any wellbore boundary condition. A new closed-form expression is derived for
the matrix diffusion solution (applicable to both homogeneous and graded
problems), improving on previous infinite series expressions.","[{'Review of Inverse Laplace Transform Algorithms for Laplace-Space\n  Numerical Approaches': 'A boundary element method (BEM) simulation is used to compare the efficiency\nof numerical inverse Laplace transform strategies, considering general\nrequirements of Laplace-space numerical approaches. The two-dimensional BEM\nsolution is used to solve the Laplace-transformed diffusion equation, producing\na time-domain solution after a numerical Laplace transform inversion. Motivated\nby the needs of numerical methods posed in Laplace-transformed space, we\ncompare five inverse Laplace transform algorithms and discuss implementation\ntechniques to minimize the number of Laplace-space function evaluations. We\ninvestigate the ability to calculate a sequence of time domain values using the\nfewest Laplace-space model evaluations. We find Fourier-series based inversion\nalgorithms work for common time behaviors, are the most robust with respect to\nfree parameters, and allow for straightforward image function evaluation re-use\nacross at least a log cycle of time.'}, {'Unsaturated Hydraulic Conductivity Models Based on Truncated Lognormal\n  Pore-size Distributions': ""We develop a closed-form three-parameter model for unsaturated hydraulic\nconductivity associated with the Kosugi three-parameter lognormal moisture\nretention model. The model derivation uses a slight modification to Mualem's\ntheory, which is nearly exact for non-clay soils. Kosugi's three-parameter\nlognormal moisture retention model uses physically meaningful parameters, but a\ncorresponding closed-form relative hydraulic conductivity model has never been\ndeveloped. The model is further extended to a four -parameter model by\ntruncating the underlying pore size distribution at physically permissible\nminimum and maximum pore radii. The proposed closed-form models are fitted to\nwell-known experimental data, to illustrate their utility. They have the same\nphysical basis as Kosugi's two-parameter model, but are more general.""}, {'Unconfined Aquifer Flow Theory - from Dupuit to present': 'Analytic and semi-analytic solution are often used by researchers and\npracticioners to estimate aquifer parameters from unconfined aquifer pumping\ntests. The non-linearities associated with unconfined (i.e., water table)\naquifer tests makes their analysis more complex than confined tests. Although\nanalytical solutions for unconfined flow began in the mid-1800s with Dupuit,\nThiem was possibly the first to use them to estimate aquifer parameters from\npumping tests in the early 1900s. In the 1950s, Boulton developed the first\ntransient well test solution specialized to unconfined flow. By the 1970s\nNeuman had developed solutions considering both primary transient storage\nmechanisms (confined storage and delayed yield) without non-physical fitting\nparameters. In the last decade, research into developing unconfined aquifer\ntest solutions has mostly focused on explicitly coupling the aquifer with the\nlinearized vadose zone. Despite the many advanced solution methods available,\nthere still exists a need for realism to accurately simulate real-world aquifer\ntests.'}, {'Uncoupling electrokinetic flow solutions': 'The continuum-scale electrokinetic porous-media flow and excess charge\nredistribution equations are uncoupled using eigenvalue decomposition. The\nuncoupling results in a pair of independent diffusion equations for\n""intermediate"" potentials subject to modified material properties and boundary\nconditions. The fluid pressure and electrostatic potential are then found by\nrecombining the solutions to the two intermediate uncoupled problems in a\nmatrix-vector multiply. Expressions for the material properties or source terms\nin the intermediate uncoupled problem may require extended precision or careful\nre-writing to avoid numerical cancellation, but the solutions themselves can be\ncomputed in typical double precision. The approach works with analytical or\ngridded numerical solutions and is illustrated through two examples. The\nsolution for flow to a pumping well is manipulated to predict streaming\npotential and electroosmosis, and a periodic one-dimensional analytical\nsolution is derived and used to predict electroosmosis and streaming potential\nin a laboratory flow cell subjected to low frequency alternating current and\npressure excitation. The examples illustrate the utility of the eigenvalue\ndecoupling approach, repurposing existing analytical solutions and leveraging\nsimpler-to-derive solutions or numerical models for coupled physics.'}, {'Multiporosity Flow in Fractured Low-Permeability Rocks': 'A multiporosity extension of classical double and triple porosity fractured\nrock flow models for slightly compressible fluids is presented. The\nmultiporosity model is an adaptation of the multirate solute transport model of\nHaggerty and Gorelick (1995) to viscous flow in fractured rock reservoirs. It\nis a generalization of both pseudo-steady-state and transient interporosity\nflow double porosity models. The model includes a fracture continuum and an\noverlapping distribution of multiple rock matrix continua, whose\nfracture-matrix exchange coefficients are specified through a discrete\nprobability mass function. Semi-analytical cylindrically symmetric solutions to\nthe multiporosity mathematical model are developed using the Laplace transform\nto illustrate its behavior. The multiporosity model presented here is\nconceptually simple, yet flexible enough to simulate common conceptualizations\nof double and triple porosity flow. This combination of generality and\nsimplicity makes the multiporosity model a good choice for flow in\nlow-permeability fractured rocks.'}, {'Core-scale solute transport model selection using Monte Carlo analysis': 'Model applicability to core-scale solute transport is evaluated using\nbreakthrough data from column experiments conducted with conservative tracers\ntritium (H-3) and sodium-22, and the retarding solute uranium-232. The three\nmodels considered are single-porosity, double-porosity with single-rate\nmobile-immobile mass-exchange, and the multirate model, which is a\ndeterministic model that admits the statistics of a random mobile-immobile\nmass-exchange rate coefficient. The experiments were conducted on intact\nCulebra Dolomite core samples. Previously, data were analyzed using single- and\ndouble-porosity models although the Culebra Dolomite is known to possess\nmultiple types and scales of porosity, and to exhibit multirate\nmobile-immobile-domain mass transfer characteristics at field scale. The data\nare reanalyzed here and null-space Monte Carlo analysis is used to facilitate\nobjective model selection. Prediction (or residual) bias is adopted as a\nmeasure of the model structural error. The analysis clearly shows single- and\ndouble-porosity models are structurally deficient, yielding late-time residual\nbias that grows with time. On the other hand, the multirate model yields\nunbiased predictions consistent with the late-time -5/2 slope diagnostic of\nmultirate mass transfer. The analysis indicates the multirate model is better\nsuited to describing core-scale solute breakthrough in the Culebra Dolomite\nthan the other two models.'}, {'Modeling cross-hole slug tests in an unconfined aquifer': 'A modified version of a published slug test model for unconfined aquifers is\napplied to cross-hole slug test data collected in field tests conducted at the\nWiden site in Switzerland. The model accounts for water-table effects using the\nlinearised kinematic condition. The model also accounts for inertial effects in\nsource and observation wells. The primary objective of this work is to\ndemonstrate applicability of this semi-analytical model to multi-well and\nmulti-level pneumatic slug tests. The pneumatic perturbation was applied at\ndiscrete intervals in a source well and monitored at discrete vertical\nintervals in observation wells. The source and observation well pairs were\nseparated by distances of up to 4 m. The analysis yielded vertical profiles of\nhydraulic conductivity, specific storage, and specific yield at observation\nwell locations. The hydraulic parameter estimates are compared to results from\nprior pumping and single-well slug tests conducted at the site, as well as to\nestimates from particle size analyses of sediment collected from boreholes\nduring well installation. The results are in general agreement with results\nfrom prior tests and are indicative of a sand and gravel aquifer. Sensitivity\nanalysis show that model identification of specific yield is strongest at\nlate-time. However, the usefulness of late-time data is limited due to the low\nsignal-to-noise ratios.'}, {'Anthropogenic influences on groundwater in the vicinity of a long-lived\n  radioactive waste repository': 'The groundwater flow system in the Culebra Dolomite Member (Culebra) of the\nPermian Rustler Formation is a potential radionuclide release pathway from the\nWaste Isolation Pilot Plant (WIPP), the only deep geological repository for\ntransuranic waste in the United States. In early conceptual models of the\nCulebra, groundwater levels were not expected to fluctuate markedly, except in\nresponse to long-term climatic changes, with response times on the order of\nhundreds to thousands of years. Recent groundwater pressures measured in\nmonitoring wells record more than 25 m of drawdown. The fluctuations are\nattributed to pumping activities at a privately-owned well that may be\nassociated with the demand of the Permian Basin hydrocarbon industry for water.\nThe unprecedented magnitude of drawdown provides an opportunity to\nquantitatively assess the influence of unplanned anthropogenic forcings near\nthe WIPP. Spatially variable realizations of Culebra saturated hydraulic\nconductivity and storativity were used to develop groundwater flow models to\nestimate a pumping rate for the private well and investigate its effect on\nadvective transport. Simulated drawdown shows reasonable agreement with\nobservations (average Model Efficiency coefficient = 0.7). Steepened hydraulic\ngradients associated with the pumping reduce estimates of conservative particle\ntravel times across the domain by one-half and shift the intersection of the\naverage particle track with the compliance boundary by more than two\nkilometers. The value of the transient simulations conducted for this study lie\nin their ability to (i) improve understanding of the Culebra groundwater flow\nsystem and (ii) challenge the notion of time-invariant land use in the vicinity\nof the WIPP.'}, {'Modeling Dynamic Helium Release as a Tracer of Rock Deformation': 'We use helium released during mechanical deformation of shales as a signal to\nexplore the effects of deformation and failure on material transport\nproperties. A dynamic dual-permeability model with evolving pore and fracture\nnetworks is used to simulate gases released from shale during deformation and\nfailure. Changes in material properties required to reproduce experimentally\nobserved gas signals are explored. We model two different experiments of $^4$He\nflow rate measured from shale undergoing mechanical deformation, a core\nparallel to bedding and a core perpendicular to bedding. We find that the\nhelium signal is sensitive to fracture development and evolution as well as\nchanges in the matrix transport properties. We constrain the timing and\neffective fracture aperture, as well as the increase in matrix porosity and\npermeability. Increases in matrix permeability are required to explain gas flow\nprior to macroscopic failure, and the short-term gas flow post failure.\nIncreased matrix porosity, is required to match the long-term, post-failure gas\nflow. Our model provides the first quantitative interpretation of helium\nrelease as a result of mechanical deformation. The sensitivity of this model to\nchanges in the fracture network, as well as to matrix properties during\ndeformation, indicates that helium release can be used as a quantitative tool\nto evaluate the state of stress and strain in earth materials.'}, {'Saturated-Unsaturated flow in a Compressible Leaky-unconfined Aquifer': 'An analytical solution is developed for three-dimensional flow towards a\npartially penetrating large-diameter well in an unconfined aquifer bounded\nbelow by an aquitard of finite or semi-infinite extent. The analytical solution\nis derived using Laplace and Hankel transforms, then inverted numerically.\nExisting solutions for flow in leaky unconfined aquifers neglect the\nunsaturated zone following an assumption of instantaneous drainage assumption\ndue to Neuman [1972]. We extend the theory of leakage in unconfined aquifers by\n(1) including water flow and storage in the unsaturated zone above the water\ntable, and (2) allowing the finite-diameter pumping well to partially penetrate\nthe aquifer. The investigation of model-predicted results shows that leakage\nfrom an underlying aquitard leads to significant departure from the unconfined\nsolution without leakage. The investigation of dimensionless time-drawdown\nrelationships shows that the aquitard drawdown also depends on unsaturated zone\nproperties and the pumping-well wellbore storage effects.'}]","Title: Analytical Solutions for Double-Porosity Flow with Variable Power Laws and Wellbore Storage

Background: 
Understanding subsurface flow in dual-porosity systems is critical for optimizing hydrocarbon extraction and managing groundwater resources. However, analytical solutions remain challenging due to the complexity of the physical processes involved.

Objective:
The primary goal of this study is to extend existing analytical solutions for double-porosity flow by incorporating a variable exponent model and wellbore storage effects. 

Innovations:
Novel innovations include the development of an analytical solution for double-porosity flow with power-law dependent permeability and porosity, and the introduction of a general wellbore storage boundary condition. These formulations capture the non-dimensionless transformed governing equation for pressure and flow with arbitrary temporal behaviors. This allows for a physical interpretation and numerical modeling of field observations without resorting to empirical fits.

Methods:
The analytical solution is derived through the use of Laplace transforms, offering a physically meaningful boundary condition for wellbore storage. The computational solution involves both Python programming and Fortran-based algorithms, enabling these tools to be utilized for further exploration and parameter estimation.

Results:
The results provide a series of flow and pressure solutions for different power-law exponents, flow dimensions, and wellbore storage effects. These solutions show that the permeability exponent κ and porosity exponent η have significant impacts, especially away from the source borehole. Wellbore storage is crucial for predicting early-time behavior accurately.

Contributions:
The contributions of this research are significant and include providing a flexible, comprehensive analytical framework for double-porosity flow modeling, accounting for both variable exponent materials and wellbore storage effects. These solutions enrich our ability to model subsurface transport, enhancing the accuracy of predicting flow behavior in complex geological formations.

Applications:
This framework is valuable for both industrial reservoir management, aiding engineers in optimizing well placement and flow rates, and scientific research, where understanding flow dynamics across various geological settings could inform the design of more effective models. Additionally, it can support groundwater management strategies, better predicting solute transport through porous media that may contain dual-porosity features, contributing to sustainable water resource management practices."
"This paper introduces Stochastic RAG--a novel approach for end-to-end
optimization of retrieval-augmented generation (RAG) models that relaxes the
simplifying assumptions of marginalization and document independence, made in
most prior work. Stochastic RAG casts the retrieval process in RAG as a
stochastic sampling without replacement process. Through this formulation, we
employ straight-through Gumbel-top-k that provides a differentiable
approximation for sampling without replacement and enables effective end-to-end
optimization for RAG. We conduct extensive experiments on seven diverse
datasets on a wide range of tasks, from open-domain question answering to fact
verification to slot-filling for relation extraction and to dialogue systems.
By applying this optimization method to a recent and effective RAG model, we
advance state-of-the-art results on six out of seven datasets.","[{'Towards Theoretical Understanding of Weak Supervision for Information\n  Retrieval': 'Neural network approaches have recently shown to be effective in several\ninformation retrieval (IR) tasks. However, neural approaches often require\nlarge volumes of training data to perform effectively, which is not always\navailable. To mitigate the shortage of labeled data, training neural IR models\nwith weak supervision has been recently proposed and received considerable\nattention in the literature. In weak supervision, an existing model\nautomatically generates labels for a large set of unlabeled data, and a machine\nlearning model is further trained on the generated ""weak"" data. Surprisingly,\nit has been shown in prior art that the trained neural model can outperform the\nweak labeler by a significant margin. Although these obtained improvements have\nbeen intuitively justified in previous work, the literature still lacks\ntheoretical justification for the observed empirical findings. In this position\npaper, we propose to theoretically study weak supervision, in particular for IR\ntasks, e.g., learning to rank. We briefly review a set of our recent\ntheoretical findings that shed light on learning from weakly supervised data,\nand provide guidelines on how train learning to rank models with weak\nsupervision.'}, {'Macaw: An Extensible Conversational Information Seeking Platform': 'Conversational information seeking (CIS) has been recognized as a major\nemerging research area in information retrieval. Such research will require\ndata and tools, to allow the implementation and study of conversational\nsystems. This paper introduces Macaw, an open-source framework with a modular\narchitecture for CIS research. Macaw supports multi-turn, multi-modal, and\nmixed-initiative interactions, and enables research for tasks such as document\nretrieval, question answering, recommendation, and structured data exploration.\nIt has a modular design to encourage the study of new CIS algorithms, which can\nbe evaluated in batch mode. It can also integrate with a user interface, which\nallows user studies and data collection in an interactive mode, where the back\nend can be fully algorithmic or a wizard of oz setup. Macaw is distributed\nunder the MIT License.'}, {'Multivariate Representation Learning for Information Retrieval': 'Dense retrieval models use bi-encoder network architectures for learning\nquery and document representations. These representations are often in the form\nof a vector representation and their similarities are often computed using the\ndot product function. In this paper, we propose a new representation learning\nframework for dense retrieval. Instead of learning a vector for each query and\ndocument, our framework learns a multivariate distribution and uses negative\nmultivariate KL divergence to compute the similarity between distributions. For\nsimplicity and efficiency reasons, we assume that the distributions are\nmultivariate normals and then train large language models to produce mean and\nvariance vectors for these distributions. We provide a theoretical foundation\nfor the proposed framework and show that it can be seamlessly integrated into\nthe existing approximate nearest neighbor algorithms to perform retrieval\nefficiently. We conduct an extensive suite of experiments on a wide range of\ndatasets, and demonstrate significant improvements compared to competitive\ndense retrieval models.'}, {'Relevance-based Word Embedding': 'Learning a high-dimensional dense representation for vocabulary terms, also\nknown as a word embedding, has recently attracted much attention in natural\nlanguage processing and information retrieval tasks. The embedding vectors are\ntypically learned based on term proximity in a large corpus. This means that\nthe objective in well-known word embedding algorithms, e.g., word2vec, is to\naccurately predict adjacent word(s) for a given word or context. However, this\nobjective is not necessarily equivalent to the goal of many information\nretrieval (IR) tasks. The primary objective in various IR tasks is to capture\nrelevance instead of term proximity, syntactic, or even semantic similarity.\nThis is the motivation for developing unsupervised relevance-based word\nembedding models that learn word representations based on query-document\nrelevance information. In this paper, we propose two learning models with\ndifferent objective functions; one learns a relevance distribution over the\nvocabulary set for each query, and the other classifies each term as belonging\nto the relevant or non-relevant class for each query. To train our models, we\nused over six million unique queries and the top ranked documents retrieved in\nresponse to each query, which are assumed to be relevant to the query. We\nextrinsically evaluate our learned word representation models using two IR\ntasks: query expansion and query classification. Both query expansion\nexperiments on four TREC collections and query classification experiments on\nthe KDD Cup 2005 dataset suggest that the relevance-based word embedding models\nsignificantly outperform state-of-the-art proximity-based embedding models,\nsuch as word2vec and GloVe.'}, {'Investigating the Successes and Failures of BERT for Passage Re-Ranking': 'The bidirectional encoder representations from transformers (BERT) model has\nrecently advanced the state-of-the-art in passage re-ranking. In this paper, we\nanalyze the results produced by a fine-tuned BERT model to better understand\nthe reasons behind such substantial improvements. To this aim, we focus on the\nMS MARCO passage re-ranking dataset and provide potential reasons for the\nsuccesses and failures of BERT for retrieval. In more detail, we empirically\nstudy a set of hypotheses and provide additional analysis to explain the\nsuccessful performance of BERT.'}, {'ICXML: An In-Context Learning Framework for Zero-Shot Extreme\n  Multi-Label Classification': 'This paper focuses on the task of Extreme Multi-Label Classification (XMC)\nwhose goal is to predict multiple labels for each instance from an extremely\nlarge label space. While existing research has primarily focused on fully\nsupervised XMC, real-world scenarios often lack supervision signals,\nhighlighting the importance of zero-shot settings. Given the large label space,\nutilizing in-context learning approaches is not trivial. We address this issue\nby introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage\nframework that cuts down the search space by generating a set of candidate\nlabels through incontext learning and then reranks them. Extensive experiments\nsuggest that ICXML advances the state of the art on two diverse public\nbenchmarks.'}, {'Evaluating Retrieval Quality in Retrieval-Augmented Generation': ""Evaluating retrieval-augmented generation (RAG) presents challenges,\nparticularly for retrieval models within these systems. Traditional end-to-end\nevaluation methods are computationally expensive. Furthermore, evaluation of\nthe retrieval model's performance based on query-document relevance labels\nshows a small correlation with the RAG system's downstream performance. We\npropose a novel evaluation approach, eRAG, where each document in the retrieval\nlist is individually utilized by the large language model within the RAG\nsystem. The output generated for each document is then evaluated based on the\ndownstream task ground truth labels. In this manner, the downstream performance\nfor each document serves as its relevance label. We employ various downstream\ntask metrics to obtain document-level annotations and aggregate them using\nset-based or ranking metrics. Extensive experiments on a wide range of datasets\ndemonstrate that eRAG achieves a higher correlation with downstream RAG\nperformance compared to baseline methods, with improvements in Kendall's $\\tau$\ncorrelation ranging from 0.168 to 0.494. Additionally, eRAG offers significant\ncomputational advantages, improving runtime and consuming up to 50 times less\nGPU memory than end-to-end evaluation.""}, {'Towards a Search Engine for Machines: Unified Ranking for Multiple\n  Retrieval-Augmented Large Language Models': 'This paper introduces uRAG--a framework with a unified retrieval engine that\nserves multiple downstream retrieval-augmented generation (RAG) systems. Each\nRAG system consumes the retrieval results for a unique purpose, such as\nopen-domain question answering, fact verification, entity linking, and relation\nextraction. We introduce a generic training guideline that standardizes the\ncommunication between the search engine and the downstream RAG systems that\nengage in optimizing the retrieval model. This lays the groundwork for us to\nbuild a large-scale experimentation ecosystem consisting of 18 RAG systems that\nengage in training and 18 unknown RAG systems that use the uRAG as the new\nusers of the search engine. Using this experimentation ecosystem, we answer a\nnumber of fundamental research questions that improve our understanding of\npromises and challenges in developing search engines for machines.'}, {'Joint Modeling and Optimization of Search and Recommendation': 'Despite the somewhat different techniques used in developing search engines\nand recommender systems, they both follow the same goal: helping people to get\nthe information they need at the right time. Due to this common goal, search\nand recommendation models can potentially benefit from each other. The recent\nadvances in neural network technologies make them effective and easily\nextendable for various tasks, including retrieval and recommendation. This\nraises the possibility of jointly modeling and optimizing search ranking and\nrecommendation algorithms, with potential benefits to both. In this paper, we\npresent theoretical and practical reasons to motivate joint modeling of search\nand recommendation as a research direction. We propose a general framework that\nsimultaneously learns a retrieval model and a recommendation model by\noptimizing a joint loss function. Our preliminary results on a dataset of\nproduct data indicate that the proposed joint modeling substantially\noutperforms the retrieval and recommendation models trained independently. We\nlist a number of future directions for this line of research that can\npotentially lead to development of state-of-the-art search and recommendation\nmodels.'}, {'Guided Transformer: Leveraging Multiple External Sources for\n  Representation Learning in Conversational Search': 'Asking clarifying questions in response to ambiguous or faceted queries has\nbeen recognized as a useful technique for various information retrieval\nsystems, especially conversational search systems with limited bandwidth\ninterfaces. Analyzing and generating clarifying questions have been studied\nrecently but the accurate utilization of user responses to clarifying questions\nhas been relatively less explored. In this paper, we enrich the representations\nlearned by Transformer networks using a novel attention mechanism from external\ninformation sources that weights each term in the conversation. We evaluate\nthis Guided Transformer model in a conversational search scenario that includes\nclarifying questions. In our experiments, we use two separate external sources,\nincluding the top retrieved documents and a set of different possible\nclarifying questions for the query. We implement the proposed representation\nlearning model for two downstream tasks in conversational search; document\nretrieval and next clarifying question selection. Our experiments use a public\ndataset for search clarification and demonstrate significant improvements\ncompared to competitive baselines.'}]","ABSTRACT

In the era of data-driven artificial intelligence, machine learning models have faced challenges with tasks requiring knowledge grounding, particularly in the context of non-stationary data where new information is continuously emerging. The paper introduces a groundbreaking retrieval-augmented generation (RAG) framework aimed to enhance machine learning systems' effectiveness via efficient knowledge retrieval and utilization. Our main goal is to mitigate the limitations of self-contained systems that lack the capacity to actively incorporate current information in their reasoning and decision-making processes.

We present innovations in utilizing stochastic expected utility maximization in the RAG framework to improve retrieval model optimization across diverse applications such as question answering, fact verification, and dialogue systems. Our method applies at inference for active selection of supporting documents, framing decisions as a stochastic expected utility problem. The framework computes the most probable answer, grounded in queried topics, by iteratively sample retrieval scores without replacement, bridging retrieval and generation in a tightly integrated system.

Through extensive experimentation on seven quality-focused datasets, our proposed model consistently outperforms state-of-the-art systems, demonstrating notable improvements except on Wizard of Wikipedia where GripRank marginally outperforms our top-performing system. Moreover, we have shown that our model benefits from scalability in downstream language model sizes, achieving superior performance on queries and passages of all lengths.

The contributions of this work are manifold: a novel system that advances RAG methodologies with stochastic expected utility optimization, proving significant performance gains across systems, models, and data. It also paves the way for dynamic, knowledge-replete AI systems that can adapt to evolving environments by effectively integrating external information.

This work has broad applications across information retrieval, question answering, and conversational AI systems, leading to more innovative and efficient solutions that can keep up with the pace of ongoing information generation and management in today's digital world."
"Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning
foundation models. It effectively reduces the number of trainable parameters by
incorporating low-rank matrices $A$ and $B$ to represent the weight change,
i.e., $\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when
handling extensive customization adaptations or larger base models. In this
work, we aim to further compress trainable parameters by enjoying the powerful
expressiveness of the Fourier transform. Specifically, we introduce FourierFT,
which treats $\Delta W$ as a matrix in the spatial domain and learns only a
small fraction of its spectral coefficients. With the trained spectral
coefficients, we implement the inverse discrete Fourier transform to recover
$\Delta W$. Empirically, our FourierFT method shows comparable or better
performance with fewer parameters than LoRA on various tasks, including natural
language understanding, natural language generation, instruction tuning, and
image classification. For example, when performing instruction tuning on the
LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable
parameters, compared to LoRA's 33.5M. Our code is released at
\url{https://github.com/Chaos96/fourierft}.","[{'Rethinking Dual-Domain Undersampled MRI reconstruction: domain-specific\n  design from the perspective of the receptive field': 'Undersampled MRI reconstruction is crucial for accelerating clinical\nscanning. Dual-domain reconstruction network is performant among SoTA deep\nlearning methods. In this paper, we rethink dual-domain model design from the\nperspective of the receptive field, which is needed for image recovery and\nK-space interpolation problems. Further, we introduce domain-specific modules\nfor dual-domain reconstruction, namely k-space global initialization and\nimage-domain parallel local detail enhancement. We evaluate our modules by\ntranslating a SoTA method DuDoRNet under different conventions of MRI\nreconstruction including image-domain, dual-domain, and reference-guided\nreconstruction on the public IXI dataset. Our model DuDoRNet+ achieves\nsignificant improvements over competing deep learning methods.'}, {'MRPD: Undersampled MRI reconstruction by prompting a large latent\n  diffusion model': ""Implicit visual knowledge in a large latent diffusion model (LLDM)\npre-trained on natural images is rich and hypothetically universal to natural\nand medical images. To test this hypothesis from a practical perspective, we\npropose a novel framework for undersampled MRI Reconstruction by Prompting a\nlarge latent Diffusion model (MRPD). While the existing methods trained on MRI\ndatasets are typically of limited generalizability toward diverse data\nacquisition scenarios, MRPD supports unsupervised and universally adaptive MRI\nreconstruction. For unsupervised reconstruction, MRSampler guides LLDM with a\nrandom-phase-modulated hard-to-soft control. With any single- or\nmultiple-source MRI dataset, MRPD's performance is boosted universally by a\nlightweight MRAdapter that only finetunes the LLDM's autoencoder. Experiments\non FastMRI and IXI show that MRPD is the only model that supports both MRI\ndatabase-free and database-available scenarios and attains the best\ngeneralizability towards out-of-domain (OOD) samplings, contrasts, and organs\namong compared unsupervised, supervised, and MRI diffusion methods. To our\nknowledge, MRPD is the first method that empirically shows the universal\nprowess of an LLDM pre-trained on vast natural images for MRI. Our official\nimplementation is at https://github.com/Z7Gao/MRPD.""}, {'Rethinking Graph Neural Networks for Anomaly Detection': ""Graph Neural Networks (GNNs) are widely applied for graph anomaly detection.\nAs one of the key components for GNN design is to select a tailored spectral\nfilter, we take the first step towards analyzing anomalies via the lens of the\ngraph spectrum. Our crucial observation is the existence of anomalies will lead\nto the `right-shift' phenomenon, that is, the spectral energy distribution\nconcentrates less on low frequencies and more on high frequencies. This fact\nmotivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed,\nBWGNN has spectral and spatial localized band-pass filters to better handle the\n`right-shift' phenomenon in anomalies. We demonstrate the effectiveness of\nBWGNN on four large-scale anomaly detection datasets. Our code and data are\nreleased at https://github.com/squareRoot3/Rethinking-Anomaly-Detection""}, {'GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection': 'With a long history of traditional Graph Anomaly Detection (GAD) algorithms\nand recently popular Graph Neural Networks (GNNs), it is still not clear (1)\nhow they perform under a standard comprehensive setting, (2) whether GNNs can\noutperform traditional algorithms such as tree ensembles, and (3) how about\ntheir efficiency on large-scale graphs. In response, we introduce GADBench -- a\nbenchmark tool dedicated to supervised anomalous node detection in static\ngraphs. GADBench facilitates a detailed comparison across 29 distinct models on\nten real-world GAD datasets, encompassing thousands to millions ($\\sim$6M)\nnodes. Our main finding is that tree ensembles with simple neighborhood\naggregation can outperform the latest GNNs tailored for the GAD task. We shed\nlight on the current progress of GAD, setting a robust groundwork for\nsubsequent investigations in this domain. GADBench is open-sourced at\nhttps://github.com/squareRoot3/GADBench.'}, {'ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node\n  Classification': 'Graph contrastive learning (GCL) has attracted a surge of attention due to\nits superior performance for learning node/graph representations without\nlabels. However, in practice, the underlying class distribution of unlabeled\nnodes for the given graph is usually imbalanced. This highly imbalanced class\ndistribution inevitably deteriorates the quality of learned node\nrepresentations in GCL. Indeed, we empirically find that most state-of-the-art\nGCL methods cannot obtain discriminative representations and exhibit poor\nperformance on imbalanced node classification. Motivated by this observation,\nwe propose a principled GCL framework on Imbalanced node classification\n(ImGCL), which automatically and adaptively balances the representations\nlearned from GCL without labels. Specifically, we first introduce the online\nclustering based progressively balanced sampling (PBS) method with theoretical\nrationale, which balances the training sets based on pseudo-labels obtained\nfrom learned representations in GCL. We then develop the node centrality based\nPBS method to better preserve the intrinsic structure of graphs, by upweighting\nthe important nodes of the given graph. Extensive experiments on multiple\nimbalanced graph datasets and imbalanced settings demonstrate the effectiveness\nof our proposed framework, which significantly improves the performance of the\nrecent state-of-the-art GCL methods. Further experimental ablations and\nanalyses show that the ImGCL framework consistently improves the representation\nquality of nodes in under-represented (tail) classes.'}, {'MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human\n  Activity Recognition': ""Multimodal sensors provide complementary information to develop accurate\nmachine-learning methods for human activity recognition (HAR), but introduce\nsignificantly higher computational load, which reduces efficiency. This paper\nproposes an efficient multimodal neural architecture for HAR using an RGB\ncamera and inertial measurement units (IMUs) called Multimodal Temporal Segment\nAttention Network (MMTSA). MMTSA first transforms IMU sensor data into a\ntemporal and structure-preserving gray-scale image using the Gramian Angular\nField (GAF), representing the inherent properties of human activities. MMTSA\nthen applies a multimodal sparse sampling method to reduce data redundancy.\nLastly, MMTSA adopts an inter-segment attention module for efficient multimodal\nfusion. Using three well-established public datasets, we evaluated MMTSA's\neffectiveness and efficiency in HAR. Results show that our method achieves\nsuperior performance improvements 11.13% of cross-subject F1-score on the MMAct\ndataset than the previous state-of-the-art (SOTA) methods. The ablation study\nand analysis suggest that MMTSA's effectiveness in fusing multimodal data for\naccurate HAR. The efficiency evaluation on an edge device showed that MMTSA\nachieved significantly better accuracy, lower computational load, and lower\ninference latency than SOTA methods.""}, {'Protein Multimer Structure Prediction via Prompt Learning': 'Understanding the 3D structures of protein multimers is crucial, as they play\na vital role in regulating various cellular processes. It has been empirically\nconfirmed that the multimer structure prediction~(MSP) can be well handled in a\nstep-wise assembly fashion using provided dimer structures and predicted\nprotein-protein interactions~(PPIs). However, due to the biological gap in the\nformation of dimers and larger multimers, directly applying PPI prediction\ntechniques can often cause a \\textit{poor generalization} to the MSP task. To\naddress this challenge, we aim to extend the PPI knowledge to multimers of\ndifferent scales~(i.e., chain numbers). Specifically, we propose\n\\textbf{\\textsc{PromptMSP}}, a pre-training and \\textbf{Prompt} tuning\nframework for \\textbf{M}ultimer \\textbf{S}tructure \\textbf{P}rediction. First,\nwe tailor the source and target tasks for effective PPI knowledge learning and\nefficient inference, respectively. We design PPI-inspired prompt learning to\nnarrow the gaps of two task formats and generalize the PPI knowledge to\nmultimers of different scales. We provide a meta-learning strategy to learn a\nreliable initialization of the prompt model, enabling our prompting framework\nto effectively adapt to limited data for large-scale multimers. Empirically, we\nachieve both significant accuracy (RMSD and TM-Score) and efficiency\nimprovements compared to advanced MSP models. The code, data and checkpoints\nare released at \\url{https://github.com/zqgao22/PromptMSP}.'}, {'DuDoUniNeXt: Dual-domain unified hybrid model for single and\n  multi-contrast undersampled MRI reconstruction': 'Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to\nincorporate a reference image of auxiliary modality to guide the reconstruction\nprocess of the target modality. Known MC reconstruction methods perform well\nwith a fully sampled reference image, but usually exhibit inferior performance,\ncompared to single-contrast (SC) methods, when the reference image is missing\nor of low quality. To address this issue, we propose DuDoUniNeXt, a unified\ndual-domain MRI reconstruction network that can accommodate to scenarios\ninvolving absent, low-quality, and high-quality reference images. DuDoUniNeXt\nadopts a hybrid backbone that combines CNN and ViT, enabling specific\nadjustment of image domain and k-space reconstruction. Specifically, an\nadaptive coarse-to-fine feature fusion module (AdaC2F) is devised to\ndynamically process the information from reference images of varying qualities.\nBesides, a partially shared shallow feature extractor (PaSS) is proposed, which\nuses shared and distinct parameters to handle consistent and discrepancy\ninformation among contrasts. Experimental results demonstrate that the proposed\nmodel surpasses state-of-the-art SC and MC models significantly. Ablation\nstudies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS,\nand the dual-domain unified learning scheme.'}, {'Deep Reinforcement Learning for Modelling Protein Complexes': 'AlphaFold can be used for both single-chain and multi-chain protein structure\nprediction, while the latter becomes extremely challenging as the number of\nchains increases. In this work, by taking each chain as a node and assembly\nactions as edges, we show that an acyclic undirected connected graph can be\nused to predict the structure of multi-chain protein complexes (a.k.a., protein\ncomplex modelling, PCM). However, there are still two challenges: 1) The huge\ncombinatorial optimization space of $N^{N-2}$ ($N$ is the number of chains) for\nthe PCM problem can easily lead to high computational cost. 2) The scales of\nprotein complexes exhibit distribution shift due to variance in chain numbers,\nwhich calls for the generalization in modelling complexes of various scales. To\naddress these challenges, we propose GAPN, a Generative Adversarial Policy\nNetwork powered by domain-specific rewards and adversarial loss through policy\ngradient for automatic PCM prediction. Specifically, GAPN learns to efficiently\nsearch through the immense assembly space and optimize the direct docking\nreward through policy gradient. Importantly, we design an adversarial reward\nfunction to enhance the receptive field of our model. In this way, GAPN will\nsimultaneously focus on a specific batch of complexes and the global assembly\nrules learned from complexes with varied chain numbers. Empirically, we have\nachieved both significant accuracy (measured by RMSD and TM-Score) and\nefficiency improvements compared to leading PCM softwares.'}, {'Handling Missing Data via Max-Entropy Regularized Graph Autoencoder': 'Graph neural networks (GNNs) are popular weapons for modeling relational\ndata. Existing GNNs are not specified for attribute-incomplete graphs, making\nmissing attribute imputation a burning issue. Until recently, many works notice\nthat GNNs are coupled with spectral concentration, which means the spectrum\nobtained by GNNs concentrates on a local part in spectral domain, e.g.,\nlow-frequency due to oversmoothing issue. As a consequence, GNNs may be\nseriously flawed for reconstructing graph attributes as graph spectral\nconcentration tends to cause a low imputation precision. In this work, we\npresent a regularized graph autoencoder for graph attribute imputation, named\nMEGAE, which aims at mitigating spectral concentration problem by maximizing\nthe graph spectral entropy. Notably, we first present the method for estimating\ngraph spectral entropy without the eigen-decomposition of Laplacian matrix and\nprovide the theoretical upper error bound. A maximum entropy regularization\nthen acts in the latent space, which directly increases the graph spectral\nentropy. Extensive experiments show that MEGAE outperforms all the other\nstate-of-the-art imputation methods on a variety of benchmark datasets.'}]","**Abstract**

Recent advancements in large foundation models (LFMs) have demonstrated impressive performance across multiple domains, including natural language processing (NLP) and computer vision (CV), underscoring the growing prominence of fine-tuning these models for a wide array of downstream tasks. The efficacy of fine-tuning can lead to significant model over-parametrization, resulting in elevated demand for storage of fine-tuned checkpoints, thereby escalating IT infrastructure consumption and strain on storage and network resources. To mitigate this issue, a novel fine-tuning method named FourierFT is developed. 

The primary objective of our research is to introduce FourierFT, leveraging Discrete Fourier Transform for parameter-efficient fine-tuning, thereby offering a scalable and storage-friendly solution that reduces the number of trainable parameters while maintaining competitive performance. This method employs a low-rank matrix decomposition that is versatile for both Continuous Fourier Transform (CFT) and Discrete Fourier Transform (DFT), and its accuracy is enhanced through the judicious selection of Fourier basis vectors to mitigate quantization errors. 

Methodologically, we integrate FourierFT into the fine-tuning process of state-of-the-art LFM models across NLP and CV tasks, comparing its efficacy against the established LoRA method. Across a spectrum of applications, FourierFT demonstrates its ability to realize substantial parameter savings, often achieving performance parity with its LoRA counterpart in fine-tuning outcomes. The research contributions include the development of a robust parameter-efficient fine-tuning method which enables efficient deployment and adaptation of LFM upon less stringent computational resources. Further, the findings reveal the method's scalability with model size, affirming its potential for scaling to ever larger LFMs. 

The potential applications of FourierFT are broad, potentially enabling the fine-tuning of larger models on resource-constrained computing environments while maintaining performance levels comparable to those of over-parametrized models. This not only reduces the infrastructure costs associated with storing fine-tuned models but also opens new avenues for deploying large-scale machine learning models in sectors such as mobile computing, edge computing, and cloud deployment, where resource optimization is critical.

In conclusion, FourierFT addresses the performance vs. parameter efficiency challenge for large foundation model fine-tuning, facilitating wider adoption and broader deployment of such models. The research opens new avenues for future studies to explore further optimization strategies and novel techniques for resource-constrained environments, facilitating democratization of AI technologies."
"In this paper, we introduce Matten, a cutting-edge latent diffusion model
with Mamba-Attention architecture for video generation. With minimal
computational cost, Matten employs spatial-temporal attention for local video
content modeling and bidirectional Mamba for global video content modeling. Our
comprehensive experimental evaluation demonstrates that Matten has competitive
performance with the current Transformer-based and GAN-based models in
benchmark performance, achieving superior FVD scores and efficiency.
Additionally, we observe a direct positive correlation between the complexity
of our designed model and the improvement in video quality, indicating the
excellent scalability of Matten.","[{'Astronomy: Starbursts near and far': 'Observations of intensely bright star-forming galaxies both close by and in\nthe distant Universe at first glance seem to emphasize their similarity. But\nlook a little closer, and differences emerge.'}, {'Represent a natural number as the sum of palindromes in various bases': 'It is shown that the set of palindromes is an additive basis for the natural\nnumbers in any base. Specifically, we prove that every natural number can be\nexpressed as the sum of $O(d)$ palindromes in base $d$.'}, {'The Global Star Formation Law: from Dense Cores to Extreme Starbursts': ""Active star formation (SF) is tightly related to the dense molecular gas in\nthe giant molecular clouds' dense cores. Our HCN (measure of the dense\nmolecular gas) survey in 65 galaxies (including 10 ultraluminous galaxies)\nreveals a tight linear correlation between HCN and IR (SF rate) luminosities,\nwhereas the correlation between IR and CO (measure of the total molecular gas)\nluminosities is nonlinear. This suggests that the global SF rate depends more\nintimately upon the amount of dense molecular gas than the total molecular gas\ncontent. This linear relationship extends to both the dense cores in the Galaxy\nand the hyperluminous extreme starbursts at high-redshift. Therefore, the\nglobal SF law in dense gas appears to be linear all the way from dense cores to\nextreme starbursts, spanning over nine orders of magnitude in IR luminosity.""}, {'A brief report on statistical study of net electric current in solar\n  active regions with longitudinal fields of opposite polarity': 'Dynamic processes occurring in solar active regions are dominated by the\nsolar magnetic field. As of now, observations using a solar magnetograph have\nsupplied us with the vector components of a solar photospheric magnetic field.\nThe two transverse components of a photospheric magnetic field allow us to\ncompute the amount of electric current. We found that the electric current in\nareas with positive (negative) polarity due to the longitudinal magnetic field\nhave both positive and negative signs in an active region, however, the net\ncurrent is found to be an order-of-magnitude less than the mean absolute\nmagnitude and has a preferred sign. In particular, we have statistically found\nthat there is a systematic net electric current from areas with negative\n(positive) polarity to areas with positive (negative) polarity in solar active\nregions in the northern (southern) hemisphere, but during the solar minimum\nthis tendency is reversed over time at some latitudes. The result indicates\nthat there is weak net electric current in areas of solar active regions with\nopposite polarity, thus providing further details about the hemispheric\nhelicity rule found in a series of previous studies.'}, {'A Quantity Characterising Variation of Observed Magnetic Twist of Solar\n  Active Regions': 'An alternative parameter $R_{J_z}$ is introduced as the ratio of one of two\nkinds of opposite-sign current to the total current and investigate the\nrelationship between the quantity and the hemispheric sign rule of helicity\n(HSR) that is established by a series of previous statistical studies. The\nclassification of current in each hemisphere is according to the following\nrule: If the product of the current and the corresponding longitudinal field\ncomponent contributes a consistent sign with reference to the HSR, it is called\n""HSR-compliant"" current, or else it is called ""HSR-noncompliant"" current.\nFirstly, the consistence between the butterfly diagram of the $R_{J_z}$ and the\ncurrent helicity was obtained in a statistical study. Active regions with\n$R_{J_z}$ smaller than 0.5 tend to obey the HSR whereas those with $R_{J_z}$\ngreater than 0.5 tend to disobey the HSR. The ""HSR-compliant"" current systems\nhave 60\\% probability of realization compared to 40\\% of ""HSR-noncompliant""\ncurrent systems. Overall, the HSR is violated for active regions in which the\n""HSR-noncompliant"" current is greater than the ""HSR-compliant"" current.\nSecondly, the $R_{J_z}$ parameter was subsequently used to study the evolution\nof current systems in the case analyses of flare-productive active regions NOAA\nAR 11158 and 11283. It is found that there were ""$R_{J_z}$-quasi-stationary""\nphase that is relatively flare quiescent and ""$R_{J_z}$-dynamic"" phase that is\ncovered by the occurrence of large flares.'}, {'The Radio Continuum, Far-Infrared Emission, And Dense Molecular Gas In\n  Galaxies': ""A tight linear correlation is established between the HCN line luminosity and\nthe radio continuum (RC) luminosity for a sample of 65 galaxies (from Gao &\nSolomon's HCN survey), including normal spiral galaxies and luminous and\nultraluminous infrared galaxies (LIRGs/ULIRGs). After analyzing the various\ncorrelations among the global far-infrared (FIR), RC, CO, and HCN luminosities\nand their various ratios, we conclude that the FIR-RC and FIR-HCN correlations\nappear to be linear and are the tightest among all correlations. The\ncombination of these two correlations could result in the tight RC-HCN\ncorrelation we observed. Meanwhile, the non-linear RC-CO correlation shows\nslightly larger scatter as compared with the RC-HCN correlation, and there is\nno correlation between ratios of either RC/HCN-CO/HCN or RC/FIR-CO/FIR. In\ncomparison, a meaningful correlation is still observed between ratios of\nRC/CO-HCN/CO. Nevertheless, the correlation between RC/FIR and HCN/FIR also\ndisappears, reflecting again the two tightest FIR-RC and FIR-HCN correlations\nas well as suggesting that FIR seems to be the bridge that connects HCN with\nRC. Interestingly, despite obvious HCN-RC and RC-CO correlations,\nmulti-parameter fits hint that while both RC and HCN contribute significantly\n(with no contribution from CO) to FIR, yet RC is primarily determined from FIR\nwith a very small contribution from CO and essentially no contribution from\nHCN. These analyses confirm independently the former conclusions that it is\npractical to use RC luminosity instead of FIR luminosity, at least globally, as\nan indicator of star formation rate in galaxies including LIRGs/ULIRGs, and HCN\nis a much better tracer of star-forming molecular gas and correlates with FIR\nmuch better than that of CO.""}, {'New Physics Opportunities in Triangle Singularity': 'We show that loop-induced processes involving new physics particles can\nreadily satisfy Landau Equation and trigger triangular singularities at high\nenergy colliders, leading to fully visible Standard Model final states.\nFour-particle vertices in new physics allow triangular singularity diagrams to\nevade large virtuality suppression. In addition, a $t$-channel triangular\nsingularity can also occur in particle scattering processes that may extend to\nlow momentum exchange. We discuss several typical scenarios in supersymmetric\nand extended Higgs models, then identify the singular component in the\nloop-integral amplitude at specific external momentum points.'}, {'Probing Light Nonthermal Dark Matter at the LHC': 'This paper investigates the collider phenomenolgy of a minimal nonthermal\ndark matter model with a 1-GeV dark matter candidate, which naturally explain\nbaryongensis. Since the light dark matter is not parity-protected, it can be\nsingly produced at the LHC. This leads to large missing energy associated with\nan energetic jet whose transverse momentum distribution is featured by a\nJacobian-like shape. The monojet, dijet, paired dijet and 2 jets + missing\nenergy channels are studied. Currently existing data at Tevatron and LHC offer\nsignificant bounds on our model.'}, {'Large time behavior, bi-Hamiltonian structure and kinetic formulation\n  for complex Burgers equation': ""We prove the existence and uniqueness of positive analytical solutions with\npositive initial data to the mean field equation (the Dyson equation) of the\nDyson Brownian motion through the complex Burgers equation with a force term on\nthe upper half complex plane. These solutions converge to a steady state given\nby Wigner's semicircle law. A unique global weak solution with nonnegative\ninitial data to the Dyson equation is obtained and some explicit solutions are\ngiven by Wigner's semicircle laws. We also construct a bi-Hamiltonian structure\nfor the system of the real and imaginary components of the complex Burgers\nequation (coupled Burgers system). We establish a kinetic formulation for the\ncoupled Burgers system and prove the existence and uniqueness of entropy\nsolutions. The coupled Burgers system in Lagrangian variable naturally leads to\ntwo interacting particle systems: Fermi-Pasta-Ulam-Tsingou model with\nnearest-neighbor interactions, and Calogero-Moser model. These two particle\nsystems yield the same Lagrangian dynamics in the continuum limit.""}, {'Comparative study of the relationships between CO isotopic luminosities\n  and infrared luminosity for the Galactic dense cores': 'Combining the 12CO(1-0), 13CO(1-0), and C18O(1-0) data with IRAS four band\ndata, we here estimate the physical parameters such as size, viral mass, and CO\nJ=1-0 isotopic and infrared luminosities for 29 dense molecular clouds from two\npublished CO samples. We further analyze the various correlations between CO\nJ=1-0 isotopic luminosities and infrared luminosity (star formation rate, SFR)\nand discuss the relationships between the molecular gas tracers and SFR. The\nresults show that 12CO(1-0), 13CO(1-0) and C18O(1-0) luminosities have tight\ncorrelations with each other. CO J=1-0 isotopic luminosities and SFR show weak\ncorrelations with lager scatter than the HCN-IR correlations of 47 dense cores\nin the Galaxy and 65 external star-forming galaxies. This might be interpreted\nas that both the SFR and star formation efficiency are mainly determined by the\nmolecular gas at high volume density rather than high column density.'}]","Title: Enhancing Mamba-based Video Diffusion Models: Analysis, Innovations, and Applications

Abstract:

Understanding and advancing the efficacy of video diffusion models, particularly in enhancing real-world applications with high-precision and contextual relevancy, forms the core focus of this research. The objective is to explore the optimization strategies for the Mamba-based models specifically addressing the FVD metrics on the SkyTimelapse dataset. Innovations in this paper include explorations focused on model大小微调, leveraging the self-attention mechanisms, scalable parallel scanning, and novel ways of integrating temporal-class information. Implemented methods analyze a range of Mamba model variants, varying in complexity to identify those yielding improved performance metrics, with a significant emphasis on localized and global information integration. The findings show substantial improvements in synthesized video generation quality and computational efficiency, accurately matching desired video characteristics. 

Contributions are multifaceted, including advancements in scalability and parallel processing capabilities of the Mamba architecture, demonstration of a superior SELF-ATTENTION-based model variant, and systematic analysis of the impact of varying model complexity, spatial information, and temporal-class actions in the diffusion and denoising process. The paper also introduces a new benchmark for Mamba-based models, establishing them as key players in the generation of visually appealing content for multiple applications, from VR content creation to real-time video enhancement.

This research promises to significantly impact the field of video analysis, synthesis, and processing, offering practical solutions and insights for developers, researchers, and industry professionals looking to harness the capabilities of diffusion models in multimedia applications, thus broadening the scope of high-quality content generation within the realm of digital entertainment."
"This study investigates the impact of magnetic turbulence on cosmic ray (CR)
electrons through Fermi-II acceleration behind merger-driven shocks in the
intracluster medium and examines how the ensuing synchrotron radio emission is
influenced by the decay of magnetic energy through dissipation in the postshock
region. We adopt simplified models for the momentum diffusion coefficient,
specifically considering transit-time-damping resonance with fast-mode waves
and gyroresonance with Alfv\'en waves. Utilizing analytic solutions derived
from diffusive shock acceleration theory, at the shock location, we introduce a
CR spectrum that is either shock-injected or shock-reaccelerated. We then track
its temporal evolution along the Lagrangian fluid element in the time domain.
The resulting CR spectra are mapped onto a spherical shell configuration to
estimate the surface brightness profile of the model radio relics. Turbulent
acceleration proves to be a significant factor in delaying the aging of
postshock CR electrons, while decaying magnetic fields have marginal impacts
due to the dominance of inverse Compton cooling over synchrotron cooling.
However, the decay of magnetic fields substantially reduces synchrotron
radiation. Consequently, the spatial distribution of the postshock magnetic
fields affects the volume-integrated radio spectrum and its spectral index. We
demonstrate that the Mach numbers estimated from the integrated spectral index
tend to be higher than the actual shock Mach numbers, highlighting the
necessity for accurate modeling of postshock magnetic turbulence in
interpreting observations of radio relics.","[{'Present Status of Diffusive Shock Acceleration': 'Diffusive shock acceleration (DSA) is now widely accepted as the model to\nexplain the production of cosmic rays (CRs) in a wide range of astrophysical\nenvironments. Despite initial successes of the theory in explaining the\nenergetics and the spectrum of CRs accelerated by supernova remnants, there\nstill remain some unresolved issues such as particle injection out of the\nthermal plasma at shocks, CR diffusion due to the self-generated MHD waves and\nyet-to-be-detected gamma-ray emission due to the ionic CRs. Recent technical\nadvancements to resolve these issues are reviewed.'}, {'Re-acceleration of Cosmic Ray Electrons by Multiple ICM Shocks': 'Radio relics could be generated by multiple shocks induced in the turbulent\nintracluster medium during galaxy mergers. Kang (2021) demonstrated that the\nre-acceleration of cosmic ray (CR) protons via diffusive shock acceleration\n(DSA) by multiple shocks could enhance the acceleration efficiency and flatten\nthe CR spectrum, compared to a single episode of DSA. Here we examine the CR\nelectron acceleration through multiple re-acceleration by considering energy\nlosses and decompression of the particle distribution and magnetic fields in\nthe postshock region between consecutive shock passages. We find that the\naccumulated effects of repeated re-acceleration are significant, if preceding\nshocks are stronger than the last shock and the shock passage interval is\n$\\lesssim20$ Myr. In such cases, both the CR spectrum and the ensuing radiation\nspectrum behind the last shock are enhanced and become flatter than the\ncanonical DSA power-law forms. As a result, the shock Mach number estimated\nfrom radio observations tends be higher than the actual Mach number of the last\nshock. Thus, multiple episodes of DSA may explain the enhanced acceleration\nefficiency for CR electrons and the discrepancy of shock Mach numbers, $M_{\\rm\nX} \\lesssim M_{\\rm rad}$, inferred for some observed radio relics.'}, {'Radio emission from weak spherical shocks in the outskirts of galaxy\n  clusters': 'In Kang (2015) we calculated the acceleration of cosmic-ray electrons and the\nensuing radio synchrotron emission at weak spherical shocks that are expected\nto form in the outskirts of galaxy clusters.There we demonstrated that, at\ndecelerating spherical shocks, the volume integrated spectra of both electrons\nand radiation deviate significantly from the test-particle power-laws predicted\nfor constant planar shocks, because the shock compression ratio and the flux of\ninjected electrons decrease in time. In this study, we consider spherical blast\nwaves propagating into a constant density core surrounded by an isothermal halo\nwith a decreasing density profile in order to explore how the deceleration rate\nof the shock speed affects the radio emission from accelerated electrons. The\nsurface brightness profile and the volume-integrated radio spectrum of the\nmodel shocks are calculated by assuming a ribbon-like shock surface on a\nspherical shell and the associated downstream region of relativistic electrons.\nIf the postshock magnetic field strength is about 7 microgauss, at the shock\nage of ~50 Myr, the volume-integrated radio spectrum steepens gradually with\nthe spectral index from alpha_{inj} to alpha_{inj}+0.5 over 0.1-10 GHz, where\nalpha_{inj} is the injection index at the shock positionexpected from the\ndiffusive shock acceleration theory. Such gradual steepening could explain the\ncurved radio spectrum of the radio relic in cluster A2266, which was\ninterpreted as a broken power-law by Trasatti et al. (2014), if the relic shock\nis young enough so that the break frequency falls in around 1 GHz.'}, {'Cosmic ray acceleration at blast waves from type Ia supernovae': 'We have calculated the cosmic ray (CR) acceleration at young remnants from\nType Ia supernovae expanding into a uniform interstellar medium (ISM). Adopting\nquasi-parallel magnetic fields, gasdynamic equations and the diffusion\nconvection equation for the particle distribution function are solved in a\ncomoving spherical grid which expands with the shock. Bohm-type diffusion due\nto self-excited Alfven waves, drift and dissipation of these waves in the\nprecursor and thermal leakage injection were included. With magnetic fields\namplified by the CR streaming instability, the particle energy can reach up to\n10^{16}Z eV at young supernova remnants (SNRs) of several thousand years old.\nThe fraction of the explosion energy transferred to the CR component asymptotes\nto 40-50 % by that time. For a typical SNR in a warm ISM, the accelerated CR\nenergy spectrum should exhibit a concave curvature with the power-law slope\nflattening from 2 to 1.6 at E>0.1 TeV.'}, {'Energy Spectrum Of Nonthermal Electrons Accelerated At A Plane Shock': 'We calculate the energy spectra of cosmic ray (CR) protons and electrons at a\nplane shock with quasi-parallel magnetic fields, using time-dependent,\ndiffusive shock acceleration (DSA) simulations, including energy losses via\nsynchrotron emission and Inverse Compton (IC) scattering. A thermal leakage\ninjection model and a Bohm type diffusion coefficient are adopted. The electron\nspectrum at the shock becomes steady after the DSA energy gains balance the\nsynchrotron/IC losses, and it cuts off at the equilibrium momentum p_{eq}. In\nthe postshock region the cutoff momentum of the electron spectrum decreases\nwith the distance from the shock due to the energy losses and the thickness of\nthe spatial distribution of electrons scales as p^{-1}. Thus the slope of the\ndownstream integrated spectrum steepens by one power of p for p_{br}<p<p_{eq},\nwhere the break momentum decrease with the shock age as p_{br}\\propto t^{-1}.\nIn a CR modified shock, both the proton and electron spectrum exhibit a concave\ncurvature and deviate from the canonical test-particle power-law, and the\nupstream integrated electron spectrum could dominate over the downstream\nintegrated spectrum near the cutoff momentum. Thus the spectral shape near the\ncutoff of X-ray synchrotron emission could reveal a signature of nonlinear DSA.'}, {'Cosmic Ray Spectrum in Supernova Remnant Shocks': ""We performed kinetic simulations of diffusive shock acceleration in Type Ia\nsupernova remnants (SNRs) expanding into a uniform interstellar medium (ISM).\nThe preshock gas temperature is the primary parameter that governs the cosmic\nray (CR) acceleration, while magnetic field strength and CR injection rate are\nsecondary parameters. SNRs in the hot ISM, with an injection fraction smaller\nthan 10^{-4}, are inefficient accelerators with less than 10 % energy getting\nconverted to CRs. The shock structure is almost test-particle like and the\nensuing CR spectrum can be steeper than E^{-2}. Although the particles can be\naccelerated to the knee energy of 10^{15.5}Z eV with amplified magnetic fields\nin the precursor, Alfv'enic drift of scattering centers softens the source\nspectrum as steep as E^{-2.1} and reduces the CR acceleration efficiency.""}, {'Nonthermal radiation from relativistic electrons accelerated at\n  spherically expanding shocks': 'We study the evolution of the energy spectrum of cosmic-ray electrons\naccelerated at spherically expanding shocks with low Mach numbers and the\nensuing spectral signatures imprinted in radio synchrotron emission.\nTime-dependent simulations of diffusive shock acceleration (DSA) of electrons\nin the test-particle limit have been performed for spherical shocks with\nparameters relevant for typical shocks in the intracluster medium. The electron\nand radiation spectra at the shock location can be described properly by the\ntest-particle DSA predictions with instantaneous shock parameters. However, the\nvolume integrated spectra of both electrons and radiation deviate significantly\nfrom the test-particle power-laws, because the shock compression ratio and the\nflux of injected electrons at the shock gradually decrease as the shock slows\ndown in time.So one needs to be cautious about interpreting observed radio\nspectra of evolving shocks based on simple DSA models in the test-particle\nregime.'}, {'Re-acceleration model for the ""Toothbrush"" Radio Relic': 'The Toothbrush radio relic associated the merging cluster 1RXS J060303.3 is\npresumed to be produced by relativistic electrons accelerated at merger-driven\nshocks. Since the shock Mach number inferred from the observed radio spectral\nindex, $M_{radio}\\approx 2.8$, is larger than that estimated from X-ray\nobservations, $M_{X-ray}\\lesssim 1.5$, we consider the re-acceleration model in\nwhich a weak shock of $M_s\\approx 1.2-1.5$ sweeps through the intracluster\nplasma with a preshock population of relativistic electrons. We find the models\nwith a power-law momentum spectrum with the slope, $s\\approx 4.6$, and the\ncutoff Lorentz factor, $\\gamma_{e,c}\\approx 7-8\\times 10^4$ can reproduce\nreasonably well the observed profiles of radio fluxes and integrated radio\nspectrum of the head portion of the Toothbrush relic. This study confirms the\nstrong connection between the ubiquitous presence of fossil relativistic plasma\noriginated from AGNs and the shock-acceleration model of radio relics in the\nintracluster medium.'}, {""Re-acceleration model for the `Sausage' Radio Relic"": 'The Sausage radio relic is the arc-like radio structure in the cluster CIZA\nJ2242.8+5301, whose observed properties can be best understood by synchrotron\nemission from relativistic electrons accelerated at a merger-driven shock.\nHowever, there remain a few puzzles that cannot be explained by the shock\nacceleration model with only in-situ injection. In particular, the Mach number\ninferred from the observed radio spectral index, $M_{\\rm radio}\\approx 4.6$,\nwhile the Mach number estimated from X-ray observations, $M_{\\rm X-ray}\\approx\n2.7$. In an attempt to resolve such a discrepancy, here we consider the\nre-acceleration model in which a shock of $M_s\\approx 3$ sweeps through the\nintracluster gas with a pre-existing population of relativistic electrons. We\nfind that observed brightness profiles at multi frequencies provide strong\nconstraints on the spectral shape of pre-existing electrons. The models with a\npower-law momentum spectrum with the slope, $s\\approx 4.1$, and the cutoff\nLorentz factor, $\\gamma_{e,c}\\approx 3-5\\times 10^4$ can reproduce reasonably\nwell the observed spatial profiles of radio fluxes and integrated radio\nspectrum of the Sausage relic. The possible origins of such relativistic\nelectrons in the intracluster medium remain to be investigated further.'}, {'Shock Acceleration Model with Postshock Turbulence for Giant Radio\n  Relics': 'We explore the shock acceleration model for giant radio relics, in which\nrelativistic electrons are accelerated via diffusive shock acceleration (DSA)\nby merger-driven shocks in the outskirts of galaxy clusters. In addition to\nDSA, turbulent acceleration by compressive MHD mode downstream of the shock is\nincluded as well as energy losses of postshock electrons by Coulomb scattering,\nsynchrotron emission, and inverse Compton scattering off the cosmic background\nradiation. Considering that only a small fraction of merging clusters host\nradio relics, we favor the reacceleration scenario in which radio relics are\ngenerated preferentially when shocks encounter the regions containing\nlow-energy ($\\gamma_{\\rm e} \\lesssim 300$) cosmic ray electrons (CRe). We\nperform time-dependent DSA simulations of spherically expanding shocks with\nphysical parameters relevant for the Sausage radio relic, and calculate the\nradio synchrotron emission from the accelerated CRe. We find that significant\nlevel of postshock turbulent acceleration is required in order to reproduce\nbroad profiles of the observed radio flux densities of the Sausage relic.\nMoreover, the spectral curvature in the observed integrated radio spectrum can\nbe explained, if the putative shock should have swept up and exited out of the\npreshock region of fossil CRe about 10~Myr ago.'}]","Title: Influence of Postshock Turbulence on Spectra of Radio Relics

Abstract:
In the context of understanding the origin of radio emissions in galaxy clusters, particularly in the weakly magnetized intracluster medium, this study presents a novel investigation into the role of postshock turbulence in shaping the properties of radio relics. Employing a unique model of spherical, postshock, CR electron acceleration, this research elucidates how turbulence, asymmetry, and decay affect the spectra and spatial profiles of radio relics. The primary objective is to examine if additional particle acceleration mechanisms, such as always turbulent postshocks, can reproduce the observed fainter spectral peaks in observational data where classical methods describe intensity peaks.

Innovatively, our model includes an extension method that accurately predicts the correlation between magnetic field and particle spectra, transitioning smoothly from the injection phase in the relativistic shock to the postshock cooling phase at a transition scale of 20–100 kpc. Key methodologies involve solving Fokker-Planck equations for CR electron transport and developing tools to calculate space-dependent intensity profiles within the model.

The results indicate that the existence of both always turbulent and decaying turbulent postshocks can influence the peak intensity and spectral index of radio relics, with decaying turbulence leading to a more gradual decrease in intensity. When compared to decaying magnetic fields only, the presence of always turbulent magnetic fields enhances the strength and broadens the peak of radio emissions. These findings contribute to the ongoing quest for understanding the astrophysical processes governing the generation and evolution of radio relics.

Applicably, tracking the evolution of particle spectra under different turbulence conditions can shed light on the conditions and history of mergers in clusters hosting these relics. These results are significant as they could provide insights into the complex interplay between astrophysical phenomena in the vicinity of major galaxy cluster mergers, potentially leading towards a more comprehensive theory of radio relic generation."
"The paper introduces AniTalker, an innovative framework designed to generate
lifelike talking faces from a single portrait. Unlike existing models that
primarily focus on verbal cues such as lip synchronization and fail to capture
the complex dynamics of facial expressions and nonverbal cues, AniTalker
employs a universal motion representation. This innovative representation
effectively captures a wide range of facial dynamics, including subtle
expressions and head movements. AniTalker enhances motion depiction through two
self-supervised learning strategies: the first involves reconstructing target
video frames from source frames within the same identity to learn subtle motion
representations, and the second develops an identity encoder using metric
learning while actively minimizing mutual information between the identity and
motion encoders. This approach ensures that the motion representation is
dynamic and devoid of identity-specific details, significantly reducing the
need for labeled data. Additionally, the integration of a diffusion model with
a variance adapter allows for the generation of diverse and controllable facial
animations. This method not only demonstrates AniTalker's capability to create
detailed and realistic facial movements but also underscores its potential in
crafting dynamic avatars for real-world applications. Synthetic results can be
viewed at https://github.com/X-LANCE/AniTalker.","[{'BHN: A Brain-like Heterogeneous Network': 'The human brain works in an unsupervised way, and more than one brain region\nis essential for lighting up intelligence. Inspired by this, we propose a\nbrain-like heterogeneous network (BHN), which can cooperatively learn a lot of\ndistributed representations and one global attention representation. By\noptimizing distributed, self-supervised, and gradient-isolated objective\nfunctions in a minimax fashion, our model improves its representations, which\nare generated from patches of pictures or frames of videos in experiments.'}, {'MLPs to Find Extrema of Functionals': 'Multilayer perceptron (MLP) is a class of networks composed of multiple\nlayers of perceptrons, and it is essentially a mathematical function. Based on\nMLP, we develop a new numerical method to find the extrema of functionals. As\ndemonstrations, we present our solutions in three physic scenes. Ideally, the\nsame method is applicable to any cases where the objective curve/surface can be\nfitted by second-order differentiable functions. This method can also be\nextended to cases where there are a finite number of non-differentiable (but\ncontinuous) points/surfaces.'}, {'A Biologically Plausible Learning Rule for Perceptual Systems of\n  organisms that Maximize Mutual Information': 'It is widely believed that the perceptual system of an organism is optimized\nfor the properties of the environment to which it is exposed. A specific\ninstance of this principle known as the Infomax principle holds that the\npurpose of early perceptual processing is to maximize the mutual information\nbetween the neural coding and the incoming sensory signal. In this article, we\npresent a method to implement this principle accurately with a local,\nspike-based, and continuous-time learning rule.'}, {'Adaptive Leader-Following Consensus for Uncertain Euler-Lagrange Systems\n  under Directed Switching Networks': 'The leader-following consensus problem for multiple Euler-Lagrange systems\nwas studied recently by the adaptive distributed observer approach under the\nassumptions that the leader system is neurally stable and the communication\nnetwork is jointly connected and undirected. In this paper, we will study the\nsame problem without assuming that the leader system is neutrally stable, and\nthe communication network is undirected. The effectiveness of this new result\nwill be illustrated by an example.'}, {'Interior inverse problem for global conservative multipeakon solutions\n  of the Camassa-Holm equation': 'We consider the interior inverse problem associated with the global\nconservative {multipeakon} solution of the Camassa-Holm equation. Based on the\ninverse spectral theory on the half-line and the oscillation property of\neigenfunctions, some {(non)}uniqueness results of {the} interior inverse\nproblem are obtained. In addition, we give the trace formula, which connects\nthe global conservative {multipeakon} solution with the corresponding\neigenvalues and normalized eigenfunctions.'}, {'Feedback Capacity of Stationary Gaussian Channels Further Examined': 'It is well known that the problem of computing the feedback capacity of a\nstationary Gaussian channel can be recast as an infinite-dimensional\noptimization problem; moreover, necessary and sufficient conditions for the\noptimality of a solution to this optimization problem have been characterized,\nand based on this characterization, an explicit formula for the feedback\ncapacity has been given for the case that the noise is a first-order\nautoregressive moving-average Gaussian process. In this paper, we further\nexamine the above-mentioned infinite-dimensional optimization problem. We prove\nthat unless the Gaussian noise is white, its optimal solution is unique, and we\npropose an algorithm to recursively compute the unique optimal solution, which\nis guaranteed to converge in theory and features an efficient implementation\nfor a suboptimal solution in practice. Furthermore, for the case that the noise\nis a k-th order autoregressive moving-average Gaussian process, we give a\nrelatively more explicit formula for the feedback capacity; more specifically,\nthe feedback capacity is expressed as a simple function evaluated at a solution\nto a system of polynomial equations, which is amenable to numerical computation\nfor the cases k=1, 2 and possibly beyond.'}, {'Discrete-Time Distributed Observers over Jointly Connected Switching\n  Networks and an Application': 'In this paper, we first establish an exponential stability result for a class\nof linear switched systems and then apply this result to show the existence of\nthe distributed observer for a discrete-time leader system over jointly\nconnected switching networks. A special case of this result leads to the\nsolution of a leader-following consensus problem of multiple discrete-time\ndouble-integrator systems over jointly connected switching networks. Then, we\nfurther develop the adaptive distributed observer for the discrete-time leader\nsystem over jointly connected switching networks, which has the advantage over\nthe distributed observer in that it does not require that every follower know\nthe system matrix of the leader system. As an application of the discrete-time\ndistributed observer, we will solve the cooperative output regulation problem\nof a discrete-time linear multi-agent system over jointly connected switching\nnetworks. A leader-following formation problem of mobile robots will be used to\nillustrate our design. This problem cannot be handled by any existing approach.'}, {'Distributed exponential state estimation of linear systems over jointly\n  connected switching networks': ""Recently, the distributed state estimation problem for continuous-time linear\nsystems over jointly connected switching networks was solved. It was shown that\nthe estimation errors will asymptotically converge to the origin by using the\ngeneralized Barbalat's Lemma. This paper further studies the same problem with\ntwo new features. First, the asymptotic convergence is strengthened to the\nexponential convergence. This strengthened result not only offers a guaranteed\nconvergence rate, but also renders the error system total stability and thus is\nable to withstand small disturbances. Second, the coupling gains of our local\nobservers can be distinct and thus offers greater design flexibility, while the\ncoupling gains in the existing result were required to be identical. These two\nnew features are achieved by establishing exponential stability for two classes\nof linear time-varying systems, which may have other applications.""}, {'BER: Balanced Error Rate For Speaker Diarization': ""DER is the primary metric to evaluate diarization performance while facing a\ndilemma: the errors in short utterances or segments tend to be overwhelmed by\nlonger ones. Short segments, e.g., `yes' or `no,' still have semantic\ninformation. Besides, DER overlooks errors in less-talked speakers. Although\nJER balances speaker errors, it still suffers from the same dilemma.\nConsidering all those aspects, duration error, segment error, and\nspeaker-weighted error constituting a complete diarization evaluation, we\npropose a Balanced Error Rate (BER) to evaluate speaker diarization. First, we\npropose a segment-level error rate (SER) via connected sub-graphs and adaptive\nIoU threshold to get accurate segment matching. Second, to evaluate diarization\nin a unified way, we adopt a speaker-specific harmonic mean between duration\nand segment, followed by a speaker-weighted average. Third, we analyze our\nmetric via the modularized system, EEND, and the multi-modal method on real\ndatasets. SER and BER are publicly available at https://github.com/X-LANCE/BER.""}, {'Robust Data-Driven Predictive Control for Unknown Linear Time-Invariant\n  Systems': 'This paper presents a new robust data-driven predictive control scheme for\nunknown linear time-invariant systems by using input-state-output or\ninput-output data based on whether the state is measurable. To remove the need\nfor the persistently exciting (PE) condition of a sufficiently high order on\npre-collected data, a set containing all systems capable of generating such\ndata is constructed. Then, at each time step, an upper bound of a given\nobjective function is derived for all systems in the set, and a feedback\ncontroller is designed to minimize this bound. The optimal control gain at each\ntime step is determined by solving a set of linear matrix inequalities. We\nprove that if the synthesis problem is feasible at the initial time step, it\nremains feasible for all future time steps. Unlike current data-driven\npredictive control schemes based on behavioral system theory, our approach\nrequires less stringent conditions for the pre-collected data, facilitating\neasier implementation. Further, the proposed predictive control scheme features\nan infinite prediction horizon, potentially resulting in superior overall\ncontrol performance compared to existing methods with finite prediction\nhorizons. The effectiveness of our proposed methods is demonstrated through\napplication to an unknown and unstable batch reactor.'}]","Title: Advancing Face Reenactment and Speech-to-Motion: AniTalker System for Diverse and Accurate Expression Reenactment

Abstract: Face reenactment technologies transform a source person’s expressions onto an identity-matched target person, while speech-to-motion systems allow the conversion of verbal scripts into coherent and natural facial movements. Despite significant advancements, current systems struggle with the decoupled representation of identity and expression, leading to stark visual inconsistencies. 

In this research, AniTalker system is presented to address the challenge by integrating a universal motion representation to decouple identity and expression, creating an animated system for expressive face reenactment. The system's innovative contributions involve the development of a motion representation handling repository with diverse expression styles, a hierarchical identity encoder to learn diverse identity embeddings, and a motion generator for multimodal generation.

In the experimental phase, AniTalker incorporates key methodologies and techniques such as LIA for robust neural reenactment, CLUB for mutual information disentanglement, and Variance Adapter for adaptable style control. The system is benchmarked against leading face and speech-driven systems, showcasing superior performance improvements in structural and perceptual image quality, natural lip-sync accuracy, and low motion jittering, supporting the claim that the representation is versatile across individuals.

The confirmed contributions to the field of face reenactment and speech-to-motion generation underpinned by AniTalker, consist of the novel movement in unifying the representation of identity and expression for more realistic-looking animations. It has the potential to empower fields like entertainment, virtual reality, and education by enabling more fluid and convincing content creation.

In conclusion, AniTalker advances the state of the art in face and speech-driven image processing, offering unique applications for the enhancement of human-computer interaction experiences. The system's progressive improvements manifest characteristically in increased adaptability and consistency among its outputs, making a substantial contribution to the field of digital human representation, particularly through optimized cross肶lication and improved fidelity in animated content generation."
"Mixture-of-experts (MoE) models facilitate efficient scaling; however,
training the router network introduces the challenge of optimizing a
non-differentiable, discrete objective. Recently, a fully-differentiable MoE
architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges
experts in the parameter space; nevertheless, its effectiveness was only
demonstrated in downstream fine-tuning on classification tasks. In this paper,
we present Lory, the first approach that scales such architectures to
autoregressive language model pre-training. Lory introduces two key techniques:
(1) a causal segment routing strategy that achieves high efficiency for expert
merging operations while preserving the autoregressive nature of language
models; (2) a similarity-based data batching method that encourages expert
specialization by grouping similar documents in training instances. We
pre-train a series of Lory models on 150B tokens from scratch, with up to 32
experts and 30B (1.5B active) parameters. Experimental results show significant
performance gains over parameter-matched dense models on both perplexity
(+13.9%) and a variety of downstream tasks (+1.5%-11.1%). Despite segment-level
routing, Lory models achieve competitive performance compared to
state-of-the-art MoE models with token-level routing. We further demonstrate
that the trained experts in Lory capture domain-level specialization without
supervision. Our work highlights the potential of fully-differentiable MoE
architectures for language model pre-training and advocates future research in
this area.","[{'A Frustratingly Easy Approach for Entity and Relation Extraction': 'End-to-end relation extraction aims to identify named entities and extract\nrelations between them. Most recent work models these two subtasks jointly,\neither by casting them in one structured prediction framework, or performing\nmulti-task learning through shared representations. In this work, we present a\nsimple pipelined approach for entity and relation extraction, and establish the\nnew state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC),\nobtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint\nmodels with the same pre-trained encoders. Our approach essentially builds on\ntwo independent encoders and merely uses the entity model to construct the\ninput for the relation model. Through a series of careful examinations, we\nvalidate the importance of learning distinct contextual representations for\nentities and relations, fusing entity information early in the relation model,\nand incorporating global context. Finally, we also present an efficient\napproximation to our approach which requires only one pass of both entity and\nrelation encoders at inference time, achieving an 8-16$\\times$ speedup with a\nslight reduction in accuracy.'}, {'Training Language Models with Memory Augmentation': 'Recent work has improved language models (LMs) remarkably by equipping them\nwith a non-parametric memory component. However, most existing approaches only\nintroduce mem-ories at testing time or represent them using a separately\ntrained encoder, resulting in suboptimal training of the language model. In\nthis work, we present TRIME, a novel yet simple training approach designed for\ntraining LMs with memory augmentation. Our approach uses a training objective\nthat directly takes in-batch examples as accessible memory. We also present new\nmethods for memory construction and data batching, which are used for adapting\nto different sets of memories--local, long-term, and external memory--at\ntesting time. We evaluate TRIME on multiple language modeling and machine\ntranslation benchmarks and show that it is able to achieve significant\nimprovements across all the settings. Concretely, TRIME reduces the perplexity\nfrom 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory\nset from the training corpus. Compared to standard LM training, TRIME adds\nnegligible computational overhead and is compatible with different neural\narchitectures, making it a versatile solution for training memory-augmented\nLMs.'}, {'Simple Entity-Centric Questions Challenge Dense Retrievers': 'Open-domain question answering has exploded in popularity recently due to the\nsuccess of dense retrieval models, which have surpassed sparse models using\nonly a few supervised training examples. However, in this paper, we demonstrate\ncurrent dense models are not yet the holy grail of retrieval. We first\nconstruct EntityQuestions, a set of simple, entity-rich questions based on\nfacts from Wikidata (e.g., ""Where was Arve Furset born?""), and observe that\ndense retrievers drastically underperform sparse methods. We investigate this\nissue and uncover that dense retrievers can only generalize to common entities\nunless the question pattern is explicitly observed during training. We discuss\ntwo simple solutions towards addressing this critical problem. First, we\ndemonstrate that data augmentation is unable to fix the generalization problem.\nSecond, we argue a more robust passage encoder helps facilitate better question\nadaptation using specialized question encoders. We hope our work can shed light\non the challenges in creating a robust, universal dense retriever that works\nwell across different input distributions.'}, {'Factual Probing Is [MASK]: Learning vs. Learning to Recall': 'Petroni et al. (2019) demonstrated that it is possible to retrieve world\nfacts from a pre-trained language model by expressing them as cloze-style\nprompts and interpret the model\'s prediction accuracy as a lower bound on the\namount of factual information it encodes. Subsequent work has attempted to\ntighten the estimate by searching for better prompts, using a disjoint set of\nfacts as training data. In this work, we make two complementary contributions\nto better understand these factual probing techniques. First, we propose\nOptiPrompt, a novel and efficient method which directly optimizes in continuous\nembedding space. We find this simple method is able to predict an additional\n6.4% of facts in the LAMA benchmark. Second, we raise a more important\nquestion: Can we really interpret these probing results as a lower bound? Is it\npossible that these prompt-search methods learn from the training data too? We\nfind, somewhat surprisingly, that the training data used by these methods\ncontains certain regularities of the underlying fact distribution, and all the\nexisting prompt methods, including ours, are able to exploit them for better\nfact prediction. We conduct a set of control experiments to disentangle\n""learning"" from ""learning to recall"", providing a more detailed picture of what\ndifferent prompts can reveal about pre-trained language models.'}, {'Poisoning Retrieval Corpora by Injecting Adversarial Passages': 'Dense retrievers have achieved state-of-the-art performance in various\ninformation retrieval tasks, but to what extent can they be safely deployed in\nreal-world applications? In this work, we propose a novel attack for dense\nretrieval systems in which a malicious user generates a small number of\nadversarial passages by perturbing discrete tokens to maximize similarity with\na provided set of training queries. When these adversarial passages are\ninserted into a large retrieval corpus, we show that this attack is highly\neffective in fooling these systems to retrieve them for queries that were not\nseen by the attacker. More surprisingly, these adversarial passages can\ndirectly generalize to out-of-domain queries and corpora with a high success\nattack rate -- for instance, we find that 50 generated passages optimized on\nNatural Questions can mislead >94% of questions posed in financial documents or\nonline forums. We also benchmark and compare a range of state-of-the-art dense\nretrievers, both unsupervised and supervised. Although different systems\nexhibit varying levels of vulnerability, we show they can all be successfully\nattacked by injecting up to 500 passages, a small fraction compared to a\nretrieval corpus of millions of passages.'}, {'Should You Mask 15% in Masked Language Modeling?': ""Masked language models (MLMs) conventionally mask 15% of tokens due to the\nbelief that more masking would leave insufficient context to learn good\nrepresentations; this masking rate has been widely used, regardless of model\nsizes or masking strategies. In this work, we revisit this important choice of\nMLM pre-training. We first establish that 15% is not universally optimal, and\nlarger models should adopt a higher masking rate. Specifically, we find that\nmasking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD.\nInterestingly, an extremely high masking rate of 80% can still preserve 95%\nfine-tuning performance and most of the accuracy in linguistic probing,\nchallenging the conventional wisdom about the role of the masking rate. We then\nexamine the interplay between masking rates and masking strategies and find\nthat uniform masking requires a higher masking rate compared to sophisticated\nmasking strategies such as span or PMI masking. Finally, we argue that\nincreasing the masking rate has two distinct effects: it leads to more\ncorruption, which makes the prediction task more difficult; it also enables\nmore predictions, which benefits optimization. Using this framework, we revisit\nBERT's 80-10-10 corruption strategy. Together, our results contribute to a\nbetter understanding of MLM pre-training.""}, {'Structured Pruning Learns Compact and Accurate Models': 'The growing size of neural language models has led to increased attention in\nmodel compression. The two predominant approaches are pruning, which gradually\nremoves weights from a pre-trained model, and distillation, which trains a\nsmaller compact model to match a larger one. Pruning methods can significantly\nreduce the model size but hardly achieve large speedups as distillation.\nHowever, distillation methods require large amounts of unlabeled data and are\nexpensive to train. In this work, we propose a task-specific structured pruning\nmethod CoFi (Coarse- and Fine-grained Pruning), which delivers highly\nparallelizable subnetworks and matches the distillation methods in both\naccuracy and latency, without resorting to any unlabeled data. Our key insight\nis to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads\nand hidden units) modules, which controls the pruning decision of each\nparameter with masks of different granularity. We also devise a layerwise\ndistillation strategy to transfer knowledge from unpruned to pruned models\nduring optimization. Our experiments on GLUE and SQuAD datasets show that CoFi\nyields models with over 10x speedups with a small accuracy drop, showing its\neffectiveness and efficiency compared to previous pruning and distillation\napproaches.'}, {'Privacy Implications of Retrieval-Based Language Models': 'Retrieval-based language models (LMs) have demonstrated improved\ninterpretability, factuality, and adaptability compared to their parametric\ncounterparts, by incorporating retrieved text from external datastores. While\nit is well known that parametric models are prone to leaking private data, it\nremains unclear how the addition of a retrieval datastore impacts model\nprivacy. In this work, we present the first study of privacy risks in\nretrieval-based LMs, particularly $k$NN-LMs. Our goal is to explore the optimal\ndesign and training procedure in domains where privacy is of concern, aiming to\nstrike a balance between utility and privacy. Crucially, we find that $k$NN-LMs\nare more susceptible to leaking private information from their private\ndatastore than parametric models. We further explore mitigations of privacy\nrisks. When privacy information is targeted and readily detected in the text,\nwe find that a simple sanitization step would completely eliminate the risks,\nwhile decoupling query and key encoders achieves an even better utility-privacy\ntrade-off. Otherwise, we consider strategies of mixing public and private data\nin both datastore and encoder training. While these methods offer modest\nimprovements, they leave considerable room for future work. Together, our\nfindings provide insights for practitioners to better understand and mitigate\nprivacy risks in retrieval-based LMs. Our code is available at:\nhttps://github.com/Princeton-SysML/kNNLM_privacy .'}, {'MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions': ""The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up\nto 175B) and outperforms previous model editors by a large margin.""}, {'$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models': 'Fine-tuning a language model on a new domain is standard practice for domain\nadaptation. However, it can be infeasible when it comes to modern large-scale\nlanguage models such as GPT-3, which can only be accessed through APIs, making\nit difficult to access the internal parameters of the model. In this paper, we\npropose $k$NN-Adapter, a method to effectively adapt these black-box large\nlanguage models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the\nretrieval-augmented language model, and adaptively learns to interpolate the\noutput of the language model with retrieval results from a datastore consisting\nof the target domain data. Our experiments on four different domains\ndemonstrate that $k$NN-Adapter significantly improves perplexity, and works\nparticularly well in settings with limited access to LLMs. Additionally, we\nshow that $k$NN-Adapter is more effective than fine-tuning when the amount of\ntraining data is limited. We also release a dataset to encourage further study.'}]","**Abstract**

In the realm of advanced machine learning, multimodaloperand expert models (MoE) present an innovative approach to enhance the efficiency of deep neural networks through dynamic allocation of computational resources among pre-defined, specialized components. This study focuses on Lory, a causal segment routing strategy for MoEs that significantly boosts model efficiency without compromising prediction accuracy. The objective of this research is to investigate Lory's performance in large-scale models, understanding its impact on training efficiency, model performance, and expert specialization. By utilizing causal segment routing, Lory enables tokens to be presented to all experts for decision-making, thereby promoting inter-expert learning and expert specialization. 

Key innovations in Lory include its unique routing strategy, which outperforms conventional methods, and the introduction of warm-up training for optimized expert utilization. The study finds that Lory MoEs can achieve superior language model perplexity across various tasks compared to dense models, especially when dealing with test data from diverse domains not included in the training set. It demonstrates that Lory not only boosts language modeling capabilities but also leads to domain-specific expert specialization, enriching the model's ability to tackle complex, domain-dependent challenges. Additionally, the research highlights the benefits of having multiple expert configurations, allowing for flexible model scaling based on task complexity and resource availability, providing practical solutions for real-world computational constraints.

Lory's findings contribute significantly to the field of neural network optimization and the evolution of large-scale models, offering a cost-effective long-term solution for deploying models in scenarios with limited computational resources. This advancement not only optimizes prediction accuracy but also enables the efficient deployment of models in diverse domains, suggesting promising applications across industries for enhanced predictive analytics and decision-making processes. The contributions of Lory lay the groundwork for more comprehensible and efficient large-scale models equipped with optimized resource allocation mechanisms for specialized tasks, fostering innovation in practical AI applications worldwide.

In summary, this research elucidates the effectiveness and versatility of Lory in MoEs, presenting a novel routing strategy, expounding on the benefits of warm-up training, analyzing classification and performance outcomes, and elucidating the implications of domain specialization. The outcomes underscore Lory's potential in resource-constrained AI systems, catalyzing advancements in predictive AI capabilities."
"Diffusion generative models have recently become a robust technique for
producing and modifying coherent, high-quality video. This survey offers a
systematic overview of critical elements of diffusion models for video
generation, covering applications, architectural choices, and the modeling of
temporal dynamics. Recent advancements in the field are summarized and grouped
into development trends. The survey concludes with an overview of remaining
challenges and an outlook on the future of the field. Website:
https://github.com/ndrwmlnk/Awesome-Video-Diffusion-Models","[{'Shape complexity estimation using VAE': 'In this paper, we compare methods for estimating the complexity of\ntwo-dimensional shapes and introduce a method that exploits reconstruction loss\nof Variational Autoencoders with different sizes of latent vectors. Although\ncomplexity of a shape is not a well defined attribute, different aspects of it\ncan be estimated. We demonstrate that our methods captures some aspects of\nshape complexity. Code and training details will be publicly available.'}, {'Faces: AI Blitz XIII Solutions': 'AI Blitz XIII Faces challenge hosted on www.aicrowd.com platform consisted of\nfive problems: Sentiment Classification, Age Prediction, Mask Prediction, Face\nRecognition, and Face De-Blurring. Our team GLaDOS took second place. Here we\npresent our solutions and results. Code implementation:\nhttps://github.com/ndrwmlnk/ai-blitz-xiii'}, {'Planning with RL and episodic-memory behavioral priors': 'The practical application of learning agents requires sample efficient and\ninterpretable algorithms. Learning from behavioral priors is a promising way to\nbootstrap agents with a better-than-random exploration policy or a safe-guard\nagainst the pitfalls of early learning. Existing solutions for imitation\nlearning require a large number of expert demonstrations and rely on\nhard-to-interpret learning methods like Deep Q-learning. In this work we\npresent a planning-based approach that can use these behavioral priors for\neffective exploration and learning in a reinforcement learning environment, and\nwe demonstrate that curated exploration policies in the form of behavioral\npriors can help an agent learn faster.'}, {'Modularization of End-to-End Learning: Case Study in Arcade Games': 'Complex environments and tasks pose a difficult problem for holistic\nend-to-end learning approaches. Decomposition of an environment into\ninteracting controllable and non-controllable objects allows supervised\nlearning for non-controllable objects and universal value function approximator\nlearning for controllable objects. Such decomposition should lead to a shorter\nlearning time and better generalisation capability. Here, we consider\narcade-game environments as sets of interacting objects (controllable,\nnon-controllable) and propose a set of functional modules that are specialized\non mastering different types of interactions in a broad range of environments.\nThe modules utilize regression, supervised learning, and reinforcement learning\nalgorithms. Results of this case study in different Atari games suggest that\nhuman-level performance can be achieved by a learning agent within a human\namount of game experience (10-15 minutes game time) when a proper decomposition\nof an environment or a task is provided. However, automatization of such\ndecomposition remains a challenging problem. This case study shows how a model\nof a causal structure underlying an environment or a task can benefit learning\ntime and generalization capability of the agent, and argues in favor of\nexploiting modular structure in contrast to using pure end-to-end learning\napproaches.'}, {'Stroke-based Rendering: From Heuristics to Deep Learning': 'In the last few years, artistic image-making with deep learning models has\ngained a considerable amount of traction. A large number of these models\noperate directly in the pixel space and generate raster images. This is however\nnot how most humans would produce artworks, for example, by planning a sequence\nof shapes and strokes to draw. Recent developments in deep learning methods\nhelp to bridge the gap between stroke-based paintings and pixel photo\ngeneration. With this survey, we aim to provide a structured introduction and\nunderstanding of common challenges and approaches in stroke-based rendering\nalgorithms. These algorithms range from simple rule-based heuristics to stroke\noptimization and deep reinforcement agents, trained to paint images with\ndifferentiable vector graphics and neural rendering.'}, {'Behavioral Cloning via Search in Video PreTraining Latent Space': ""Our aim is to build autonomous agents that can solve tasks in environments\nlike Minecraft. To do so, we used an imitation learning-based approach. We\nformulate our control problem as a search problem over a dataset of experts'\ndemonstrations, where the agent copies actions from a similar demonstration\ntrajectory of image-action pairs. We perform a proximity search over the BASALT\nMineRL-dataset in the latent representation of a Video PreTraining model. The\nagent copies the actions from the expert trajectory as long as the distance\nbetween the state representations of the agent and the selected expert\ntrajectory from the dataset do not diverge. Then the proximity search is\nrepeated. Our approach can effectively recover meaningful demonstration\ntrajectories and show human-like behavior of an agent in the Minecraft\nenvironment.""}, {'YOLO -- You only look 10647 times': 'With this work we are explaining the ""You Only Look Once"" (YOLO) single-stage\nobject detection approach as a parallel classification of 10647 fixed region\nproposals. We support this view by showing that each of YOLOs output pixel is\nattentive to a specific sub-region of previous layers, comparable to a local\nregion proposal. This understanding reduces the conceptual gap between\nYOLO-like single-stage object detection models, RCNN-like two-stage region\nproposal based models, and ResNet-like image classification models. In\naddition, we created interactive exploration tools for a better visual\nunderstanding of the YOLO information processing streams:\nhttps://limchr.github.io/yolo_visualization'}, {'Solving Physics Puzzles by Reasoning about Paths': 'We propose a new deep learning model for goal-driven tasks that require\nintuitive physical reasoning and intervention in the scene to achieve a desired\nend goal. Its modular structure is motivated by hypothesizing a sequence of\nintuitive steps that humans apply when trying to solve such a task. The model\nfirst predicts the path the target object would follow without intervention and\nthe path the target object should follow in order to solve the task. Next, it\npredicts the desired path of the action object and generates the placement of\nthe action object. All components of the model are trained jointly in a\nsupervised way; each component receives its own learning signal but learning\nsignals are also backpropagated through the entire architecture. To evaluate\nthe model we use PHYRE - a benchmark test for goal-driven physical reasoning in\n2D mechanics puzzles.'}, {'Critic Guided Segmentation of Rewarding Objects in First-Person Views': ""This work discusses a learning approach to mask rewarding objects in images\nusing sparse reward signals from an imitation learning dataset. For that, we\ntrain an Hourglass network using only feedback from a critic model. The\nHourglass network learns to produce a mask to decrease the critic's score of a\nhigh score image and increase the critic's score of a low score image by\nswapping the masked areas between these two images. We trained the model on an\nimitation learning dataset from the NeurIPS 2020 MineRL Competition Track,\nwhere our model learned to mask rewarding objects in a complex interactive 3D\nenvironment with a sparse reward signal. This approach was part of the 1st\nplace winning solution in this competition. Video demonstration and code:\nhttps://rebrand.ly/critic-guided-segmentation""}, {'Benchmarks for Physical Reasoning AI': 'Physical reasoning is a crucial aspect in the development of general AI\nsystems, given that human learning starts with interacting with the physical\nworld before progressing to more complex concepts. Although researchers have\nstudied and assessed the physical reasoning of AI approaches through various\nspecific benchmarks, there is no comprehensive approach to evaluating and\nmeasuring progress. Therefore, we aim to offer an overview of existing\nbenchmarks and their solution approaches and propose a unified perspective for\nmeasuring the physical reasoning capacity of AI systems. We select benchmarks\nthat are designed to test algorithmic performance in physical reasoning tasks.\nWhile each of the selected benchmarks poses a unique challenge, their ensemble\nprovides a comprehensive proving ground for an AI generalist agent with a\nmeasurable skill level for various physical reasoning concepts. This gives an\nadvantage to such an ensemble of benchmarks over other holistic benchmarks that\naim to simulate the real world by intertwining its complexity and many\nconcepts. We group the presented set of physical reasoning benchmarks into\nsubcategories so that more narrow generalist AI agents can be tested first on\nthese groups.'}]","Title: Advances in Video Diffusion Models: Innovations, Applications, and Future Directions

Abstract:

Video diffusion models, a fascinating and adaptive domain within generative AI, serve critical functions for creating and conditioning videos with both text and visual cues. Exploiting the potential of video diffusion models, this paper delves into their evolution from simplistic natural language processing tasks to sophisticated applications across domains such as healthcare, multimedia content creation, and beyond. The research integrates concepts like the UNet architecture, latent diffusion models, and vision transformers to enhance model capability and realism in video synthesis.

The main objective is to comprehensively review the current landscape of video diffusion models, with a spotlight on their advancements and effectiveness in text, image, and video conditioning. We introduce not just LLNL's GAIA-1, as a foundational example, but also elaborate on models that push boundaries, such as MagicVideo for its capacity to generate 1024×1024 resolution videos and automated metrics for accuracy assessment. Innovations also include text-to-video zero-shot synthesis and advancements in talking head generation through diffusion models.

Key methods discussed are UNet architected for denoising, latent diffusion models that facilitate higher fidelity outputs at lower computational costs, and vision transformers for attention-based interactions in video synthesis. We emphasize the use of human studies for relative performance evaluation and the importance of automated evaluation metrics like Fréchet distance for set-to-set comparison.

The paper highlights the main contributions to the field, including improved human-computer interaction, scalability of video generation models, and the superiority of diffusion models in generative tasks compared to traditional sequential models. Additionally, it outlines the potential massive scalability of video generation for various applications, including personalized multimedia content creation, health diagnostics, and future Internet video streaming technologies.

Ongoing challenges span the weaponization of AI for ethical concerns, personal privacy implications in large-scale data use, and the need for more diverse and representative data. The future of video diffusion models is thus marked by the imperative to balance the technical advancements with responsible AI usage and societal impact.

To summarize, this comprehensive overview of video diffusion models not only reflects the state-of-the-art but also anticipates new challenges, innovations, and applications that will re-imagine our interaction with digital video content, catalyzing further advancements in the evolving field of AI-driven content creation."
"Diffusion models have made significant contributions to computer vision,
sparking a growing interest in the community recently regarding the application
of them to graph generation. Existing discrete graph diffusion models exhibit
heightened computational complexity and diminished training efficiency. A
preferable and natural way is to directly diffuse the graph within the latent
space. However, due to the non-Euclidean structure of graphs is not isotropic
in the latent space, the existing latent diffusion models effectively make it
difficult to capture and preserve the topological information of graphs. To
address the above challenges, we propose a novel geometrically latent diffusion
framework HypDiff. Specifically, we first establish a geometrically latent
space with interpretability measures based on hyperbolic geometry, to define
anisotropic latent diffusion processes for graphs. Then, we propose a
geometrically latent diffusion process that is constrained by both radial and
angular geometric properties, thereby ensuring the preservation of the original
topological properties in the generative graphs. Extensive experimental results
demonstrate the superior effectiveness of HypDiff for graph generation with
various topologies.","[{'Dynamic Graph Information Bottleneck': 'Dynamic Graphs widely exist in the real world, which carry complicated\nspatial and temporal feature patterns, challenging their representation\nlearning. Dynamic Graph Neural Networks (DGNNs) have shown impressive\npredictive abilities by exploiting the intrinsic dynamics. However, DGNNs\nexhibit limited robustness, prone to adversarial attacks. This paper presents\nthe novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust\nand discriminative representations. Leveraged by the Information Bottleneck\n(IB) principle, we first propose the expected optimal representations should\nsatisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress\nredundant as well as conserve meritorious information into latent\nrepresentation, DGIB iteratively directs and refines the structural and feature\ninformation flow passing through graph snapshots. To meet the MSC Condition, we\ndecompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the\nDGIB$_{MS}$ channel aims to learn the minimal and sufficient representations,\nwith the DGIB$_{MS}$ channel guarantees the predictive consensus. Extensive\nexperiments on real-world and synthetic dynamic graph datasets demonstrate the\nsuperior robustness of DGIB against adversarial attacks compared with\nstate-of-the-art baselines in the link prediction task. To the best of our\nknowledge, DGIB is the first work to learn robust representations of dynamic\ngraphs grounded in the information-theoretic IB principle.'}, {'Curvature Graph Generative Adversarial Networks': 'Generative adversarial network (GAN) is widely used for generalized and\nrobust learning on graph data. However, for non-Euclidean graph data, the\nexisting GAN-based graph representation methods generate negative samples by\nrandom walk or traverse in discrete space, leading to the information loss of\ntopological properties (e.g. hierarchy and circularity). Moreover, due to the\ntopological heterogeneity (i.e., different densities across the graph\nstructure) of graph data, they suffer from serious topological distortion\nproblems. In this paper, we proposed a novel Curvature Graph Generative\nAdversarial Networks method, named \\textbf{\\modelname}, which is the first\nGAN-based graph representation method in the Riemannian geometric manifold. To\nbetter preserve the topological properties, we approximate the discrete\nstructure as a continuous Riemannian geometric manifold and generate negative\nsamples efficiently from the wrapped normal distribution. To deal with the\ntopological heterogeneity, we leverage the Ricci curvature for local structures\nwith different topological properties, obtaining to low-distortion\nrepresentations. Extensive experiments show that CurvGAN consistently and\nsignificantly outperforms the state-of-the-art methods across multiple tasks\nand shows superior robustness and generalization.'}, {'Self-organization Preserved Graph Structure Learning with Principle of\n  Relevant Information': 'Most Graph Neural Networks follow the message-passing paradigm, assuming the\nobserved structure depicts the ground-truth node relationships. However, this\nfundamental assumption cannot always be satisfied, as real-world graphs are\nalways incomplete, noisy, or redundant. How to reveal the inherent graph\nstructure in a unified way remains under-explored. We proposed PRI-GSL, a Graph\nStructure Learning framework guided by the Principle of Relevant Information,\nproviding a simple and unified framework for identifying the self-organization\nand revealing the hidden structure. PRI-GSL learns a structure that contains\nthe most relevant yet least redundant information quantified by von Neumann\nentropy and Quantum Jensen-Shannon divergence. PRI-GSL incorporates the\nevolution of quantum continuous walk with graph wavelets to encode node\nstructural roles, showing in which way the nodes interplay and self-organize\nwith the graph structure. Extensive experiments demonstrate the superior\neffectiveness and robustness of PRI-GSL.'}, {'Environment-Aware Dynamic Graph Learning for Out-of-Distribution\n  Generalization': 'Dynamic graph neural networks (DGNNs) are increasingly pervasive in\nexploiting spatio-temporal patterns on dynamic graphs. However, existing works\nfail to generalize under distribution shifts, which are common in real-world\nscenarios. As the generation of dynamic graphs is heavily influenced by latent\nenvironments, investigating their impacts on the out-of-distribution (OOD)\ngeneralization is critical. However, it remains unexplored with the following\ntwo major challenges: (1) How to properly model and infer the complex\nenvironments on dynamic graphs with distribution shifts? (2) How to discover\ninvariant patterns given inferred spatio-temporal environments? To solve these\nchallenges, we propose a novel Environment-Aware dynamic Graph LEarning (EAGLE)\nframework for OOD generalization by modeling complex coupled environments and\nexploiting spatio-temporal invariant patterns. Specifically, we first design\nthe environment-aware EA-DGNN to model environments by multi-channel\nenvironments disentangling. Then, we propose an environment instantiation\nmechanism for environment diversification with inferred distributions. Finally,\nwe discriminate spatio-temporal invariant patterns for out-of-distribution\nprediction by the invariant pattern recognition mechanism and perform\nfine-grained causal interventions node-wisely with a mixture of instantiated\nenvironment samples. Experiments on real-world and synthetic dynamic graph\ndatasets demonstrate the superiority of our method against state-of-the-art\nbaselines under distribution shifts. To the best of our knowledge, we are the\nfirst to study OOD generalization on dynamic graphs from the environment\nlearning perspective.'}, {'Poincaré Differential Privacy for Hierarchy-Aware Graph Embedding': ""Hierarchy is an important and commonly observed topological property in\nreal-world graphs that indicate the relationships between supervisors and\nsubordinates or the organizational behavior of human groups. As hierarchy is\nintroduced as a new inductive bias into the Graph Neural Networks (GNNs) in\nvarious tasks, it implies latent topological relations for attackers to improve\ntheir inference attack performance, leading to serious privacy leakage issues.\nIn addition, existing privacy-preserving frameworks suffer from reduced\nprotection ability in hierarchical propagation due to the deficiency of\nadaptive upper-bound estimation of the hierarchical perturbation boundary. It\nis of great urgency to effectively leverage the hierarchical property of data\nwhile satisfying privacy guarantees. To solve the problem, we propose the\nPoincar\\'e Differential Privacy framework, named PoinDP, to protect the\nhierarchy-aware graph embedding based on hyperbolic geometry. Specifically,\nPoinDP first learns the hierarchy weights for each entity based on the\nPoincar\\'e model in hyperbolic space. Then, the Personalized Hierarchy-aware\nSensitivity is designed to measure the sensitivity of the hierarchical\nstructure and adaptively allocate the privacy protection strength. Besides, the\nHyperbolic Gaussian Mechanism (HGM) is proposed to extend the Gaussian\nmechanism in Euclidean space to hyperbolic space to realize random\nperturbations that satisfy differential privacy under the hyperbolic space\nmetric. Extensive experiment results on five real-world datasets demonstrate\nthe proposed PoinDP's advantages of effective privacy protection while\nmaintaining good performance on the node classification task.""}, {'A Robust and Generalized Framework for Adversarial Graph Embedding': ""Graph embedding is essential for graph mining tasks. With the prevalence of\ngraph data in real-world applications, many methods have been proposed in\nrecent years to learn high-quality graph embedding vectors various types of\ngraphs. However, most existing methods usually randomly select the negative\nsamples from the original graph to enhance the training data without\nconsidering the noise. In addition, most of these methods only focus on the\nexplicit graph structures and cannot fully capture complex semantics of edges\nsuch as various relationships or asymmetry. In order to address these issues,\nwe propose a robust and generalized framework for adversarial graph embedding\nbased on generative adversarial networks. Inspired by generative adversarial\nnetwork, we propose a robust and generalized framework for adversarial graph\nembedding, named AGE. AGE generates the fake neighbor nodes as the enhanced\nnegative samples from the implicit distribution, and enables the discriminator\nand generator to jointly learn each node's robust and generalized\nrepresentation. Based on this framework, we propose three models to handle\nthree types of graph data and derive the corresponding optimization algorithms,\ni.e., UG-AGE and DG-AGE for undirected and directed homogeneous graphs,\nrespectively, and HIN-AGE for heterogeneous information networks. Extensive\nexperiments show that our methods consistently and significantly outperform\nexisting state-of-the-art methods across multiple graph mining tasks, including\nlink prediction, node classification, and graph reconstruction.""}, {'ACE-HGNN: Adaptive Curvature Exploration Hyperbolic Graph Neural Network': 'Graph Neural Networks (GNNs) have been widely studied in various graph data\nmining tasks. Most existingGNNs embed graph data into Euclidean space and thus\nare less effective to capture the ubiquitous hierarchical structures in\nreal-world networks. Hyperbolic Graph Neural Networks(HGNNs) extend GNNs to\nhyperbolic space and thus are more effective to capture the hierarchical\nstructures of graphs in node representation learning. In hyperbolic geometry,\nthe graph hierarchical structure can be reflected by the curvatures of the\nhyperbolic space, and different curvatures can model different hierarchical\nstructures of a graph. However, most existing HGNNs manually set the curvature\nto a fixed value for simplicity, which achieves a suboptimal performance of\ngraph learning due to the complex and diverse hierarchical structures of the\ngraphs. To resolve this problem, we propose an Adaptive Curvature Exploration\nHyperbolic Graph NeuralNetwork named ACE-HGNN to adaptively learn the optimal\ncurvature according to the input graph and downstream tasks. Specifically,\nACE-HGNN exploits a multi-agent reinforcement learning framework and contains\ntwo agents, ACE-Agent andHGNN-Agent for learning the curvature and node\nrepresentations, respectively. The two agents are updated by a NashQ-leaning\nalgorithm collaboratively, seeking the optimal hyperbolic space indexed by the\ncurvature. Extensive experiments on multiple real-world graph datasets\ndemonstrate a significant and consistent performance improvement in model\nquality with competitive performance and good generalization ability.'}, {'Hyperbolic Geometric Graph Representation Learning for\n  Hierarchy-imbalance Node Classification': 'Learning unbiased node representations for imbalanced samples in the graph\nhas become a more remarkable and important topic. For the graph, a significant\nchallenge is that the topological properties of the nodes (e.g., locations,\nroles) are unbalanced (topology-imbalance), other than the number of training\nlabeled nodes (quantity-imbalance). Existing studies on topology-imbalance\nfocus on the location or the local neighborhood structure of nodes, ignoring\nthe global underlying hierarchical properties of the graph, i.e., hierarchy. In\nthe real-world scenario, the hierarchical structure of graph data reveals\nimportant topological properties of graphs and is relevant to a wide range of\napplications. We find that training labeled nodes with different hierarchical\nproperties have a significant impact on the node classification tasks and\nconfirm it in our experiments. It is well known that hyperbolic geometry has a\nunique advantage in representing the hierarchical structure of graphs.\nTherefore, we attempt to explore the hierarchy-imbalance issue for node\nclassification of graph neural networks with a novelty perspective of\nhyperbolic geometry, including its characteristics and causes. Then, we propose\na novel hyperbolic geometric hierarchy-imbalance learning framework, named\nHyperIMBA, to alleviate the hierarchy-imbalance issue caused by uneven\nhierarchy-levels and cross-hierarchy connectivity patterns of labeled\nnodes.Extensive experimental results demonstrate the superior effectiveness of\nHyperIMBA for hierarchy-imbalance node classification tasks.'}, {'Graph Structure Learning with Variational Information Bottleneck': 'Graph Neural Networks (GNNs) have shown promising results on a broad spectrum\nof applications. Most empirical studies of GNNs directly take the observed\ngraph as input, assuming the observed structure perfectly depicts the accurate\nand complete relations between nodes. However, graphs in the real world are\ninevitably noisy or incomplete, which could even exacerbate the quality of\ngraph representations. In this work, we propose a novel Variational Information\nBottleneck guided Graph Structure Learning framework, namely VIB-GSL, in the\nperspective of information theory. VIB-GSL advances the Information Bottleneck\n(IB) principle for graph structure learning, providing a more elegant and\nuniversal framework for mining underlying task-relevant relations. VIB-GSL\nlearns an informative and compressive graph structure to distill the actionable\ninformation for specific downstream tasks. VIB-GSL deduces a variational\napproximation for irregular graph data to form a tractable IB objective\nfunction, which facilitates training stability. Extensive experimental results\ndemonstrate that the superior effectiveness and robustness of VIB-GSL.'}, {'Position-aware Structure Learning for Graph Topology-imbalance by\n  Relieving Under-reaching and Over-squashing': ""Topology-imbalance is a graph-specific imbalance problem caused by the uneven\ntopology positions of labeled nodes, which significantly damages the\nperformance of GNNs. What topology-imbalance means and how to measure its\nimpact on graph learning remain under-explored. In this paper, we provide a new\nunderstanding of topology-imbalance from a global view of the supervision\ninformation distribution in terms of under-reaching and over-squashing, which\nmotivates two quantitative metrics as measurements. In light of our analysis,\nwe propose a novel position-aware graph structure learning framework named\nPASTEL, which directly optimizes the information propagation path and solves\nthe topology-imbalance issue in essence. Our key insight is to enhance the\nconnectivity of nodes within the same class for more supervision information,\nthereby relieving the under-reaching and over-squashing phenomena.\nSpecifically, we design an anchor-based position encoding mechanism, which\nbetter incorporates relative topology position and enhances the intra-class\ninductive bias by maximizing the label influence. We further propose a\nclass-wise conflict measure as the edge weights, which benefits the separation\nof different node classes. Extensive experiments demonstrate the superior\npotential and adaptability of PASTEL in enhancing GNNs' power in different data\nannotation scenarios.""}]","Title: 6D Hyperbolic Geometric Latent Diffusion Model for Graph Generation and Node Classification

Abstract:

Within the dense realm of graph data, including social networks, biological networks, and information networks, the task of learning from and generating complex graph structures remains challenging. These datasets are characterized by intricate connectivity and a wealth of attributes. To address this challenge, an innovative 6D Hyperbolic Geometric Latent Diffusion Model has been developed, aiming to navigate the limitations of Euclidean and flat representations while leveraging hyperbolic geometry's potential for representing hierarchical and scale-free networks more accurately.

Objective:
The primary objective of this research is to introduce and validate a novel graph generation and node classification technique that utilizes hyperbolic geometry to effectively encode, diffuse, and generate complex graph structures over non-Euclidean manifolds, with a specific focus on scenarios where nodes exhibit anisotropic properties.

Innovations:
This work introduces a 'Hyperbolic Anisotropic Diffusion' model. The key innovation lies in the geometrically constrained diffusion process that ensures the preservation of both metrics and cluster properties during graph embedding. It features anisotropic diffusion, spatial anisotropy encoding, and a clustering technique that enhances graph generation quality and node classification accuracy.

Methods:
The methodology draws from variational autoencoder and graph diffusion model principles, with the addition of an anisotropic diffusion layer that accelerates the learning process of graph structure. The proposed model utilizes a deep learning architecture optimized for hyperbolic space, guiding nodes into hyperbolic embeddings and facilitating the diffusion of embedded graphs towards more coherent non-Euclidean settings. Node classification utilizes the learned embeddings in hyperbolic space, achieving superior accuracy.

Results:
The experimental outcomes demonstrate that the proposed model excels in generating synthetic and real-world graph structures with a significantly improved F1 score of up to 1.1 compared to existing models. Moreover, it achieves an average reduction in MMD distance by an order of magnitude, which underscores its robustness in faithfully reproducing graphs. 

Contributions:
The research contributes to the field by enabling more accurate and efficient representation, generation, and classification of non-Euclidean graph data, primarily represented as weighted graphs. This advancement offers a novel parallel to Euclidean vector space models for graph data analysis, particularly targeting weighted graphs and their anisotropic properties.

Applications:
The introduced techniques can be applied to a wide range of fields, including social network analysis, recommendation systems, biological network modeling, and information retrieval, where weighted graphs are prevalent. By enhancing the representation and generation capabilities for such graphs, the model significantly contributes to the holistic understanding and comprehensiveness of the underlying network data.

In summary, this research introduces a robust framework for graph generation and node classification that leverages advanced geometric insights for non-Euclidean datasets, thereby offering a critical tool for complex network analytics."
"The softmax activation function plays a crucial role in the success of large
language models (LLMs), particularly in the self-attention mechanism of the
widely adopted Transformer architecture. However, the underlying learning
dynamics that contribute to the effectiveness of softmax remain largely
unexplored. As a step towards better understanding, this paper provides a
theoretical study of the optimization and generalization properties of
two-layer softmax neural networks, providing theoretical insights into their
superior performance as other activation functions, such as ReLU and
exponential. Leveraging the Neural Tangent Kernel (NTK) framework, our analysis
reveals that the normalization effect of the softmax function leads to a good
perturbation property of the induced NTK matrix, resulting in a good convex
region of the loss landscape. Consequently, softmax neural networks can learn
the target function in the over-parametrization regime. To demonstrate the
broad applicability of our theoretical findings, we apply them to the task of
learning score estimation functions in diffusion models, a promising approach
for generative modeling. Our analysis shows that gradient-based algorithms can
learn the score function with a provable accuracy. Our work provides a deeper
understanding of the effectiveness of softmax neural networks and their
potential in various domains, paving the way for further advancements in
natural language processing and beyond.","[{'An Empirical Study of Language CNN for Image Captioning': 'Language Models based on recurrent neural networks have dominated recent\nimage caption generation tasks. In this paper, we introduce a Language CNN\nmodel which is suitable for statistical language modeling tasks and shows\ncompetitive performance in image captioning. In contrast to previous models\nwhich predict next word based on one previous word and hidden state, our\nlanguage CNN is fed with all the previous words and can model the long-range\ndependencies of history words, which are critical for image captioning. The\neffectiveness of our approach is validated on two datasets MS COCO and\nFlickr30K. Our extensive experimental results show that our method outperforms\nthe vanilla recurrent neural network based language models and is competitive\nwith the state-of-the-art methods.'}, {'Stack-Captioning: Coarse-to-Fine Learning for Image Captioning': ""The existing image captioning approaches typically train a one-stage sentence\ndecoder, which is difficult to generate rich fine-grained descriptions. On the\nother hand, multi-stage image caption model is hard to train due to the\nvanishing gradient problem. In this paper, we propose a coarse-to-fine\nmulti-stage prediction framework for image captioning, composed of multiple\ndecoders each of which operates on the output of the previous stage, producing\nincreasingly refined image descriptions. Our proposed learning approach\naddresses the difficulty of vanishing gradients during training by providing a\nlearning objective function that enforces intermediate supervisions.\nParticularly, we optimize our model with a reinforcement learning approach\nwhich utilizes the output of each intermediate decoder's test-time inference\nalgorithm as well as the output of its preceding decoder to normalize the\nrewards, which simultaneously solves the well-known exposure bias problem and\nthe loss-evaluation mismatch problem. We extensively evaluate the proposed\napproach on MSCOCO and show that our approach can achieve the state-of-the-art\nperformance.""}, {'Unpaired Image Captioning by Language Pivoting': 'Image captioning is a multimodal task involving computer vision and natural\nlanguage processing, where the goal is to learn a mapping from the image to its\nnatural language description. In general, the mapping function is learned from\na training set of image-caption pairs. However, for some language, large scale\nimage-caption paired corpus might not be available. We present an approach to\nthis unpaired image captioning problem by language pivoting. Our method can\neffectively capture the characteristics of an image captioner from the pivot\nlanguage (Chinese) and align it to the target language (English) using another\npivot-target (Chinese-English) sentence parallel corpus. We evaluate our method\non two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative\ncomparisons against several baseline approaches demonstrate the effectiveness\nof our method.'}, {'Watch It Twice: Video Captioning with a Refocused Video Encoder': 'With the rapid growth of video data and the increasing demands of various\napplications such as intelligent video search and assistance toward\nvisually-impaired people, video captioning task has received a lot of attention\nrecently in computer vision and natural language processing fields. The\nstate-of-the-art video captioning methods focus more on encoding the temporal\ninformation, while lack of effective ways to remove irrelevant temporal\ninformation and also neglecting the spatial details. However, the current RNN\nencoding module in single time order can be influenced by the irrelevant\ntemporal information, especially the irrelevant temporal information is at the\nbeginning of the encoding. In addition, neglecting spatial information will\nlead to the relationship confusion of the words and detailed loss. Therefore,\nin this paper, we propose a novel recurrent video encoding method and a novel\nvisual spatial feature for the video captioning task. The recurrent encoding\nmodule encodes the video twice with the predicted key frame to avoid the\nirrelevant temporal information often occurring at the beginning and the end of\na video. The novel spatial features represent the spatial information in\ndifferent regions of a video and enrich the details of a caption. Experiments\non two benchmark datasets show superior performance of the proposed method.'}, {'Customization Assistant for Text-to-image Generation': 'Customizing pre-trained text-to-image generation model has attracted massive\nresearch interest recently, due to its huge potential in real-world\napplications. Although existing methods are able to generate creative content\nfor a novel concept contained in single user-input image, their capability are\nstill far from perfection. Specifically, most existing methods require\nfine-tuning the generative model on testing images. Some existing methods do\nnot require fine-tuning, while their performance are unsatisfactory.\nFurthermore, the interaction between users and models are still limited to\ndirective and descriptive prompts such as instructions and captions. In this\nwork, we build a customization assistant based on pre-trained large language\nmodel and diffusion model, which can not only perform customized generation in\na tuning-free manner, but also enable more user-friendly interactions: users\ncan chat with the assistant and input either ambiguous text or clear\ninstruction. Specifically, we propose a new framework consists of a new model\ndesign and a novel training strategy. The resulting assistant can perform\ncustomized generation in 2-5 seconds without any test time fine-tuning.\nExtensive experiments are conducted, competitive results have been obtained\nacross different domains, illustrating the effectiveness of the proposed\nmethod.'}, {'Video Captioning with Boundary-aware Hierarchical Language Decoding and\n  Joint Video Prediction': 'The explosion of video data on the internet requires effective and efficient\ntechnology to generate captions automatically for people who are not able to\nwatch the videos. Despite the great progress of video captioning research,\nparticularly on video feature encoding, the language decoder is still largely\nbased on the prevailing RNN decoder such as LSTM, which tends to prefer the\nfrequent word that aligns with the video. In this paper, we propose a\nboundary-aware hierarchical language decoder for video captioning, which\nconsists of a high-level GRU based language decoder, working as a global\n(caption-level) language model, and a low-level GRU based language decoder,\nworking as a local (phrase-level) language model. Most importantly, we\nintroduce a binary gate into the low-level GRU language decoder to detect the\nlanguage boundaries. Together with other advanced components including joint\nvideo prediction, shared soft attention, and boundary-aware video encoding, our\nintegrated video captioning framework can discover hierarchical language\ninformation and distinguish the subject and the object in a sentence, which are\nusually confusing during the language generation. Extensive experiments on two\nwidely-used video captioning datasets, MSR-Video-to-Text (MSR-VTT)\n\\cite{xu2016msr} and YouTube-to-Text (MSVD) \\cite{chen2011collecting} show that\nour method is highly competitive, compared with the state-of-the-art methods.'}, {'Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval\n  with Generative Models': 'Textual-visual cross-modal retrieval has been a hot research topic in both\ncomputer vision and natural language processing communities. Learning\nappropriate representations for multi-modal data is crucial for the cross-modal\nretrieval performance. Unlike existing image-text retrieval approaches that\nembed image-text pairs as single feature vectors in a common representational\nspace, we propose to incorporate generative processes into the cross-modal\nfeature embedding, through which we are able to learn not only the global\nabstract features but also the local grounded features. Extensive experiments\nshow that our framework can well match images and sentences with complex\ncontent, and achieve the state-of-the-art cross-modal retrieval results on\nMSCOCO dataset.'}, {'Open-Vocabulary Instance Segmentation via Robust Cross-Modal\n  Pseudo-Labeling': 'Open-vocabulary instance segmentation aims at segmenting novel classes\nwithout mask annotations. It is an important step toward reducing laborious\nhuman supervision. Most existing works first pretrain a model on captioned\nimages covering many novel classes and then finetune it on limited base classes\nwith mask annotations. However, the high-level textual information learned from\ncaption pretraining alone cannot effectively encode the details required for\npixel-wise segmentation. To address this, we propose a cross-modal\npseudo-labeling framework, which generates training pseudo masks by aligning\nword semantics in captions with visual features of object masks in images.\nThus, our framework is capable of labeling novel classes in captions via their\nword semantics to self-train a student model. To account for noises in pseudo\nmasks, we design a robust student model that selectively distills mask\nknowledge by estimating the mask noise levels, hence mitigating the adverse\nimpact of noisy pseudo masks. By extensive experiments, we show the\neffectiveness of our framework, where we significantly improve mAP score by\n4.5% on MS-COCO and 5.1% on the large-scale Open Images & Conceptual Captions\ndatasets compared to the state-of-the-art.'}, {'Unpaired Image Captioning via Scene Graph Alignments': 'Most of current image captioning models heavily rely on paired image-caption\ndatasets. However, getting large scale image-caption paired data is\nlabor-intensive and time-consuming. In this paper, we present a scene\ngraph-based approach for unpaired image captioning. Our framework comprises an\nimage scene graph generator, a sentence scene graph generator, a scene graph\nencoder, and a sentence decoder. Specifically, we first train the scene graph\nencoder and the sentence decoder on the text modality. To align the scene\ngraphs between images and sentences, we propose an unsupervised feature\nalignment method that maps the scene graph features from the image to the\nsentence modality. Experimental results show that our proposed model can\ngenerate quite promising results without using any image-caption training\npairs, outperforming existing methods by a wide margin.'}, {'Resilient Load Restoration in Microgrids Considering Mobile Energy\n  Storage Fleets: A Deep Reinforcement Learning Approach': 'Mobile energy storage systems (MESSs) provide mobility and flexibility to\nenhance distribution system resilience. The paper proposes a Markov decision\nprocess (MDP) formulation for an integrated service restoration strategy that\ncoordinates the scheduling of MESSs and resource dispatching of microgrids. The\nuncertainties in load consumption are taken into account. The deep\nreinforcement learning (DRL) algorithm is utilized to solve the MDP for optimal\nscheduling. Specifically, the twin delayed deep deterministic policy gradient\n(TD3) is applied to train the deep Q-network and policy network, then the well\ntrained policy can be deployed in on-line manner to perform multiple actions\nsimultaneously. The proposed model is demonstrated on an integrated test system\nwith three microgrids connected by Sioux Falls transportation network. The\nsimulation results indicate that mobile and stationary energy resources can be\nwell coordinated to improve system resilience.'}]","Title: Neural Tangent Kernel Analysis for Two-Layer Softmax Networks: Applications to Diﬀusion Models

Abstract:
The rise of Large Language Models (LLMs) in recent years has transformed the landscape of artificial intelligence, with innovative models such as GPT4 and Claude. Despite significant strides with Transformer architectures and self-attention mechanisms, understanding the inner workings remains crucial. This research delves into two-layer neural networks with a softmax activation function, which have evolved to incorporate rich feature learning dynamics.

Our primary goal is to provide a strong theoretical foundation for two-layer softmax networks, complementing earlier analysis on over-parameterized neural networks. The study revolves around the Neural Tangent Kernel, enabling the derivation of epsilon-GIN (Guaranteed Inference Number) bounds, which characterize the model's requirements for guaranteed optimization. 

Our innovations lie in: (a) Analysing the component-wise Neural Tangent Kernel for the two-layer softmax network, offering insights seldom explored in similar studies. (b) Extending analysis to diffusion models, demonstrating the network's ability to learn and regulate score estimation functions under noisy labels, a crucial aspect in image generation and other related areas.

In terms of methods, we leverage truncated vector decomposition and kernel perturbation techniques, as well as an inductive scheme, to rigorously quantify the generalization bounds around initialization. Substantial concentration inequalities, prompted by the neurons' spectral properties, are also employed to refine these bounds.

Our discussion reveals that with an indication of the model's capacity (hidden neuron number and training steps), one can achieve a guaranteed level of training loss, demonstrating the efficacy of two-layer softmax networks. This study not only validates but also validates the computational fluidity of neural networks, capable of addressing a diversified range of practical applications in image processing and beyond.

Contributions from this work might catalyze further advancements in understanding neural networks with non-linear activation functions, enhancing their ability to learn complex patterns and improving their applicability across diverse domains. From an applied viewpoint, it could lead to more efficient and adaptable models in the era of AI, with potential implications in areas like autonomous systems, smart technologies, and personalized healthcare."
"This paper presents a novel method for parsing and vectorizing
semi-structured data to enhance the functionality of Retrieval-Augmented
Generation (RAG) within Large Language Models (LLMs). We developed a
comprehensive pipeline for converting various data formats into .docx, enabling
efficient parsing and structured data extraction. The core of our methodology
involves the construction of a vector database using Pinecone, which integrates
seamlessly with LLMs to provide accurate, context-specific responses,
particularly in environmental management and wastewater treatment operations.
Through rigorous testing with both English and Chinese texts in diverse
document formats, our results demonstrate a marked improvement in the precision
and reliability of LLMs outputs. The RAG-enhanced models displayed enhanced
ability to generate contextually rich and technically accurate responses,
underscoring the potential of vector knowledge bases in significantly boosting
the performance of LLMs in specialized domains. This research not only
illustrates the effectiveness of our method but also highlights its potential
to revolutionize data processing and analysis in environmental sciences,
setting a precedent for future advancements in AI-driven applications. Our code
is available at https://github.com/linancn/TianGong-AI-Unstructure.git.","[{'Blow Up Of A Hyperbolic SQG Model': 'This paper studies of a variation of the hyperbolic blow up scenario\nsuggested by Hou and Luo\'s recent numerical simulation [12]. In particular, we\npropose a ""hyperbolic"" surface quasi-geostrophic equation characterized by a\nincompressible velocity field with a modified Biot-Savart law. For this model,\nwe will show Finite time blow up for a wide class of initial data.'}, {'Local Well-posedness of A Non-local Burgers Equation': 'In this paper, we explore a nonlocal inviscid Burgers equation. Fixing a\nparameter $h$, we prove existence and uniqueness of the local solution of the\nequation $\\InviscidBurgersNonlocal{u}$ with periodic initial condition. We also\nexplore the blow up properties of solutions to these kinds of equations with\ngiven periodic initial data, and show that there exists solutions that blow up\nin finite time and solutions that are globally regular. This contrasts with the\nclassical inviscid Burgers equation, for which all non-constant smooth periodic\ninitial data lead to finite time blow up. Finally, we present results of\nsimulations to illustrate our findings.'}, {'Injective symmetric quantaloid-enriched categories': 'We characterize injective objects, injective hulls and essential embeddings\nin the category of symmetric categories enriched in a small, integral and\ninvolutive quantaloid. In particular, injective partial metric spaces are\nprecisely formulated.'}, {'Local and Global Results for Shape optimization problems with weighted\n  source': 'We consider shape optimization problems of maximizing the averaged heat under\nvarious boundary conditions. Assuming that the heat source is radial, we obtain\nseveral local stability and global optimality results on ball shape. As a\nbyproduct of stability analysis, we show that Talenti type pointwise comparison\nresult is no longer true under Robin conditions even if the domain is a smooth\nsmall perturbation of a ball.'}, {'Analysis of a Singular Boussinesq Model': 'Recently, a new singularity formation scenario for the 3D axi-symmetric Euler\nequation and the 2D inviscid Boussinesq system has been proposed by Hu and Luo\nbased on extensive numerical simulations [15, 16]. As the firrst step to\nunderstand the scenario, models with simplified sign-definite Biot-Savart law\nand forcing have recently been studied in [7, 6, 8, 12, 14, 18]. In this paper,\nwe aim to bring back one of the complications encountered in the original\nequation - the sign changing kernel in the Biot-Savart law. This makes analysis\nharder, as there are two competing terms in the fluid velocity integral whose\nbalance determines the regularity properties of the solution. The equation we\nstudy here is based on the CKY model introduced in [6]. We prove that finite\ntime blow up persists in a certain range of parameters.'}, {'Depth Image Upsampling based on Guided Filter with Low Gradient\n  Minimization': 'In this paper, we present a novel upsampling framework to enhance the spatial\nresolution of the depth image. In our framework, the upscaling of a\nlow-resolution depth image is guided by a corresponding intensity images, we\nformulate it as a cost aggregation problem with the guided filter. However, the\nguided filter does not make full use of the properties of the depth image.\nSince depth images have quite sparse gradients, it inspires us to regularize\nthe gradients for improving depth upscaling results. Statistics show a special\nproperty of depth images, that is, there is a non-ignorable part of pixels\nwhose horizontal or vertical derivatives are equal to $\\pm 1$. Considering this\nspecial property, we propose a low gradient regularization method which reduces\nthe penalty for horizontal or vertical derivative $\\pm1$. The proposed low\ngradient regularization is integrated with the guided filter into the depth\nimage upsampling method. Experimental results demonstrate the effectiveness of\nour proposed approach both qualitatively and quantitatively compared with the\nstate-of-the-art methods.'}, {'An Adaptive Parameter Estimation for Guided Filter based Image\n  Deconvolution': ""Image deconvolution is still to be a challenging ill-posed problem for\nrecovering a clear image from a given blurry image, when the point spread\nfunction is known. Although competitive deconvolution methods are numerically\nimpressive and approach theoretical limits, they are becoming more complex,\nmaking analysis, and implementation difficult. Furthermore, accurate estimation\nof the regularization parameter is not easy for successfully solving image\ndeconvolution problems. In this paper, we develop an effective approach for\nimage restoration based on one explicit image filter - guided filter. By\napplying the decouple of denoising and deblurring techniques to the\ndeconvolution model, we reduce the optimization complexity and achieve a simple\nbut effective algorithm to automatically compute the parameter in each\niteration, which is based on Morozov's discrepancy principle. Experimental\nresults demonstrate that the proposed algorithm outperforms many\nstate-of-the-art deconvolution methods in terms of both ISNR and visual\nquality.""}, {'Guided Filter based Edge-preserving Image Non-blind Deconvolution': 'In this work, we propose a new approach for efficient edge-preserving image\ndeconvolution. Our algorithm is based on a novel type of explicit image filter\n- guided filter. The guided filter can be used as an edge-preserving smoothing\noperator like the popular bilateral filter, but has better behaviors near\nedges. We propose an efficient iterative algorithm with the decouple of\ndeblurring and denoising steps in the restoration process. In deblurring step,\nwe proposed two cost function which could be computed with fast Fourier\ntransform efficiently. The solution of the first one is used as the guidance\nimage, and another solution will be filtered in next step. In the denoising\nstep, the guided filter is used with the two obtained images for efficient\nedge-preserving filtering. Furthermore, we derive a simple and effective method\nto automatically adjust the regularization parameter at each iteration. We\ncompare our deconvolution algorithm with many competitive deconvolution\ntechniques in terms of ISNR and visual quality.'}, {'Select Good Regions for Deblurring based on Convolutional Neural\n  Networks': 'The goal of blind image deblurring is to recover sharp image from one input\nblurred image with an unknown blur kernel. Most of image deblurring approaches\nfocus on developing image priors, however, there is not enough attention to the\ninfluence of image details and structures on the blur kernel estimation. What\nis the useful image structure and how to choose a good deblurring region? In\nthis work, we propose a deep neural network model method for selecting good\nregions to estimate blur kernel. First we construct image patches with labels\nand train a deep neural networks, then the learned model is applied to\ndetermine which region of the image is most suitable to deblur. Experimental\nresults illustrate that the proposed approach is effective, and could be able\nto select good regions for image deblurring.'}, {'The single-particle space-momentum angle distribution effect on two-pion\n  HBT correlation with the Coulomb interaction': 'We calculate the HBT radius Rs for {\\pi}+ particles with the Coulomb\ninteraction by using the string melting version of a multiphase transport(AMPT)\nmodel. We study the relationship between the single-particle space-momentum\nangle and the particle sources and discuss HBT radii without single-particle\nspace-momentum correlation. Additionally, we study the Coulomb interaction\neffect on the numerical connection between the single-particle space-momentum\nangle distribution and the transverse momentum dependence of Rs.'}]","**Abstract:**

This study explores the integration of Retrieval-Augmented Generation (RAG) systems and domain-specific vector knowledge bases to enhance Large Language Models (LLMs) in specialized fields, focusing on ecological environmental management and related disciplines. The primary objective is to address the challenges in accurate and context-specific knowledge acquisition, particularly the generation of 'hallucinations' or factually incorrect responses, by merging LLMs with comprehensive domain knowledge.

Innovations introduced encompass a vector-based knowledge base that is designed for accurate data retrieval and augmentation, specifically tailored to enrich LLM outputs with contextually relevant information. This framework was implemented using GPT 4.0, which processed and evaluated fifty questions across English and Chinese, showcasing its effectiveness in obtaining accurate responses for specialized questions.

The methodologies employed involved a robust data cleaning process for textual, pictorial, and tabular elements, ensuring the quality and integrity of the input data fed into the LLM models. This phase involved manual and automated approaches to filter and organize data, aiming to mitigate the risks associated with superfluous and qualitative materials that could detract from model efficiency.

The results demonstrated significant improvements in the quality and accuracy of LLM responses, particularly for specialized questions drawn from ecological environmental management. The RAG approach, supported by the vector knowledge base, was shown to augment the domain-specific expertise of GPT 4.0, reflecting its capability to provide precise and relevant answers.

This research contributes to the field by presenting a novel methodology to enhance the capabilities of LLMs, particularly in integrating them with domain knowledge to address the challenges inherent in the specialized work. The findings have immediate implications for the automation and optimization of tasks within ecological environmental management and related sectors, offering more effective natural language processing solutions and potentially leading to improved decision-making processes based on accurate and contextually enriched information."
"We introduce Vidu, a high-performance text-to-video generator that is capable
of producing 1080p videos up to 16 seconds in a single generation. Vidu is a
diffusion model with U-ViT as its backbone, which unlocks the scalability and
the capability for handling long videos. Vidu exhibits strong coherence and
dynamism, and is capable of generating both realistic and imaginative videos,
as well as understanding some professional photography techniques, on par with
Sora -- the most powerful reported text-to-video generator. Finally, we perform
initial experiments on other controllable video generation, including
canny-to-video generation, video prediction and subject-driven generation,
which demonstrate promising results.","[{'Boosting Generative Models by Leveraging Cascaded Meta-Models': 'Deep generative models are effective methods of modeling data. However, it is\nnot easy for a single generative model to faithfully capture the distributions\nof complex data such as images. In this paper, we propose an approach for\nboosting generative models, which cascades meta-models together to produce a\nstronger model. Any hidden variable meta-model (e.g., RBM and VAE) which\nsupports likelihood evaluation can be leveraged. We derive a decomposable\nvariational lower bound of the boosted model, which allows each meta-model to\nbe trained separately and greedily. Besides, our framework can be extended to\nsemi-supervised boosting, where the boosted model learns a joint distribution\nof data and labels. Finally, we combine our boosting framework with the\nmultiplicative boosting framework, which further improves the learning power of\ngenerative models.'}, {'Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in\n  Diffusion Probabilistic Models': 'Diffusion probabilistic models (DPMs) represent a class of powerful\ngenerative models. Despite their success, the inference of DPMs is expensive\nsince it generally needs to iterate over thousands of timesteps. A key problem\nin the inference is to estimate the variance in each timestep of the reverse\nprocess. In this work, we present a surprising result that both the optimal\nreverse variance and the corresponding optimal KL divergence of a DPM have\nanalytic forms w.r.t. its score function. Building upon it, we propose\nAnalytic-DPM, a training-free inference framework that estimates the analytic\nforms of the variance and KL divergence using the Monte Carlo method and a\npretrained score-based model. Further, to correct the potential bias caused by\nthe score-based model, we derive both lower and upper bounds of the optimal\nvariance and clip the estimate for a better result. Empirically, our\nanalytic-DPM improves the log-likelihood of various DPMs, produces high-quality\nsamples, and meanwhile enjoys a 20x to 80x speed up.'}, {'Why Are Conditional Generative Models Better Than Unconditional Ones?': 'Extensive empirical evidence demonstrates that conditional generative models\nare easier to train and perform better than unconditional ones by exploiting\nthe labels of data. So do score-based diffusion models. In this paper, we\nanalyze the phenomenon formally and identify that the key of conditional\nlearning is to partition the data properly. Inspired by the analyses, we\npropose self-conditioned diffusion models (SCDM), which is trained conditioned\non indices clustered by the k-means algorithm on the features extracted by a\nmodel pre-trained in a self-supervised manner. SCDM significantly improves the\nunconditional model across various datasets and achieves a record-breaking FID\nof 3.94 on ImageNet 64x64 without labels. Besides, SCDM achieves a slightly\nbetter FID than the corresponding conditional model on CIFAR10.'}, {'Stability and Generalization of Bilevel Programming in Hyperparameter\n  Optimization': 'The (gradient-based) bilevel programming framework is widely used in\nhyperparameter optimization and has achieved excellent performance empirically.\nPrevious theoretical work mainly focuses on its optimization properties, while\nleaving the analysis on generalization largely open. This paper attempts to\naddress the issue by presenting an expectation bound w.r.t. the validation set\nbased on uniform stability. Our results can explain some mysterious behaviours\nof the bilevel programming in practice, for instance, overfitting to the\nvalidation set. We also present an expectation bound for the classical\ncross-validation algorithm. Our results suggest that gradient-based algorithms\ncan be better than cross-validation under certain conditions in a theoretical\nperspective. Furthermore, we prove that regularization terms in both the outer\nand inner levels can relieve the overfitting problem in gradient-based\nalgorithms. In experiments on feature learning and data reweighting for noisy\nlabels, we corroborate our theoretical findings.'}, {'Estimating the Optimal Covariance with Imperfect Mean in Diffusion\n  Probabilistic Models': 'Diffusion probabilistic models (DPMs) are a class of powerful deep generative\nmodels (DGMs). Despite their success, the iterative generation process over the\nfull timesteps is much less efficient than other DGMs such as GANs. Thus, the\ngeneration performance on a subset of timesteps is crucial, which is greatly\ninfluenced by the covariance design in DPMs. In this work, we consider diagonal\nand full covariances to improve the expressive power of DPMs. We derive the\noptimal result for such covariances, and then correct it when the mean of DPMs\nis imperfect. Both the optimal and the corrected ones can be decomposed into\nterms of conditional expectations over functions of noise. Building upon it, we\npropose to estimate the optimal covariance and its correction given imperfect\nmean by learning these conditional expectations. Our method can be applied to\nDPMs with both discrete and continuous timesteps. We consider the diagonal\ncovariance in our implementation for computational efficiency. For an efficient\npractical implementation, we adopt a parameter sharing scheme and a two-stage\ntraining process. Empirically, our method outperforms a wide variety of\ncovariance design on likelihood results, and improves the sample quality\nespecially on a small number of timesteps.'}, {'Towards Interpretable Deep Neural Networks by Leveraging Adversarial\n  Examples': 'Deep neural networks (DNNs) have demonstrated impressive performance on a\nwide array of tasks, but they are usually considered opaque since internal\nstructure and learned parameters are not interpretable. In this paper, we\nre-examine the internal representations of DNNs using adversarial images, which\nare generated by an ensemble-optimization algorithm. We find that: (1) the\nneurons in DNNs do not truly detect semantic objects/parts, but respond to\nobjects/parts only as recurrent discriminative patches; (2) deep visual\nrepresentations are not robust distributed codes of visual concepts because the\nrepresentations of adversarial images are largely not consistent with those of\nreal images, although they have similar visual appearance, both of which are\ndifferent from previous findings. To further improve the interpretability of\nDNNs, we propose an adversarial training scheme with a consistent loss such\nthat the neurons are endowed with human-interpretable concepts. The induced\ninterpretable representations enable us to trace eventual outcomes back to\ninfluential neurons. Therefore, human users can know how the models make\npredictions, as well as when and why they make errors.'}, {'EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic\n  Differential Equations': 'Score-based diffusion models (SBDMs) have achieved the SOTA FID results in\nunpaired image-to-image translation (I2I). However, we notice that existing\nmethods totally ignore the training data in the source domain, leading to\nsub-optimal solutions for unpaired I2I. To this end, we propose energy-guided\nstochastic differential equations (EGSDE) that employs an energy function\npretrained on both the source and target domains to guide the inference process\nof a pretrained SDE for realistic and faithful unpaired I2I. Building upon two\nfeature extractors, we carefully design the energy function such that it\nencourages the transferred image to preserve the domain-independent features\nand discard domain-specific ones. Further, we provide an alternative\nexplanation of the EGSDE as a product of experts, where each of the three\nexperts (corresponding to the SDE and two feature extractors) solely\ncontributes to faithfulness or realism. Empirically, we compare EGSDE to a\nlarge family of baselines on three widely-adopted unpaired I2I tasks under four\nmetrics. EGSDE not only consistently outperforms existing SBDMs-based methods\nin almost all settings but also achieves the SOTA realism results without\nharming the faithful performance. Furthermore, EGSDE allows for flexible\ntrade-offs between realism and faithfulness and we improve the realism results\nfurther (e.g., FID of 51.04 in Cat to Dog and FID of 50.43 in Wild to Dog on\nAFHQ) by tuning hyper-parameters. The code is available at\nhttps://github.com/ML-GSAI/EGSDE.'}, {'Bi-level Score Matching for Learning Energy-based Latent Variable Models': 'Score matching (SM) provides a compelling approach to learn energy-based\nmodels (EBMs) by avoiding the calculation of partition function. However, it\nremains largely open to learn energy-based latent variable models (EBLVMs),\nexcept some special cases. This paper presents a bi-level score matching (BiSM)\nmethod to learn EBLVMs with general structures by reformulating SM as a\nbi-level optimization problem. The higher level introduces a variational\nposterior of the latent variables and optimizes a modified SM objective, and\nthe lower level optimizes the variational posterior to fit the true posterior.\nTo solve BiSM efficiently, we develop a stochastic optimization algorithm with\ngradient unrolling. Theoretically, we analyze the consistency of BiSM and the\nconvergence of the stochastic algorithm. Empirically, we show the promise of\nBiSM in Gaussian restricted Boltzmann machines and highly nonstructural EBLVMs\nparameterized by deep convolutional neural networks. BiSM is comparable to the\nwidely adopted contrastive divergence and SM methods when they are applicable;\nand can learn complex EBLVMs with intractable posteriors to generate natural\nimages.'}, {'Variational (Gradient) Estimate of the Score Function in Energy-based\n  Latent Variable Models': 'The learning and evaluation of energy-based latent variable models (EBLVMs)\nwithout any structural assumptions are highly challenging, because the true\nposteriors and the partition functions in such models are generally\nintractable. This paper presents variational estimates of the score function\nand its gradient with respect to the model parameters in a general EBLVM,\nreferred to as VaES and VaGES respectively. The variational posterior is\ntrained to minimize a certain divergence to the true model posterior and the\nbias in both estimates can be bounded by the divergence theoretically. With a\nminimal model assumption, VaES and VaGES can be applied to the kernelized Stein\ndiscrepancy (KSD) and score matching (SM)-based methods to learn EBLVMs.\nBesides, VaES can also be used to estimate the exact Fisher divergence between\nthe data and general EBLVMs.'}, {'Equivariant Energy-Guided SDE for Inverse Molecular Design': 'Inverse molecular design is critical in material science and drug discovery,\nwhere the generated molecules should satisfy certain desirable properties. In\nthis paper, we propose equivariant energy-guided stochastic differential\nequations (EEGSDE), a flexible framework for controllable 3D molecule\ngeneration under the guidance of an energy function in diffusion models.\nFormally, we show that EEGSDE naturally exploits the geometric symmetry in 3D\nmolecular conformation, as long as the energy function is invariant to\northogonal transformations. Empirically, under the guidance of designed energy\nfunctions, EEGSDE significantly improves the baseline on QM9, in inverse\nmolecular design targeted to quantum properties and molecular structures.\nFurthermore, EEGSDE is able to generate molecules with multiple target\nproperties by combining the corresponding energy functions linearly.'}]","Title: Innovating Video Generation with Vidu: A Multifaceted Approach to Realistic and Imaginative Video Creation

Abstract:

In the fast-evolving landscape of artificial intelligence, the capability to generate high-quality, visually compelling videos has become paramount, especially in areas like entertainment, education, and artistic expression. This paper, featuring Vidu – a groundbreaking AI model, addresses the key areas of content generation across different lengths of videos, 3D consistency, cuts, camera movements, lighting effects, emotional portrayal, and imaginative ability, offering unparalleled realism. 

The main objective is to present a versatile AI system capable of generating 1080p videos extending up to 16 seconds, accompanied by the novelty of real and imagined scenarios. Vidu incorporates innovative advancements in text-to-video generation, with a focus on 3D consistency, transitioning seamlessly between scenes and subjects, diverse camera movements, and evocative lighting effects. Importantly, it distinguishes itself through its ability to capture and express the subtle nuances of emotions and generate imaginative, fictional scenes.

Methodologically, artificial intelligence techniques and large, diverse video datasets power Vidu, aiding its proficiency in crafting content that spans various narrative demands with attention to detail, aesthetic appeal, and immersive realism. Rigorous testing across predetermined prompts demonstrates Vidu's effectiveness in meeting these objectives.

The breakdown of findings highlights Vidu’s capacity to derive meaningful transitions between scenes, execute multi-dimensional camera maneuvers that enrich visual storytelling, render dynamic 3D effects, and vividly manifest emotional atmospheres. Quantitatively and qualitatively, it scores high against comparable state-of-the-art models like Sora in the generation of videos, staking its claim as a leader in the field.

This research contributes to an enhanced understanding of AI-generated content, illustrating the potential for AI in comprehensive video generation – a practical and accessible means of amplifying creativity across industries. At its core, Vidu underscores the pivotal role of AI in shaping the art and science of multimedia content creation, enabling the realization of projects with vastly increased efficiency, cost-effectiveness, and potential for pushing creative boundaries."
"We present a new multi-modal face image generation method that converts a
text prompt and a visual input, such as a semantic mask or scribble map, into a
photo-realistic face image. To do this, we combine the strengths of Generative
Adversarial networks (GANs) and diffusion models (DMs) by employing the
multi-modal features in the DM into the latent space of the pre-trained GANs.
We present a simple mapping and a style modulation network to link two models
and convert meaningful representations in feature maps and attention maps into
latent codes. With GAN inversion, the estimated latent codes can be used to
generate 2D or 3D-aware facial images. We further present a multi-step training
strategy that reflects textual and structural representations into the
generated image. Our proposed network produces realistic 2D, multi-view, and
stylized face images, which align well with inputs. We validate our method by
using pre-trained 2D and 3D GANs, and our results outperform existing methods.
Our project page is available at
https://github.com/1211sh/Diffusion-driven_GAN-Inversion/.","[{'Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal\n  Latent Representations': 'In this paper, we propose a multi-speaker face-to-speech waveform generation\nmodel that also works for unseen speaker conditions. Using a generative\nadversarial network (GAN) with linguistic and speaker characteristic features\nas auxiliary conditions, our method directly converts face images into speech\nwaveforms under an end-to-end training framework. The linguistic features are\nextracted from lip movements using a lip-reading model, and the speaker\ncharacteristic features are predicted from face images using cross-modal\nlearning with a pre-trained acoustic model. Since these two features are\nuncorrelated and controlled independently, we can flexibly synthesize speech\nwaveforms whose speaker characteristics vary depending on the input face\nimages. We show the superiority of our proposed model over conventional methods\nin terms of objective and subjective evaluation results. Specifically, we\nevaluate the performances of linguistic features by measuring their accuracy on\nan automatic speech recognition task. In addition, we estimate speaker and\ngender similarity for multi-speaker and unseen conditions, respectively. We\nalso evaluate the aturalness of the synthesized speech waveforms using a mean\nopinion score (MOS) test and non-intrusive objective speech quality assessment\n(NISQA).The demo samples of the proposed and other models are available at\nhttps://sam-0927.github.io/'}, {'Correlation between Ultra-high Energy Cosmic Rays and Active Galactic\n  Nuclei from Fermi Large Area Telescope': 'We study the possibility that the $\\gamma$-ray loud active galactic nuclei\n(AGN) are the sources of ultra-high energy cosmic rays (UHECR), through the\ncorrelation analysis of their locations and the arrival directions of UHECR. We\nuse the $\\gamma$-ray loud AGN with $d\\le 100 {\\rm Mpc}$ from the second Fermi\nLarge Area Telescope AGN catalog and the UHECR data with $E\\ge 55 {\\rm EeV}$\nobserved by Pierre Auger Observatory. The distribution of arrival directions\nexpected from the $\\gamma$-ray loud AGN is compared with that of the observed\nUHECR using the correlational angular distance distribution and the\nKolmogorov-Smirnov test. We conclude that the hypothesis that the $\\gamma$-ray\nloud AGN are the dominant sources of UHECR is disfavored unless there is a\nlarge smearing effect due to the intergalactic magnetic fields.'}, {'Statistical Analysis of the Correlation between Active Galactic Nuclei\n  and Ultra-High Energy Cosmic Rays': ""We develop the statistical methods for comparing two sets of arrival\ndirections of cosmic rays in which the two-dimensional distribution of arrival\ndirections is reduced to the one-dimensional distributions so that the standard\none-dimensional Kolmogorov-Smirnov test can be applied. Then we apply them to\nthe analysis of correlation between the ultra-high energy cosmic rays (UHECR)\nwith energies above $5.7\\times10^{19}$ eV, observed by Pierre Auger Observatory\n(PAO) and Akeno Giant Air Shower Array (AGASA), and the active galactic nuclei\n(AGN) within the distance 100 Mpc. For statistical test, we set up the simple\nAGN model for UHECR sources in which a certain fraction of observed UHECR are\noriginated from AGN within a chosen distance, assuming that all AGN have equal\nluminosity and smearing angle of UHECR, and the remaining fraction are from the\nisotropic background contribution. For the PAO data, our methods exclude not\nonly a hypotheses that the observed UHECR are simply isotropically distributed\nbut also a hypothesis that they are completely originated from the selected\nAGN. But, the addition of appropriate amount of isotropic component either\nthrough the background contribution or through the large smearing effect\nimproves the correlation greatly and makes the AGN hypothesis for UHECR sources\na viable one. We also point out that restricting AGN within the distance bin of\n40-60 Mpc happens to yield a good correlation without appreciable isotropic\ncomponent and large smearing effect. For the AGASA data, we don't find any\nsignificant correlation with AGN.""}, {'Revisit of Correlation Analysis between Active Galactic Nuclei and\n  Ultra-High Energy Cosmic Rays': ""We update the previous analysis of correlation between ultra-high energy\ncosmic rays (UHECR) and active galactic nuclei (AGN), using 69 UHECR events\nwith energy $E\\ge55\\,{\\rm EeV}$ released in 2010 by Pierre Auger observatory\nand 862 AGN within the distance $d\\le100\\,{\\rm Mpc}$ listed in the 13th edition\nof V\\'eron-Cetty and V\\'eron AGN catalog. To make the test hypothesis definite,\nwe use the simple AGN source model in which UHECR are originated both from AGN,\nwith the fraction $f_A$, and from the isotropic background. We treat all AGN as\nequal sources of UHECR, and introduce the smearing angle $\\theta_s$ to\nincorporate the effects of intervening magnetic fields. We compare the arrival\ndirection distributions observed by PAO and expected from the model by the\ncorrelational angular distance distribution (CADD) method. CADD method rules\nout the AGN dominance model with a small smearing angle ($f_A\\gtrsim0.7$ and\n$\\theta_s\\lesssim6^\\circ$). Concerning the isotropy, CADD shows that the\ndistribution of PAO data is marginally consistent with isotropy. The best fit\nmodel lies around the AGN fraction $f_A=0.4$ and the moderate smearing angle\n$\\theta_s=10^\\circ$. For the fiducial value $f_A=0.7$, the best probability of\nCADD was obtained at a rather large smearing angle $\\theta_{\\rm s}=46^\\circ$.\nOur results imply that for the whole AGN to be viable sources of UHECR, either\nan appreciable amount of additional isotropic background or the large smearing\neffect is required. Thus, we try to bin the distance range of AGN to narrow\ndown the UHECR sources and found that the AGN residing in the distance range\n$60-80\\,{\\rm Mpc}$ have good correlation with the updated PAO data. It is an\nindication that further study on the subclass of AGN as the UHECR source may be\nquite interesting.""}, {'New robust inference for predictive regressions': 'We propose two robust methods for testing hypotheses on unknown parameters of\npredictive regression models under heterogeneous and persistent volatility as\nwell as endogenous, persistent and/or fat-tailed regressors and errors. The\nproposed robust testing approaches are applicable both in the case of discrete\nand continuous time models. Both of the methods use the Cauchy estimator to\neffectively handle the problems of endogeneity, persistence and/or\nfat-tailedness in regressors and errors. The difference between our two methods\nis how the heterogeneous volatility is controlled. The first method relies on\nrobust t-statistic inference using group estimators of a regression parameter\nof interest proposed in Ibragimov and Muller, 2010. It is simple to implement,\nbut requires the exogenous volatility assumption. To relax the exogenous\nvolatility assumption, we propose another method which relies on the\nnonparametric correction of volatility. The proposed methods perform well\ncompared with widely used alternative inference procedures in terms of their\nfinite sample properties.'}, {'Propagation of ultra-high-energy cosmic rays in the magnetized cosmic\n  web': 'A high concentration of ultra-high-energy cosmic ray (UHECR) events, called a\nhotspot, was reported by the Telescope Array (TA) experiment, but its origin\nstill remains unsolved. One of the obstacles is that there is no astronomical\nobject, which could be the source, behind the TA hotpot. In an effort to\nunderstand the origin of the TA hotspot, we suggested a model based on the\nmagnetized cosmic web structure. The UHECRs were produced from sources in the\nVirgo cluster and were initially confined by cluster magnetic fields for a\ncertain period. Next, some of them preferentially escaped to and propagated\nalong filaments. Eventually, they were scattered by filament magnetic fields,\nand come to us. To examine the model, we followed the propagation trajectories\nof UHE protons in a simulated universe with clusters, filaments, and voids, by\nemploying a number of models for cosmic magnetic fields. In this study, we\npresent some of the initial results, such as the ratio between the particles\ndirectly escaping from the clusters to the voids and particles escaping from\nthe clusters to the filaments. We also discuss the feasibility of our model for\nthe origin of the hotspot by examining the trajectories of the UHE protons.'}, {'Design of Ga2O3 Modulation Doped Field Effect Transistors': 'The design of beta-Ga2O3-based modulation doped field effect transistors\n(MODFETs) is discussed with a focus on the role of self-heating and resultant\nmodification of the electron mobility profile. Temperature- and\ndoping-dependent model of the electron mobility as well as temperature- and\norientation-dependent approximations of the thermal conductivity of beta-Ga2O3\nare presented. A decrease in drain current was attributed to a\nposition-dependent mobility reduction caused by a coupled self-heating\nmechanism and a high electric-field mobility reduction mechanism. A simple\nthermal management solution is presented where heat is extracted through the\nsource contact metal. Additionally, it is shown that an undesired secondary\nchannel can form at the modulation doped layer that is distinguished by an\ninflection in the transconductance curve.'}, {'Hybridization-Controlled Pseudogap State in the Quantum Critical\n  Superconductor CeCoIn5': 'The origin of the partial suppression of the electronic density states in the\nenigmatic pseudogap behavior, which is at the core of understanding high-$T_c$\nsuperconductivity, has been hotly contested as either a hallmark of preformed\nCooper pairs or an incipient order of competing interactions nearby. Here, we\nreport the quasi-particle scattering spectroscopy of the quantum critical\nsuperconductor CeCoIn$_5$, where a pseudogap with energy $\\Delta_g$ was\nmanifested as a dip in the differential conductance ($dI/dV$) below the\ncharacteristic temperature of $T_g$. When subjected to external pressure, $T_g$\nand $\\Delta_g$ gradually increase, following the trend of increase in quantum\nentangled hybridization between Ce 4$f$ moment and conduction electrons. On the\nother hand, the superconducting (SC) energy gap and its phase transition\ntemperature shows a maximum, revealing a dome shape under pressure. The\ndisparate dependence on pressure between the two quantum states shows that the\npseudogap is less likely involved in the formation of SC Cooper pairs, but\nrather is controlled by Kondo hybridization, indicating that a novel type of\npseudogap is realized in CeCoIn$_5$.'}, {'Crystalline symmetry-dependent magnon formation in itinerant ferromagnet\n  SrRuO3': 'SrRuO3 (SRO) is an itinerant ferromagnet with strong coupling between the\ncharge, spin, and lattice degrees of freedom. This strong coupling suggests\nthat the electronic and magnetic behaviors of SRO are highly susceptible to\nchanges in the lattice distortion. Here we show how the spin interaction and\nresultant magnon formation change with the modification in the crystallographic\norientation. We fabricated SRO epitaxial thin films with (100), (110), and\n(111) surface orientations, to systematically modulate the spin interaction and\nspin dimensionality. The reduced spin dimensionality and enhanced exchange\ninteraction in the (111)-oriented SRO thin film significantly suppresses magnon\nformation. Our study comprehensively demonstrates the facile tunability of\nmagnon formation and spin interaction in correlated oxide thin films.'}, {'Evidence for charge delocalization crossover in the quantum critical\n  superconductor CeRhIn$_5$': 'The nature of charge degrees-of-freedom distinguishes scenarios for\ninterpreting the character of a second order magnetic transition at zero\ntemperature, that is, a magnetic quantum critical point (QCP). Heavy-fermion\nsystems are prototypes of this paradigm, and in those, the relevant question is\nwhere, relative to a magnetic QCP, does the Kondo effect delocalize their\n$f$-electron degrees-of-freedom. Herein, we use pressure-dependent Hall\nmeasurements to identify a finite-temperature scale $E_\\text{loc}$ that signals\na crossover from $f$-localized to $f$-delocalized character. As a function of\npressure, $E_\\text{loc}(P)$ extrapolates smoothly to zero temperature at the\nantiferromagnetic QCP of CeRhIn$_5$ where its Fermi surface reconstructs,\nhallmarks of Kondo-breakdown criticality that generates critical magnetic and\ncharge fluctuations. In 4.4% Sn-doped CeRhIn$_5$, however, $E_\\text{loc}(P)$\nextrapolates into its magnetically ordered phase and is decoupled from the\npressure-induced magnetic QCP, which implies a spin-density-wave (SDW) type of\ncriticality that produces only critical fluctuations of the SDW order\nparameter. Our results demonstrate the importance of experimentally determining\n$E_\\text{loc}$ to characterize quantum criticality and the associated\nconsequences for understanding the pairing mechanism of superconductivity that\nreaches a maximum $T_\\text{c}$ in both materials at their respective magnetic\nQCP.'}]","Title: Multi-Modal 3D-Aware Face Image Generation: Methods, Innovations, and Applications

Background: In the era of AI-driven identity representation, accurate and diverse facial image generation, particularly in 3D, is increasingly demanded for various applications including virtual fitting, human-computer interaction, and digital content creation.

Objective: The objective is to improve multi-modal 3D-aware face image generation by extending the capacity to leverage textual descriptions alongside visual conditions from semantic and scribble masks.

Innovations: The paper presents a refined model incorporating a conditionally-aware mapping network, a semantic mask text encoder, and disjoint attention maps. Two modes – one purely text-driven, and the other using both text and visual inputs – are considered. The model enables disentangled processing of facial attributes aligned across multi-modal data.

Methods: Experiments are conducted on CelebAMask-HQ dataset, with positive validation results, outperforming state-of-the-art methods. The multi-step training and flexible attention maps facilitate better integration of input modes while preserving structural integrity and textual fidelity.

Results: The proposed model demonstrates superior performance particularly for face images that require details beyond overlapping regions, showcasing strength in text understanding and visual representation.

Contributions: The main contribution is the development of an effective, data-efficient framework for 3D-aware face generation based on multi-modal inputs, advancing AI technology for diverse identity representation fields.

Applications: The method is applicable for virtual fashion try-on, 3D digital humans in gaming or animation, more personalized AI-driven communication interfaces, and broader digital content creation spanning media and entertainment.

This abstract succinctly describes the research, conveying its critical elements – from the foundational context and objectives to the innovative techniques, outcomes, and impactful areas of application."
"Understanding how humans would behave during hand-object interaction is vital
for applications in service robot manipulation and extended reality. To achieve
this, some recent works have been proposed to simultaneously predict hand
trajectories and object affordances on human egocentric videos. They are
regarded as the representation of future hand-object interactions, indicating
potential human motion and motivation. However, the existing approaches mostly
adopt the autoregressive paradigm for unidirectional prediction, which lacks
mutual constraints within the holistic future sequence, and accumulates errors
along the time axis. Meanwhile, these works basically overlook the effect of
camera egomotion on first-person view predictions. To address these
limitations, we propose a novel diffusion-based interaction prediction method,
namely Diff-IP2D, to forecast future hand trajectories and object affordances
concurrently in an iterative non-autoregressive manner. We transform the
sequential 2D images into latent feature space and design a denoising diffusion
model to predict future latent interaction features conditioned on past ones.
Motion features are further integrated into the conditional denoising process
to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate
interaction prediction. The experimental results show that our method
significantly outperforms the state-of-the-art baselines on both the
off-the-shelf metrics and our proposed new evaluation protocol. This highlights
the efficacy of leveraging a generative paradigm for 2D hand-object interaction
prediction. The code of Diff-IP2D will be released at
https://github.com/IRMVLab/Diff-IP2D.","[{'SeqOT: A Spatial-Temporal Transformer Network for Place Recognition\n  Using Sequential LiDAR Data': 'Place recognition is an important component for autonomous vehicles to\nachieve loop closing or global localization. In this paper, we tackle the\nproblem of place recognition based on sequential 3D LiDAR scans obtained by an\nonboard LiDAR sensor. We propose a transformer-based network named SeqOT to\nexploit the temporal and spatial information provided by sequential range\nimages generated from the LiDAR data. It uses multi-scale transformers to\ngenerate a global descriptor for each sequence of LiDAR range images in an\nend-to-end fashion. During online operation, our SeqOT finds similar places by\nmatching such descriptors between the current query sequence and those stored\nin the map. We evaluate our approach on four datasets collected with different\ntypes of LiDAR sensors in different environments. The experimental results show\nthat our method outperforms the state-of-the-art LiDAR-based place recognition\nmethods and generalizes well across different environments. Furthermore, our\nmethod operates online faster than the frame rate of the sensor. The\nimplementation of our method is released as open source at:\nhttps://github.com/BIT-MJY/SeqOT.'}, {'CVTNet: A Cross-View Transformer Network for Place Recognition Using\n  LiDAR Data': ""LiDAR-based place recognition (LPR) is one of the most crucial components of\nautonomous vehicles to identify previously visited places in GPS-denied\nenvironments. Most existing LPR methods use mundane representations of the\ninput point cloud without considering different views, which may not fully\nexploit the information from LiDAR sensors. In this paper, we propose a\ncross-view transformer-based network, dubbed CVTNet, to fuse the range image\nviews (RIVs) and bird's eye views (BEVs) generated from the LiDAR data. It\nextracts correlations within the views themselves using intra-transformers and\nbetween the two different views using inter-transformers. Based on that, our\nproposed CVTNet generates a yaw-angle-invariant global descriptor for each\nlaser scan end-to-end online and retrieves previously seen places by descriptor\nmatching between the current query scan and the pre-built database. We evaluate\nour approach on three datasets collected with different sensor setups and\nenvironmental conditions. The experimental results show that our method\noutperforms the state-of-the-art LPR methods with strong robustness to\nviewpoint changes and long-time spans. Furthermore, our approach has a good\nreal-time performance that can run faster than the typical LiDAR frame rate.\nThe implementation of our method is released as open source at:\nhttps://github.com/BIT-MJY/CVTNet.""}, {'Anti-Fall: A Non-intrusive and Real-time Fall Detector Leveraging CSI\n  from Commodity WiFi Devices': 'Fall is one of the major health threats and obstacles to independent living\nfor elders, timely and reliable fall detection is crucial for mitigating the\neffects of falls. In this paper, leveraging the fine-grained Channel State\nInformation (CSI) and multi-antenna setting in commodity WiFi devices, we\ndesign and implement a real-time, non-intrusive, and low-cost indoor fall\ndetector, called Anti-Fall. For the first time, the CSI phase difference over\ntwo antennas is identified as the salient feature to reliably segment the fall\nand fall-like activities, both phase and amplitude information of CSI is then\nexploited to accurately separate the fall from other fall-like activities.\nExperimental results in two indoor scenarios demonstrate that Anti-Fall\nconsistently outperforms the state-of-the-art approach WiFall, with 10% higher\ndetection rate and 10% less false alarm rate on average.'}, {'LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for\n  Place Recognition': 'Place recognition is one of the most crucial modules for autonomous vehicles\nto identify places that were previously visited in GPS-invalid environments.\nSensor fusion is considered an effective method to overcome the weaknesses of\nindividual sensors. In recent years, multimodal place recognition fusing\ninformation from multiple sensors has gathered increasing attention. However,\nmost existing multimodal place recognition methods only use limited\nfield-of-view camera images, which leads to an imbalance between features from\ndifferent modalities and limits the effectiveness of sensor fusion. In this\npaper, we present a novel neural network named LCPR for robust multimodal place\nrecognition, which fuses LiDAR point clouds with multi-view RGB images to\ngenerate discriminative and yaw-rotation invariant representations of the\nenvironment. A multi-scale attention-based fusion module is proposed to fully\nexploit the panoramic views from different modalities of the environment and\ntheir correlations. We evaluate our method on the nuScenes dataset, and the\nexperimental results show that our method can effectively utilize multi-view\ncamera and LiDAR data to improve the place recognition performance while\nmaintaining strong robustness to viewpoint changes. Our open-source code and\npre-trained models are available at https://github.com/ZhouZijie77/LCPR .'}, {'PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point\n  Cloud Prediction': 'The ability to predict future structure features of environments based on\npast perception information is extremely needed by autonomous vehicles, which\nhelps to make the following decision-making and path planning more reasonable.\nRecently, point cloud prediction (PCP) is utilized to predict and describe\nfuture environmental structures by the point cloud form. In this letter, we\npropose a novel efficient Transformer-based network to predict the future LiDAR\npoint clouds exploiting the past point cloud sequences. We also design a\nsemantic auxiliary training strategy to make the predicted LiDAR point cloud\nsequence semantically similar to the ground truth and thus improves the\nsignificance of the deployment for more tasks in real-vehicle applications. Our\napproach is completely self-supervised, which means it does not require any\nmanual labeling and has a solid generalization ability toward different\nenvironments. The experimental results show that our method outperforms the\nstate-of-the-art PCP methods on the prediction results and semantic similarity,\nand has a good real-time performance. Our open-source code and pre-trained\nmodels are available at https://github.com/Blurryface0814/PCPNet.'}, {'A Comparative Study of Deep Reinforcement Learning-based Transferable\n  Energy Management Strategies for Hybrid Electric Vehicles': 'The deep reinforcement learning-based energy management strategies (EMS) have\nbecome a promising solution for hybrid electric vehicles (HEVs). When driving\ncycles are changed, the neural network will be retrained, which is a\ntime-consuming and laborious task. A more efficient way of choosing EMS is to\ncombine deep reinforcement learning (DRL) with transfer learning, which can\ntransfer knowledge of one domain to the other new domain, making the network of\nthe new domain reach convergence values quickly. Different exploration methods\nof DRL, including adding action space noise and parameter space noise, are\ncompared against each other in the transfer learning process in this work.\nResults indicate that the network added parameter space noise is more stable\nand faster convergent than the others. In conclusion, the best exploration\nmethod for transferable EMS is to add noise in the parameter space, while the\ncombination of action space noise and parameter space noise generally performs\npoorly. Our code is available at\nhttps://github.com/BIT-XJY/RL-based-Transferable-EMS.git.'}, {'PC-NeRF: Parent-Child Neural Radiance Fields under Partial Sensor Data\n  Loss in Autonomous Driving Environments': 'Reconstructing large-scale 3D scenes is essential for autonomous vehicles,\nespecially when partial sensor data is lost. Although the recently developed\nneural radiance fields (NeRF) have shown compelling results in implicit\nrepresentations, the large-scale 3D scene reconstruction using partially lost\nLiDAR point cloud data still needs to be explored. To bridge this gap, we\npropose a novel 3D scene reconstruction framework called parent-child neural\nradiance field (PC-NeRF). The framework comprises two modules, the parent NeRF\nand the child NeRF, to simultaneously optimize scene-level, segment-level, and\npoint-level scene representations. Sensor data can be utilized more efficiently\nby leveraging the segment-level representation capabilities of child NeRFs, and\nan approximate volumetric representation of the scene can be quickly obtained\neven with limited observations. With extensive experiments, our proposed\nPC-NeRF is proven to achieve high-precision 3D reconstruction in large-scale\nscenes. Moreover, PC-NeRF can effectively tackle situations where partial\nsensor data is lost and has high deployment efficiency with limited training\ntime. Our approach implementation and the pre-trained models will be available\nat https://github.com/biter0088/pc-nerf.'}, {'PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames\n  in Autonomous Driving Environments': 'Large-scale 3D scene reconstruction and novel view synthesis are vital for\nautonomous vehicles, especially utilizing temporally sparse LiDAR frames.\nHowever, conventional explicit representations remain a significant bottleneck\ntowards representing the reconstructed and synthetic scenes at unlimited\nresolution. Although the recently developed neural radiance fields (NeRF) have\nshown compelling results in implicit representations, the problem of\nlarge-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR\nframes remains unexplored. To bridge this gap, we propose a 3D scene\nreconstruction and novel view synthesis framework called parent-child neural\nradiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF,\nthe framework implements hierarchical spatial partitioning and multi-level\nscene representation, including scene, segment, and point levels. The\nmulti-level scene representation enhances the efficient utilization of sparse\nLiDAR point cloud data and enables the rapid acquisition of an approximate\nvolumetric scene representation. With extensive experiments, PC-NeRF is proven\nto achieve high-precision novel LiDAR view synthesis and 3D reconstruction in\nlarge-scale scenes. Moreover, PC-NeRF can effectively handle situations with\nsparse LiDAR frames and demonstrate high deployment efficiency with limited\ntraining epochs. Our approach implementation and the pre-trained models are\navailable at https://github.com/biter0088/pc-nerf.'}, {'Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in\n  Autonomous Driving Applications': 'Understanding how the surrounding environment changes is crucial for\nperforming downstream tasks safely and reliably in autonomous driving\napplications. Recent occupancy estimation techniques using only camera images\nas input can provide dense occupancy representations of large-scale scenes\nbased on the current observation. However, they are mostly limited to\nrepresenting the current 3D space and do not consider the future state of\nsurrounding objects along the time axis. To extend camera-only occupancy\nestimation into spatiotemporal prediction, we propose Cam4DOcc, a new benchmark\nfor camera-only 4D occupancy forecasting, evaluating the surrounding scene\nchanges in a near future. We build our benchmark based on multiple publicly\navailable datasets, including nuScenes, nuScenes-Occupancy, and Lyft-Level5,\nwhich provides sequential occupancy states of general movable and static\nobjects, as well as their 3D backward centripetal flow. To establish this\nbenchmark for future research with comprehensive comparisons, we introduce four\nbaseline types from diverse camera-based perception and prediction\nimplementations, including a static-world occupancy model, voxelization of\npoint cloud prediction, 2D-3D instance-based prediction, and our proposed novel\nend-to-end 4D occupancy forecasting network. Furthermore, the standardized\nevaluation protocol for preset multiple tasks is also provided to compare the\nperformance of all the proposed baselines on present and future occupancy\nestimation with respect to objects of interest in autonomous driving scenarios.\nThe dataset and our implementation of all four baselines in the proposed\nCam4DOcc benchmark will be released here: https://github.com/haomo-ai/Cam4DOcc.'}, {'OverlapTransformer: An Efficient and Rotation-Invariant Transformer\n  Network for LiDAR-Based Place Recognition': 'Place recognition is an important capability for autonomously navigating\nvehicles operating in complex environments and under changing conditions. It is\na key component for tasks such as loop closing in SLAM or global localization.\nIn this paper, we address the problem of place recognition based on 3D LiDAR\nscans recorded by an autonomous vehicle. We propose a novel lightweight neural\nnetwork exploiting the range image representation of LiDAR sensors to achieve\nfast execution with less than 2 ms per frame. We design a yaw-angle-invariant\narchitecture exploiting a transformer network, which boosts the place\nrecognition performance of our method. We evaluate our approach on the KITTI\nand Ford Campus datasets. The experimental results show that our method can\neffectively detect loop closures compared to the state-of-the-art methods and\ngeneralizes well across different environments. To evaluate long-term place\nrecognition performance, we provide a novel dataset containing LiDAR sequences\nrecorded by a mobile robot in repetitive places at different times. The\nimplementation of our method and dataset are released here:\nhttps://github.com/haomo-ai/OverlapTransformer'}]","Title: Improved Temporal Forecasting of Human-Object Interactions: A Deep Iterative Approach

_Abstract_

Research Problem and Background: Human-object interactions (HOI) are a crucial component of natural human behavior. Precise forecasting of these interactions involves predicting the optimal physical execution of human actions towards objects, a challenge driven by complex decision-making and environmental dynamics. Existing approaches often struggle with incorporating long-term dependencies and maintaining human-object constraints under sequential feature sequences.

Objective and Innovations: This research introduces an improved denoising diffusion model, named Diff-IP2D, which advances the current state-of-the-art in forecasting future HOsIs. Our work proposes an iterative neural autoregressive (iter-NAR) paradigm in the latent feature space, enhancing the forecasting of both hand trajectories and object affordances over a future time span. We operationalize egomotion-aware denoising to mitigate artifacts in the sequence, improving the coherence and realism of HOI predictions.

Methodology: The Diff-IP2D model features partial denoising techniques in both forward evolution and reverse value estimation phases. This framework is augmented with a motion encoder to predict egomotion from 2D/3D keyframes, which is used as guidance in the denoising process. Inference adopts the score-based diffusion model and Denoising Automatically Muse()(DAMT) for reconstruction, with key innovations in the iterative denoising step for handling the syntax-articulated and sequential nature of HOIs.

Results and Findings: Our experiments on the Epic-Kitchens dataset amount to a clear demonstration of performance enhancement over prior state-of-the-art methods. The use of iterative denoising in Diff-IP2D resulted in more accurate predictions of both hand trajectories and object interactions, with improvements measured by various quantification techniques.

Contributions and Implications: Diff-IP2D makes significant advances by improving temporal correlation and forecasting accuracy in human-object interaction predictions. Notably, the integration of egomotion guidance and the iterative denoising framework substantially enriches our understanding and capability to model complex multi-object interactions in real-world scenarios. This framework holds promise for applications in robotics, AI-assisted video analysis, and interactive human-computer interfaces for more intelligent, contextually aware systems.

Thus, this research not only advances the technical landscape of forecasting human-object interactions but also paves new avenues for intelligent interface design, aiming to facilitate seamless and efficient human-machine encounters in various technological domains."
"Unstructured data formats account for over 80% of the data currently stored,
and extracting value from such formats remains a considerable challenge. In
particular, current approaches for managing unstructured documents do not
support ad-hoc analytical queries on document collections. Moreover, Large
Language Models (LLMs) directly applied to the documents themselves, or on
portions of documents through a process of Retrieval-Augmented Generation
(RAG), fail to provide high accuracy query results, and in the LLM-only case,
additionally incur high costs. Since many unstructured documents in a
collection often follow similar templates that impart a common semantic
structure, we introduce ZenDB, a document analytics system that leverages this
semantic structure, coupled with LLMs, to answer ad-hoc SQL queries on document
collections. ZenDB efficiently extracts semantic hierarchical structures from
such templatized documents, and introduces a novel query engine that leverages
these structures for accurate and cost-effective query execution. Users can
impose a schema on their documents, and query it, all via SQL. Extensive
experiments on three real-world document collections demonstrate ZenDB's
benefits, achieving up to 30% cost savings compared to LLM-based baselines,
while maintaining or improving accuracy, and surpassing RAG-based baselines by
up to 61% in precision and 80% in recall, at a marginally higher cost.","[{'QUIP: Query-driven Missing Value Imputation': 'Missing values widely exist in real-world data sets, and failure to clean the\nmissing data may result in the poor quality of answers to queries.\n\\yiming{Traditionally, missing value imputation has been studied as an offline\nprocess as part of preparing data for analysis.} This paper studies query-time\nmissing value imputation and proposes QUIP, which only imputes minimal missing\nvalues to answer the query. Specifically, by taking a reasonable good query\nplan as input, QUIP tries to minimize the missing value imputation cost and\nquery processing overhead. QUIP proposes a new implementation of outer join to\npreserve missing values in query processing and a bloom filter based index\nstructure to optimize the space and runtime overhead. QUIP also designs a\ncost-based decision function to automatically guide each operator to impute\nmissing values now or delay imputations. Efficient optimizations are proposed\nto speed-up aggregate operations in QUIP, such as MAX/MIN operator. Extensive\nexperiments on both real and synthetic data sets demonstrates the effectiveness\nand efficiency of QUIP, which outperforms the state-of-the-art ImputeDB by 2 to\n10 times on different query sets and data sets, and achieves the\norder-of-magnitudes improvement over the offline approach.'}, {'RelicVR: A Virtual Reality Game for Active Exploration of Archaeological\n  Relics': ""Digitalization is changing how people visit museums and explore the artifacts\nthey house. Museums, as important educational venues outside classrooms, need\nto actively explore the application of digital interactive media, including\ngames that can balance entertainment and knowledge acquisition. In this paper,\nwe introduce RelicVR, a virtual reality (VR) game that encourages players to\ndiscover artifacts through physical interaction in a game-based approach.\nPlayers need to unearth artifacts hidden in a clod enclosure by using available\ntools and physical movements. The game relies on the dynamic voxel deformation\ntechnique to allow players to chip away earth covering the artifacts. We added\nuncertainty in the exploration process to bring it closer to how archaeological\ndiscovery happens in real life. Players do not know the shape or features of\nthe hidden artifact and have to take away the earth gradually but strategically\nwithout hitting the artifact itself. From playtesting sessions with eight\nparticipants, we found that the uncertainty elements are conducive to their\nengagement and exploration experience. Overall, RelicVR is an innovative game\nthat can improve players' learning motivation and outcomes of ancient\nartifacts.""}, {'MobiFace: A Novel Dataset for Mobile Face Tracking in the Wild': 'Face tracking serves as the crucial initial step in mobile applications\ntrying to analyse target faces over time in mobile settings. However, this\nproblem has received little attention, mainly due to the scarcity of dedicated\nface tracking benchmarks. In this work, we introduce MobiFace, the first\ndataset for single face tracking in mobile situations. It consists of 80\nunedited live-streaming mobile videos captured by 70 different smartphone users\nin fully unconstrained environments. Over $95K$ bounding boxes are manually\nlabelled. The videos are carefully selected to cover typical smartphone usage.\nThe videos are also annotated with 14 attributes, including 6 newly proposed\nattributes and 8 commonly seen in object tracking. 36 state-of-the-art\ntrackers, including facial landmark trackers, generic object trackers and\ntrackers that we have fine-tuned or improved, are evaluated. The results\nsuggest that mobile face tracking cannot be solved through existing approaches.\nIn addition, we show that fine-tuning on the MobiFace training data\nsignificantly boosts the performance of deep learning-based trackers,\nsuggesting that MobiFace captures the unique characteristics of mobile face\ntracking. Our goal is to offer the community a diverse dataset to enable the\ndesign and evaluation of mobile face trackers. The dataset, annotations and the\nevaluation server will be on \\url{https://mobiface.github.io/}.'}, {'RoI Tanh-polar Transformer Network for Face Parsing in the Wild': 'Face parsing aims to predict pixel-wise labels for facial components of a\ntarget face in an image. Existing approaches usually crop the target face from\nthe input image with respect to a bounding box calculated during\npre-processing, and thus can only parse inner facial Regions of\nInterest~(RoIs). Peripheral regions like hair are ignored and nearby faces that\nare partially included in the bounding box can cause distractions. Moreover,\nthese methods are only trained and evaluated on near-frontal portrait images\nand thus their performance for in-the-wild cases has been unexplored. To\naddress these issues, this paper makes three contributions. First, we introduce\niBugMask dataset for face parsing in the wild, which consists of 21,866\ntraining images and 1,000 testing images. The training images are obtained by\naugmenting an existing dataset with large face poses. The testing images are\nmanually annotated with $11$ facial regions and there are large variations in\nsizes, poses, expressions and background. Second, we propose RoI Tanh-polar\ntransform that warps the whole image to a Tanh-polar representation with a\nfixed ratio between the face area and the context, guided by the target\nbounding box. The new representation contains all information in the original\nimage, and allows for rotation equivariance in the convolutional neural\nnetworks~(CNNs). Third, we propose a hybrid residual representation learning\nblock, coined HybridBlock, that contains convolutional layers in both the\nTanh-polar space and the Tanh-Cartesian space, allowing for receptive fields of\ndifferent shapes in CNNs. Through extensive experiments, we show that the\nproposed method improves the state-of-the-art for face parsing in the wild and\ndoes not require facial landmarks for alignment.'}, {'FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in\n  the Wild': ""Image-based age estimation aims to predict a person's age from facial images.\nIt is used in a variety of real-world applications. Although end-to-end deep\nmodels have achieved impressive results for age estimation on benchmark\ndatasets, their performance in-the-wild still leaves much room for improvement\ndue to the challenges caused by large variations in head pose, facial\nexpressions, and occlusions. To address this issue, we propose a simple yet\neffective method to explicitly incorporate facial semantics into age\nestimation, so that the model would learn to correctly focus on the most\ninformative facial components from unaligned facial images regardless of head\npose and non-rigid deformation. To this end, we design a face parsing-based\nnetwork to learn semantic information at different scales and a novel face\nparsing attention module to leverage these semantic features for age\nestimation. To evaluate our method on in-the-wild data, we also introduce a new\nchallenging large-scale benchmark called IMDB-Clean. This dataset is created by\nsemi-automatically cleaning the noisy IMDB-WIKI dataset using a constrained\nclustering method. Through comprehensive experiment on IMDB-Clean and other\nbenchmark datasets, under both intra-dataset and cross-dataset evaluation\nprotocols, we show that our method consistently outperforms all existing age\nestimation methods and achieves a new state-of-the-art performance. To the best\nof our knowledge, our work presents the first attempt of leveraging face\nparsing attention to achieve semantic-aware age estimation, which may be\ninspiring to other high level facial analysis tasks. Code and data are\navailable on \\url{https://github.com/ibug-group/fpage}.""}, {'Deep Polarization Imaging for 3D shape and SVBRDF Acquisition': 'We present a novel method for efficient acquisition of shape and spatially\nvarying reflectance of 3D objects using polarization cues. Unlike previous\nworks that have exploited polarization to estimate material or object\nappearance under certain constraints (known shape or multiview acquisition), we\nlift such restrictions by coupling polarization imaging with deep learning to\nachieve high quality estimate of 3D object shape (surface normals and depth)\nand SVBRDF using single-view polarization imaging under frontal flash\nillumination. In addition to acquired polarization images, we provide our deep\nnetwork with strong novel cues related to shape and reflectance, in the form of\na normalized Stokes map and an estimate of diffuse color. We additionally\ndescribe modifications to network architecture and training loss which provide\nfurther qualitative improvements. We demonstrate our approach to achieve\nsuperior results compared to recent works employing deep learning in\nconjunction with flash illumination.'}, {'Auto-BI: Automatically Build BI-Models Leveraging Local Join Prediction\n  and Global Schema Graph': 'Business Intelligence (BI) is crucial in modern enterprises and\nbillion-dollar business. Traditionally, technical experts like database\nadministrators would manually prepare BI-models (e.g., in star or snowflake\nschemas) that join tables in data warehouses, before less-technical business\nusers can run analytics using end-user dashboarding tools. However, the\npopularity of self-service BI (e.g., Tableau and Power-BI) in recent years\ncreates a strong demand for less technical end-users to build BI-models\nthemselves.\n  We develop an Auto-BI system that can accurately predict BI models given a\nset of input tables, using a principled graph-based optimization problem we\npropose called \\textit{k-Min-Cost-Arborescence} (k-MCA), which holistically\nconsiders both local join prediction and global schema-graph structures,\nleveraging a graph-theoretical structure called \\textit{arborescence}. While we\nprove k-MCA is intractable and inapproximate in general, we develop novel\nalgorithms that can solve k-MCA optimally, which is shown to be efficient in\npractice with sub-second latency and can scale to the largest BI-models we\nencounter (with close to 100 tables).\n  Auto-BI is rigorously evaluated on a unique dataset with over 100K real BI\nmodels we harvested, as well as on 4 popular TPC benchmarks. It is shown to be\nboth efficient and accurate, achieving over 0.9 F1-score on both real and\nsynthetic benchmarks.'}, {'Efficient Entity Resolution on Heterogeneous Records': 'Entity resolution (ER) is the problem of identifying and merging records that\nrefer to the same real-world entity. In many scenarios, raw records are stored\nunder heterogeneous environment. Specifically, the schemas of records may\ndiffer from each other. To leverage such records better, most existing work\nassume that schema matching and data exchange have been done to convert records\nunder different schemas to those under a predefined schema. However, we observe\nthat schema matching would lose information in some cases, which could be\nuseful or even crucial to ER.\n  To leverage sufficient information from heterogeneous sources, in this paper,\nwe address several challenges of ER on heterogeneous records and show that none\nof existing similarity metrics or their transformations could be applied to\nfind similar records under heterogeneous settings. Motivated by this, we design\nthe similarity function and propose a novel framework to iteratively find\nrecords which refer to the same entity. Regarding efficiency, we build an index\nto generate candidates and accelerate similarity computation. Evaluations on\nreal-world datasets show the effectiveness and efficiency of our methods.'}, {'Data Source Selection for Information Integration in Big Data Era': 'In Big data era, information integration often requires abundant data\nextracted from massive data sources. Due to a large number of data sources,\ndata source selection plays a crucial role in information integration, since it\nis costly and even impossible to access all data sources. Data Source selection\nshould consider both efficiency and effectiveness issues. For efficiency, the\napproach should achieve high performance and be scalability to fit large data\nsource amount. From effectiveness aspect, data quality and overlapping of\nsources are to be considered, since data quality varies much from data sources,\nwith significant differences in the accuracy and coverage of the data provided,\nand the overlapping of sources can even lower the quality of data integrated\nfrom selected data sources.\n  In this paper, we study source selection problem in \\textit{Big Data Era} and\npropose methods which can scale to datasets with up to millions of data sources\nand guarantee the quality of results. Motivated by this, we propose a new\nobject function taking the expected number of true values a source can provide\nas a criteria to evaluate the contribution of a data source. Based on our\nproposed index we present a scalable algorithm and two pruning strategies to\nimprove the efficiency without sacrificing precision. Experimental results on\nboth real world and synthetic data sets show that our methods can select\nsources providing a large proportion of true values efficiently and can scale\nto massive data sources.'}, {'Context Does Matter: End-to-end Panoptic Narrative Grounding with\n  Deformable Attention Refined Matching Network': 'Panoramic Narrative Grounding (PNG) is an emerging visual grounding task that\naims to segment visual objects in images based on dense narrative captions. The\ncurrent state-of-the-art methods first refine the representation of phrase by\naggregating the most similar $k$ image pixels, and then match the refined text\nrepresentations with the pixels of the image feature map to generate\nsegmentation results. However, simply aggregating sampled image features\nignores the contextual information, which can lead to phrase-to-pixel\nmis-match. In this paper, we propose a novel learning framework called\nDeformable Attention Refined Matching Network (DRMN), whose main idea is to\nbring deformable attention in the iterative process of feature learning to\nincorporate essential context information of different scales of pixels. DRMN\niteratively re-encodes pixels with the deformable attention network after\nupdating the feature representation of the top-$k$ most similar pixels. As\nsuch, DRMN can lead to accurate yet discriminative pixel representations,\npurify the top-$k$ most similar pixels, and consequently alleviate the\nphrase-to-pixel mis-match substantially.Experimental results show that our\nnovel design significantly improves the matching results between text phrases\nand image pixels. Concretely, DRMN achieves new state-of-the-art performance on\nthe PNG benchmark with an average recall improvement 3.5%. The codes are\navailable in: https://github.com/JaMesLiMers/DRMN.'}]","Title: Advancing Document Analytics with ZenDB: Semantic Hierarchical Tables for Enhanced Querying and Extraction 

Abstract:

As unstructured data continues to overwhelm traditional analytical capabilities, there is a pressing need for innovative solutions that enable efficient querying and extraction of valuable insights. This paper introduces ZenDB, a system that significantly enhances document analytics through the utilization of structured hierarchical tables (SHTs) with semantic structure and dynamic tables (DTables). ZenDB exemplifies groundbreaking innovation by marrying the strengths of large language models (LLMs) with hierarchical structure in semantically rich documents such as scientific papers, civic agenda reports, and notice of violations.

The primary objective is to develop and implement an efficient querying framework for various document collections, with the goal of surpassing existing methods in terms of accuracy and cost efficiency. Byzantine LLMs, while powerful, are prone to errors, especially in handling large contexts. This limitations make them less suitable for complex query processing. To address these issues, ZenDB leverages the inherent semantic structure of templated documents and combines structured and visual hint methods with robust LLM-based and rule-based approaches.

Key innovations in ZenDB include its ability to relavantly segment documents and extract meaningful information through a unique hierarchy constructed as part of the system, as opposed to traditional access measures. It enables accurate and efficient querying mechanisms costly to traditional methods like language models or retrieval-augmented generation (RAG) approaches for long documents with queries involving aggregations and filters. 

Methodologically, ZenDB consists of a Semantic Hierarchical Table construction, allowing detailed information retrieval based on semantically organized entries. Additionally, it includes a system-maintained SHT, a user-defined DTable system where users can utilize SQL to design their own tables for queries, and a metadata management system for tracking user-created tables. These functionalities enable users to construct complex queries that surpass those produced by standalone LLMs or RAG algorithms while significantly reducing costs. 

Findings reveal that ZenDB surpasses both LLMs and RAG methods in accuracy (25% increase), recall (48% increase), and lowers cost by up to 93%. Through an implementation in real-world datasets of scientific publications, civic agenda reports, and notices of violations, ZenDB showcases its value in enhancing query performance at a fraction of the cost, demonstrating its applicability for broad sectors and its potential to transform data-driven decision-making processes across industries. Consequently, ZenDB addresses a significant gap in the area of efficient document querying and extraction, setting a new benchmark for future research in the field."
"Recent advances in diffusion-based generative modeling have led to the
development of text-to-video (T2V) models that can generate high-quality videos
conditioned on a text prompt. Most of these T2V models often produce
single-scene video clips that depict an entity performing a particular action
(e.g., `a red panda climbing a tree'). However, it is pertinent to generate
multi-scene videos since they are ubiquitous in the real-world (e.g., `a red
panda climbing a tree' followed by `the red panda sleeps on the top of the
tree'). To generate multi-scene videos from the pretrained T2V model, we
introduce Time-Aligned Captions (TALC) framework. Specifically, we enhance the
text-conditioning mechanism in the T2V architecture to recognize the temporal
alignment between the video scenes and scene descriptions. For instance, we
condition the visual features of the earlier and later scenes of the generated
video with the representations of the first scene description (e.g., `a red
panda climbing a tree') and second scene description (e.g., `the red panda
sleeps on the top of the tree'), respectively. As a result, we show that the
T2V model can generate multi-scene videos that adhere to the multi-scene text
descriptions and be visually consistent (e.g., entity and background). Further,
we finetune the pretrained T2V model with multi-scene video-text data using the
TALC framework. We show that the TALC-finetuned model outperforms the baseline
methods by 15.5 points in the overall score, which averages visual consistency
and text adherence using human evaluation. The project website is
https://talc-mst2v.github.io/.","[{'Resting state-fMRI approach towards understanding impairments in mTLE': 'Mesial temporal lobe epilepsy (mTLE) is the most common form of epilepsy.\nWhile it is characterized by an epileptogenic focus in the mesial temporal\nlobe, it is increasingly understood as a network disorder. Hence, understanding\nthe nature of impairments on a network level is essential for its diagnosis and\ntreatment. In this work, we review recent works that apply resting-state\nfunctional MRI to provide key insights into the impairments to the functional\narchitecture in mTLE. We discuss changes on both regional and global scales.\nFinally, we describe how Machine Learning can be applied to rs-fMRI data to\nextract resting-state networks specific to mTLE and for automated diagnosis of\nthis disease.'}, {'Leaving Reality to Imagination: Robust Classification via Generated\n  Datasets': 'Recent research on robustness has revealed significant performance gaps\nbetween neural image classifiers trained on datasets that are similar to the\ntest set, and those that are from a naturally shifted distribution, such as\nsketches, paintings, and animations of the object categories observed during\ntraining. Prior work focuses on reducing this gap by designing engineered\naugmentations of training data or through unsupervised pretraining of a single\nlarge model on massive in-the-wild training datasets scraped from the Internet.\nHowever, the notion of a dataset is also undergoing a paradigm shift in recent\nyears. With drastic improvements in the quality, ease-of-use, and access to\nmodern generative models, generated data is pervading the web. In this light,\nwe study the question: How do these generated datasets influence the natural\nrobustness of image classifiers? We find that Imagenet classifiers trained on\nreal data augmented with generated data achieve higher accuracy and effective\nrobustness than standard training and popular augmentation strategies in the\npresence of natural distribution shifts. We analyze various factors influencing\nthese results, including the choice of conditioning strategies and the amount\nof generated data. Additionally, we find that the standard ImageNet classifiers\nsuffer a performance degradation of upto 20\\% on the generated data, indicating\ntheir fragility at accurately classifying the objects under novel variations.\nLastly, we demonstrate that the image classifiers, which have been trained on\nreal data augmented with generated data from the base generative model, exhibit\ngreater resilience to natural distribution shifts compared to the classifiers\ntrained on real data augmented with generated data from the finetuned\ngenerative model on the real data. The code, models, and datasets are available\nat https://github.com/Hritikbansal/generative-robustness.'}, {""An improved sex specific and age dependent classification model for\n  Parkinson's diagnosis using handwriting measurement"": ""Accurate diagnosis is crucial for preventing the progression of Parkinson's,\nas well as improving the quality of life with individuals with Parkinson's\ndisease. In this paper, we develop a sex-specific and age-dependent\nclassification method to diagnose the Parkinson's disease using the online\nhandwriting recorded from individuals with\nParkinson's(n=37;m/f-19/18;age-69.3+-10.9years) and healthy\ncontrols(n=38;m/f-20/18;age-62.4+-11.3 years).The sex specific and age\ndependent classifier was observed significantly outperforming the generalized\nclassifier. An improved accuracy of 83.75%(SD+1.63) with female specific\nclassifier, and 79.55%(SD=1.58) with old age dependent classifier was observed\nin comparison to 75.76%(SD=1.17) accuracy with the generalized classifier.\nFinally, combining the age and sex information proved to be encouraging in\nclassification. We performed a rigorous analysis to observe the dominance of\nsex specific and age dependent features for Parkinson's detection and ranked\nthem using the support vector machine(SVM) ranking method. Distinct set of\nfeatures were observed to be dominating for higher classification accuracy in\ndifferent category of classification.""}, {'Peering Through Preferences: Unraveling Feedback Acquisition for\n  Aligning Large Language Models': ""Aligning large language models (LLMs) with human values and intents\ncritically involves the use of human or AI feedback. While dense feedback\nannotations are expensive to acquire and integrate, sparse feedback presents a\nstructural design choice between ratings (e.g., score Response A on a scale of\n1-7) and rankings (e.g., is Response A better than Response B?). In this work,\nwe analyze the effect of this design choice for the alignment and evaluation of\nLLMs. We uncover an inconsistency problem wherein the preferences inferred from\nratings and rankings significantly disagree 60% for both human and AI\nannotators. Our subsequent analysis identifies various facets of annotator\nbiases that explain this phenomena, such as human annotators would rate denser\nresponses higher while preferring accuracy during pairwise judgments. To our\nsurprise, we also observe that the choice of feedback protocol also has a\nsignificant effect on the evaluation of aligned LLMs. In particular, we find\nthat LLMs that leverage rankings data for alignment (say model X) are preferred\nover those that leverage ratings data (say model Y), with a rank-based\nevaluation protocol (is X/Y's response better than reference response?) but not\nwith a rating-based evaluation protocol (score Rank X/Y's response on a scale\nof 1-7). Our findings thus shed light on critical gaps in methods for\nevaluating the real-world utility of language models and their strong\ndependence on the feedback protocol used for alignment. Our code and data are\navailable at https://github.com/Hritikbansal/sparse_feedback.""}, {'Can RNNs trained on harder subject-verb agreement instances still\n  perform well on easier ones?': ""Previous work suggests that RNNs trained on natural language corpora can\ncapture number agreement well for simple sentences but perform less well when\nsentences contain agreement attractors: intervening nouns between the verb and\nthe main subject with grammatical number opposite to the latter. This suggests\nthese models may not learn the actual syntax of agreement, but rather infer\nshallower heuristics such as `agree with the recent noun'. In this work, we\ninvestigate RNN models with varying inductive biases trained on selectively\nchosen `hard' agreement instances, i.e., sentences with at least one agreement\nattractor. For these the verb number cannot be predicted using a simple linear\nheuristic, and hence they might help provide the model additional cues for\nhierarchical syntax. If RNNs can learn the underlying agreement rules when\ntrained on such hard instances, then they should generalize well to other\nsentences, including simpler ones. However, we observe that several RNN types,\nincluding the ONLSTM which has a soft structural inductive bias, surprisingly\nfail to perform well on sentences without attractors when trained solely on\nsentences with attractors. We analyze how these selectively trained RNNs\ncompare to the baseline (training on a natural distribution of agreement\nattractors) along the dimensions of number agreement accuracy, representational\nsimilarity, and performance across different syntactic constructions. Our\nfindings suggest that RNNs trained on our hard agreement instances still do not\ncapture the underlying syntax of agreement, but rather tend to overfit the\ntraining distribution in a way which leads them to perform poorly on `easy'\nout-of-distribution instances. Thus, while RNNs are powerful models which can\npick up non-trivial dependency patterns, inducing them to do so at the level of\nsyntax rather than surface remains a challenge.""}, {'How much complexity does an RNN architecture need to learn\n  syntax-sensitive dependencies?': 'Long short-term memory (LSTM) networks and their variants are capable of\nencapsulating long-range dependencies, which is evident from their performance\non a variety of linguistic tasks. On the other hand, simple recurrent networks\n(SRNs), which appear more biologically grounded in terms of synaptic\nconnections, have generally been less successful at capturing long-range\ndependencies as well as the loci of grammatical errors in an unsupervised\nsetting. In this paper, we seek to develop models that bridge the gap between\nbiological plausibility and linguistic competence. We propose a new\narchitecture, the Decay RNN, which incorporates the decaying nature of neuronal\nactivations and models the excitatory and inhibitory connections in a\npopulation of neurons. Besides its biological inspiration, our model also shows\ncompetitive performance relative to LSTMs on subject-verb agreement, sentence\ngrammaticality, and language modeling tasks. These results provide some\npointers towards probing the nature of the inductive biases required for RNN\narchitectures to model linguistic phenomena successfully.'}, {'Systematic Generalization in Neural Networks-based Multivariate Time\n  Series Forecasting Models': 'Systematic generalization aims to evaluate reasoning about novel combinations\nfrom known components, an intrinsic property of human cognition. In this work,\nwe study systematic generalization of NNs in forecasting future time series of\ndependent variables in a dynamical system, conditioned on past time series of\ndependent variables, and past and future control variables. We focus on\nsystematic generalization wherein the NN-based forecasting model should perform\nwell on previously unseen combinations or regimes of control variables after\nbeing trained on a limited set of the possible regimes. For NNs to depict such\nout-of-distribution generalization, they should be able to disentangle the\nvarious dependencies between control variables and dependent variables. We\nhypothesize that a modular NN architecture guided by the readily-available\nknowledge of independence of control variables as a potentially useful\ninductive bias to this end. Through extensive empirical evaluation on a toy\ndataset and a simulated electric motor dataset, we show that our proposed\nmodular NN architecture serves as a simple yet highly effective inductive bias\nthat enabling better forecasting of the dependent variables up to large\nhorizons in contrast to standard NNs, and indeed capture the true dependency\nrelations between the dependent and the control variables.'}, {'How well can Text-to-Image Generative Models understand Ethical Natural\n  Language Interventions?': ""Text-to-image generative models have achieved unprecedented success in\ngenerating high-quality images based on natural language descriptions. However,\nit is shown that these models tend to favor specific social groups when\nprompted with neutral text descriptions (e.g., 'a photo of a lawyer').\nFollowing Zhao et al. (2021), we study the effect on the diversity of the\ngenerated images when adding ethical intervention that supports equitable\njudgment (e.g., 'if all individuals can be a lawyer irrespective of their\ngender') in the input prompts. To this end, we introduce an Ethical NaTural\nLanguage Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset\nto evaluate the change in image generations conditional on ethical\ninterventions across three social axes -- gender, skin color, and culture.\nThrough ENTIGEN framework, we find that the generations from minDALL.E,\nDALL.E-mini and Stable Diffusion cover diverse social groups while preserving\nthe image quality. Preliminary studies indicate that a large change in the\nmodel predictions is triggered by certain phrases such as 'irrespective of\ngender' in the context of gender bias in the ethical interventions. We release\ncode and annotated data at https://github.com/Hritikbansal/entigen_emnlp.""}, {'ClimateLearn: Benchmarking Machine Learning for Weather and Climate\n  Modeling': 'Modeling weather and climate is an essential endeavor to understand the near-\nand long-term impacts of climate change, as well as inform technology and\npolicymaking for adaptation and mitigation efforts. In recent years, there has\nbeen a surging interest in applying data-driven methods based on machine\nlearning for solving core problems such as weather forecasting and climate\ndownscaling. Despite promising results, much of this progress has been impaired\ndue to the lack of large-scale, open-source efforts for reproducibility,\nresulting in the use of inconsistent or underspecified datasets, training\nsetups, and evaluations by both domain scientists and artificial intelligence\nresearchers. We introduce ClimateLearn, an open-source PyTorch library that\nvastly simplifies the training and evaluation of machine learning models for\ndata-driven climate science. ClimateLearn consists of holistic pipelines for\ndataset processing (e.g., ERA5, CMIP6, PRISM), implementation of\nstate-of-the-art deep learning models (e.g., Transformers, ResNets), and\nquantitative and qualitative evaluation for standard weather and climate\nmodeling tasks. We supplement these functionalities with extensive\ndocumentation, contribution guides, and quickstart tutorials to expand access\nand promote community growth. We have also performed comprehensive forecasting\nand downscaling experiments to showcase the capabilities and key features of\nour library. To our knowledge, ClimateLearn is the first large-scale,\nopen-source effort for bridging research in weather and climate modeling with\nmodern machine learning systems. Our library is available publicly at\nhttps://github.com/aditya-grover/climate-learn.'}, {'ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in\n  Large Multimodal Models': ""Many real-world tasks require an agent to reason jointly over text and visual\nobjects, (e.g., navigating in public spaces), which we refer to as\ncontext-sensitive text-rich visual reasoning. Specifically, these tasks require\nan understanding of the context in which the text interacts with visual\nelements within an image. However, there is a lack of existing datasets to\nbenchmark the state-of-the-art multimodal models' capability on\ncontext-sensitive text-rich visual reasoning. In this paper, we introduce\nConTextual, a novel dataset featuring human-crafted instructions that require\ncontext-sensitive reasoning for text-rich images. We conduct experiments to\nassess the performance of 14 foundation models (GPT-4V, Gemini-Pro-Vision,\nLLaVA-Next) and establish a human performance baseline. Further, we perform\nhuman evaluations of the model responses and observe a significant performance\ngap of 30.8% between GPT-4V (the current best-performing Large Multimodal\nModel) and human performance. Our fine-grained analysis reveals that GPT-4V\nencounters difficulties interpreting time-related data and infographics.\nHowever, it demonstrates proficiency in comprehending abstract visual contexts\nsuch as memes and quotes. Finally, our qualitative analysis uncovers various\nfactors contributing to poor performance including lack of precise visual\nperception and hallucinations. Our dataset, code, and leaderboard can be found\non the project page https://con-textual.github.io/""}]","Title: Enhanced Text-to-Video Generation for Multi-Scene Scenarios: Creating Consistent, Realistic Videos 

Abstract:

As demand for creating diverse, high-quality video content outpaces human capacity, the need for efficient state-of-the-art text-to-video generation (T2V) tools has grown. This research investigates a novel Text-to-Video Conditioning (TALC) framework that significantly improves upon existing techniques for multi-scene video creation. Our objective is to systematically generate consistent, coherent videos across transitions between different scenes. The key innovation is theTemporal Alignment Layer Conditioning (TALC) framework that ensures seamless text-video alignment and background consistency by conditioning earlier visual representations on earlier text and later visual representations on later text.

We integrate this method with existing Text-to-Video models, enabling creation of multi-scene videos from a handcrafted text summarization or natural edits, enhancing scene adherence while retaining visual uniformity. By assessing performance through quantitative measures like visual consistency and text adherence, our method outperforms conventional approaches without any fine-tuning. This research introduces a refined and easily-integrated additive approach for robust multi-scene video generation, offering specific use cases in documentary filmmaking, education, and entertainment contexts. The development of TALC opens new possibilities for rapid, accurate video content creation, greatly expanding the role of AI in multimedia production."
"Schr\""odinger bridge (SB) has emerged as the go-to method for optimizing
transportation plans in diffusion models. However, SB requires estimating the
intractable forward score functions, inevitably resulting in the costly
implicit training loss based on simulated trajectories. To improve the
scalability while preserving efficient transportation plans, we leverage
variational inference to linearize the forward score functions (variational
scores) of SB and restore simulation-free properties in training backward
scores. We propose the variational Schr\""odinger diffusion model (VSDM), where
the forward process is a multivariate diffusion and the variational scores are
adaptively optimized for efficient transport. Theoretically, we use stochastic
approximation to prove the convergence of the variational scores and show the
convergence of the adaptively generated samples based on the optimal
variational scores. Empirically, we test the algorithm in simulated examples
and observe that VSDM is efficient in generations of anisotropic shapes and
yields straighter sample trajectories compared to the single-variate diffusion.
We also verify the scalability of the algorithm in real-world data and achieve
competitive unconditional generation performance in CIFAR10 and conditional
generation in time series modeling. Notably, VSDM no longer depends on warm-up
initializations and has become tuning-friendly in training large-scale
experiments.","[{'Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte\n  Carlo': 'The rise of artificial intelligence (AI) hinges on the efficient training of\nmodern deep neural networks (DNNs) for non-convex optimization and uncertainty\nquantification, which boils down to a non-convex Bayesian learning problem. A\nstandard tool to handle the problem is Langevin Monte Carlo, which proposes to\napproximate the posterior distribution with theoretical guarantees. In this\nthesis, we start with the replica exchange Langevin Monte Carlo (also known as\nparallel tempering), which proposes appropriate swaps between exploration and\nexploitation to achieve accelerations. However, the na\\""ive extension of swaps\nto big data problems leads to a large bias, and bias-corrected swaps are\nrequired. Such a mechanism leads to few effective swaps and insignificant\naccelerations. To alleviate this issue, we first propose a control variates\nmethod to reduce the variance of noisy energy estimators and show a potential\nto accelerate the exponential convergence. We also present the population-chain\nreplica exchange based on non-reversibility and obtain an optimal round-trip\nrate for deep learning. In the second part of the thesis, we study scalable\ndynamic importance sampling algorithms based on stochastic approximation.\nTraditional dynamic importance sampling algorithms have achieved success,\nhowever, the lack of scalability has greatly limited their extensions to big\ndata. To handle this scalability issue, we resolve the vanishing gradient\nproblem and propose two dynamic importance sampling algorithms. Theoretically,\nwe establish the stability condition for the underlying ordinary differential\nequation (ODE) system and guarantee the asymptotic convergence of the latent\nvariable to the desired fixed point. Interestingly, such a result still holds\ngiven non-convex energy landscapes.'}, {'Cosmological implications of Fast Radio Burst / Gamma-Ray Burst\n  Associations': 'If a small fraction of Fast Radio Bursts (FRBs) are associated with Gamma-Ray\nBursts (GRBs), as recently suggested by Zhang, the combination of redshift\nmeasurements of GRBs and dispersion measure (DM) measurements of FRBs opens a\nnew window to study cosmology. At $z<2$ where the universe is essentially fully\nionized, detections of FRB/GRB pairs can give an independent measurement of the\nintergalactic medium portion of the baryon mass fraction, $\\Omega_b f_{\\rm\nIGM}$, of the universe. If a good sample of FRB/GRB associations are discovered\nat higher redshifts, the free electron column density history can be mapped,\nwhich can be used to probe the reionization history of both hydrogen and helium\nin the universe. We apply our formulation to GRBs 101011A and 100704A that each\nmight have an associated FRB, and constrained $\\Omega_b f_{\\rm IGM}$ to be\nconsistent with the value derived from other methods. The methodology developed\nhere is also applicable, if the redshifts of FRBs not associated with GRBs can\nbe measured by other means.'}, {'Low Energy Spectral Index and $E_{p}$ evolution of Quasi-thermal\n  Photosphere Emission of Gamma-Ray Bursts': ""Recent observations by the Fermi satellite suggest that a photosphere\nemission component is contributing to the observed spectrum of many GRBs. One\nimportant question is whether the photosphere component can interpret the\ntypical ``Band'' function of GRBs with a typical low energy photon spectral\nindex $\\alpha \\sim -1$. We perform a detailed study of the photosphere emission\nspectrum by progressively introducing several physical ingredients previously\nnot fully incorporated, including the probability distribution of the location\nof a dynamically evolving photosphere, superposition of emission from an\nequal-arrival-time ``volume'' in a continuous wind, the evolution of optical\ndepth of a wind with finite but evolving outer boundary, as well as the effect\nof different top-hat wind luminosity ($L_w$) profiles. By assuming a co-moving\nblackbody spectrum emerging from the photosphere, we find that for an outflow\nwith a constant or increasing $L_w$, the low-energy spectrum below the peak\nenergy ($E_{p}$), can be modified to $F_\\nu \\sim \\nu^{1.5}$ ($\\alpha \\sim\n+0.5$). A softer ($-1<\\alpha<+0.5$) or flat ($\\alpha=-1$) spectrum can be\nobtained during the $L_w$ decreasing phase or high-latitude-emission-dominated\nphase. We also study the evolution of $E_{p}$ as a function of wind and\nphotosphere luminosity in this photosphere model. An $E_p-L$ tracking pattern\ncan be reproduced if a certain positive dependence between the dimensionless\nentropy $\\eta$ and $L_w$ is introduced. However, the hard-to-soft evolution\npattern cannot be reproduced unless a contrived condition is invoked. In order\nto interpret the Band spectrum, a more complicated photosphere model or a\ndifferent energy dissipation and radiation mechanism are needed.""}, {'Non-uniform dependence for higher dimensional Camassa-Holm equations in\n  Besov spaces': 'In this paper, we investigate the dependence on initial data of solutions to\nhigher dimensional Camassa-Holm equations. We show that the data-to-solution\nmap is not uniformly continuous dependence in Besov spaces\n$B^s_{p,r}(\\mathbb{R}^d),s>\\max\\{1+\\frac d2,\\frac32\\}$.'}, {'Interacting Contour Stochastic Gradient Langevin Dynamics': 'We propose an interacting contour stochastic gradient Langevin dynamics\n(ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic\ngradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show\nthat ICSGLD can be theoretically more efficient than a single-chain CSGLD with\nan equivalent computational budget. We also present a novel random-field\nfunction, which facilitates the estimation of self-adapting parameters in big\ndata and obtains free mode explorations. Empirically, we compare the proposed\nalgorithm with popular benchmark methods for posterior sampling. The numerical\nresults show a great potential of ICSGLD for large-scale uncertainty estimation\ntasks.'}, {'Relativistic MHD simulations of collision-induced magnetic dissipation\n  in Poynting-flux-dominated jets/outflows': ""We perform 3D relativistic ideal MHD simulations to study the collisions\nbetween high-$\\sigma$ (Poynting-flux-dominated) blobs which contain both\npoloidal and toroidal magnetic field components. This is meant to mimic the\ninteractions inside a highly variable Poynting-flux-dominated jet. We discover\na significant electromagnetic field (EMF) energy dissipation with an Alfv\\'enic\nrate with the efficiency around 35\\%. Detailed analyses show that this\ndissipation is mostly facilitated by the collision-induced magnetic\nreconnection. Additional resolution and parameter studies show a robust result\nthat the relative EMF energy dissipation efficiency is nearly independent of\nthe numerical resolution or most physical parameters in the relevant parameter\nrange. The reconnection outflows in our simulation can potentially form the\nmulti-orientation relativistic mini-jets as needed for several analytical\nmodels. We also find a linear relationship between the $\\sigma$ values before\nand after the major EMF energy dissipation process. Our results give support to\nthe proposed astrophysical models that invoke significant magnetic energy\ndissipation in Poynting-flux-dominated jets, such as the internal\ncollision-induced magnetic reconnection and turbulence (ICMART) model for GRBs,\nand reconnection triggered mini-jets model for AGNs.""}, {'An Adaptive Empirical Bayesian Method for Sparse Deep Learning': 'We propose a novel adaptive empirical Bayesian method for sparse deep\nlearning, where the sparsity is ensured via a class of self-adaptive\nspike-and-slab priors. The proposed method works by alternatively sampling from\nan adaptive hierarchical posterior distribution using stochastic gradient\nMarkov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters\nusing stochastic approximation (SA). We further prove the convergence of the\nproposed method to the asymptotically correct distribution under mild\nconditions. Empirical applications of the proposed method lead to the\nstate-of-the-art performance on MNIST and Fashion MNIST with shallow\nconvolutional neural networks and the state-of-the-art compression performance\non CIFAR10 with Residual Networks. The proposed method also improves resistance\nto adversarial attacks.'}, {'A Contour Stochastic Gradient Langevin Dynamics Algorithm for\n  Simulations of Multi-modal Distributions': 'We propose an adaptively weighted stochastic gradient Langevin dynamics\nalgorithm (SGLD), so-called contour stochastic gradient Langevin dynamics\n(CSGLD), for Bayesian learning in big data statistics. The proposed algorithm\nis essentially a \\emph{scalable dynamic importance sampler}, which\nautomatically \\emph{flattens} the target distribution such that the simulation\nfor a multi-modal distribution can be greatly facilitated. Theoretically, we\nprove a stability condition and establish the asymptotic convergence of the\nself-adapting parameter to a {\\it unique fixed-point}, regardless of the\nnon-convexity of the original energy function; we also present an error\nanalysis for the weighted averaging estimators. Empirically, the CSGLD\nalgorithm is tested on multiple benchmark datasets including CIFAR10 and\nCIFAR100. The numerical results indicate its superiority to avoid the local\ntrap problem in training deep neural networks.'}, {'Information Directed Sampling for Sparse Linear Bandits': 'Stochastic sparse linear bandits offer a practical model for high-dimensional\nonline decision-making problems and have a rich information-regret structure.\nIn this work we explore the use of information-directed sampling (IDS), which\nnaturally balances the information-regret trade-off. We develop a class of\ninformation-theoretic Bayesian regret bounds that nearly match existing lower\nbounds on a variety of problem instances, demonstrating the adaptivity of IDS.\nTo efficiently implement sparse IDS, we propose an empirical Bayesian approach\nfor sparse posterior sampling using a spike-and-slab Gaussian-Laplace prior.\nNumerical results demonstrate significant regret reductions by sparse IDS\nrelative to several baselines.'}, {'Ill-posedness for a generalized Camassa-Holm equation with higher-order\n  nonlinearity in the critical Besov space': 'In this paper, we prove that the Cauchy problem for a generalized\nCamassa-Holm equation with higher-order nonlinearity is ill-posed in the\ncritical Besov space $B^1_{\\infty,1}(\\R)$. It is shown in (J. Differ. Equ.,\n327:127-144,2022) that the Camassa-Holm equation is ill-posed in\n$B^1_{\\infty,1}(\\R)$, here we turn our attention to a higher-order nonlinear\ngeneralization of Camassa-Holm equation proposed by Hakkaev and Kirchev (Commun\nPartial Differ Equ 30:761-781,2005). With newly constructed initial data, we\nget the norm inflation in the critical space $B^1_{\\infty,1}(\\R)$ which leads\nto ill-posedness.'}]","Abstract:

High-fidelity generation in diffusion models, pivotal in image processing, faces limitations in optimal transport and cost-efficiency. We introduce Variational Schrödinger Diffusion Models (VSDMs), a novel approach that leverages theoretical underpinnings from Schrödinger bridges for enhanced transport optimization and efficiency. Building on both variational inference and diffusion concepts, VSDMs extend diffusion models by coupling a forward diffusion process and an optimized backward process to facilitate data conversion. 

Our main objective is to create models better suited for straighter data trajectories while maintaining computational efficiency and optimal transport properties, addressing key issues in existing diffusion models. We introduce two types of models: Blend-to-point VSDMs for versatile image manipulation and Point-to-point VSDMs for enhanced time series forecasting.

Utilizing closed-form expressions and parametric learning for backward processes调料ensaves computational cost and yields more efficient transport solutions. Empirical testing on synthetic datasets, real-world image generation, and time series forecasting validates VSDMs' ability to improve generation quality, model complex data distributions effectively, and reduce computational costs. The integration of variational inference enhances model scalability and applicability across multiple domains, making VSDMs a significant contribution to the field of generative modeling.

Notably, VSDMs provide improved scalability in handling high-dimensional data, efficient convergence, and apply equally well to tasks such as image generation and time-series forecasting. They represent a significant step forward in optimizing and enhancing the practicality of diffusion models for a wide range of applications, showcasing the versatility and power of Schrödinger bridge-based approaches in linearizing data flow and enhancing optimal transport mechanisms while maintaining efficiency."
"Controllable text-to-image (T2I) diffusion models generate images conditioned
on both text prompts and semantic inputs of other modalities like edge maps.
Nevertheless, current controllable T2I methods commonly face challenges related
to efficiency and faithfulness, especially when conditioning on multiple inputs
from either the same or diverse modalities. In this paper, we propose a novel
Flexible and Efficient method, FlexEControl, for controllable T2I generation.
At the core of FlexEControl is a unique weight decomposition strategy, which
allows for streamlined integration of various input types. This approach not
only enhances the faithfulness of the generated image to the control, but also
significantly reduces the computational overhead typically associated with
multimodal conditioning. Our approach achieves a reduction of 41% in trainable
parameters and 30% in memory usage compared with Uni-ControlNet. Moreover, it
doubles data efficiency and can flexibly generate images under the guidance of
multiple input conditions of various modalities.","[{'Multimodal Graph Transformer for Multimodal Question Answering': 'Despite the success of Transformer models in vision and language tasks, they\noften learn knowledge from enormous data implicitly and cannot utilize\nstructured input data directly. On the other hand, structured learning\napproaches such as graph neural networks (GNNs) that integrate prior\ninformation can barely compete with Transformer models. In this work, we aim to\nbenefit from both worlds and propose a novel Multimodal Graph Transformer for\nquestion answering tasks that requires performing reasoning across multiple\nmodalities. We introduce a graph-involved plug-and-play quasi-attention\nmechanism to incorporate multimodal graph information, acquired from text and\nvisual data, to the vanilla self-attention as effective prior. In particular,\nwe construct the text graph, dense region graph, and semantic graph to generate\nadjacency matrices, and then compose them with input vision and language\nfeatures to perform downstream reasoning. Such a way of regularizing\nself-attention with graph information significantly improves the inferring\nability and helps align features from different modalities. We validate the\neffectiveness of Multimodal Graph Transformer over its Transformer baselines on\nGQA, VQAv2, and MultiModalQA datasets.'}, {'Learning by Ignoring, with Application to Domain Adaptation': 'Learning by ignoring, which identifies less important things and excludes\nthem from the learning process, is broadly practiced in human learning and has\nshown ubiquitous effectiveness. There has been psychological studies showing\nthat learning to ignore certain things is a powerful tool for helping people\nfocus. In this paper, we explore whether this useful human learning methodology\ncan be borrowed to improve machine learning. We propose a novel machine\nlearning framework referred to as learning by ignoring (LBI). Our framework\nautomatically identifies pretraining data examples that have large domain shift\nfrom the target distribution by learning an ignoring variable for each example\nand excludes them from the pretraining process. We formulate LBI as a\nthree-level optimization framework where three learning stages are involved:\npretraining by minimizing the losses weighed by ignoring variables; finetuning;\nupdating the ignoring variables by minimizing the validation loss. A\ngradient-based algorithm is developed to efficiently solve the three-level\noptimization problem in LBI. Experiments on various datasets demonstrate the\neffectiveness of our framework.'}, {'MiniGPT-5: Interleaved Vision-and-Language Generation via Generative\n  Vokens': 'The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a\nprofound capability in multimodal understanding. However, the simultaneous\ngeneration of images with coherent texts is still underdeveloped. Addressing\nthis, we introduce a novel interleaved vision-and-language generation method,\ncentered around the concept of ``generative vokens"". These vokens serve as\npivotal elements contributing to coherent image-text outputs. Our method is\nmarked by a unique two-stage training strategy for description-free multimodal\ngeneration, which does not necessitate extensive descriptions of images. We\nintegrate classifier-free guidance to enhance the alignment of generated images\nand texts, ensuring more seamless and contextually relevant multimodal\ninteractions. Our model, MiniGPT-5, exhibits substantial improvement over the\nbaseline models on multimodal generation datasets, including MMDialog and VIST.\nThe human evaluation shows MiniGPT-5 is better than the baseline model on more\nthan 56\\% cases for multimodal generation, highlighting its efficacy across\ndiverse benchmarks.'}, {'PathVQA: 30000+ Questions for Medical Visual Question Answering': 'Is it possible to develop an ""AI Pathologist"" to pass the board-certified\nexamination of the American Board of Pathology? To achieve this goal, the first\nstep is to create a visual question answering (VQA) dataset where the AI agent\nis presented with a pathology image together with a question and is asked to\ngive the correct answer. Our work makes the first attempt to build such a\ndataset. Different from creating general-domain VQA datasets where the images\nare widely accessible and there are many crowdsourcing workers available and\ncapable of generating question-answer pairs, developing a medical VQA dataset\nis much more challenging. First, due to privacy concerns, pathology images are\nusually not publicly available. Second, only well-trained pathologists can\nunderstand pathology images, but they barely have time to help create datasets\nfor AI research. To address these challenges, we resort to pathology textbooks\nand online digital libraries. We develop a semi-automated pipeline to extract\npathology images and captions from textbooks and generate question-answer pairs\nfrom captions using natural language processing. We collect 32,799 open-ended\nquestions from 4,998 pathology images where each question is manually checked\nto ensure correctness. To our best knowledge, this is the first dataset for\npathology VQA. Our dataset will be released publicly to promote research in\nmedical VQA.'}, {'ComCLIP: Training-Free Compositional Image and Text Matching': ""Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for matching images and text. However, it is still\nchallenging to adapt vision-lanaguage pretrained models like CLIP to\ncompositional image and text matching -- a more challenging image and text\nmatching task requiring the model understanding of compositional word concepts\nand visual components. Towards better compositional generalization in zero-shot\nimage and text matching, in this paper, we study the problem from a causal\nperspective: the erroneous semantics of individual entities are essentially\nconfounders that cause the matching failure. Therefore, we propose a novel\n\\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP\ndisentangles input images into subjects, objects, and action sub-images and\ncomposes CLIP's vision encoder and text encoder to perform evolving matching\nover compositional text embedding and sub-image embeddings. In this way,\nComCLIP can mitigate spurious correlations introduced by the pretrained CLIP\nmodels and dynamically evaluate the importance of each component. Experiments\non four compositional image-text matching datasets: SVO, ComVG, Winoground, and\nVL-checklist, and two general image-text retrieval datasets: Flick30K, and\nMSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts\nthe \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even\nwithout further training or fine-tuning. Our codes can be found at\nhttps://github.com/eric-ai-lab/ComCLIP.""}, {'Parameter-efficient Model Adaptation for Vision Transformers': 'In computer vision, it has achieved great transfer learning performance via\nadapting large-scale pretrained vision models (e.g., vision transformers) to\ndownstream tasks. Common approaches for model adaptation either update all\nmodel parameters or leverage linear probes. In this paper, we aim to study\nparameter-efficient model adaptation strategies for vision transformers on the\nimage classification task. We formulate efficient model adaptation as a\nsubspace training problem and perform a comprehensive benchmarking over\ndifferent efficient adaptation methods. We conduct an empirical study on each\nefficient model adaptation method focusing on its performance alongside\nparameter cost. Furthermore, we propose a parameter-efficient model adaptation\nframework, which first selects submodules by measuring local intrinsic\ndimensions and then projects them into subspace for further decomposition via a\nnovel Kronecker Adaptation (KAdaptation) method. We analyze and compare our\nmethod with a diverse set of baseline model adaptation methods (including\nstate-of-the-art methods for pretrained language models). Our method performs\nthe best in terms of the tradeoff between accuracy and parameter efficiency\nacross 20 image classification datasets under the few-shot setting and 7 image\nclassification datasets under the full-shot setting.'}, {'Pathological Visual Question Answering': 'Is it possible to develop an ""AI Pathologist"" to pass the board-certified\nexamination of the American Board of Pathology (ABP)? To build such a system,\nthree challenges need to be addressed. First, we need to create a visual\nquestion answering (VQA) dataset where the AI agent is presented with a\npathology image together with a question and is asked to give the correct\nanswer. Due to privacy concerns, pathology images are usually not publicly\navailable. Besides, only well-trained pathologists can understand pathology\nimages, but they barely have time to help create datasets for AI research. The\nsecond challenge is: since it is difficult to hire highly experienced\npathologists to create pathology visual questions and answers, the resulting\npathology VQA dataset may contain errors. Training pathology VQA models using\nthese noisy or even erroneous data will lead to problematic models that cannot\ngeneralize well on unseen images. The third challenge is: the medical concepts\nand knowledge covered in pathology question-answer (QA) pairs are very diverse\nwhile the number of QA pairs available for modeling training is limited. How to\nlearn effective representations of diverse medical concepts based on limited\ndata is technically demanding. In this paper, we aim to address these three\nchallenges. To our best knowledge, our work represents the first one addressing\nthe pathology VQA problem. To deal with the issue that a publicly available\npathology VQA dataset is lacking, we create PathVQA dataset. To address the\nsecond challenge, we propose a learning-by-ignoring approach. To address the\nthird challenge, we propose to use cross-modal self-supervised learning. We\nperform experiments on our created PathVQA dataset and the results demonstrate\nthe effectiveness of our proposed learning-by-ignoring method and cross-modal\nself-supervised learning methods.'}, {'COVID-CT-Dataset: A CT Scan Dataset about COVID-19': 'During the outbreak time of COVID-19, computed tomography (CT) is a useful\nmanner for diagnosing COVID-19 patients. Due to privacy issues, publicly\navailable COVID-19 CT datasets are highly difficult to obtain, which hinders\nthe research and development of AI-powered diagnosis methods of COVID-19 based\non CTs. To address this issue, we build an open-sourced dataset -- COVID-CT,\nwhich contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19\nCTs. The utility of this dataset is confirmed by a senior radiologist who has\nbeen diagnosing and treating COVID-19 patients since the outbreak of this\npandemic. We also perform experimental studies which further demonstrate that\nthis dataset is useful for developing AI-based diagnosis models of COVID-19.\nUsing this dataset, we develop diagnosis methods based on multi-task learning\nand self-supervised learning, that achieve an F1 of 0.90, an AUC of 0.98, and\nan accuracy of 0.89. According to the senior radiologist, models with such\nperformance are good enough for clinical usage. The data and code are available\nat https://github.com/UCSD-AI4H/COVID-CT'}, {'Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining\n  Paradigms': 'Pretraining has become a standard technique in computer vision and natural\nlanguage processing, which usually helps to improve performance substantially.\nPreviously, the most dominant pretraining method is transfer learning (TL),\nwhich uses labeled data to learn a good representation network. Recently, a new\npretraining approach -- self-supervised learning (SSL) -- has demonstrated\npromising results on a wide range of applications. SSL does not require\nannotated labels. It is purely conducted on input data by solving auxiliary\ntasks defined on the input data examples. The current reported results show\nthat in certain applications, SSL outperforms TL and the other way around in\nother applications. There has not been a clear understanding on what properties\nof data and tasks render one approach outperforms the other. Without an\ninformed guideline, ML researchers have to try both methods to find out which\none is better empirically. It is usually time-consuming to do so. In this work,\nwe aim to address this problem. We perform a comprehensive comparative study\nbetween SSL and TL regarding which one works better under different properties\nof data and tasks, including domain difference between source and target tasks,\nthe amount of pretraining data, class imbalance in source data, and usage of\ntarget data for additional pretraining, etc. The insights distilled from our\ncomparative studies can help ML researchers decide which method to use based on\nthe properties of their applications.'}, {'JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for\n  Conversational Embodied Agents': 'Building a conversational embodied agent to execute real-life tasks has been\na long-standing yet quite challenging research goal, as it requires effective\nhuman-agent communication, multi-modal understanding, long-range sequential\ndecision making, etc. Traditional symbolic methods have scaling and\ngeneralization issues, while end-to-end deep learning models suffer from data\nscarcity and high task complexity, and are often hard to explain. To benefit\nfrom both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning\nframework for modular, generalizable, and interpretable conversational embodied\nagents. First, it acquires symbolic representations by prompting large language\nmodels (LLMs) for language understanding and sub-goal planning, and by\nconstructing semantic maps from visual observations. Then the symbolic module\nreasons for sub-goal planning and action generation based on task- and\naction-level common sense. Extensive experiments on the TEACh dataset validate\nthe efficacy and efficiency of our JARVIS framework, which achieves\nstate-of-the-art (SOTA) results on all three dialog-based embodied tasks,\nincluding Execution from Dialog History (EDH), Trajectory from Dialog (TfD),\nand Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen\nSuccess Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze\nthe essential factors that affect the task performance and also demonstrate the\nsuperiority of our method in few-shot settings. Our JARVIS model ranks first in\nthe Alexa Prize SimBot Public Benchmark Challenge.'}]","Abstract:

In the rapidly evolving field of AI-generated imagery, the ability to control image generation based on textual prompts has garnered significant interest. Yet, traditional methods struggle with efficiency, particularly when managing multiple heterogeneous conditions. This study introduces FlexEControl, a pioneering text-to-image generation framework that leverages low-rank adaptations to efficiently distill image details stipulated in the text. 

Notably, the paper advances the state of the art by proposing a unified approach to reduce memory cost, number of trainable parameters, and training time in comparison to existing methods, such as Uni-ControlNet and LowRank Adaptation (LoRA). A core innovation involves a new method for extracting structural inputs from conditions, optimizing local features for more precise control. The framework also includes a condition-friendly discriminative diffusion loss, enabling efficient, targeted denoising based on image conditions. Comparative evaluations across diverse conditions highlight FlexEControl's superior performance, merely requiring half the training time and 41% fewer parameters than Uni-ControlNet, but outperforming it in both image quality and alignment metrics. These contributions significantly enhance the controllability and efficiency of diffusion models, significantly benefiting applications in content generation, personalized imagery creation, and user-driven AI art.

With its sophisticated yet streamlined design, FlexEControl could serve as a technological cornerstone for advanced text-to-image synthesis in professional and consumer AI applications, offering improved responsiveness, cost-effectiveness, and user interactivity compared to current systems."
"Automatic medical image segmentation technology has the potential to expedite
pathological diagnoses, thereby enhancing the efficiency of patient care.
However, medical images often have complex textures and structures, and the
models often face the problem of reduced image resolution and information loss
due to downsampling. To address this issue, we propose HC-Mamba, a new medical
image segmentation model based on the modern state space model Mamba.
Specifically, we introduce the technique of dilated convolution in the HC-Mamba
model to capture a more extensive range of contextual information without
increasing the computational cost by extending the perceptual field of the
convolution kernel. In addition, the HC-Mamba model employs depthwise separable
convolutions, significantly reducing the number of parameters and the
computational power of the model. By combining dilated convolution and
depthwise separable convolutions, HC-Mamba is able to process large-scale
medical image data at a much lower computational cost while maintaining a high
level of performance. We conduct comprehensive experiments on segmentation
tasks including skin lesion, and conduct extensive experiments on ISIC17 and
ISIC18 to demonstrate the potential of the HC-Mamba model in medical image
segmentation. The experimental results show that HC-Mamba exhibits competitive
performance on all these datasets, thereby proving its effectiveness and
usefulness in medical image segmentation.","[{'Self-supervised Model Based on Masked Autoencoders Advance CT Scans\n  Classification': 'The coronavirus pandemic has been going on since the year 2019, and the trend\nis still not abating. Therefore, it is particularly important to classify\nmedical CT scans to assist in medical diagnosis. At present, Supervised Deep\nLearning algorithms have made a great success in the classification task of\nmedical CT scans, but medical image datasets often require professional image\nannotation, and many research datasets are not publicly available. To solve\nthis problem, this paper is inspired by the self-supervised learning algorithm\nMAE and uses the MAE model pre-trained on ImageNet to perform transfer learning\non CT Scans dataset. This method improves the generalization performance of the\nmodel and avoids the risk of overfitting on small datasets. Through extensive\nexperiments on the COVID-CT dataset and the SARS-CoV-2 dataset, we compare the\nSSL-based method in this paper with other state-of-the-art supervised\nlearning-based pretraining methods. Experimental results show that our method\nimproves the generalization performance of the model more effectively and\navoids the risk of overfitting on small datasets. The model achieved almost the\nsame accuracy as supervised learning on both test datasets. Finally, ablation\nexperiments aim to fully demonstrate the effectiveness of our method and how it\nworks.'}, {'Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical\n  Relation Extraction?': 'Two key obstacles in biomedical relation extraction (RE) are the scarcity of\nannotations and the prevalence of instances without explicitly pre-defined\nlabels due to low annotation coverage. Existing approaches, which treat\nbiomedical RE as a multi-class classification task, often result in poor\ngeneralization in low-resource settings and do not have the ability to make\nselective prediction on unknown cases but give a guess from seen relations,\nhindering the applicability of those approaches. We present NBR, which converts\nbiomedical RE as natural language inference formulation through indirect\nsupervision. By converting relations to natural language hypotheses, NBR is\ncapable of exploiting semantic cues to alleviate annotation scarcity. By\nincorporating a ranking-based loss that implicitly calibrates abstinent\ninstances, NBR learns a clearer decision boundary and is instructed to abstain\non uncertain instances. Extensive experiments on three widely-used biomedical\nRE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in\nboth full-set and low-resource regimes. Our analysis demonstrates that indirect\nsupervision benefits biomedical RE even when a domain gap exists, and combining\nNLI knowledge with biomedical knowledge leads to the best performance gains.'}, {'Unified Semantic Typing with Meaningful Label Inference': 'Semantic typing aims at classifying tokens or spans of interest in a textual\ncontext into semantic categories such as relations, entity types, and event\ntypes. The inferred labels of semantic categories meaningfully interpret how\nmachines understand components of text. In this paper, we present UniST, a\nunified framework for semantic typing that captures label semantics by\nprojecting both inputs and labels into a joint semantic embedding space. To\nformulate different lexical and relational semantic typing tasks as a unified\ntask, we incorporate task descriptions to be jointly encoded with the input,\nallowing UniST to be adapted to different tasks without introducing\ntask-specific model components. UniST optimizes a margin ranking loss such that\nthe semantic relatedness of the input and labels is reflected from their\nembedding similarity. Our experiments demonstrate that UniST achieves strong\nperformance across three semantic typing tasks: entity typing, relation\nclassification and event typing. Meanwhile, UniST effectively transfers\nsemantic knowledge of labels and substantially improves generalizability on\ninferring rarely seen and unseen types. In addition, multiple semantic typing\ntasks can be jointly trained within the unified framework, leading to a single\ncompact multi-tasking model that performs comparably to dedicated single-task\nmodels, while offering even better transferability.'}, {'Instructions as Backdoors: Backdoor Vulnerabilities of Instruction\n  Tuning for Large Language Models': 'We investigate security concerns of the emergent instruction tuning paradigm,\nthat models are trained on crowdsourced datasets with task instructions to\nachieve superior performance. Our studies demonstrate that an attacker can\ninject backdoors by issuing very few malicious instructions (~1000 tokens) and\ncontrol model behavior through data poisoning, without even the need to modify\ndata instances or labels themselves. Through such instruction attacks, the\nattacker can achieve over 90% attack success rate across four commonly used NLP\ndatasets. As an empirical study on instruction attacks, we systematically\nevaluated unique perspectives of instruction attacks, such as poison transfer\nwhere poisoned models can transfer to 15 diverse generative datasets in a\nzero-shot manner; instruction transfer where attackers can directly apply\npoisoned instruction on many other datasets; and poison resistance to continual\nfinetuning. Lastly, we show that RLHF and clean demonstrations might mitigate\nsuch backdoors to some degree. These findings highlight the need for more\nrobust defenses against poisoning attacks in instruction-tuning models and\nunderscore the importance of ensuring data quality in instruction\ncrowdsourcing.'}, {'Test-time Backdoor Mitigation for Black-Box Large Language Models with\n  Defensive Demonstrations': 'Existing studies in backdoor defense have predominantly focused on the\ntraining phase, overlooking the critical aspect of testing time defense. This\ngap becomes particularly pronounced in the context of Large Language Models\n(LLMs) deployed as Web Services, which typically offer only black-box access,\nrendering training-time defenses impractical. To bridge this gap, our work\nintroduces defensive demonstrations, an innovative backdoor defense strategy\nfor blackbox large language models. Our method involves identifying the task\nand retrieving task-relevant demonstrations from an uncontaminated pool. These\ndemonstrations are then combined with user queries and presented to the model\nduring testing, without requiring any modifications/tuning to the black-box\nmodel or insights into its internal mechanisms. Defensive demonstrations are\ndesigned to counteract the adverse effects of triggers, aiming to recalibrate\nand correct the behavior of poisoned models during test-time evaluations.\nExtensive experiments show that defensive demonstrations are effective in\ndefending both instance-level and instruction-level backdoor attacks, not only\nrectifying the behavior of poisoned models but also surpassing existing\nbaselines in most scenarios.'}, {'SalKG: Learning From Knowledge Graph Explanations for Commonsense\n  Reasoning': ""Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, to\nmotivate saliency-based supervision, we analyze oracle KG-augmented models\nwhich directly use saliency explanations as extra inputs for guiding their\nattention. Third, we propose SalKG, a framework for KG-augmented models to\nlearn from coarse and/or fine saliency explanations. Given saliency\nexplanations created from a task's training set, SalKG jointly trains the model\nto predict the explanations, then solve the task by attending to KG features\nhighlighted by the predicted explanations. On three commonsense QA benchmarks\n(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can\nyield considerable performance gains -- up to 2.76% absolute improvement on\nCSQA.""}, {'DALL-E for Detection: Language-driven Compositional Image Synthesis for\n  Object Detection': 'We propose a new paradigm to automatically generate training data with\naccurate labels at scale using the text-toimage synthesis frameworks (e.g.,\nDALL-E, Stable Diffusion, etc.). The proposed approach decouples training data\ngeneration into foreground object mask generation and background (context)\nimage generation. For foreground object mask generation, we use a simple\ntextual template with object class name as input to DALL-E to generate a\ndiverse set of foreground images. A foreground-background segmentation\nalgorithm is then used to generate foreground object masks. Next, in order to\ngenerate context images, first a language description of the context is\ngenerated by applying an image captioning method on a small set of images\nrepresenting the context. These language descriptions are then used to generate\ndiverse sets of context images using the DALL-E framework. These are then\ncomposited with object masks generated in the first step to provide an\naugmented training set for a classifier. We demonstrate the advantages of our\napproach on four object detection datasets including on Pascal VOC and COCO\nobject detection tasks. Furthermore, we also highlight the compositional nature\nof our data generation approach on out-of-distribution and zero-shot data\ngeneration scenarios.'}, {'EM-Paste: EM-guided Cut-Paste with DALL-E Augmentation for Image-level\n  Weakly Supervised Instance Segmentation': 'We propose EM-PASTE: an Expectation Maximization(EM) guided Cut-Paste\ncompositional dataset augmentation approach for weakly-supervised instance\nsegmentation using only image-level supervision. The proposed method consists\nof three main components. The first component generates high-quality foreground\nobject masks. To this end, an EM-like approach is proposed that iteratively\nrefines an initial set of object mask proposals generated by a generic region\nproposal method. Next, in the second component, high-quality context-aware\nbackground images are generated using a text-to-image compositional synthesis\nmethod like DALL-E. Finally, the third component creates a large-scale\npseudo-labeled instance segmentation training dataset by compositing the\nforeground object masks onto the original and generated background images. The\nproposed approach achieves state-of-the-art weakly-supervised instance\nsegmentation results on both the PASCAL VOC 2012 and MS COCO datasets by using\nonly image-level, weak label information. In particular, it outperforms the\nbest baseline by +7.4 and +2.8 mAP0.50 on PASCAL and COCO, respectively.\nFurther, the method provides a new solution to the long-tail weakly-supervised\ninstance segmentation problem (when many classes may only have few training\nsamples), by selectively augmenting under-represented classes.'}, {'Beyond Generation: Harnessing Text to Image Models for Object Detection\n  and Segmentation': 'We propose a new paradigm to automatically generate training data with\naccurate labels at scale using the text-to-image synthesis frameworks (e.g.,\nDALL-E, Stable Diffusion, etc.). The proposed approach1 decouples training data\ngeneration into foreground object generation, and contextually coherent\nbackground generation. To generate foreground objects, we employ a\nstraightforward textual template, incorporating the object class name as input\nprompts. This is fed into a text-to-image synthesis framework, producing\nvarious foreground images set against isolated backgrounds. A\nforeground-background segmentation algorithm is then used to generate\nforeground object masks. To generate context images, we begin by creating\nlanguage descriptions of the context. This is achieved by applying an image\ncaptioning method to a small set of images representing the desired context.\nThese textual descriptions are then transformed into a diverse array of context\nimages via a text-to-image synthesis framework. Subsequently, we composite\nthese with the foreground object masks produced in the initial step, utilizing\na cut-and-paste method, to formulate the training data. We demonstrate the\nadvantages of our approach on five object detection and segmentation datasets,\nincluding Pascal VOC and COCO. We found that detectors trained solely on\nsynthetic data produced by our method achieve performance comparable to those\ntrained on real data (Fig. 1). Moreover, a combination of real and synthetic\ndata yields even much better results. Further analysis indicates that the\nsynthetic data distribution complements the real data distribution effectively.\nAdditionally, we emphasize the compositional nature of our data generation\napproach in out-of-distribution and zero-shot data generation scenarios. We\nopen-source our code at https://github.com/gyhandy/Text2Image-for-Detection'}, {'Instructional Fingerprinting of Large Language Models': 'The exorbitant cost of training Large language models (LLMs) from scratch\nmakes it essential to fingerprint the models to protect intellectual property\nvia ownership authentication and to ensure downstream users and developers\ncomply with their license terms (e.g. restricting commercial use). In this\nstudy, we present a pilot study on LLM fingerprinting as a form of very\nlightweight instruction tuning. Model publisher specifies a confidential\nprivate key and implants it as an instruction backdoor that causes the LLM to\ngenerate specific text when the key is present. Results on 11 popularly-used\nLLMs showed that this approach is lightweight and does not affect the normal\nbehavior of the model. It also prevents publisher overclaim, maintains\nrobustness against fingerprint guessing and parameter-efficient training, and\nsupports multi-stage fingerprinting akin to MIT License. Code is available in\nhttps://cnut1648.github.io/Model-Fingerprint/.'}]","Title: HC-Mamba: Enhancing Medical Image Segmentation with Structured State Space Modules

Abstract:

As medical imaging techniques advance, there is a critical need for innovative solutions in processing and interpreting complex images. The paper introduces HC-Mamba, an advanced architecture aimed at superior performance in medical image segmentation tasks. The objective is to disrupt existing paradigms and achieve high accuracy in recognizing and contrasting medical anomalies and normal tissue in digital images.

Our innovation focuses on the integration of AI models based on State Space Models (SSM), particularly focusing on the Enhanced Comprehensive Structured State Space Module (HC-SSM). This novel approach utilizes two structured state space components for improved feature extraction—Scan Expansion, ensuring multidirectional information coverage, and a Selectivity Module for parameter fine-tuning. Combined with a Patch Embedding Layer and Patch Merging Layer, HC-Mamba structures its functions to optimize input processing, feature extraction, and output spatial resolution.

The HC-Mamba model was rigorously evaluated on the ISIC'17 and ISIC'18 datasets, demonstrating its superior performance compared to existing state-of-the-art models. Our results show substantial increases in Mean Intersection over Union (mIoU) and Dice Score, indicating its potential for enhancing diagnostic precision in automated medical imaging analysis.

The key contributions include the development of a robust, efficient, and adaptable architecture that effectively leverages SSM concepts for medical image segmentation. The model's practical implications lie in its potential to significantly augment the capabilities of clinical decision-making through improved accuracy and streamlined analysis processes, ultimately aiming to enhance patient outcomes and the efficiency of healthcare delivery systems. 

HC-Mamba thus advances the field of medical imaging by providing a novel framework that not only improves the technical standards of segmentation tasks but also paves the way for a more intelligent and effective use of AI in diagnosing diseases within medical contexts."
"Event-based semantic segmentation (ESS) is a fundamental yet challenging task
for event camera sensing. The difficulties in interpreting and annotating event
data limit its scalability. While domain adaptation from images to event data
can help to mitigate this issue, there exist data representational differences
that require additional effort to resolve. In this work, for the first time, we
synergize information from image, text, and event-data domains and introduce
OpenESS to enable scalable ESS in an open-world, annotation-efficient manner.
We achieve this goal by transferring the semantically rich CLIP knowledge from
image-text pairs to event streams. To pursue better cross-modality adaptation,
we propose a frame-to-event contrastive distillation and a text-to-event
semantic consistency regularization. Experimental results on popular ESS
benchmarks showed our approach outperforms existing methods. Notably, we
achieve 53.93% and 43.31% mIoU on DDD17 and DSEC-Semantic without using either
event or frame labels.","[{'Kinematic Resolutions of Redundant Robot Manipulators using\n  Integration-Enhanced RNNs': 'Recently, a time-varying quadratic programming (QP) framework that describes\nthe tracking operations of redundant robot manipulators is introduced to handle\nthe kinematic resolutions of many robot control tasks. Based on the\ngeneralization of such a time-varying QP framework, two schemes, i.e., the\nRepetitive Motion Scheme and the Hybrid Torque Scheme, are proposed. However,\nmeasurement noises are unavoidable when a redundant robot manipulator is\nexecuting a tracking task. To solve this problem, a novel integration-enhanced\nrecurrent neural network (IE-RNN) is proposed in this paper. Associating with\nthe aforementioned two schemes, the tracking task can be accurately completed\nby IE-RNN. Both theoretical analyses and simulations results prove that the\nresidual errors of IE-RNN can converge to zero under different kinds of\nmeasurement noises. Moreover, practical experiments are elaborately made to\nverify the excellent convergence and strong robustness properties of the\nproposed IE-RNN.'}, {'ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via\n  Regularized Domain Concatenation': 'Transferring knowledge learned from the labeled source domain to the raw\ntarget domain for unsupervised domain adaptation (UDA) is essential to the\nscalable deployment of autonomous driving systems. State-of-the-art methods in\nUDA often employ a key idea: utilizing joint supervision signals from both\nsource and target domains for self-training. In this work, we improve and\nextend this aspect. We present ConDA, a concatenation-based domain adaptation\nframework for LiDAR segmentation that: 1) constructs an intermediate domain\nconsisting of fine-grained interchange signals from both source and target\ndomains without destabilizing the semantic coherency of objects and background\naround the ego-vehicle; and 2) utilizes the intermediate domain for\nself-training. To improve the network training on the source domain and\nself-training on the intermediate domain, we propose an anti-aliasing\nregularizer and an entropy aggregator to reduce the negative effect caused by\nthe aliasing artifacts and noisy pseudo labels. Through extensive studies, we\ndemonstrate that ConDA significantly outperforms prior arts in mitigating\ndomain gaps.'}, {'Modification of Gesture-Determined-Dynamic Function with Consideration\n  of Margins for Motion Planning of Humanoid Robots': 'The gesture-determined-dynamic function (GDDF) offers an effective way to\nhandle the control problems of humanoid robots. Specifically, GDDF is utilized\nto constrain the movements of dual arms of humanoid robots and steer specific\ngestures to conduct demanding tasks under certain conditions. However, there is\nstill a deficiency in this scheme. Through experiments, we found that the\njoints of the dual arms, which can be regarded as the redundant manipulators,\ncould exceed their limits slightly at the joint angle level. The performance\nstraightly depends on the parameters designed beforehand for the GDDF, which\ncauses a lack of adaptability to the practical applications of this method. In\nthis paper, a modified scheme of GDDF with consideration of margins (MGDDF) is\nproposed. This MGDDF scheme is based on quadratic programming (QP) framework,\nwhich is widely applied to solving the redundancy resolution problems of robot\narms. Moreover, three margins are introduced in the proposed MGDDF scheme to\navoid joint limits. With consideration of these margins, the joints of\nmanipulators of the humanoid robots will not exceed their limits, and the\npotential damages which might be caused by exceeding limits will be completely\navoided. Computer simulations conducted on MATLAB further verify the\nfeasibility and superiority of the proposed MGDDF scheme.'}, {'LaserMix for Semi-Supervised LiDAR Semantic Segmentation': 'Densely annotating LiDAR point clouds is costly, which restrains the\nscalability of fully-supervised learning methods. In this work, we study the\nunderexplored semi-supervised learning (SSL) in LiDAR segmentation. Our core\nidea is to leverage the strong spatial cues of LiDAR point clouds to better\nexploit unlabeled data. We propose LaserMix to mix laser beams from different\nLiDAR scans, and then encourage the model to make consistent and confident\npredictions before and after mixing. Our framework has three appealing\nproperties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g.,\nrange view and voxel), and hence our SSL framework can be universally applied.\n2) Statistically grounded: We provide a detailed analysis to theoretically\nexplain the applicability of the proposed framework. 3) Effective:\nComprehensive experimental analysis on popular LiDAR segmentation datasets\n(nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and\nsuperiority. Notably, we achieve competitive results over fully-supervised\ncounterparts with 2x to 5x fewer labels and improve the supervised-only\nbaseline significantly by 10.8% on average. We hope this concise yet\nhigh-performing framework could facilitate future research in semi-supervised\nLiDAR segmentation. Code is publicly available.'}, {'Optimizing LiDAR Placements for Robust Driving Perception in Adverse\n  Conditions': 'The robustness of driving perception systems under unprecedented conditions\nis crucial for safety-critical usages. Latest advancements have prompted\nincreasing interests towards multi-LiDAR perception. However, prevailing\ndriving datasets predominantly utilize single-LiDAR systems and collect data\ndevoid of adverse conditions, failing to capture the complexities of real-world\nenvironments accurately. Addressing these gaps, we proposed Place3D, a\nfull-cycle pipeline that encompasses LiDAR placement optimization, data\ngeneration, and downstream evaluations. Our framework makes three appealing\ncontributions. 1) To identify the most effective configurations for multi-LiDAR\nsystems, we introduce a Surrogate Metric of the Semantic Occupancy Grids\n(M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we\npropose a novel optimization strategy to refine multi-LiDAR placements. 3)\nCentered around the theme of multi-condition multi-LiDAR perception, we collect\na 364,000-frame dataset from both clean and adverse conditions. Extensive\nexperiments demonstrate that LiDAR placements optimized using our approach\noutperform various baselines. We showcase exceptional robustness in both 3D\nobject detection and LiDAR semantic segmentation tasks, under diverse adverse\nweather and sensor failure conditions. Code and benchmark toolkit are publicly\navailable.'}, {'FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation': 'LiDAR segmentation has become a crucial component in advanced autonomous\ndriving systems. Recent range-view LiDAR segmentation approaches show promise\nfor real-time processing. However, they inevitably suffer from corrupted\ncontextual information and rely heavily on post-processing techniques for\nprediction refinement. In this work, we propose FRNet, a simple yet powerful\nmethod aimed at restoring the contextual information of range image pixels\nusing corresponding frustum LiDAR points. Firstly, a frustum feature encoder\nmodule is used to extract per-point features within the frustum region, which\npreserves scene consistency and is crucial for point-level predictions. Next, a\nfrustum-point fusion module is introduced to update per-point features\nhierarchically, enabling each point to extract more surrounding information via\nthe frustum features. Finally, a head fusion module is used to fuse features at\ndifferent levels for final semantic prediction. Extensive experiments conducted\non four popular LiDAR segmentation benchmarks under various task setups\ndemonstrate the superiority of FRNet. Notably, FRNet achieves 73.3% and 82.5%\nmIoU scores on the testing sets of SemanticKITTI and nuScenes. While achieving\ncompetitive performance, FRNet operates 5 times faster than state-of-the-art\napproaches. Such high efficiency opens up new possibilities for more scalable\nLiDAR segmentation. The code has been made publicly available at\nhttps://github.com/Xiangxu-0103/FRNet.'}, {'Unsupervised Video Domain Adaptation for Action Recognition: A\n  Disentanglement Perspective': 'Unsupervised video domain adaptation is a practical yet challenging task. In\nthis work, for the first time, we tackle it from a disentanglement view. Our\nkey idea is to handle the spatial and temporal domain divergence separately\nthrough disentanglement. Specifically, we consider the generation of\ncross-domain videos from two sets of latent factors, one encoding the static\ninformation and another encoding the dynamic information. A Transfer Sequential\nVAE (TranSVAE) framework is then developed to model such generation. To better\nserve for adaptation, we propose several objectives to constrain the latent\nfactors. With these constraints, the spatial divergence can be readily removed\nby disentangling the static domain-specific information out, and the temporal\ndivergence is further reduced from both frame- and video-levels through\nadversarial learning. Extensive experiments on the UCF-HMDB, Jester, and\nEpic-Kitchens datasets verify the effectiveness and superiority of TranSVAE\ncompared with several state-of-the-art approaches. Code is publicly available.'}, {'Free Lunch for Co-Saliency Detection: Context Adjustment': 'We unveil a long-standing problem in the prevailing co-saliency detection\nsystems: there is indeed inconsistency between training and testing.\nConstructing a high-quality co-saliency detection dataset involves\ntime-consuming and labor-intensive pixel-level labeling, which has forced most\nrecent works to rely instead on semantic segmentation or saliency detection\ndatasets for training. However, the lack of proper co-saliency and the absence\nof multiple foreground objects in these datasets can lead to spurious\nvariations and inherent biases learned by models. To tackle this, we introduce\nthe idea of counterfactual training through context adjustment and propose a\n""cost-free"" group-cut-paste (GCP) procedure to leverage off-the-shelf images\nand synthesize new samples. Following GCP, we collect a novel dataset called\nContext Adjustment Training (CAT). CAT consists of 33,500 images, which is four\ntimes larger than the current co-saliency detection datasets. All samples are\nautomatically annotated with high-quality mask annotations, object categories,\nand edge maps. Extensive experiments on recent benchmarks are conducted, show\nthat CAT can improve various state-of-the-art models by a large margin (5% ~\n25%). We hope that the scale, diversity, and quality of our dataset can benefit\nresearchers in this area and beyond. Our dataset will be publicly accessible\nthrough our project page.'}, {""RoboBEV: Towards Robust Bird's Eye View Perception under Corruptions"": ""The recent advances in camera-based bird's eye view (BEV) representation\nexhibit great potential for in-vehicle 3D perception. Despite the substantial\nprogress achieved on standard benchmarks, the robustness of BEV algorithms has\nnot been thoroughly examined, which is critical for safe operations. To bridge\nthis gap, we introduce RoboBEV, a comprehensive benchmark suite that\nencompasses eight distinct corruptions, including Bright, Dark, Fog, Snow,\nMotion Blur, Color Quant, Camera Crash, and Frame Lost. Based on it, we\nundertake extensive evaluations across a wide range of BEV-based models to\nunderstand their resilience and reliability. Our findings indicate a strong\ncorrelation between absolute performance on in-distribution and\nout-of-distribution datasets. Nonetheless, there are considerable variations in\nrelative performance across different approaches. Our experiments further\ndemonstrate that pre-training and depth-free BEV transformation has the\npotential to enhance out-of-distribution robustness. Additionally, utilizing\nlong and rich temporal information largely helps with robustness. Our findings\nprovide valuable insights for designing future BEV models that can achieve both\naccuracy and robustness in real-world deployments.""}, {'Visual Foundation Models Boost Cross-Modal Unsupervised Domain\n  Adaptation for 3D Semantic Segmentation': 'Unsupervised domain adaptation (UDA) is vital for alleviating the workload of\nlabeling 3D point cloud data and mitigating the absence of labels when facing a\nnewly defined domain. Various methods of utilizing images to enhance the\nperformance of cross-domain 3D segmentation have recently emerged. However, the\npseudo labels, which are generated from models trained on the source domain and\nprovide additional supervised signals for the unseen domain, are inadequate\nwhen utilized for 3D segmentation due to their inherent noisiness and\nconsequently restrict the accuracy of neural networks. With the advent of 2D\nvisual foundation models (VFMs) and their abundant knowledge prior, we propose\na novel pipeline VFMSeg to further enhance the cross-modal unsupervised domain\nadaptation framework by leveraging these models. In this work, we study how to\nharness the knowledge priors learned by VFMs to produce more accurate labels\nfor unlabeled target domains and improve overall performance. We first utilize\na multi-modal VFM, which is pre-trained on large scale image-text pairs, to\nprovide supervised labels (VFM-PL) for images and point clouds from the target\ndomain. Then, another VFM trained on fine-grained 2D masks is adopted to guide\nthe generation of semantically augmented images and point clouds to enhance the\nperformance of neural networks, which mix the data from source and target\ndomains like view frustums (FrustumMixing). Finally, we merge class-wise\nprediction across modalities to produce more accurate annotations for unlabeled\ntarget domains. Our method is evaluated on various autonomous driving datasets\nand the results demonstrate a significant improvement for 3D segmentation task.'}]","Title: Open-ESS: Flexible and Efficient Event Scene Segmentation via Cross-Modal Contrastive Learning

Abstract:

In the realm of computer vision, event-based scene understanding (ESS) aims to interpret complex real-world scenarios from sparse and irregular events acquired through the photodiode camera. The primary objective of this research paperis to propose Open-ESS, a groundbreaking framework for open-vocabulary ESS that reduces annotation requirements and enhances the scalability and flexibility of scene understanding for diverse applications. 

The novelty of Open-ESS lies in its utilization of contrastive distillation for semantic event segmentation, which coherently integrates knowledge between dense visual frames and sparse event streams through the dedicated CLIP model pre-trained on a massive dataset. By leveraging the strengths of computer vision techniques (VGG, CLIP) over traditional event streams, the method achieves state-of-the-art performance in both open-world and annotation-limited scenarios on datasets like DDD17-Seg and DSEC-Semantic. 

Key findings demonstrate significant improvements over existing models, including competitive results with full supervision and unprecedented zero-shot capabilities which surpass previous state-of-the-art methods in scene understanding. Notably, Open-ESS advances event-based scene segmentation by enabling flexible interpretation from text prompts, supporting open-vocabulary learning, and facilitating efficient cross-domain knowledge transfer, thus paving new avenues for automated scene comprehension in smart surveillance, robotics, and augmented reality systems.

The practical implications of this research lie in expanding the scope of easy-to-acquire event data for scene understanding applications, reducing dependency on annotated events, lowering data requirements, and enhancing real-world scenario processing capabilities. The dissemination of the Open-ESS framework thus opens new possibilities for robust AI systems that can interpret and interact with their environment from a large pool of event data efficiently and effectively."
"The chemical abundances of Milky Way's satellites reflect their star
formation histories (SFHs), yet, due to the difficulty of determining the ages
of old stars, the SFHs of most satellites are poorly measured. Ongoing and
upcoming surveys will obtain around ten times more medium-resolution spectra
for stars in satellites than are currently available. To correctly extract SFHs
from large samples of chemical abundances, the relationship between chemical
abundances and SFHs needs to be clarified. Here, we perform a high-resolution
cosmological zoom-in simulation of a Milky Way-like galaxy with detailed models
of star formation, supernova feedback, and metal diffusion. We quantify SFHs,
metallicity distribution functions, and the $\alpha$-element (Mg, Ca, and Si)
abundances in satellites of the host galaxy. We find that star formation in
most simulated satellites is quenched before infalling to their host. Star
formation episodes in simulated satellites are separated by a few hundred Myr
owing to supernova feedback; each star formation event produces groups of stars
with similar [$\alpha$/Fe] and [Fe/H]. We then perform a mock observation of
the upcoming Subaru Prime Focus Spectrograph (PFS) observations. We find that
Subaru PFS will be able to detect distinct groups of stars in [$\alpha$/Fe] vs.
[Fe/H] space, produced by episodic star formation. This result means that
episodic SFHs can be estimated from the chemical abundances of $\gtrsim$ 1,000
stars determined with medium-resolution spectroscopy.","[{'Efficiency of metal mixing in dwarf galaxies': 'Metal mixing plays critical roles in the enrichment of metals in galaxies.\nThe abundance of elements such as Mg, Fe, and Ba in metal-poor stars help us\nunderstand the metal mixing in galaxies. However, the efficiency of metal\nmixing in galaxies is not yet understood. Here we report a series of\n$N$-body/smoothed particle hydrodynamics simulations of dwarf galaxies with\ndifferent efficiencies of metal mixing using turbulence-induced mixing model.\nWe show that metal mixing apparently occurs in dwarf galaxies from Mg and Ba\nabundance. We find that the scaling factor for metal diffusion larger than 0.01\nis necessary to reproduce the observation of Ba abundance in dwarf galaxies.\nThis value is consistent with the value expected from turbulence theory and\nexperiment. We also find that timescale of metal mixing is less than 40 Myr.\nThis timescale is shorter than that of typical dynamical times of dwarf\ngalaxies. We demonstrate that the determination of a degree of scatters of Ba\nabundance by the observation will help us to constrain the efficiency of metal\nmixing more precisely.'}, {'Enrichment of Strontium in Dwarf Galaxies': 'Light trans-iron elements such as Sr serve as the key to understanding the\nastrophysical sites of heavy elements. Spectroscopic studies of metal-poor\nstars have revealed large star-to-star scatters in the ratios of [Sr/Ba], which\nindicates that there are multiple sites for the production of Sr. Here we\npresent the enrichment history of Sr by a series of the $N$-body/smoothed\nparticle hydrodynamics simulations of a dwarf galaxy with a stellar mass of 3\n$\\times$ 10$^{6}$ $M_{\\odot}$. We show that binary neutron star mergers (NSMs)\nand asymptotic giant branch (AGB) stars contribute to the enrichment of Sr in\nthe metallicity ranges [Fe/H] $\\gtrsim$ $-$3 and [Fe/H] $\\gtrsim$ $-$1,\nrespectively. It appears insufficient, however, to explain the overall\nobservational trends of Sr by considering only these sites. We find that the\nmodels including electron-capture supernovae (ECSNe) and rotating massive stars\n(RMSs), in addition to NSMs and AGBs, reasonably reproduce the enrichment\nhistories of Sr in dwarf galaxies. The contributions of both ECSNe and NSMs\nmake scatters of $\\approx$ 0.2 dex in [Sr/Fe], [Sr/Ba], and [Sr/Zn] as can be\nseen for observed stars in the metallicity range [Fe/H] $<$ $-2$. We also find\nthat the mass range of ECSN progenitors should be substantially smaller than\n$1\\, M_\\odot$ (e.g., 0.1-$0.2\\, M_\\odot$) to avoid over-prediction of [Sr/Ba]\nand [Sr/Zn] ratios. Our results demonstrate that NSMs, AGBs, ECSNe, and RMSs\nall play roles in the enrichment histories of Local Group dwarf galaxies,\nalthough more observational data are required to disentangle the relative\ncontributions of these sources.'}, {'The Origin of the Large Magellanic Cloud Globular Cluster NGC 2005': 'The ancient Large Magellanic Cloud (LMC) globular cluster NGC 2005 has\nrecently been reported to have an ex-situ origin, thus, setting precedents that\nthe LMC could have partially formed from smaller merged dwarf galaxies. We here\nprovide additional arguments from which we conclude that is also fairly\nplausible an in-situ origin of NGC 2005, based on the abundance spread of a\nvariety of chemical elements measured in dwarf galaxies, their minimum mass in\norder to form globular clusters, the globular cluster formation imprints kept\nin their kinematics, and the recent modeling showing that explosions of\nsupernovae are responsible for the observed chemical abundance spread in dwarf\ngalaxies. The present analysis points to the need for further development of\nnumerical simulations and observational indices that can help us to\ndifferentiate between two mechanisms of galaxy formation for the LMC, namely, a\nprimordial dwarf or an initial merging event of smaller dwarfs.'}, {'Enrichment of Zinc in galactic chemodynamical evolution models': 'The heaviest iron-peak element, Zn has been used as an important tracer of\ncosmic chemical evolution. Spectroscopic observations of the metal-poor stars\nin Local Group galaxies show that an increasing trend of [Zn/Fe] ratios toward\nlower metallicity. However, enrichment of Zn in galaxies is not well understood\ndue to the poor knowledge of astrophysical sites of Zn as well as metal mixing\nin galaxies. Here we show possible explanations for the observed trend by\ntaking into account electron-capture supernovae (ECSNe) as one of the sources\nof Zn in our chemodynamical simulations of dwarf galaxies. We find that the\nejecta from ECSNe contribute to stars with [Zn/Fe] $\\gtrsim$ 0.5. We also find\nthat scatters of [Zn/Fe] in higher metallicity originate from the ejecta of\ntype Ia supernovae. On the other hand, it appears difficult to explain the\nobserved trends if we do not consider ECSNe as a source of Zn. These results\ncome from inhomogeneous spatial metallicity distribution due to the\ninefficiency of metal mixing. We find that the optimal value of scaling factor\nfor metal diffusion coefficient is $\\sim$ 0.01 in the shear-based metal mixing\nmodel in smoothed particle hydrodynamics simulations. These results suggest\nthat ECSNe can be one of the contributors to the enrichment of Zn in galaxies.'}, {'Dark matter halo properties of the Galactic dwarf satellites:\n  implication for chemo-dynamical evolution of the satellites and a challenge\n  to $Λ$CDM': 'Elucidating dark matter density profiles in the Galactic dwarf satellites is\nessential to understanding not only the quintessence of dark matter, but also\nthe evolution of the satellites themselves. In this work, we present the\ncurrent constraints on dark matter densities in the Galactic ultra-faint dwarf\n(UFD) and diffuse galaxies. Applying our constructed non-spherical mass models\nto the currently available kinematic data of the 25 UFDs and 2 diffuse\nsatellites, we find that whereas most of the galaxies have huge uncertainties\non the inferred dark matter density profiles, Eridanus~II, Segue~I, and\nWillman~1 favor cuspy central profiles even considering effects of a prior\nbias. We compare our results with the simulated subhalos on the plane between\nthe dark matter density at 150~pc and the pericenter distance. We find that the\nmost observed satellites and the simulated subhalos are similarly distributed\non this plane, except for Antlia~2, Crater~2, and Tucana~3, which are less than\none tenth of the density. Despite considerable tidal effects, the subhalos\ndetected by commonly-used subhalo finders have difficulty in explaining such a\nhuge deviation. We also estimate the dynamical mass-to-light ratios of the\nsatellites and confirm the ratio is linked to stellar mass and metallicity.\nTucana~3 deviates largely from these relations, while it follows the\nmass-metallicity relation. This indicates that Tucana~3 has a cored dark matter\nhalo, despite a significant uncertainty in its ratios.'}, {'Neutron star mergers as the astrophysical site of the r-process in the\n  Milky Way and its satellite galaxies': ""Recent progress of nucleosynthesis work as well as the discovery of a\nkilonova associated with the gravitational-wave source GW170817 indicates that\nneutron star mergers (NSM) can be a site of the r-process. Several studies of\ngalactic chemical evolution, however, have pointed out inconsistencies between\nthis idea and the observed stellar abundance signatures in the Milky Way: (a)\nthe presence of Eu at low (halo) metallicity and (b) the descending trend of\nEu/Fe at high (disc) metallicity. In this study, we explore the galactic\nchemical evolution of the Milky Way's halo, disc and satellite dwarf galaxies.\nParticular attention is payed to the forms of delay-time distributions for both\ntype Ia supernovae (SN Ia) and NSMs. The Galactic halo is modeled as an\nensemble of independently evolving building-block galaxies with different\nmasses. The single building blocks as well as the disc and satellite dwarfs are\ntreated as well-mixed one-zone systems. Our results indicate that the\naforementioned inconsistencies can be resolved and thus NSMs can be the unique\nr-process site in the Milky Way, provided that the delay-time distributions\nsatisfy the following conditions: (i) a long delay (~1 Gyr) for the appearance\nof the first SN Ia (or a slow early increase of its number) and (ii) an\nadditional early component providing >~ 50% of all NSMs with a delay of ~0.1\nGyr. In our model, r-process-enhanced and r-process-deficient stars in the halo\nappear to have originated from ultra-faint dwarf-sized and massive building\nblocks, respectively. Our results also imply that the natal kicks of binary\nneutron stars have a little impact on the evolution of Eu in the disc.""}, {'SIRIUS project. I. Star formation models for star-by-star simulations of\n  star clusters and galaxy formation': 'Most stars are formed as star clusters in galaxies, which then disperse into\ngalactic disks. Upcoming exascale supercomputational facilities will enable\nperforming simulations of galaxies and their formation by resolving individual\nstars (star-by-star simulations). This will substantially advance our\nunderstanding of star formation in galaxies, star cluster formation, and\nassembly histories of galaxies. In previous galaxy simulations, a simple\nstellar population approximation was used. It is, however, difficult to improve\nthe mass resolution with this approximation. Therefore, a model for forming\nindividual stars that can be used in simulations of galaxies must be\nestablished. In this first paper of a series of the SIRIUS (SImulations\nResolving IndividUal Stars) project, we demonstrate a stochastic star formation\nmodel for star-by-star simulations. An assumed stellar initial mass function\n(IMF) is randomly assigned to newly formed stars. We introduce a maximum search\nradius to assemble the mass from surrounding gas particles to form star\nparticles. In this study, we perform a series of N-body/smoothed particle\nhydrodynamics simulations of star cluster formations from turbulent molecular\nclouds and ultra-faint dwarf galaxies as test cases. The IMF can be correctly\nsampled if a maximum search radius that is larger than the value estimated from\nthe threshold density for star formation is adopted. In small clouds, the\nformation of massive stars is highly stochastic because of the small number of\nstars. We confirm that the star formation efficiency and threshold density do\nnot strongly affect the results. We find that our model can naturally reproduce\nthe relationship between the most massive stars and the total stellar mass of\nstar clusters. Herein, we demonstrate that our models can be applied to\nsimulations varying from star clusters to galaxies for a wide range of\nresolutions.'}, {'Early chemo-dynamical evolution of dwarf galaxies deduced from\n  enrichment of r-process elements': 'The abundance of elements synthesized by the rapid neutron-capture process\n(r-process elements) of extremely metal-poor (EMP) stars in the Local Group\ngalaxies gives us clues to clarify the early evolutionary history of the Milky\nWay halo. The Local Group dwarf galaxies would have similarly evolved with\nbuilding blocks of the Milky Way halo. However, how the chemo-dynamical\nevolution of the building blocks affects the abundance of r-process elements is\nnot yet clear. In this paper, we perform a series of simulations using dwarf\ngalaxy models with various dynamical times and total mass, which determine\nstar-formation histories. We find that galaxies with dynamical times longer\nthan 100 Myr have star formation rates less than $10^{-3} M_{\\odot}$ yr$^{-1}$\nand slowly enrich metals in their early phase. These galaxies can explain the\nobserved large scatters of r-process abundance in EMP stars in the Milky Way\nhalo regardless of their total mass. On the other hand, the first neutron star\nmerger appears at a higher metallicity in galaxies with a dynamical time\nshorter than typical neutron star merger times. The scatters of r-process\nelements mainly come from inhomogeneity of the metals in the interstellar\nmedium whereas the scatters of $\\alpha$-elements are mostly due to the\ndifference in the yield of each supernova. Our results demonstrate that the\nfuture observations of r-process elements in EMP stars will be able to\nconstrain the early chemo-dynamical evolution of the Local Group galaxies.'}, {'$R$-process enhancements of Gaia-Enceladus in GALAH DR3': 'The dominant site of production of $r$-process elements remains unclear\ndespite recent observations of a neutron star merger. Observational constraints\non the properties of the sites can be obtained by comparing $r$-process\nabundances in different environments. The recent Gaia data releases and large\nsamples from high-resolution optical spectroscopic surveys are enabling us to\ncompare $r$-process element abundances between stars formed in an accreted\ndwarf galaxy, Gaia-Enceladus, and those formed in the Milky Way. We aim to\nunderstand the origin of $r$-process elements in Gaia-Enceladus. We first\nconstruct a sample of stars to study Eu abundances without being affected by\nthe detection limit. We then kinematically select 71 Gaia-Enceladus stars and\n93 in-situ stars from the Galactic Archaeology with HERMES (GALAH) DR3, of\nwhich 50 and 75 stars can be used to study Eu reliably. Gaia-Enceladus stars\nclearly show higher ratios of [{Eu}/{Mg}] than in-situ stars. High [{Eu}/{Mg}]\nalong with low [{Mg}/{Fe}] are also seen in relatively massive satellite\ngalaxies such as the LMC, Fornax, and Sagittarius dwarfs. On the other hand,\nunlike these galaxies, Gaia-Enceladus does not show enhanced [{Ba}/{Eu}] or\n[{La}/{Eu}] ratios suggesting a lack of significant $s$-process contribution.\nFrom comparisons with simple chemical evolution models, we show that the high\n[{Eu}/{Mg}] of Gaia-Enceladus can naturally be explained by considering\n$r$-process enrichment by neutron-star mergers with delay time distribution\nthat follows a similar power-law as type~Ia supernovae but with a shorter\nminimum delay time.'}, {'SIRIUS Project. III. Star-by-star simulations of star cluster formation\n  using a direct N-body integrator with stellar feedback': 'One of the computational challenges of cluster formation simulations is\nresolving individual stars and simulating massive clusters with masses of more\nthan $10^4 M_{\\odot}$ without gravitational softening. Combining direct\n$N$-body code with smoothed-particle hydrodynamics (SPH) code, we have\ndeveloped a new code, \\textsc{ASURA+BRIDGE}, in which we can integrate stellar\nparticles without softening. We add a feedback model for \\HII regions into this\ncode, in which thermal and momentum feedback is given within the Str{\\""o}mgren\nradius. We perform $N$-body/SPH simulations of star cluster formation. Without\nsoftening, a portion of massive stars are ejected from the forming clusters. As\na result, the stellar feedback works outside the clusters. This\nenhances/suppresses the star formation in initially sub-virial/super-virial\nclouds. We find that the formed star clusters are denser than currently\nobserved open clusters, but the mass--density relation is consistent with or\neven higher than that is estimated as an initial cluster density. We also find\nthat some clusters have multiple peaks in their stellar age distribution as a\nconsequence of their hierarchical formation.'}]","The research paper titled ""Analyzing Dwarf Galaxy Formation and Evolution: Insights from Cosmological Simulations and Observational Studies"" aims to unravel the complexities of star formation in dwarf galaxies that are satellite systems of the Milky Way. Through state-of-the-art computational simulations and comparison with observational data, the study advances our understanding of dwarf galaxy formation processes and their impacts on the chemical abundances of stars.

**Background**: Dwarf galaxies, crucial components of the Milky Way's satellite system, have provided unprecedented insights into star formation and chemical evolution in the early universe. However, understanding the varying star formation and chemical enrichment histories of these galaxies remains elusive.

**Objective**: The primary goal of the study is to investigate the conditions and mechanisms that influence star formation and driving gas outflows in dwarf galaxies, focusing on their interactions with the Milky Way. 

**Innovations**: The research combines multi-scale cosmological simulations with detailed chemical modeling to simulate dwarf galaxies, enabling unprecedented exploration of the star formation histories (SFHs) and the mass-loading factors (MLFs) responsible for outflows under different SN feedback scenarios. Unique simulations facilitate the identification of key parameters governing the SFH and outflow dynamics.

**Methods**: The study relies on N-body simulations, which model dark matter dynamics and gas cooling, star formation, and feedback from supernovae (SN). Chemical enrichment and nucleosynthesis are traced through phosphate and boron isotope ratios, as well as α-element abundances. Various feedback models are tested to determine their impact on the SFH and outflows.

**Results**: The research reveals that dwarf galaxies with enhanced star formation typically possess a high normalization of cold gas compared to their halo virial mass, with pericentric distances exceeding 10 kpc. This triggers strong feedback, curbing star formation within <1 Gyr. The MLF in MW satellites is significantly lower than predicted in simulations, implying more efficient turbulence or higher mass outflows in real dwarf galaxies.

**Contributions**: This work advances our understanding of the role of supernovae and cosmological gas inflows in regulating star formation and supporting outflows in dwarf galaxies. Insights into the formation and evolution of the Milky Way's satellite system enhance our knowledge of galaxy formation across cosmic time.

**Applications**: The findings have implications for mapping the Chemical Evolution of the Milky Way, constraining the history of star formation and outflows in dwarf galaxies, and informing theories of galactic feedback. They also support the identification of SN signatures in dwarf galaxies during field surveys with wide-field spectroscopic facilities, aiding in deciphering the connection between environment, feedback mechanisms, and chemical evolution.

This research constitutes a pivotal step in unraveling the mystery of star formation and chemical evolution at low metallicities, contributing to the broader goal of mapping the Milky Way's formation history."
"The conformational and dynamical properties of active ring polymers are
studied by numerical simulations. The two-dimensionally confined polymer is
modeled as a closed bead-spring chain, driven by tangential forces, put in
contact with a heat bath described by the Brownian multiparticle collision
dynamics. Both phantom polymers and chains comprising excluded volume
interactions are considered for different bending rigidities. The size and
shape are found to be dependent on persistence length, driving force, and bead
mutual exclusion. The lack of excluded volume interactions is responsible for a
shrinkage of active rings when increasing driving force in the flexible limit
while the presence induces a moderate swelling of chains. Internal dynamics of
flexible phantom active rings shows activity-enhanced diffusive behavior at
large activity values while, in the case of self-avoiding active chains, it is
characterized by active ballistic motion not depending on stiffness. The
long-time dynamics of active rings is marked by rotational motion whose period
scales as the inverse of the applied tangential force, irrespective of
persistence length and beads self-exclusion.","[{'A lattice Boltzmann study of phase separation in liquid-vapor systems\n  with gravity': 'Phase separation of a two-dimensional van der Waals fluid subject to a\ngravitational force is studied by numerical simulations based on lattice\nBoltzmann methods (LBM) implemented with a finite difference scheme. A growth\nexponent $\\alpha=1$ is measured in the direction of the external force.'}, {'Phase separation of binary fluids with dynamic temperature': 'Phase separation of binary fluids quenched by contact with cold external\nwalls is considered. Navier-Stokes, convection-diffusion, and energy equations\nare solved by lattice Boltzmann method coupled with finite-difference schemes.\nAt high viscosity, different morphologies are observed by varying the thermal\ndiffusivity. In the range of thermal diffusivities with domains growing\nparallel to the walls, temperature and phase separation fronts propagate\ntowards the inner of the system with power-law behavior. At low viscosity\nhydrodynamics favors rounded shapes, and complex patterns with different\nlengthscales appear. Off-symmetrical systems behave similarly but with more\nordered configurations.'}, {'Lattice Boltzmann simulations of segregating binary fluid mixtures in\n  shear flow': 'We apply lattice Boltzmann method to study the phase separation of a\ntwo-dimensional binary fluid mixture in shear flow. The algorithm can simulate\nsystems described by the Navier-Stokes and convection-diffusion equations. We\npropose a new scheme for imposing the shear flow which has the advantage of\npreserving mass and momentum conservation on the boundary walls without\nintroducing slip velocities. Our main results concern the presence of two\ntypical lenght scales in the phase separation process, corresponding to domains\nwith two different thicknesses. Our simulations at low viscosity confirm\nprevious results only valid in the limit of infinite viscosity.'}, {'Computer simulations of domain growth in off-critical quenches of\n  two-dimensional binary mixtures': 'The phase separation of two-dimensional binary mixtures has been studied\nthrough numerical Langevin simulations based on a Ginzburg-Landau free energy.\nWe have considered not symmetric mixtures with and without imposed shear flow.\nIn the sheared case our main results are as follows: (1) domains are distorted\nby the flow; (2) the structure factor has four peaks; (3) excess viscosity\nshows a peak whose position is independent of shear rate but its height\ndecreases increasing shear rate.'}, {'A lattice Boltzmann model with random dynamical constraints': 'In this paper we introduce a modified lattice Boltzmann model (LBM) with the\ncapability of mimicking a fluid system with dynamic heterogeneities. The\nphysical system is modeled as a one-dimensional fluid, interacting with\nfinite-lifetime moving obstacles. Fluid motion is described by a lattice\nBoltzmann equation and obstacles are randomly distributed semi-permeable\nbarriers which constrain the motion of the fluid particles. After a lifetime\ndelay, obstacles move to new random positions. It is found that the\nnon-linearly coupled dynamics of the fluid and obstacles produces heterogeneous\npatterns in fluid density and non-exponential relaxation of two-time\nautocorrelation function.'}, {'A lattice mesoscopic model of dynamically heterogeneous fluids': 'We introduce a mesoscopic three-dimensional Lattice Boltzmann Model which\nattempts to mimick the physical features associated with cage effects in\ndynamically heterogeneous fluids. To this purpose, we extend the standard\nLattice Boltzmann dynamics with self-consistent constraints based on the\nnon-local density of the surrounding fluid. The resulting dynamics exhibits\ntypical features of dynamic heterogeneous fluids, such as non-Gaussian density\ndistributions and long-time relaxation. Due to its intrinsically parallel\ndynamics, and absence of statistical noise, the method is expected to compute\nsignificantly faster than molecular dynamics, Monte Carlo and lattice glass\nmodels.'}, {'Dynamics and Rheology of Vesicle Suspensions in Wall-Bounded Shear Flow': 'The dynamics and rheology of suspensions of fluid vesicles or red blood cells\nis investigated by a combination of molecular dynamics and mesoscale\nhydrodynamics simulations in two dimensions. The vesicle suspension is confined\nbetween two no-slip walls, which are driven externally to generate a shear flow\nwith shear rate $\\dot\\gamma$. The flow behavior is studied as a function of\n$\\dot\\gamma$, the volume fraction of vesicles, and the viscosity contrast\nbetween inside and outside fluids. Results are obtained for the encounter and\ninteractions of two vesicles, the intrinsic viscosity of the suspension, and\nthe cell-free layer near the walls.'}, {'Rheological properties of sheared vesicle and cell suspensions': 'Numerical simulations of vesicle suspensions are performed in two dimensions\nto study their dynamical and rheological properties. An hybrid method is\nadopted, which combines a mesoscopic approach for the solvent with a\ncurvature-elasticity model for the membrane. Shear flow is induced by two\ncounter-sliding parallel walls, which generate a linear flow profile. The flow\nbehavior is studied for various vesicle concentrations and viscosity ratios\nbetween the internal and the external fluid. Both the intrinsic viscosity and\nthe thickness of depletion layers near the walls are found to increase with\nincreasing viscosity ratio.'}, {'Sheared phase-separating binary mixtures with surface diffusion': 'The phase-separation process of a binary mixture with\norder-parameter-dependent mobility under shear flow is numerically studied. The\nordering is characterized by an alternate stretching and bursting of domains\nwhich produce oscillations in the physical observables. The amplitude of such\nmodulations reduce in time when the mobility vanishes in the bulk phase,\ndisfavoring the growth of bubbles coming from bursted domains. We propose two\nequations for the typical sizes $R_x$ and $R_y$ of domains finding the\nlong-time behaviors $R_x \\sim t^{5/4}$ and $R_y \\sim t^{1/4}$ in the flow and\nshear directions, respectively, in the case of surface diffusion. A reduction\nof the excess viscosity with increasing shear rate is observed in simulations.'}, {'Finite difference lattice Boltzmann model with flux limiters for\n  liquid-vapor systems': 'In this paper we apply a finite difference lattice Boltzmann model to study\nthe phase separation in a two-dimensional liquid-vapor system. Spurious\nnumerical effects in macroscopic equations are discussed and an appropriate\nnumerical scheme involving flux limiter techniques is proposed to minimize them\nand guarantee a better numerical stability at very low viscosity. The phase\nseparation kinetics is investigated and we find evidence of two different\ngrowth regimes depending on the value of the fluid viscosity as well as on the\nliquid-vapor ratio.'}]","### Abstract

This paper constitutes an in-depth investigation into the conformational dynamics of active rings—discrete representations of biological filaments such as microtubules, under the influence of modified Péclet numbers that mimic the effect of active forces acting on the filaments. The study innovates by elucidating how varied values of the persistence length and the Péclet number unravel distinct internal elbowings around elongated structures and compact shapes through computational molecular dynamics simulations.

**Background**: Understanding filaments under functional anisotropic conditions, particularly the impact of active forces in shaping the physical organization of biological structures, is crucial for advances in biophysics. Our work focuses on realizing this through a precise discrete model of single closed chains.

**Objective**: The primary objective is to systematically analyze the interplay between the Péclet number, persistence length, and the resulting conformations of active rings, both phantom and self-avoiding, in elucidating their underlying dynamics.

**Innovations**: This research introduces a novel method for quantifying the dynamic interplay between filaments' bending rigidity, length, and active forces through the Péclet number, revealing new relational patterns, such as non-monotonic dependence of structure on active forces.

**Methods**: We employ a fully discrete model, integrating biophysical principles and adopting innovative approaches in computational algorithms, namely the Brownian multiparticle collision method, to simulate the dynamics of the active rings across a wide range of parameter values, providing insights into the complex behavior of filaments under active forces.

**Results**: The simulations expose a detailed landscape of filament configurations, revealing the transformation from elongated through compact to elongated and back, as the Péclet number and persistence length are varied. Probability distributions highlighting modes and tails of filament sizes elucidate the transition character.

**Contributions**: By presenting comprehensive findings on the small-scale dynamics of active filaments, the research offers a pivotal contribution to the mechanical understanding of emergent structures in biological systems, paving the way for further theoretical extensions and practical applications in biophysics, polymer sciences, and material sciences.

**Applications**: The insights gained are fundamental to the field of biophysics, informing the development of new models and technologies that can mimic or enhance the functionality of biological filaments. Simultaneously, this research underscores the importance of considering active forces in the design of synthetic materials and in elucidating the mechanisms of cellular dynamics."
"The past few years have witnessed substantial advancement in text-guided
image generation powered by diffusion models. However, it was shown that
text-to-image diffusion models are vulnerable to training image memorization,
raising concerns on copyright infringement and privacy invasion. In this work,
we perform practical analysis of memorization in text-to-image diffusion
models. Targeting a set of images to protect, we conduct quantitive analysis on
them without need to collect any prompts. Specifically, we first formally
define the memorization of image and identify three necessary conditions of
memorization, respectively similarity, existence and probability. We then
reveal the correlation between the model's prediction error and image
replication. Based on the correlation, we propose to utilize inversion
techniques to verify the safety of target images against memorization and
measure the extent to which they are memorized. Model developers can utilize
our analysis method to discover memorized images or reliably claim safety
against memorization. Extensive experiments on the Stable Diffusion, a popular
open-source text-to-image diffusion model, demonstrate the effectiveness of our
analysis method.","[{'Efficient Progressive High Dynamic Range Image Restoration via Attention\n  and Alignment Network': 'HDR is an important part of computational photography technology. In this\npaper, we propose a lightweight neural network called Efficient\nAttention-and-alignment-guided Progressive Network (EAPNet) for the challenge\nNTIRE 2022 HDR Track 1 and Track 2. We introduce a multi-dimensional\nlightweight encoding module to extract features. Besides, we propose\nProgressive Dilated U-shape Block (PDUB) that can be a progressive\nplug-and-play module for dynamically tuning MAccs and PSNR. Finally, we use\nfast and low-power feature-align module to deal with misalignment problem in\nplace of the time-consuming Deformable Convolutional Network (DCN). The\nexperiments show that our method achieves about 20 times compression on MAccs\nwith better mu-PSNR and PSNR compared to the state-of-the-art method. We got\nthe second place of both two tracks during the testing phase. Figure1. shows\nthe visualized result of NTIRE 2022 HDR challenge.'}, {'Direct Learning-Based Deep Spiking Neural Networks: A Review': 'The spiking neural network (SNN), as a promising brain-inspired computational\nmodel with binary spike information transmission mechanism, rich\nspatially-temporal dynamics, and event-driven characteristics, has received\nextensive attention. However, its intricately discontinuous spike mechanism\nbrings difficulty to the optimization of the deep SNN. Since the surrogate\ngradient method can greatly mitigate the optimization difficulty and shows\ngreat potential in directly training deep SNNs, a variety of direct\nlearning-based deep SNN works have been proposed and achieved satisfying\nprogress in recent years. In this paper, we present a comprehensive survey of\nthese direct learning-based deep SNN works, mainly categorized into accuracy\nimprovement methods, efficiency improvement methods, and temporal dynamics\nutilization methods. In addition, we also divide these categorizations into\nfiner granularities further to better organize and introduce them. Finally, the\nchallenges and trends that may be faced in future research are prospected.'}, {'Model-Driven Deep Learning for Non-Coherent Massive Machine-Type\n  Communications': 'In this paper, we investigate the joint device activity and data detection in\nmassive machine-type communications (mMTC) with a one-phase non-coherent\nscheme, where data bits are embedded in the pilot sequences and the base\nstation simultaneously detects active devices and their embedded data bits\nwithout explicit channel estimation. Due to the correlated sparsity pattern\nintroduced by the non-coherent transmission scheme, the traditional approximate\nmessage passing (AMP) algorithm cannot achieve satisfactory performance.\nTherefore, we propose a deep learning (DL) modified AMP network (DL-mAMPnet)\nthat enhances the detection performance by effectively exploiting the pilot\nactivity correlation. The DL-mAMPnet is constructed by unfolding the AMP\nalgorithm into a feedforward neural network, which combines the principled\nmathematical model of the AMP algorithm with the powerful learning capability,\nthereby benefiting from the advantages of both techniques. Trainable parameters\nare introduced in the DL-mAMPnet to approximate the correlated sparsity pattern\nand the large-scale fading coefficient. Moreover, a refinement module is\ndesigned to further advance the performance by utilizing the spatial feature\ncaused by the correlated sparsity pattern. Simulation results demonstrate that\nthe proposed DL-mAMPnet can significantly outperform traditional algorithms in\nterms of the symbol error rate performance.'}, {'NeuroCLIP: Neuromorphic Data Understanding by CLIP and SNN': 'Recently, the neuromorphic vision sensor has received more and more interest.\nHowever, the neuromorphic data consists of asynchronous event spikes, which\nmakes it difficult to construct a big benchmark to train a power general neural\nnetwork model, thus limiting the neuromorphic data understanding for ``unseen""\nobjects by deep learning. While for the frame image, since the training data\ncan be obtained easily, the zero-shot and few-shot learning for ``unseen"" task\nvia the large Contrastive Vision-Language Pre-training (CLIP) model, which is\npre-trained by large-scale image-text pairs in 2D, have shown inspirational\nperformance. We wonder whether the CLIP could be transferred to neuromorphic\ndata recognition to handle the ``unseen"" problem. To this end, we materialize\nthis idea with NeuroCLIP in the paper. The NeuroCLIP consists of 2D CLIP and\ntwo specially designed modules for neuromorphic data understanding. First, an\nevent-frame module that could convert the event spikes to the sequential frame\nimage with a simple discrimination strategy. Second, an inter-timestep adapter,\nwhich is a simple fine-tuned adapter based on a spiking neural network (SNN)\nfor the sequential features coming from the visual encoder of CLIP to improve\nthe few-shot performance. Various experiments on neuromorphic datasets\nincluding N-MNIST, CIFAR10-DVS, and ES-ImageNet demonstrate the effectiveness\nof NeuroCLIP. Our code is open-sourced at\nhttps://github.com/yfguo91/NeuroCLIP.git.'}, {'ECKPN: Explicit Class Knowledge Propagation Network for Transductive\n  Few-shot Learning': 'Recently, the transductive graph-based methods have achieved great success in\nthe few-shot classification task. However, most existing methods ignore\nexploring the class-level knowledge that can be easily learned by humans from\njust a handful of samples. In this paper, we propose an Explicit Class\nKnowledge Propagation Network (ECKPN), which is composed of the comparison,\nsqueeze and calibration modules, to address this problem. Specifically, we\nfirst employ the comparison module to explore the pairwise sample relations to\nlearn rich sample representations in the instance-level graph. Then, we squeeze\nthe instance-level graph to generate the class-level graph, which can help\nobtain the class-level visual knowledge and facilitate modeling the relations\nof different classes. Next, the calibration module is adopted to characterize\nthe relations of the classes explicitly to obtain the more discriminative\nclass-level knowledge representations. Finally, we combine the class-level\nknowledge with the instance-level sample representations to guide the inference\nof the query samples. We conduct extensive experiments on four few-shot\nclassification benchmarks, and the experimental results show that the proposed\nECKPN significantly outperforms the state-of-the-art methods.'}, {'Hierarchical Similarity Learning for Language-based Product Image\n  Retrieval': 'This paper aims for the language-based product image retrieval task. The\nmajority of previous works have made significant progress by designing network\nstructure, similarity measurement, and loss function. However, they typically\nperform vision-text matching at certain granularity regardless of the intrinsic\nmultiple granularities of images. In this paper, we focus on the cross-modal\nsimilarity measurement, and propose a novel Hierarchical Similarity Learning\n(HSL) network. HSL first learns multi-level representations of input data by\nstacked encoders, and object-granularity similarity and image-granularity\nsimilarity are computed at each level. All the similarities are combined as the\nfinal hierarchical cross-modal similarity. Experiments on a large-scale product\nretrieval dataset demonstrate the effectiveness of our proposed method. Code\nand data are available at https://github.com/liufh1/hsl.'}, {'Fine-Grained Fashion Similarity Prediction by Attribute-Specific\n  Embedding Learning': 'This paper strives to predict fine-grained fashion similarity. In this\nsimilarity paradigm, one should pay more attention to the similarity in terms\nof a specific design/attribute between fashion items. For example, whether the\ncollar designs of the two clothes are similar. It has potential value in many\nfashion related applications, such as fashion copyright protection. To this\nend, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn\nmultiple attribute-specific embeddings, thus measure the fine-grained\nsimilarity in the corresponding space. The proposed ASEN is comprised of a\nglobal branch and a local branch. The global branch takes the whole image as\ninput to extract features from a global perspective, while the local branch\ntakes as input the zoomed-in region-of-interest (RoI) w.r.t. the specified\nattribute thus able to extract more fine-grained features. As the global branch\nand the local branch extract the features from different perspectives, they are\ncomplementary to each other. Additionally, in each branch, two attention\nmodules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel\nAttention, are integrated to make ASEN be able to locate the related regions\nand capture the essential patterns under the guidance of the specified\nattribute, thus make the learned attribute-specific embeddings better reflect\nthe fine-grained similarity. Extensive experiments on three fashion-related\ndatasets, i.e., FashionAI, DARN, and DeepFashion, show the effectiveness of\nASEN for fine-grained fashion similarity prediction and its potential for\nfashion reranking. Code and data are available at\nhttps://github.com/maryeon/asenpp .'}, {'Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding\n  Network': 'This paper strives to learn fine-grained fashion similarity. In this\nsimilarity paradigm, one should pay more attention to the similarity in terms\nof a specific design/attribute among fashion items, which has potential values\nin many fashion related applications such as fashion copyright protection. To\nthis end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly\nlearn multiple attribute-specific embeddings in an end-to-end manner, thus\nmeasure the fine-grained similarity in the corresponding space. With two\nattention modules, i.e., Attribute-aware Spatial Attention and Attribute-aware\nChannel Attention, ASEN is able to locate the related regions and capture the\nessential patterns under the guidance of the specified attribute, thus make the\nlearned attribute-specific embeddings better reflect the fine-grained\nsimilarity. Extensive experiments on four fashion-related datasets show the\neffectiveness of ASEN for fine-grained fashion similarity learning and its\npotential for fashion reranking.'}, {'Thermoelastic properties and thermal evolution of the Martian core from\n  ab initio calculated ferromagnetic Fe-S liquid': 'The accurate thermoelastic properties and thermal conductivity are crucial in\nunderstanding the thermal evolution of the Martian core. A fitting method based\non the ab initio calculated pressure-volume-temperature data is proposed in the\nformulation of the equation of state with high accuracy, by which the pressure\nand temperature dependent thermoelastic properties can be directly calculated\nby definitions. The ab initio results show that the liquid Fe0.75S0.25 under\nMartian core condition is thoroughly in the ferromagnetic state, without\nexisting spin crossover. The liquid Fe0.75S0.25 in magnetic calculation owns a\nlow thermal conductivity (21~23 W/m/K) when compared with non-magnetic\ncalculation at the same state. Based on the Insight estimated and ab initio\ncalculated properties of the Martian core, the iron snow model is verified when\nthe current temperature at the core-mantle boundary is below the core melting\ntemperature, and the simply secular cooling model is verified on the contrary.'}, {'UNMAS: Multi-Agent Reinforcement Learning for Unshaped Cooperative\n  Scenarios': 'Multi-agent reinforcement learning methods such as VDN, QMIX, and QTRAN that\nadopt centralized training with decentralized execution (CTDE) framework have\nshown promising results in cooperation and competition. However, in some\nmulti-agent scenarios, the number of agents and the size of action set actually\nvary over time. We call these unshaped scenarios, and the methods mentioned\nabove fail in performing satisfyingly. In this paper, we propose a new method\ncalled Unshaped Networks for Multi-Agent Systems (UNMAS) that adapts to the\nnumber and size changes in multi-agent systems. We propose the self-weighting\nmixing network to factorize the joint action-value. Its adaption to the change\nin agent number is attributed to the nonlinear mapping from each-agent Q value\nto the joint action-value with individual weights. Besides, in order to address\nthe change in action set, each agent constructs an individual action-value\nnetwork that is composed of two streams to evaluate the constant\nenvironment-oriented subset and the varying unit-oriented subset. We evaluate\nUNMAS on various StarCraft II micro-management scenarios and compare the\nresults with several state-of-the-art MARL algorithms. The superiority of UNMAS\nis demonstrated by its highest winning rates especially on the most difficult\nscenario 3s5z_vs_3s6z. The agents learn to perform effectively cooperative\nbehaviors while other MARL algorithms fail in. Animated demonstrations and\nsource code are provided in https://sites.google.com/view/unmas.'}]","Abstract:

In the evolving landscape of text-to-image models, a critical area of vulnerability has emerged: memorization. Driven by the recent recognition of this threat in popular models like Stable Diffusion, this research introduces a framework to measure the extent of memorization in text-to-image models, offering a fundamental advancement in understanding and mitigating risks in the field. 

The primary objective of this study is to develop a metric, referred to as L(𝑥0,𝜖0,𝑒), that quantifies the similarity between real image replicas and model outputs, marking a significant contribution toward practical analysis methods. The research elucidates the novelty of the proposed metric in terms of its effectiveness in recognizing image similarity and its ability to trigger model replication. 

Employing the unconditional diffusion model as a baseline, the study highlights the model's inherent resilience to image memorization, underlining its robust foundation for establishing a criteria of safety. Methods involve evaluating simulation accuracy, temporal analysis of distortion, and quantifying model replication across varying image complexities. 

Results demonstrate that the metric L(𝑥0,𝜖0,𝑒) can effectively distinguish between replicated and ordinary images, showcasing a sensible measurement for assessing memorization. The research emphasizes continuous embeddings as a critical consideration for binary replication, while underscoring the unconditional model's safety from memoization, ensuring a benchmark for model reliability. 

Contributions include the formulation of a robust and inclusive replication measurement, establishing a safety metric rooted in the unconditional model's superiority against memorization, and advancing theory on memorization analysis in text-to-image models. Potential implications encompass enhancing the security and preventing misuse of commercial AI models, and informing the development of future, more resilient text-to-image technologies. 

This research provides a comprehensive toolkit for quantifying and mitigating the risk of memorization in text-to-image algorithms, positioning advancements in AI ethics and safety."
"Sora unveils the potential of scaling Diffusion Transformer for generating
photorealistic images and videos at arbitrary resolutions, aspect ratios, and
durations, yet it still lacks sufficient implementation details. In this
technical report, we introduce the Lumina-T2X family - a series of Flow-based
Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized
attention, as a unified framework designed to transform noise into images,
videos, multi-view 3D objects, and audio clips conditioned on text
instructions. By tokenizing the latent spatial-temporal space and incorporating
learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X
seamlessly unifies the representations of different modalities across various
spatial-temporal resolutions. This unified approach enables training within a
single framework for different modalities and allows for flexible generation of
multimodal data at any resolution, aspect ratio, and length during inference.
Advanced techniques like RoPE, RMSNorm, and flow matching enhance the
stability, flexibility, and scalability of Flag-DiT, enabling models of
Lumina-T2X to scale up to 7 billion parameters and extend the context window to
128K tokens. This is particularly beneficial for creating ultra-high-definition
images with our Lumina-T2I model and long 720p videos with our Lumina-T2V
model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT,
requires only 35% of the training computational costs of a
600-million-parameter naive DiT. Our further comprehensive analysis underscores
Lumina-T2X's preliminary capability in resolution extrapolation,
high-resolution editing, generating consistent 3D views, and synthesizing
videos with seamless transitions. We expect that the open-sourcing of
Lumina-T2X will further foster creativity, transparency, and diversity in the
generative AI community.","[{'Hardy-type Inequalities Via Auxiliary Sequences': 'We prove some Hardy-type inequalities via an approach that involves\nconstructing auxiliary sequences.'}, {'Sums of Powers and Majorization': 'We study certain sequences involving sums of powers of positive integers and\nin connection with this, we give examples to show that power majorization does\nnot imply majorization.'}, {'A Note on Sums of Powers': 'We improve a result of Bennett concerning certain sequences involving sums of\npowers of positive integers.'}, {""A Note on Carleman's Inequality"": ""We study a weighted version of Carleman's inequality via Carleman's original\napproach. As an application of our result, we prove a conjecture of Bennett.""}, {""Finite Sections of Weighted Carleman's Inequality"": ""We study finite sections of weighted Carleman's inequality following the\napproach of De Bruijn. Similar to the unweighted case, we obtain an asymptotic\nexpression for the optimal constant.""}, {'A Note On Mixed Mean Inequalities': 'We give a simpler proof of a result of Holland concerning a mixed\narithmetic-geometric mean inequality. We also prove a result of mixed mean\ninequality involving the symmetric means.'}, {""Finite Sections of Weighted Hardy's Inequality"": ""We study finite sections of weighted Hardy's inequality following the\napproach of De Bruijn. Similar to the unweighted case, we obtain an asymptotic\nexpression for the optimal constant.""}, {'A note on $l^p$ norms of weighted mean matrices': ""We present some results concerning the $l^p$ norms of weighted mean matrices.\nThese results can be regarded as analogues to a result of Bennett concerning\nweighted Carleman's inequalities.""}, {'On a result of Levin and Stečkin': ""We extend a result of Levin and Ste\\v{c}kin concerning an inequality\nanalogous to Hardy's inequality.""}, {'On an inequality suggested by Littlewood': 'We study an inequality suggested by Littlewood, our result refines a result\nof Bennett.'}]","Title: Lumina-T2X: Large-Scale Generative Diffusion Transformers for High-Resolution Text-to-Image Generation

Abstract:

As a flagship contribution to the expanding field of generative models for text-to-image synthesis, the paper introduces Lumina-T2X, spearheaded by a family of large diffusion transformers. Lumina-T2X catapults into the forefront of the technology by delivering transformative quality, scalability to large resolutions, and showcasing robust control capabilities, all while leveraging advanced mechanisms and a novel formulation. The core innovation, embodied in Lumina-T2X, modulates a prefix conditioning kernel to merge N textual prompts with discrete visual cues, thereby facilitating the generation of images with intricate details and high fidelity across multiple concepts. This is coupled with a robust training methodology, which includes the employment of advanced techniques such as Mixed Precision Training, Large Learning Rates, and specialized architectural modifications. The research demonstrates that parameter scaling significantly improves sample quality, avoiding reliance on traditional tricks.

A key finding is the superiority of simple yet potent techniques like Rotary Position Encoding (RoPE), exemplified by a streamlined integration of this mechanism into diffusion transformers, championed by Flag-DiT. Lumina-T2X exemplifies this through its superior performance, resolution extrapolation abilities, and the seamless handling of multi-concept text-to-image generation. This streamlines the pipeline for generating high-resolution images from textual descriptions, catering to diverse fields and applications that demand high-quality visual output. The research embodies these advancements, promising a leap forward in text-to-image synthesis through its introduction and validation of Lumina-T2X.

Moreover, Lumina-T2X showcases various manipulation applications, supporting style-consistent generation and compositional text-to-image tasks. By enabling the generation of multiple subjects at different regions of a single image, it paves the way for innovative output variation and customization within the generative framework. The paper underscores the practical utility and potential applications of Lumina-T2X within domains such as digital art, fashion, virtual reality, and interactive media, highlighting its contribution to generating content that accurately interprets textual instructions into vivid, high-quality images.

The research stands at the confluence of efficiency, innovation, and usability, serving as a cornerstone for future developments in text-to-image synthesis. Lumina-T2X pushes the boundaries of large-scale generative models for high-resolution outputs, showcasing a harmonious blend of theoretical advancements and practical implications, thus making a significant contribution to the field."
"Recent advancements in Multimodal Large Language Models (LLMs) have focused
primarily on scaling by increasing text-image pair data and enhancing LLMs to
improve performance on multimodal tasks. However, these scaling approaches are
computationally expensive and overlook the significance of improving model
capabilities from the vision side. Inspired by the successful applications of
Mixture-of-Experts (MoE) in LLMs, which improves model scalability during
training while keeping inference costs similar to those of smaller models, we
propose CuMo. CuMo incorporates Co-upcycled Top-K sparsely-gated
Mixture-of-experts blocks into both the vision encoder and the MLP connector,
thereby enhancing the multimodal LLMs with minimal additional activated
parameters during inference. CuMo first pre-trains the MLP blocks and then
initializes each expert in the MoE block from the pre-trained MLP block during
the visual instruction tuning stage. Auxiliary losses are used to ensure a
balanced loading of experts. CuMo outperforms state-of-the-art multimodal LLMs
across various VQA and visual-instruction-following benchmarks using models
within each model size group, all while training exclusively on open-sourced
datasets. The code and model weights for CuMo are open-sourced at
https://github.com/SHI-Labs/CuMo.","[{'Prototypical Contrastive Learning-based CLIP Fine-tuning for Object\n  Re-identification': 'This work aims to adapt large-scale pre-trained vision-language models, such\nas contrastive language-image pretraining (CLIP), to enhance the performance of\nobject reidentification (Re-ID) across various supervision settings. Although\nprompt learning has enabled a recent work named CLIP-ReID to achieve promising\nperformance, the underlying mechanisms and the necessity of prompt learning\nremain unclear due to the absence of semantic labels in ReID tasks. In this\nwork, we first analyze the role prompt learning in CLIP-ReID and identify its\nlimitations. Based on our investigations, we propose a simple yet effective\napproach to adapt CLIP for supervised object Re-ID. Our approach directly\nfine-tunes the image encoder of CLIP using a prototypical contrastive learning\n(PCL) loss, eliminating the need for prompt learning. Experimental results on\nboth person and vehicle Re-ID datasets demonstrate the competitiveness of our\nmethod compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP\nfine-tuning approach to unsupervised scenarios, where we achieve state-of-the\nart performance.'}, {""Renormalized Singles with Correlation in $GW$ Green's Function Theory\n  for Accurate Quasiparticle Energies"": ""We apply the renormalized singles with correlation (RSc) Green's function in\nthe $GW$ approximation to calculate accurate quasiparticle (QP) energies and\norbitals. The RSc Green's function includes all orders of singles contributions\nfrom the associated density functional approximation (DFA) and treats higher\norder contributions in a perturbative manner. The\n$G_{\\text{RSc}}W_{\\text{RSc}}$ method uses the RSc Green's function as the new\nstarting point and calculates the screened interaction with the RSc Green's\nfunction. The $G_{\\text{RSc}}W_0$ methods fixes the screened interaction at the\nDFA level. For the calculations of ionization potentials in the GW100 set,\n$G_{\\text{RSc}}W_{\\text{RSc}}$ significantly reduces the dependence on the\nstarting point of DFAs used and provides accurate results with the mean\nabsolute error (MAE) of $0.34$ eV comparable to ev$GW$. For the calculations of\ncore-level binding energies in the CORE65 set, $G_{\\text{RSc}}W_{\\text{RSc}}$\nslightly overestimates the results because of underscreening, but\n$G_{\\text{RSc}}W_0$ with GGA functionals provides the optimal accuracy of\n$0.40$ eV MAE comparable to ev$GW_0$. We also show that\n$G_{\\text{RSc}}W_{\\text{RSc}}$ predicts accurate dipole moments of small\nmolecules. These two methods, $G_{\\text{RSc}}W_{\\text{RSc}}$ and\n$G_{\\text{RSc}}W_0$, are computationally much favorable than any flavor of\nself-consistent $GW$ methods. Thus, the RSc approach is promising for making\n$GW$ and other Green's function methods efficient and robust.""}, {'Chemical Potentials and the One-Electron Hamiltonian of the Second-Order\n  Perturbation Theory from the Functional Derivative Approach': ""We develop a functional derivative approach to calculate the chemical\npotentials of the second-order perturbation theory (MP2). In the functional\nderivative approach, the correlation part of the MP2 chemical potential, which\nis the derivative of the MP2 correlation energy with respect to the occupation\nnumber of frontier orbitals, is obtained from the chain rule via the\nnon-interacting Green's function. First, the MP2 correlation energy is\nexpressed in terms of the non-interacting Green's function and its functional\nderivative to the non-interacting Green's function is the second-order\nself-energy. Then the derivative of the non-interacting Green's function to the\noccupation number is obtained by including the orbital relaxation effect. We\nshow that the MP2 chemical potentials obtained from the functional derivative\napproach agrees with that obtained from the finite difference approach. The\none-electron Hamiltonian, defined as the derivative of the MP2 energy with\nrespect to the one particle density matrix, is also derived using the\nfunctional derivative approach, which can be used in the self-consistent\ncalculations of MP2 and double-hybrid density functionals. The developed\nfunctional derivative approach is promising for calculating the chemical\npotentials and the one-electron Hamiltonian of approximate functionals and\nmany-body perturbation approaches dependent explicitly on the non-interacting\nGreen's function.""}, {'Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein\n  Graph Double-Attention Network': 'Effective understanding of the environment and accurate trajectory prediction\nof surrounding dynamic obstacles are indispensable for intelligent mobile\nsystems (like autonomous vehicles and social robots) to achieve safe and\nhigh-quality planning when they navigate in highly interactive and crowded\nscenarios. Due to the existence of frequent interactions and uncertainty in the\nscene evolution, it is desired for the prediction system to enable relational\nreasoning on different entities and provide a distribution of future\ntrajectories for each agent. In this paper, we propose a generic generative\nneural system (called Social-WaGDAT) for multi-agent trajectory prediction,\nwhich makes a step forward to explicit interaction modeling by incorporating\nrelational inductive biases with a dynamic graph representation and leverages\nboth trajectory and scene context information. We also employ an efficient\nkinematic constraint layer applied to vehicle trajectory prediction which not\nonly ensures physical feasibility but also enhances model performance. The\nproposed system is evaluated on three public benchmark datasets for trajectory\nprediction, where the agents cover pedestrians, cyclists and on-road vehicles.\nThe experimental results demonstrate that our model achieves better performance\nthan various baseline approaches in terms of prediction accuracy.'}, {'Symbolic Expression Transformer: A Computer Vision Approach for Symbolic\n  Regression': ""Symbolic Regression (SR) is a type of regression analysis to automatically\nfind the mathematical expression that best fits the data. Currently, SR still\nbasically relies on various searching strategies so that a sample-specific\nmodel is required to be optimized for every expression, which significantly\nlimits the model's generalization and efficiency. Inspired by the fact that\nhuman beings can infer a mathematical expression based on the curve of it, we\npropose Symbolic Expression Transformer (SET), a sample-agnostic model from the\nperspective of computer vision for SR. Specifically, the collected data is\nrepresented as images and an image caption model is employed for translating\nimages to symbolic expressions. A large-scale dataset without overlap between\ntraining and testing sets in the image domain is released. Our results\ndemonstrate the effectiveness of SET and suggest the promising direction of\nimage-based model for solving the challenging SR problem.""}, {""Renormalized Singles Green's Function in the T-Matrix Approximation for\n  Accurate Quasiparticle Energy Calculation"": ""We combine the renormalized singles (RS) Green's function with the T-Matrix\napproximation for the single-particle Green's function to compute quasiparticle\nenergies for valence and core states of molecular systems. The\n$G_{\\text{RS}}T_0$ method uses the RS Green's function that incorporates\nsingles contributions as the initial Green's function. The\n$G_{\\text{RS}}T_{\\text{RS}}$ method further calculates the generalized\neffective interaction with the RS Green's function by using RS eigenvalues in\nthe T-Matrix calculation through the particle-particle random phase\napproximation. The $G_{\\text{RS}}T_{\\text{RS}}$ method provides significant\nimprovements over the one-shot T-Matrix method $G_0T_0$ as demonstrated in\ncalculations for GW100 and CORE65 test sets. It also systematically eliminates\nthe dependence of $G_{0}T_{0}$ on the choice of density functional\napproximations (DFAs). For valence states, the $G_{\\text{RS}}T_{\\text{RS}}$\nmethod provides an excellent accuracy, which is better than $G_0T_0$ with\nHartree-Fock (HF) or other DFAs. For core states, the\n$G_{\\text{RS}}T_{\\text{RS}}$ method correctly identifies desired peaks in the\nspectral function and significantly outperforms $G_0T_0$ on core level binding\nenergies (CLBEs) and relative CLBEs, with any commonly used DFAs.""}, {'Multireference Density Functional Theory for Describing Ground and\n  Excited States with Renormalized Singles': 'We applied renormalized singles (RS) in the multireference density functional\ntheory (DFT) to calculate accurate energies of ground and excited states. The\nmultireference DFT approach determines the total energy of the $N$-electron\nsystem as the sum of the ($N-2$)-electron energy from a density functional\napproximation (DFA) and the two-electron addition energies from the\nparticle-particle Tamm-Dancoff approximation (ppTDA), naturally including\nmultireference description. The ppTDA@RS-DFA approach uses the RS Hamiltonian\ncapturing all singles contributions in calculating two-electron addition\nenergies, and its total energy is optimized with the optimized effective\npotential method. It significantly improves the original ppTDA@DFA. For ground\nstates, ppTDA@RS-DFA properly describes dissociation curves tested and the\ndouble bond rotation of ethylene. For excited states, ppTDA@RS-DFA provides\naccurate excitation energies and largely eliminates the DFA dependence.\nppTDA@RS-DFA thus provides an efficient multireference approach to systems with\nstatic correlation.'}, {'ConvNeXt-backbone HoVerNet for nuclei segmentation and classification': 'This manuscript gives a brief description of the algorithm used to\nparticipate in CoNIC Challenge 2022. After the baseline was made available, we\nfollow the method in it and replace the ResNet baseline with ConvNeXt one.\nMoreover, we propose to first convert RGB space to Haematoxylin-Eosin-DAB(HED)\nspace, then use Haematoxylin composition of origin image to smooth semantic one\nhot label. Afterwards, nuclei distribution of train and valid set are explored\nto select the best fold split for training model for final test phase\nsubmission. Results on validation set shows that even with channel of each\nstage smaller in number, HoVerNet with ConvNeXt-tiny backbone still improves\nthe mPQ+ by 0.04 and multi r2 by 0.0144'}, {'Transformer Based Multi-Grained Features for Unsupervised Person\n  Re-Identification': 'Multi-grained features extracted from convolutional neural networks (CNNs)\nhave demonstrated their strong discrimination ability in supervised person\nre-identification (Re-ID) tasks. Inspired by them, this work investigates the\nway of extracting multi-grained features from a pure transformer network to\naddress the unsupervised Re-ID problem that is label-free but much more\nchallenging. To this end, we build a dual-branch network architecture based\nupon a modified Vision Transformer (ViT). The local tokens output in each\nbranch are reshaped and then uniformly partitioned into multiple stripes to\ngenerate part-level features, while the global tokens of two branches are\naveraged to produce a global feature. Further, based upon offline-online\nassociated camera-aware proxies (O2CAP) that is a top-performing unsupervised\nRe-ID method, we define offline and online contrastive learning losses with\nrespect to both global and part-level features to conduct unsupervised\nlearning. Extensive experiments on three person Re-ID datasets show that the\nproposed method outperforms state-of-the-art unsupervised methods by a\nconsiderable margin, greatly mitigating the gap to supervised counterparts.\nCode will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.'}, {'""Always Nice and Confident, Sometimes wrong"": Developer\'s Experiences\n  Engaging Generative AI Chatbots Versus Human-Powered Q&A Platforms': ""Software engineers have historically relied on human-powered Q&A platforms,\nlike Stack Overflow (SO), as coding aids. With the rise of generative AI,\ndevelopers have adopted AI chatbots, such as ChatGPT, in their software\ndevelopment process. Recognizing the potential parallels between human-powered\nQ&A platforms and AI-powered question-based chatbots, we investigate and\ncompare how developers integrate this assistance into their real-world coding\nexperiences by conducting thematic analysis of Reddit posts. Through a\ncomparative study of SO and ChatGPT, we identified each platform's strengths,\nuse cases, and barriers. Our findings suggest that ChatGPT offers fast, clear,\ncomprehensive responses and fosters a more respectful environment than SO.\nHowever, concerns about ChatGPT's reliability stem from its overly confident\ntone and the absence of validation mechanisms like SO's voting system. Based on\nthese findings, we recommend leveraging each platform's unique features to\nimprove developer experiences in the future.""}]","**Abstract**

As the emergence of multimodal large language models (LLMs) enhances the capability of dealing with visual-instruction tasks, their performance and scalability remain critical avenues of investigation. The research focus herein:  
**Background**: The landscape of multimodal LLMs is characterized by a trade-off between model size, capabilities, and efficiency, making them potent but demanding to scale effectively.

**Objective**: To introduce, evaluate, and optimize the Cohort-Mixture-of-Experts (CuMo) model, aiming at boosting the performance and stability of multimodal LLMs on visual-instruction tasks while maintaining an efficient scaling model.

**Innovations**: The paper introduces CuMo, a multimodal LLM framework that leverages sparse Top-K mixture-of-experts blocks across CLIP vision encoders and vision-language MLP connectors. It realizes efficient model scaling using a pre-fine-tuning stage and auxiliary losses for load balance. 

**Methods**: The CuMo model utilizes a CLIP vision encoder, vision-language MLP connector, and Mistral-7B as the base large language model. Training is conducted on diverse datasets following visual instruction formats. The model incorporates auxiliary losses to ensure balanced load distribution among the expert modules during inference.

**Results**: CuMo demonstrated competitive performance across academic and language-following benchmark datasets, surpassing state-of-the-art models, especially on challenging tasks like MMMU and MathVista, while showing performance advantages under limited data conditions.

**Contributions**: CuMo contributes a robust multimodal LLM approach that effectively scales with improvements in visual-instruction handling. Framework innovations enable more efficient model training and inference, showcasing enhancement potential for similar models.

**Applications**: The advancements are critical for applications requiring high-dimensional multimodal understanding and instruction execution, encompassing areas like visual search, interactive AI, and semantic-driven robot navigation."
